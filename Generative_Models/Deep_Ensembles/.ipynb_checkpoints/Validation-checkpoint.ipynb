{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras import optimizers, models, regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_model(Model):\n",
    "    def __init__(self,data_tuple,arch_type='baseline',lrate=0.001,num_epochs=1000,eps_range=0.01):\n",
    "        super(toy_model, self).__init__()\n",
    "\n",
    "        train_data = data_tuple[0]\n",
    "        valid_data = data_tuple[1]\n",
    "        test_data = data_tuple[2]\n",
    "        \n",
    "        self.input_train_data = train_data[:,0].reshape(-1,1)\n",
    "        self.input_valid_data = valid_data[:,0].reshape(-1,1)\n",
    "        self.input_test_data = test_data[:,0].reshape(-1,1)\n",
    "        \n",
    "        self.output_train_data = train_data[:,1].reshape(-1,1)\n",
    "        self.output_valid_data = valid_data[:,1].reshape(-1,1)\n",
    "        self.output_test_data = train_data[:,1].reshape(-1,1)\n",
    "\n",
    "        self.ntrain = self.input_train_data.shape[0]\n",
    "        self.nvalid = self.input_valid_data.shape[0]\n",
    "        self.ntest = self.input_test_data.shape[0]\n",
    "        self.arch_type = arch_type\n",
    "        self.num_latent = 6\n",
    "\n",
    "        self.init_architecture_baseline()\n",
    "        self.train_op = tf.keras.optimizers.Adam(learning_rate=lrate)\n",
    "\n",
    "        # If adversarial training is used\n",
    "        self.eps_range = eps_range\n",
    "        \n",
    "        # num epochs\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def init_architecture_baseline(self):\n",
    "\n",
    "        # Define model architecture\n",
    "        ## Encoder\n",
    "        self.l1 = Dense(50,activation='relu')\n",
    "\n",
    "        if self.arch_type == 'baseline':\n",
    "            self.out = Dense(1,activation='linear')\n",
    "        \n",
    "        elif self.arch_type =='mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa':\n",
    "            self.out_mean = Dense(1,activation='linear')\n",
    "            self.out_logvar = Dense(1,activation='linear') #sigma^2\n",
    "\n",
    "        if self.arch_type == 'dropout':\n",
    "            self.out = Dense(1,activation='linear')\n",
    "            self.dropout_layer = Dropout(0.1)\n",
    "            \n",
    "        if self.arch_type == 'bayesian':\n",
    "            self.l1 = tfp.layers.DenseFlipout(50, activation=\"relu\")\n",
    "            self.out = Dense(1,activation='linear')\n",
    "    \n",
    "\n",
    "    def call_baseline(self,X):\n",
    "\n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        out = self.out(hh)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call_dropout(self,X):\n",
    "        \n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        hh = self.dropout_layer(hh,training=True)\n",
    "        out = self.out(hh)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def call_mixture(self,X):\n",
    "\n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        mean = self.out_mean(hh)\n",
    "        logvar = self.out_logvar(hh)\n",
    "\n",
    "        return mean, logvar\n",
    "    \n",
    "    def call_bayesian(self,X):\n",
    "\n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        out = self.out(hh)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Running the model\n",
    "    def call(self,X):\n",
    "        if self.arch_type == 'baseline':\n",
    "            recon = self.call_baseline(X)\n",
    "            return recon\n",
    "        elif self.arch_type =='dropout':\n",
    "            recon = self.call_dropout(X)\n",
    "            return recon\n",
    "        elif self.arch_type =='mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa':\n",
    "            recon_mean, recon_logvar = self.call_mixture(X)\n",
    "            return recon_mean, recon_logvar\n",
    "        elif self.arch_type == 'bayesian':\n",
    "            recon = self.call_bayesian(X)\n",
    "            return recon\n",
    "    \n",
    "    # Regular MSE\n",
    "    def get_loss(self,X,Y):\n",
    "\n",
    "        if self.arch_type == 'mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa': # Log likelihood optimization\n",
    "            op_mean, op_logvar = self.call(X)\n",
    "            \n",
    "            op_var = tf.math.exp(op_logvar)\n",
    "            half_logvar = 0.5*op_logvar\n",
    "            \n",
    "            mse = (tf.math.square(op_mean-Y))*0.5/(op_var+K.epsilon())\n",
    "            loss = tf.reduce_mean(half_logvar+mse)\n",
    "            \n",
    "        elif self.arch_type == 'bayesian':\n",
    "            \n",
    "            loss = self.elbo_loss(X,Y)\n",
    "            \n",
    "        else: \n",
    "\n",
    "            op = self.call(X)\n",
    "            loss = tf.reduce_mean(tf.math.square(op-Y))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def elbo_loss(self, X, Y):\n",
    "        \n",
    "        op = self.call(X)\n",
    "        \n",
    "        mse_loss = tf.reduce_mean(tf.math.square(op-Y))\n",
    "        loss_kl = tf.reduce_mean(tf.keras.losses.KLD(Y, op))\n",
    "        \n",
    "        return mse_loss + loss_kl\n",
    "\n",
    "    # Regular MSE\n",
    "    def get_adversarial_loss(self,X,X_adv,Y):\n",
    "\n",
    "        op_mean, op_logvar = self.call(X_adv)\n",
    "        \n",
    "        op_var = tf.math.exp(op_logvar)\n",
    "        half_logvar = 0.5*op_logvar\n",
    "\n",
    "        mse = tf.math.square(op_mean-Y)*0.5/(op_var+K.epsilon())\n",
    "        loss = tf.reduce_mean(half_logvar+mse)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # get gradients - regular\n",
    "    def get_grad(self,X,Y):\n",
    "        if self.arch_type == 'ensemble':\n",
    "            # Adversarial training\n",
    "            X = tf.convert_to_tensor(X)\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                L = self.get_loss(X,Y)\n",
    "                g_temp = tape.gradient(L,X)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(self.trainable_variables)\n",
    "                X_adv = X + self.eps_range*tf.math.sign(g_temp)\n",
    "                L_adv = self.get_adversarial_loss(X,X_adv,Y)\n",
    "                g = tape.gradient(L+L_adv, self.trainable_variables)\n",
    "        else:\n",
    "            # Regular training\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(self.trainable_variables)\n",
    "                L = self.get_loss(X,Y)\n",
    "                g = tape.gradient(L, self.trainable_variables)            \n",
    "        \n",
    "        return g\n",
    "    \n",
    "    # perform gradient descent - regular\n",
    "    def network_learn(self,X,Y):\n",
    "        g = self.get_grad(X,Y)\n",
    "        self.train_op.apply_gradients(zip(g, self.trainable_variables))\n",
    "\n",
    "    # Train the model\n",
    "    def train_model(self):\n",
    "        plot_iter = 0\n",
    "        stop_iter = 0\n",
    "        patience = 100\n",
    "        best_valid_loss = np.inf # Some large number \n",
    "        swa_iter = 0\n",
    "\n",
    "        self.num_batches = 1\n",
    "        self.train_batch_size = int(self.ntrain/self.num_batches)\n",
    "        self.valid_batch_size = int(self.nvalid/self.num_batches)\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "            # Training loss\n",
    "            print('Training iteration:',i)\n",
    "            \n",
    "            for batch in range(self.num_batches):\n",
    "                input_batch = self.input_train_data[batch*self.train_batch_size:(batch+1)*self.train_batch_size]\n",
    "                output_batch = self.output_train_data[batch*self.train_batch_size:(batch+1)*self.train_batch_size]\n",
    "                self.network_learn(input_batch,output_batch)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            for batch in range(self.num_batches):\n",
    "                input_batch = self.input_valid_data[batch*self.valid_batch_size:(batch+1)*self.valid_batch_size]\n",
    "                output_batch = self.output_valid_data[batch*self.valid_batch_size:(batch+1)*self.valid_batch_size]\n",
    "                valid_loss = valid_loss + self.get_loss(input_batch,output_batch).numpy()\n",
    "\n",
    "            valid_loss = valid_loss/self.nvalid\n",
    "\n",
    "            # Check early stopping criteria\n",
    "            if valid_loss < best_valid_loss:\n",
    "                \n",
    "                print('Improved validation loss from:',best_valid_loss,' to:', valid_loss)                \n",
    "                best_valid_loss = valid_loss\n",
    "                \n",
    "                if self.arch_type == 'baseline':\n",
    "                    self.save_weights('./checkpoints/baseline_checkpoint')\n",
    "                elif self.arch_type == 'dropout':\n",
    "                    self.save_weights('./checkpoints/dropout_checkpoint')\n",
    "                elif self.arch_type == 'mixture':\n",
    "                    self.save_weights('./checkpoints/mixture_checkpoint')\n",
    "                elif self.arch_type == 'ensemble':\n",
    "                    self.save_weights('./checkpoints/ensemble_checkpoint')\n",
    "                elif self.arch_type == 'swa':\n",
    "                    self.save_weights('./checkpoints/swa_checkpoint')\n",
    "                elif self.arch_type == 'bayesian':\n",
    "                    self.save_weights('./checkpoints/bayesian_checkpoint')\n",
    "                \n",
    "                stop_iter = 0\n",
    "            else:\n",
    "                print('Validation loss (no improvement):',valid_loss)\n",
    "                stop_iter = stop_iter + 1\n",
    "\n",
    "            if stop_iter == patience:\n",
    "                if self.arch_type == 'swa' and swa_iter < 5:\n",
    "                    self.save_weights('./checkpoints/swa_checkpoint_'+str(swa_iter))\n",
    "                    swa_iter +=1\n",
    "                    \n",
    "                    stop_iter = 0    \n",
    "                else:   \n",
    "                    break\n",
    "                \n",
    "    # Load weights\n",
    "    def restore_model(self):\n",
    "        if self.arch_type == 'baseline':\n",
    "            self.load_weights('./checkpoints/baseline_checkpoint')\n",
    "        elif self.arch_type == 'dropout':\n",
    "            self.load_weights('./checkpoints/dropout_checkpoint')\n",
    "        elif self.arch_type == 'mixture':\n",
    "            self.load_weights('./checkpoints/mixture_checkpoint')\n",
    "        elif self.arch_type == 'ensemble':\n",
    "            self.load_weights('./checkpoints/ensemble_checkpoint')\n",
    "        elif self.arch_type == 'bayesian':\n",
    "            self.load_weights('./checkpoints/bayesian_checkpoint')\n",
    "\n",
    "    # Do some testing\n",
    "    def model_inference(self,mc_num=100):\n",
    "        # Restore from checkpoint\n",
    "        self.restore_model()\n",
    "\n",
    "        if self.arch_type == 'baseline':\n",
    "\n",
    "            predictions = self.call(self.input_test_data)\n",
    "            \n",
    "            np.save('Baseline_validation.npy',predictions.numpy())\n",
    "            \n",
    "            return None\n",
    "\n",
    "        \n",
    "        elif self.arch_type == 'dropout':\n",
    "            \n",
    "            prediction_list = []\n",
    "            for i in range(mc_num):\n",
    "                recon = self.call(self.input_test_data)\n",
    "                recon = recon.numpy()\n",
    "                prediction_list.append(recon)\n",
    "\n",
    "            prediction_list = np.asarray(prediction_list)\n",
    "            \n",
    "            np.save('Dropout_validation.npy',prediction_list)\n",
    "            \n",
    "            return None\n",
    "                    \n",
    "        elif self.arch_type == 'mixture' or self.arch_type == 'ensemble':       \n",
    "\n",
    "            mean, var = self.call(self.input_test_data)\n",
    "            mean = mean.numpy()\n",
    "            var = var.numpy()\n",
    "            \n",
    "            np.save(self.arch_type+'_mean_validation.npy',mean)\n",
    "            np.save(self.arch_type+'_var_validation.npy',var)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        elif self.arch_type == 'swa':\n",
    "            # Average weights - initialization\n",
    "            self.load_weights('./checkpoints/swa_checkpoint')\n",
    "            \n",
    "            mean, logvar = self.call(self.input_test_data)\n",
    "            mean = mean.numpy()\n",
    "            var = np.exp(logvar.numpy()) + mean**2\n",
    "            \n",
    "            for i in range(5):\n",
    "                self.load_weights('./checkpoints/swa_checkpoint_'+str(i))\n",
    "            \n",
    "                t1, t2 = self.call(self.input_test_data)\n",
    "                mean = mean + t1.numpy()\n",
    "                var = var + np.exp(t2.numpy()) + mean**2\n",
    "                \n",
    "            ensemble_mean = mean/6.0\n",
    "            ensemble_var = var/6.0 - ensemble_mean**2\n",
    "            ensemble_std = np.sqrt(ensemble_var)\n",
    "            \n",
    "            np.save('SWA_ensembles_mean_multiple.npy',ensemble_mean)\n",
    "            np.save('SWA_ensembles_std_multiple.npy',ensemble_std)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        elif self.arch_type == 'bayesian':\n",
    "            # Average weights - initialization\n",
    "            self.load_weights('./checkpoints/bayesian_checkpoint')\n",
    "            \n",
    "            prediction_list = []\n",
    "            for i in range(mc_num):\n",
    "                recon = self.call(self.input_test_data)\n",
    "                recon = recon.numpy()\n",
    "                prediction_list.append(recon)\n",
    "\n",
    "            prediction_list = np.asarray(prediction_list)\n",
    "            \n",
    "            np.save('Bayesian_validation.npy',prediction_list)\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition_data():\n",
    "    # Load data\n",
    "    x = np.random.uniform(low=-4.0,high=4.0,size=(200,1))\n",
    "    y = x**3 + np.random.normal(0,9)\n",
    "    \n",
    "    total_data = np.concatenate((x,y),axis=-1)\n",
    "    \n",
    "    train_data = total_data[:40]\n",
    "    valid_data = total_data[40:60]\n",
    "    test_data = total_data[60:]\n",
    "\n",
    "    data_tuple = (train_data, valid_data, test_data)\n",
    "\n",
    "    return data_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuple = load_partition_data()\n",
    "eps_range = 0.01*(np.max(data_tuple[0][:,0])-np.min(data_tuple[0][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = toy_model(data_tuple,arch_type='baseline',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "bayesian_model = toy_model(data_tuple,arch_type='bayesian',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "dropout_model = toy_model(data_tuple,arch_type='dropout',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "mixture_model = toy_model(data_tuple,arch_type='mixture',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "swa_model = toy_model(data_tuple,arch_type='swa',lrate=0.01,num_epochs=4000,eps_range=eps_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 29.02362060546875\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 29.02362060546875  to: 28.67264404296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmlans/anaconda3/envs/tf2_tfp_env/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2\n",
      "Improved validation loss from: 28.67264404296875  to: 28.41582336425781\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 28.41582336425781  to: 28.307891845703125\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 28.307891845703125  to: 28.017391967773438\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 28.017391967773438  to: 27.56883544921875\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 27.56883544921875  to: 27.13878479003906\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 27.13878479003906  to: 26.99324951171875\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 26.99324951171875  to: 26.570281982421875\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 26.570281982421875  to: 26.3744140625\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 26.3744140625  to: 25.7891845703125\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 25.7891845703125  to: 25.419891357421875\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 25.419891357421875  to: 24.880369567871092\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 24.880369567871092  to: 24.410475158691405\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 24.410475158691405  to: 24.029466247558595\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 24.029466247558595  to: 23.706338500976564\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 23.706338500976564  to: 23.029051208496092\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 23.029051208496092  to: 22.379310607910156\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 22.379310607910156  to: 22.116775512695312\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 22.116775512695312  to: 21.328990173339843\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 21.328990173339843  to: 20.887496948242188\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 20.887496948242188  to: 20.108401489257812\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 20.108401489257812  to: 19.618690490722656\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 19.618690490722656  to: 18.825454711914062\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 18.825454711914062  to: 18.308633422851564\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 18.308633422851564  to: 17.684164428710936\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 17.684164428710936  to: 16.951727294921874\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 16.951727294921874  to: 16.185342407226564\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 16.185342407226564  to: 15.549368286132813\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 15.549368286132813  to: 14.997120666503907\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 14.997120666503907  to: 14.096560668945312\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 14.096560668945312  to: 13.475123596191406\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 13.475123596191406  to: 12.718276214599609\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 12.718276214599609  to: 12.326337432861328\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 12.326337432861328  to: 11.537718963623046\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 11.537718963623046  to: 10.888265228271484\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 10.888265228271484  to: 10.258755493164063\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 10.258755493164063  to: 9.822072601318359\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 9.822072601318359  to: 9.602439880371094\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 9.602439880371094  to: 8.85361099243164\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 8.85361099243164  to: 8.343183135986328\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 8.343183135986328  to: 7.886003112792968\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 7.886003112792968  to: 7.5409904479980465\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 7.5409904479980465  to: 7.3111015319824215\n",
      "Training iteration: 44\n",
      "Validation loss (no improvement): 7.393635559082031\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 7.3111015319824215  to: 7.187010192871094\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 7.187010192871094  to: 7.180309295654297\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 7.180309295654297  to: 6.876381683349609\n",
      "Training iteration: 48\n",
      "Validation loss (no improvement): 7.216823577880859\n",
      "Training iteration: 49\n",
      "Validation loss (no improvement): 7.0609992980957035\n",
      "Training iteration: 50\n",
      "Validation loss (no improvement): 7.044696807861328\n",
      "Training iteration: 51\n",
      "Validation loss (no improvement): 7.131500244140625\n",
      "Training iteration: 52\n",
      "Validation loss (no improvement): 7.343540954589844\n",
      "Training iteration: 53\n",
      "Validation loss (no improvement): 7.2001594543457035\n",
      "Training iteration: 54\n",
      "Validation loss (no improvement): 7.554356384277344\n",
      "Training iteration: 55\n",
      "Validation loss (no improvement): 7.734263610839844\n",
      "Training iteration: 56\n",
      "Validation loss (no improvement): 7.958061981201172\n",
      "Training iteration: 57\n",
      "Validation loss (no improvement): 7.904019165039062\n",
      "Training iteration: 58\n",
      "Validation loss (no improvement): 8.150749206542969\n",
      "Training iteration: 59\n",
      "Validation loss (no improvement): 8.308039093017578\n",
      "Training iteration: 60\n",
      "Validation loss (no improvement): 8.290406799316406\n",
      "Training iteration: 61\n",
      "Validation loss (no improvement): 8.549446868896485\n",
      "Training iteration: 62\n",
      "Validation loss (no improvement): 8.390628814697266\n",
      "Training iteration: 63\n",
      "Validation loss (no improvement): 8.37644271850586\n",
      "Training iteration: 64\n",
      "Validation loss (no improvement): 8.26929931640625\n",
      "Training iteration: 65\n",
      "Validation loss (no improvement): 8.530876159667969\n",
      "Training iteration: 66\n",
      "Validation loss (no improvement): 8.376152801513673\n",
      "Training iteration: 67\n",
      "Validation loss (no improvement): 8.607247161865235\n",
      "Training iteration: 68\n",
      "Validation loss (no improvement): 8.344760131835937\n",
      "Training iteration: 69\n",
      "Validation loss (no improvement): 8.164391326904298\n",
      "Training iteration: 70\n",
      "Validation loss (no improvement): 8.572919464111328\n",
      "Training iteration: 71\n",
      "Validation loss (no improvement): 8.430399322509766\n",
      "Training iteration: 72\n",
      "Validation loss (no improvement): 8.195308685302734\n",
      "Training iteration: 73\n",
      "Validation loss (no improvement): 8.246045684814453\n",
      "Training iteration: 74\n",
      "Validation loss (no improvement): 7.972634124755859\n",
      "Training iteration: 75\n",
      "Validation loss (no improvement): 7.733849334716797\n",
      "Training iteration: 76\n",
      "Validation loss (no improvement): 7.953128814697266\n",
      "Training iteration: 77\n",
      "Validation loss (no improvement): 7.956976318359375\n",
      "Training iteration: 78\n",
      "Validation loss (no improvement): 7.5986991882324215\n",
      "Training iteration: 79\n",
      "Validation loss (no improvement): 7.701570892333985\n",
      "Training iteration: 80\n",
      "Validation loss (no improvement): 7.713653564453125\n",
      "Training iteration: 81\n",
      "Validation loss (no improvement): 7.446356964111328\n",
      "Training iteration: 82\n",
      "Validation loss (no improvement): 7.678021240234375\n",
      "Training iteration: 83\n",
      "Validation loss (no improvement): 7.417112731933594\n",
      "Training iteration: 84\n",
      "Validation loss (no improvement): 7.323355102539063\n",
      "Training iteration: 85\n",
      "Validation loss (no improvement): 7.35836181640625\n",
      "Training iteration: 86\n",
      "Validation loss (no improvement): 7.248914337158203\n",
      "Training iteration: 87\n",
      "Validation loss (no improvement): 7.149422454833984\n",
      "Training iteration: 88\n",
      "Validation loss (no improvement): 7.141765594482422\n",
      "Training iteration: 89\n",
      "Validation loss (no improvement): 7.211040496826172\n",
      "Training iteration: 90\n",
      "Validation loss (no improvement): 7.092450714111328\n",
      "Training iteration: 91\n",
      "Validation loss (no improvement): 7.042595672607422\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 6.876381683349609  to: 6.832776641845703\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 6.832776641845703  to: 6.808733367919922\n",
      "Training iteration: 94\n",
      "Validation loss (no improvement): 6.879926300048828\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 6.808733367919922  to: 6.5557708740234375\n",
      "Training iteration: 96\n",
      "Validation loss (no improvement): 6.7688652038574215\n",
      "Training iteration: 97\n",
      "Validation loss (no improvement): 6.866423034667969\n",
      "Training iteration: 98\n",
      "Validation loss (no improvement): 6.771946716308594\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 6.5557708740234375  to: 6.546721649169922\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 6.546721649169922  to: 6.479295349121093\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 6.479295349121093  to: 6.45006103515625\n",
      "Training iteration: 102\n",
      "Validation loss (no improvement): 6.660143280029297\n",
      "Training iteration: 103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 6.587323760986328\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 6.45006103515625  to: 6.444902801513672\n",
      "Training iteration: 105\n",
      "Validation loss (no improvement): 6.72418441772461\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 6.444902801513672  to: 6.383843231201172\n",
      "Training iteration: 107\n",
      "Validation loss (no improvement): 6.398265838623047\n",
      "Training iteration: 108\n",
      "Validation loss (no improvement): 6.574095916748047\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 6.383843231201172  to: 6.232241439819336\n",
      "Training iteration: 110\n",
      "Validation loss (no improvement): 6.378755950927735\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 6.232241439819336  to: 6.217427062988281\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 6.217427062988281  to: 6.109133529663086\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 6.109133529663086  to: 5.943874740600586\n",
      "Training iteration: 114\n",
      "Validation loss (no improvement): 6.166240692138672\n",
      "Training iteration: 115\n",
      "Validation loss (no improvement): 6.326852416992187\n",
      "Training iteration: 116\n",
      "Validation loss (no improvement): 5.964057159423828\n",
      "Training iteration: 117\n",
      "Validation loss (no improvement): 6.067360305786133\n",
      "Training iteration: 118\n",
      "Validation loss (no improvement): 5.9873504638671875\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 5.943874740600586  to: 5.8959003448486325\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 5.8959003448486325  to: 5.884808731079102\n",
      "Training iteration: 121\n",
      "Validation loss (no improvement): 6.01581039428711\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 5.884808731079102  to: 5.773073196411133\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 5.773073196411133  to: 5.535533905029297\n",
      "Training iteration: 124\n",
      "Validation loss (no improvement): 5.885651779174805\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 5.535533905029297  to: 5.481748580932617\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 5.481748580932617  to: 5.428474426269531\n",
      "Training iteration: 127\n",
      "Validation loss (no improvement): 5.458412551879883\n",
      "Training iteration: 128\n",
      "Validation loss (no improvement): 5.472207641601562\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 5.428474426269531  to: 5.338534164428711\n",
      "Training iteration: 130\n",
      "Validation loss (no improvement): 5.411675643920899\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 5.338534164428711  to: 5.161662292480469\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 5.161662292480469  to: 5.150913238525391\n",
      "Training iteration: 133\n",
      "Validation loss (no improvement): 5.222695159912109\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 5.150913238525391  to: 4.895277404785157\n",
      "Training iteration: 135\n",
      "Validation loss (no improvement): 5.154507446289062\n",
      "Training iteration: 136\n",
      "Validation loss (no improvement): 4.965579986572266\n",
      "Training iteration: 137\n",
      "Validation loss (no improvement): 4.979119873046875\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 4.895277404785157  to: 4.772173309326172\n",
      "Training iteration: 139\n",
      "Validation loss (no improvement): 4.88375244140625\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 4.772173309326172  to: 4.745951843261719\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 4.745951843261719  to: 4.664187622070313\n",
      "Training iteration: 142\n",
      "Validation loss (no improvement): 4.7499755859375\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 4.664187622070313  to: 4.649848937988281\n",
      "Training iteration: 144\n",
      "Validation loss (no improvement): 4.663478088378906\n",
      "Training iteration: 145\n",
      "Validation loss (no improvement): 4.673612213134765\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 4.649848937988281  to: 4.612732696533203\n",
      "Training iteration: 147\n",
      "Validation loss (no improvement): 4.655198669433593\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 4.612732696533203  to: 4.472699737548828\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 4.472699737548828  to: 4.465853881835938\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 4.465853881835938  to: 4.241319274902343\n",
      "Training iteration: 151\n",
      "Validation loss (no improvement): 4.483134460449219\n",
      "Training iteration: 152\n",
      "Validation loss (no improvement): 4.251884460449219\n",
      "Training iteration: 153\n",
      "Validation loss (no improvement): 4.322073364257813\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 4.241319274902343  to: 4.237687683105468\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 4.237687683105468  to: 4.2085121154785154\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 4.2085121154785154  to: 4.090294647216797\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 4.090294647216797  to: 4.051006317138672\n",
      "Training iteration: 158\n",
      "Validation loss (no improvement): 4.100979614257812\n",
      "Training iteration: 159\n",
      "Validation loss (no improvement): 4.161357879638672\n",
      "Training iteration: 160\n",
      "Validation loss (no improvement): 4.122917175292969\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 4.051006317138672  to: 3.9023208618164062\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 3.9023208618164062  to: 3.747023010253906\n",
      "Training iteration: 163\n",
      "Validation loss (no improvement): 3.9822677612304687\n",
      "Training iteration: 164\n",
      "Validation loss (no improvement): 3.7562965393066405\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 3.747023010253906  to: 3.6770980834960936\n",
      "Training iteration: 166\n",
      "Validation loss (no improvement): 3.771135711669922\n",
      "Training iteration: 167\n",
      "Validation loss (no improvement): 3.711445617675781\n",
      "Training iteration: 168\n",
      "Validation loss (no improvement): 3.7746231079101564\n",
      "Training iteration: 169\n",
      "Validation loss (no improvement): 3.7271263122558596\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 3.6770980834960936  to: 3.5839618682861327\n",
      "Training iteration: 171\n",
      "Validation loss (no improvement): 3.6238895416259767\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 3.5839618682861327  to: 3.4929962158203125\n",
      "Training iteration: 173\n",
      "Validation loss (no improvement): 3.58081169128418\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 3.4929962158203125  to: 3.48836669921875\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 3.48836669921875  to: 3.2933177947998047\n",
      "Training iteration: 176\n",
      "Validation loss (no improvement): 3.4606075286865234\n",
      "Training iteration: 177\n",
      "Validation loss (no improvement): 3.3597270965576174\n",
      "Training iteration: 178\n",
      "Validation loss (no improvement): 3.388388824462891\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 3.2933177947998047  to: 3.2357593536376954\n",
      "Training iteration: 180\n",
      "Validation loss (no improvement): 3.3105728149414064\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 3.2357593536376954  to: 3.0779491424560548\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 3.0779491424560548  to: 3.0310733795166014\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 3.0310733795166014  to: 2.997451972961426\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 2.997451972961426  to: 2.882221221923828\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 2.882221221923828  to: 2.7117898941040037\n",
      "Training iteration: 186\n",
      "Validation loss (no improvement): 2.7543624877929687\n",
      "Training iteration: 187\n",
      "Validation loss (no improvement): 2.7548852920532227\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 2.7117898941040037  to: 2.618841361999512\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 2.618841361999512  to: 2.6048067092895506\n",
      "Training iteration: 190\n",
      "Validation loss (no improvement): 2.779292869567871\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 2.6048067092895506  to: 2.4850255966186525\n",
      "Training iteration: 192\n",
      "Validation loss (no improvement): 2.5735355377197267\n",
      "Training iteration: 193\n",
      "Validation loss (no improvement): 2.5187007904052736\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 2.4850255966186525  to: 2.364578628540039\n",
      "Training iteration: 195\n",
      "Validation loss (no improvement): 2.5027183532714843\n",
      "Training iteration: 196\n",
      "Validation loss (no improvement): 2.608785057067871\n",
      "Training iteration: 197\n",
      "Validation loss (no improvement): 2.5108179092407226\n",
      "Training iteration: 198\n",
      "Validation loss (no improvement): 2.4448848724365235\n",
      "Training iteration: 199\n",
      "Validation loss (no improvement): 2.4923776626586913\n",
      "Training iteration: 200\n",
      "Validation loss (no improvement): 2.5941667556762695\n",
      "Training iteration: 201\n",
      "Validation loss (no improvement): 2.5621118545532227\n",
      "Training iteration: 202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 2.364578628540039  to: 2.3043067932128904\n",
      "Training iteration: 203\n",
      "Validation loss (no improvement): 2.5711696624755858\n",
      "Training iteration: 204\n",
      "Validation loss (no improvement): 2.3067291259765623\n",
      "Training iteration: 205\n",
      "Validation loss (no improvement): 2.5193605422973633\n",
      "Training iteration: 206\n",
      "Validation loss (no improvement): 2.4001697540283202\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 2.3043067932128904  to: 2.227345848083496\n",
      "Training iteration: 208\n",
      "Validation loss (no improvement): 2.2559093475341796\n",
      "Training iteration: 209\n",
      "Validation loss (no improvement): 2.2991430282592775\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 2.227345848083496  to: 2.1724992752075196\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 2.1724992752075196  to: 2.1379798889160155\n",
      "Training iteration: 212\n",
      "Validation loss (no improvement): 2.187521553039551\n",
      "Training iteration: 213\n",
      "Validation loss (no improvement): 2.1496124267578125\n",
      "Training iteration: 214\n",
      "Validation loss (no improvement): 2.199140167236328\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 2.1379798889160155  to: 2.1249248504638674\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 2.1249248504638674  to: 2.102570152282715\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 2.102570152282715  to: 1.984039878845215\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 1.984039878845215  to: 1.9763463973999023\n",
      "Training iteration: 219\n",
      "Validation loss (no improvement): 2.005437660217285\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 1.9763463973999023  to: 1.9141332626342773\n",
      "Training iteration: 221\n",
      "Validation loss (no improvement): 1.9196413040161133\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 1.9141332626342773  to: 1.869300079345703\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 1.869300079345703  to: 1.7627233505249023\n",
      "Training iteration: 224\n",
      "Validation loss (no improvement): 1.8756431579589843\n",
      "Training iteration: 225\n",
      "Validation loss (no improvement): 1.9467538833618163\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 1.7627233505249023  to: 1.7445640563964844\n",
      "Training iteration: 227\n",
      "Validation loss (no improvement): 1.902031135559082\n",
      "Training iteration: 228\n",
      "Validation loss (no improvement): 1.9598033905029297\n",
      "Training iteration: 229\n",
      "Validation loss (no improvement): 1.8410406112670898\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 1.7445640563964844  to: 1.7425268173217774\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 1.7425268173217774  to: 1.6637557983398437\n",
      "Training iteration: 232\n",
      "Validation loss (no improvement): 1.7202037811279296\n",
      "Training iteration: 233\n",
      "Validation loss (no improvement): 1.7576196670532227\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 1.6637557983398437  to: 1.5887307167053222\n",
      "Training iteration: 235\n",
      "Validation loss (no improvement): 1.7094715118408204\n",
      "Training iteration: 236\n",
      "Validation loss (no improvement): 1.6272821426391602\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 1.5887307167053222  to: 1.5081412315368652\n",
      "Training iteration: 238\n",
      "Validation loss (no improvement): 1.5363883972167969\n",
      "Training iteration: 239\n",
      "Validation loss (no improvement): 1.5710379600524902\n",
      "Training iteration: 240\n",
      "Validation loss (no improvement): 1.5786767959594727\n",
      "Training iteration: 241\n",
      "Validation loss (no improvement): 1.6347349166870118\n",
      "Training iteration: 242\n",
      "Validation loss (no improvement): 1.6152814865112304\n",
      "Training iteration: 243\n",
      "Validation loss (no improvement): 1.6285541534423829\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 1.5081412315368652  to: 1.3478849411010743\n",
      "Training iteration: 245\n",
      "Validation loss (no improvement): 1.6095325469970703\n",
      "Training iteration: 246\n",
      "Validation loss (no improvement): 1.486569881439209\n",
      "Training iteration: 247\n",
      "Validation loss (no improvement): 1.521395492553711\n",
      "Training iteration: 248\n",
      "Validation loss (no improvement): 1.4917811393737792\n",
      "Training iteration: 249\n",
      "Validation loss (no improvement): 1.4891119956970216\n",
      "Training iteration: 250\n",
      "Validation loss (no improvement): 1.351849365234375\n",
      "Training iteration: 251\n",
      "Validation loss (no improvement): 1.3690619468688965\n",
      "Training iteration: 252\n",
      "Validation loss (no improvement): 1.4114626884460448\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 1.3478849411010743  to: 1.2607542991638183\n",
      "Training iteration: 254\n",
      "Validation loss (no improvement): 1.3437151908874512\n",
      "Training iteration: 255\n",
      "Validation loss (no improvement): 1.449768352508545\n",
      "Training iteration: 256\n",
      "Validation loss (no improvement): 1.2651076316833496\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 1.2607542991638183  to: 1.1733637809753419\n",
      "Training iteration: 258\n",
      "Validation loss (no improvement): 1.2561805725097657\n",
      "Training iteration: 259\n",
      "Validation loss (no improvement): 1.2281707763671874\n",
      "Training iteration: 260\n",
      "Validation loss (no improvement): 1.216986846923828\n",
      "Training iteration: 261\n",
      "Validation loss (no improvement): 1.2124063491821289\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 1.1733637809753419  to: 1.1603543281555175\n",
      "Training iteration: 263\n",
      "Validation loss (no improvement): 1.4045374870300293\n",
      "Training iteration: 264\n",
      "Validation loss (no improvement): 1.3012870788574218\n",
      "Training iteration: 265\n",
      "Validation loss (no improvement): 1.160544204711914\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 1.1603543281555175  to: 1.1258252143859864\n",
      "Training iteration: 267\n",
      "Validation loss (no improvement): 1.2132226943969726\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 1.1258252143859864  to: 1.0564562797546386\n",
      "Training iteration: 269\n",
      "Validation loss (no improvement): 1.1644877433776855\n",
      "Training iteration: 270\n",
      "Validation loss (no improvement): 1.1057082176208497\n",
      "Training iteration: 271\n",
      "Validation loss (no improvement): 1.159705352783203\n",
      "Training iteration: 272\n",
      "Validation loss (no improvement): 1.05958833694458\n",
      "Training iteration: 273\n",
      "Validation loss (no improvement): 1.118236255645752\n",
      "Training iteration: 274\n",
      "Validation loss (no improvement): 1.1410452842712402\n",
      "Training iteration: 275\n",
      "Validation loss (no improvement): 1.1487306594848632\n",
      "Training iteration: 276\n",
      "Validation loss (no improvement): 1.1507411003112793\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 1.0564562797546386  to: 0.993680191040039\n",
      "Training iteration: 278\n",
      "Validation loss (no improvement): 1.0878666877746581\n",
      "Training iteration: 279\n",
      "Validation loss (no improvement): 1.1845250129699707\n",
      "Training iteration: 280\n",
      "Validation loss (no improvement): 1.055350399017334\n",
      "Training iteration: 281\n",
      "Validation loss (no improvement): 1.1075444221496582\n",
      "Training iteration: 282\n",
      "Validation loss (no improvement): 1.012629222869873\n",
      "Training iteration: 283\n",
      "Validation loss (no improvement): 1.0177230834960938\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.993680191040039  to: 0.787535572052002\n",
      "Training iteration: 285\n",
      "Validation loss (no improvement): 1.0272783279418944\n",
      "Training iteration: 286\n",
      "Validation loss (no improvement): 0.9699844360351563\n",
      "Training iteration: 287\n",
      "Validation loss (no improvement): 0.8078038215637207\n",
      "Training iteration: 288\n",
      "Validation loss (no improvement): 0.9593769073486328\n",
      "Training iteration: 289\n",
      "Validation loss (no improvement): 0.9140310287475586\n",
      "Training iteration: 290\n",
      "Validation loss (no improvement): 0.9041204452514648\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.787535572052002  to: 0.7340094089508057\n",
      "Training iteration: 292\n",
      "Validation loss (no improvement): 0.8603160858154297\n",
      "Training iteration: 293\n",
      "Validation loss (no improvement): 0.9970205307006836\n",
      "Training iteration: 294\n",
      "Validation loss (no improvement): 0.8359438896179199\n",
      "Training iteration: 295\n",
      "Validation loss (no improvement): 0.8893905639648437\n",
      "Training iteration: 296\n",
      "Validation loss (no improvement): 0.9168710708618164\n",
      "Training iteration: 297\n",
      "Validation loss (no improvement): 0.7980898380279541\n",
      "Training iteration: 298\n",
      "Validation loss (no improvement): 0.8306715965270997\n",
      "Training iteration: 299\n",
      "Validation loss (no improvement): 0.803046989440918\n",
      "Training iteration: 300\n",
      "Validation loss (no improvement): 0.8043709754943847\n",
      "Training iteration: 301\n",
      "Validation loss (no improvement): 0.8722024917602539\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.7340094089508057  to: 0.7128928661346435\n",
      "Training iteration: 303\n",
      "Validation loss (no improvement): 0.8152588844299317\n",
      "Training iteration: 304\n",
      "Validation loss (no improvement): 0.8466405868530273\n",
      "Training iteration: 305\n",
      "Validation loss (no improvement): 0.8868870735168457\n",
      "Training iteration: 306\n",
      "Validation loss (no improvement): 0.7946613788604736\n",
      "Training iteration: 307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.8044639587402344\n",
      "Training iteration: 308\n",
      "Validation loss (no improvement): 0.8669649124145508\n",
      "Training iteration: 309\n",
      "Validation loss (no improvement): 0.8116086006164551\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.7128928661346435  to: 0.6577919006347657\n",
      "Training iteration: 311\n",
      "Validation loss (no improvement): 0.7538279056549072\n",
      "Training iteration: 312\n",
      "Validation loss (no improvement): 0.6896100997924804\n",
      "Training iteration: 313\n",
      "Validation loss (no improvement): 0.690008544921875\n",
      "Training iteration: 314\n",
      "Validation loss (no improvement): 0.7398799419403076\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.6577919006347657  to: 0.6022832870483399\n",
      "Training iteration: 316\n",
      "Validation loss (no improvement): 0.7011582851409912\n",
      "Training iteration: 317\n",
      "Validation loss (no improvement): 0.7464758396148682\n",
      "Training iteration: 318\n",
      "Validation loss (no improvement): 0.8349873542785644\n",
      "Training iteration: 319\n",
      "Validation loss (no improvement): 0.7577160835266114\n",
      "Training iteration: 320\n",
      "Validation loss (no improvement): 0.6489470958709717\n",
      "Training iteration: 321\n",
      "Validation loss (no improvement): 0.760522985458374\n",
      "Training iteration: 322\n",
      "Validation loss (no improvement): 0.7242112636566163\n",
      "Training iteration: 323\n",
      "Validation loss (no improvement): 0.7034165859222412\n",
      "Training iteration: 324\n",
      "Validation loss (no improvement): 0.6289922714233398\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.6022832870483399  to: 0.5928227424621582\n",
      "Training iteration: 326\n",
      "Validation loss (no improvement): 0.7087208747863769\n",
      "Training iteration: 327\n",
      "Validation loss (no improvement): 0.6072650909423828\n",
      "Training iteration: 328\n",
      "Validation loss (no improvement): 0.7227608680725097\n",
      "Training iteration: 329\n",
      "Validation loss (no improvement): 0.6144174098968506\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.5928227424621582  to: 0.5645541191101074\n",
      "Training iteration: 331\n",
      "Validation loss (no improvement): 0.6223496913909912\n",
      "Training iteration: 332\n",
      "Validation loss (no improvement): 0.5969680309295654\n",
      "Training iteration: 333\n",
      "Validation loss (no improvement): 0.7422313213348388\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.5645541191101074  to: 0.5477607250213623\n",
      "Training iteration: 335\n",
      "Validation loss (no improvement): 0.5843218326568603\n",
      "Training iteration: 336\n",
      "Validation loss (no improvement): 0.6255124568939209\n",
      "Training iteration: 337\n",
      "Validation loss (no improvement): 0.6753041267395019\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.5477607250213623  to: 0.44136781692504884\n",
      "Training iteration: 339\n",
      "Validation loss (no improvement): 0.6056098937988281\n",
      "Training iteration: 340\n",
      "Validation loss (no improvement): 0.647837781906128\n",
      "Training iteration: 341\n",
      "Validation loss (no improvement): 0.543622350692749\n",
      "Training iteration: 342\n",
      "Validation loss (no improvement): 0.6183888912200928\n",
      "Training iteration: 343\n",
      "Validation loss (no improvement): 0.6133572578430175\n",
      "Training iteration: 344\n",
      "Validation loss (no improvement): 0.6510011196136475\n",
      "Training iteration: 345\n",
      "Validation loss (no improvement): 0.6138119697570801\n",
      "Training iteration: 346\n",
      "Validation loss (no improvement): 0.5298655033111572\n",
      "Training iteration: 347\n",
      "Validation loss (no improvement): 0.5484137535095215\n",
      "Training iteration: 348\n",
      "Validation loss (no improvement): 0.4682731628417969\n",
      "Training iteration: 349\n",
      "Validation loss (no improvement): 0.5286373615264892\n",
      "Training iteration: 350\n",
      "Validation loss (no improvement): 0.556780195236206\n",
      "Training iteration: 351\n",
      "Validation loss (no improvement): 0.6009925842285156\n",
      "Training iteration: 352\n",
      "Validation loss (no improvement): 0.45578827857971194\n",
      "Training iteration: 353\n",
      "Validation loss (no improvement): 0.6176576137542724\n",
      "Training iteration: 354\n",
      "Validation loss (no improvement): 0.4856693267822266\n",
      "Training iteration: 355\n",
      "Validation loss (no improvement): 0.4738768577575684\n",
      "Training iteration: 356\n",
      "Validation loss (no improvement): 0.6039019584655761\n",
      "Training iteration: 357\n",
      "Validation loss (no improvement): 0.6298086643218994\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.44136781692504884  to: 0.42360529899597166\n",
      "Training iteration: 359\n",
      "Validation loss (no improvement): 0.6385925769805908\n",
      "Training iteration: 360\n",
      "Validation loss (no improvement): 0.6378376007080078\n",
      "Training iteration: 361\n",
      "Validation loss (no improvement): 0.4516590118408203\n",
      "Training iteration: 362\n",
      "Validation loss (no improvement): 0.5747127532958984\n",
      "Training iteration: 363\n",
      "Validation loss (no improvement): 0.5037374019622802\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.42360529899597166  to: 0.4005420684814453\n",
      "Training iteration: 365\n",
      "Validation loss (no improvement): 0.4348424434661865\n",
      "Training iteration: 366\n",
      "Validation loss (no improvement): 0.5126612186431885\n",
      "Training iteration: 367\n",
      "Validation loss (no improvement): 0.4652410984039307\n",
      "Training iteration: 368\n",
      "Validation loss (no improvement): 0.43064584732055666\n",
      "Training iteration: 369\n",
      "Validation loss (no improvement): 0.44044008255004885\n",
      "Training iteration: 370\n",
      "Validation loss (no improvement): 0.4526200294494629\n",
      "Training iteration: 371\n",
      "Validation loss (no improvement): 0.43921899795532227\n",
      "Training iteration: 372\n",
      "Validation loss (no improvement): 0.4105218410491943\n",
      "Training iteration: 373\n",
      "Validation loss (no improvement): 0.447128963470459\n",
      "Training iteration: 374\n",
      "Validation loss (no improvement): 0.45212492942810056\n",
      "Training iteration: 375\n",
      "Validation loss (no improvement): 0.4518420696258545\n",
      "Training iteration: 376\n",
      "Validation loss (no improvement): 0.5172411441802979\n",
      "Training iteration: 377\n",
      "Validation loss (no improvement): 0.5197526454925537\n",
      "Training iteration: 378\n",
      "Validation loss (no improvement): 0.4878836154937744\n",
      "Training iteration: 379\n",
      "Validation loss (no improvement): 0.4383989334106445\n",
      "Training iteration: 380\n",
      "Validation loss (no improvement): 0.475286865234375\n",
      "Training iteration: 381\n",
      "Validation loss (no improvement): 0.5605602741241456\n",
      "Training iteration: 382\n",
      "Validation loss (no improvement): 0.4579652786254883\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.4005420684814453  to: 0.35653510093688967\n",
      "Training iteration: 384\n",
      "Validation loss (no improvement): 0.4908240795135498\n",
      "Training iteration: 385\n",
      "Validation loss (no improvement): 0.4650729179382324\n",
      "Training iteration: 386\n",
      "Validation loss (no improvement): 0.49479899406433103\n",
      "Training iteration: 387\n",
      "Validation loss (no improvement): 0.4015041351318359\n",
      "Training iteration: 388\n",
      "Validation loss (no improvement): 0.4456284523010254\n",
      "Training iteration: 389\n",
      "Validation loss (no improvement): 0.4335886001586914\n",
      "Training iteration: 390\n",
      "Validation loss (no improvement): 0.5544441699981689\n",
      "Training iteration: 391\n",
      "Validation loss (no improvement): 0.42207913398742675\n",
      "Training iteration: 392\n",
      "Validation loss (no improvement): 0.44161739349365237\n",
      "Training iteration: 393\n",
      "Validation loss (no improvement): 0.435321569442749\n",
      "Training iteration: 394\n",
      "Validation loss (no improvement): 0.38169901371002196\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.35653510093688967  to: 0.2917789936065674\n",
      "Training iteration: 396\n",
      "Validation loss (no improvement): 0.42764711380004883\n",
      "Training iteration: 397\n",
      "Validation loss (no improvement): 0.3578137636184692\n",
      "Training iteration: 398\n",
      "Validation loss (no improvement): 0.48720245361328124\n",
      "Training iteration: 399\n",
      "Validation loss (no improvement): 0.4153908729553223\n",
      "Training iteration: 400\n",
      "Validation loss (no improvement): 0.387175178527832\n",
      "Training iteration: 401\n",
      "Validation loss (no improvement): 0.43608994483947755\n",
      "Training iteration: 402\n",
      "Validation loss (no improvement): 0.47581052780151367\n",
      "Training iteration: 403\n",
      "Validation loss (no improvement): 0.4122284412384033\n",
      "Training iteration: 404\n",
      "Validation loss (no improvement): 0.33489749431610105\n",
      "Training iteration: 405\n",
      "Validation loss (no improvement): 0.3900209665298462\n",
      "Training iteration: 406\n",
      "Validation loss (no improvement): 0.3519673109054565\n",
      "Training iteration: 407\n",
      "Validation loss (no improvement): 0.4279904842376709\n",
      "Training iteration: 408\n",
      "Validation loss (no improvement): 0.4228946685791016\n",
      "Training iteration: 409\n",
      "Validation loss (no improvement): 0.38166489601135256\n",
      "Training iteration: 410\n",
      "Validation loss (no improvement): 0.36603846549987795\n",
      "Training iteration: 411\n",
      "Validation loss (no improvement): 0.3485686779022217\n",
      "Training iteration: 412\n",
      "Validation loss (no improvement): 0.3410106420516968\n",
      "Training iteration: 413\n",
      "Validation loss (no improvement): 0.36316781044006347\n",
      "Training iteration: 414\n",
      "Validation loss (no improvement): 0.44100031852722166\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.2917789936065674  to: 0.2710383176803589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 416\n",
      "Validation loss (no improvement): 0.350659704208374\n",
      "Training iteration: 417\n",
      "Validation loss (no improvement): 0.28720242977142335\n",
      "Training iteration: 418\n",
      "Validation loss (no improvement): 0.35820162296295166\n",
      "Training iteration: 419\n",
      "Validation loss (no improvement): 0.29257049560546877\n",
      "Training iteration: 420\n",
      "Validation loss (no improvement): 0.2718286752700806\n",
      "Training iteration: 421\n",
      "Validation loss (no improvement): 0.32338454723358157\n",
      "Training iteration: 422\n",
      "Validation loss (no improvement): 0.3636874437332153\n",
      "Training iteration: 423\n",
      "Validation loss (no improvement): 0.3385987043380737\n",
      "Training iteration: 424\n",
      "Validation loss (no improvement): 0.33365650177001954\n",
      "Training iteration: 425\n",
      "Validation loss (no improvement): 0.32815914154052733\n",
      "Training iteration: 426\n",
      "Validation loss (no improvement): 0.42777304649353026\n",
      "Training iteration: 427\n",
      "Validation loss (no improvement): 0.32334587574005125\n",
      "Training iteration: 428\n",
      "Validation loss (no improvement): 0.36609883308410646\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.2710383176803589  to: 0.21571109294891358\n",
      "Training iteration: 430\n",
      "Validation loss (no improvement): 0.38874914646148684\n",
      "Training iteration: 431\n",
      "Validation loss (no improvement): 0.32185442447662355\n",
      "Training iteration: 432\n",
      "Validation loss (no improvement): 0.4100686550140381\n",
      "Training iteration: 433\n",
      "Validation loss (no improvement): 0.30391685962677\n",
      "Training iteration: 434\n",
      "Validation loss (no improvement): 0.3588600158691406\n",
      "Training iteration: 435\n",
      "Validation loss (no improvement): 0.3611281871795654\n",
      "Training iteration: 436\n",
      "Validation loss (no improvement): 0.28279149532318115\n",
      "Training iteration: 437\n",
      "Validation loss (no improvement): 0.3186869382858276\n",
      "Training iteration: 438\n",
      "Validation loss (no improvement): 0.34673619270324707\n",
      "Training iteration: 439\n",
      "Validation loss (no improvement): 0.402464771270752\n",
      "Training iteration: 440\n",
      "Validation loss (no improvement): 0.3625248193740845\n",
      "Training iteration: 441\n",
      "Validation loss (no improvement): 0.31284844875335693\n",
      "Training iteration: 442\n",
      "Validation loss (no improvement): 0.3799983263015747\n",
      "Training iteration: 443\n",
      "Validation loss (no improvement): 0.3012258768081665\n",
      "Training iteration: 444\n",
      "Validation loss (no improvement): 0.2717085599899292\n",
      "Training iteration: 445\n",
      "Validation loss (no improvement): 0.27095637321472166\n",
      "Training iteration: 446\n",
      "Validation loss (no improvement): 0.23861267566680908\n",
      "Training iteration: 447\n",
      "Validation loss (no improvement): 0.31588699817657473\n",
      "Training iteration: 448\n",
      "Validation loss (no improvement): 0.23581068515777587\n",
      "Training iteration: 449\n",
      "Validation loss (no improvement): 0.2613854885101318\n",
      "Training iteration: 450\n",
      "Validation loss (no improvement): 0.3804697275161743\n",
      "Training iteration: 451\n",
      "Validation loss (no improvement): 0.3853041410446167\n",
      "Training iteration: 452\n",
      "Validation loss (no improvement): 0.22980663776397706\n",
      "Training iteration: 453\n",
      "Validation loss (no improvement): 0.2768268346786499\n",
      "Training iteration: 454\n",
      "Validation loss (no improvement): 0.21727161407470702\n",
      "Training iteration: 455\n",
      "Validation loss (no improvement): 0.340735650062561\n",
      "Training iteration: 456\n",
      "Validation loss (no improvement): 0.2988744735717773\n",
      "Training iteration: 457\n",
      "Validation loss (no improvement): 0.28135147094726565\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.21571109294891358  to: 0.1660294532775879\n",
      "Training iteration: 459\n",
      "Validation loss (no improvement): 0.33277924060821534\n",
      "Training iteration: 460\n",
      "Validation loss (no improvement): 0.2783110857009888\n",
      "Training iteration: 461\n",
      "Validation loss (no improvement): 0.29013619422912595\n",
      "Training iteration: 462\n",
      "Validation loss (no improvement): 0.30072698593139646\n",
      "Training iteration: 463\n",
      "Validation loss (no improvement): 0.2748763084411621\n",
      "Training iteration: 464\n",
      "Validation loss (no improvement): 0.3686011791229248\n",
      "Training iteration: 465\n",
      "Validation loss (no improvement): 0.26865150928497317\n",
      "Training iteration: 466\n",
      "Validation loss (no improvement): 0.22956399917602538\n",
      "Training iteration: 467\n",
      "Validation loss (no improvement): 0.2450197458267212\n",
      "Training iteration: 468\n",
      "Validation loss (no improvement): 0.2603192090988159\n",
      "Training iteration: 469\n",
      "Validation loss (no improvement): 0.28928542137145996\n",
      "Training iteration: 470\n",
      "Validation loss (no improvement): 0.1776035785675049\n",
      "Training iteration: 471\n",
      "Validation loss (no improvement): 0.1936091184616089\n",
      "Training iteration: 472\n",
      "Validation loss (no improvement): 0.27028181552886965\n",
      "Training iteration: 473\n",
      "Validation loss (no improvement): 0.3027146577835083\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1660294532775879  to: 0.16173343658447265\n",
      "Training iteration: 475\n",
      "Validation loss (no improvement): 0.22057020664215088\n",
      "Training iteration: 476\n",
      "Validation loss (no improvement): 0.2624450445175171\n",
      "Training iteration: 477\n",
      "Validation loss (no improvement): 0.2958916187286377\n",
      "Training iteration: 478\n",
      "Validation loss (no improvement): 0.21376562118530273\n",
      "Training iteration: 479\n",
      "Validation loss (no improvement): 0.3165736198425293\n",
      "Training iteration: 480\n",
      "Validation loss (no improvement): 0.2341580867767334\n",
      "Training iteration: 481\n",
      "Validation loss (no improvement): 0.1860440731048584\n",
      "Training iteration: 482\n",
      "Validation loss (no improvement): 0.21197681427001952\n",
      "Training iteration: 483\n",
      "Validation loss (no improvement): 0.29446887969970703\n",
      "Training iteration: 484\n",
      "Validation loss (no improvement): 0.2762941837310791\n",
      "Training iteration: 485\n",
      "Validation loss (no improvement): 0.3072237491607666\n",
      "Training iteration: 486\n",
      "Validation loss (no improvement): 0.23331959247589112\n",
      "Training iteration: 487\n",
      "Validation loss (no improvement): 0.21420886516571044\n",
      "Training iteration: 488\n",
      "Validation loss (no improvement): 0.2479710102081299\n",
      "Training iteration: 489\n",
      "Validation loss (no improvement): 0.20109694004058837\n",
      "Training iteration: 490\n",
      "Validation loss (no improvement): 0.2872843027114868\n",
      "Training iteration: 491\n",
      "Validation loss (no improvement): 0.27751009464263915\n",
      "Training iteration: 492\n",
      "Validation loss (no improvement): 0.17725417613983155\n",
      "Training iteration: 493\n",
      "Validation loss (no improvement): 0.2886474370956421\n",
      "Training iteration: 494\n",
      "Validation loss (no improvement): 0.21453194618225097\n",
      "Training iteration: 495\n",
      "Validation loss (no improvement): 0.2181483268737793\n",
      "Training iteration: 496\n",
      "Validation loss (no improvement): 0.27689006328582766\n",
      "Training iteration: 497\n",
      "Validation loss (no improvement): 0.21717889308929444\n",
      "Training iteration: 498\n",
      "Validation loss (no improvement): 0.22822418212890624\n",
      "Training iteration: 499\n",
      "Validation loss (no improvement): 0.20647139549255372\n",
      "Training iteration: 500\n",
      "Validation loss (no improvement): 0.21343812942504883\n",
      "Training iteration: 501\n",
      "Validation loss (no improvement): 0.27706069946289064\n",
      "Training iteration: 502\n",
      "Validation loss (no improvement): 0.22450876235961914\n",
      "Training iteration: 503\n",
      "Validation loss (no improvement): 0.25545334815979004\n",
      "Training iteration: 504\n",
      "Validation loss (no improvement): 0.19936978816986084\n",
      "Training iteration: 505\n",
      "Validation loss (no improvement): 0.2524984121322632\n",
      "Training iteration: 506\n",
      "Validation loss (no improvement): 0.18461076021194459\n",
      "Training iteration: 507\n",
      "Validation loss (no improvement): 0.2622933864593506\n",
      "Training iteration: 508\n",
      "Validation loss (no improvement): 0.2546199083328247\n",
      "Training iteration: 509\n",
      "Validation loss (no improvement): 0.21429333686828614\n",
      "Training iteration: 510\n",
      "Validation loss (no improvement): 0.2039550542831421\n",
      "Training iteration: 511\n",
      "Validation loss (no improvement): 0.21612770557403566\n",
      "Training iteration: 512\n",
      "Validation loss (no improvement): 0.20206048488616943\n",
      "Training iteration: 513\n",
      "Validation loss (no improvement): 0.2179654836654663\n",
      "Training iteration: 514\n",
      "Validation loss (no improvement): 0.20769329071044923\n",
      "Training iteration: 515\n",
      "Validation loss (no improvement): 0.1922242283821106\n",
      "Training iteration: 516\n",
      "Validation loss (no improvement): 0.24875543117523194\n",
      "Training iteration: 517\n",
      "Validation loss (no improvement): 0.25126848220825193\n",
      "Training iteration: 518\n",
      "Validation loss (no improvement): 0.16341862678527833\n",
      "Training iteration: 519\n",
      "Validation loss (no improvement): 0.28142244815826417\n",
      "Training iteration: 520\n",
      "Validation loss (no improvement): 0.17165406942367553\n",
      "Training iteration: 521\n",
      "Validation loss (no improvement): 0.22751405239105224\n",
      "Training iteration: 522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.2195526361465454\n",
      "Training iteration: 523\n",
      "Validation loss (no improvement): 0.27407407760620117\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.16173343658447265  to: 0.1329134702682495\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1329134702682495  to: 0.12274774312973022\n",
      "Training iteration: 526\n",
      "Validation loss (no improvement): 0.20503242015838624\n",
      "Training iteration: 527\n",
      "Validation loss (no improvement): 0.16705820560455323\n",
      "Training iteration: 528\n",
      "Validation loss (no improvement): 0.2234565019607544\n",
      "Training iteration: 529\n",
      "Validation loss (no improvement): 0.1349366307258606\n",
      "Training iteration: 530\n",
      "Validation loss (no improvement): 0.20359406471252442\n",
      "Training iteration: 531\n",
      "Validation loss (no improvement): 0.17233861684799195\n",
      "Training iteration: 532\n",
      "Validation loss (no improvement): 0.16314517259597777\n",
      "Training iteration: 533\n",
      "Validation loss (no improvement): 0.13299551010131835\n",
      "Training iteration: 534\n",
      "Validation loss (no improvement): 0.1386652946472168\n",
      "Training iteration: 535\n",
      "Validation loss (no improvement): 0.18411706686019896\n",
      "Training iteration: 536\n",
      "Validation loss (no improvement): 0.19461604356765747\n",
      "Training iteration: 537\n",
      "Validation loss (no improvement): 0.18871405124664306\n",
      "Training iteration: 538\n",
      "Validation loss (no improvement): 0.22126712799072265\n",
      "Training iteration: 539\n",
      "Validation loss (no improvement): 0.1999765396118164\n",
      "Training iteration: 540\n",
      "Validation loss (no improvement): 0.19726758003234862\n",
      "Training iteration: 541\n",
      "Validation loss (no improvement): 0.17568336725234984\n",
      "Training iteration: 542\n",
      "Validation loss (no improvement): 0.23993234634399413\n",
      "Training iteration: 543\n",
      "Validation loss (no improvement): 0.18266950845718383\n",
      "Training iteration: 544\n",
      "Validation loss (no improvement): 0.2817242622375488\n",
      "Training iteration: 545\n",
      "Validation loss (no improvement): 0.15572009086608887\n",
      "Training iteration: 546\n",
      "Validation loss (no improvement): 0.19971318244934083\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12274774312973022  to: 0.11681090593338013\n",
      "Training iteration: 548\n",
      "Validation loss (no improvement): 0.1826430916786194\n",
      "Training iteration: 549\n",
      "Validation loss (no improvement): 0.25335254669189455\n",
      "Training iteration: 550\n",
      "Validation loss (no improvement): 0.18845449686050414\n",
      "Training iteration: 551\n",
      "Validation loss (no improvement): 0.2121199369430542\n",
      "Training iteration: 552\n",
      "Validation loss (no improvement): 0.19897490739822388\n",
      "Training iteration: 553\n",
      "Validation loss (no improvement): 0.1527460217475891\n",
      "Training iteration: 554\n",
      "Validation loss (no improvement): 0.14191421270370483\n",
      "Training iteration: 555\n",
      "Validation loss (no improvement): 0.1750830292701721\n",
      "Training iteration: 556\n",
      "Validation loss (no improvement): 0.16470569372177124\n",
      "Training iteration: 557\n",
      "Validation loss (no improvement): 0.14040931463241577\n",
      "Training iteration: 558\n",
      "Validation loss (no improvement): 0.15834716558456421\n",
      "Training iteration: 559\n",
      "Validation loss (no improvement): 0.12493295669555664\n",
      "Training iteration: 560\n",
      "Validation loss (no improvement): 0.1801094651222229\n",
      "Training iteration: 561\n",
      "Validation loss (no improvement): 0.173541259765625\n",
      "Training iteration: 562\n",
      "Validation loss (no improvement): 0.16497666835784913\n",
      "Training iteration: 563\n",
      "Validation loss (no improvement): 0.14557830095291138\n",
      "Training iteration: 564\n",
      "Validation loss (no improvement): 0.1648935079574585\n",
      "Training iteration: 565\n",
      "Validation loss (no improvement): 0.18098703622817994\n",
      "Training iteration: 566\n",
      "Validation loss (no improvement): 0.177346134185791\n",
      "Training iteration: 567\n",
      "Validation loss (no improvement): 0.13351577520370483\n",
      "Training iteration: 568\n",
      "Validation loss (no improvement): 0.15334411859512329\n",
      "Training iteration: 569\n",
      "Validation loss (no improvement): 0.18841969966888428\n",
      "Training iteration: 570\n",
      "Validation loss (no improvement): 0.1657833695411682\n",
      "Training iteration: 571\n",
      "Validation loss (no improvement): 0.15363171100616455\n",
      "Training iteration: 572\n",
      "Validation loss (no improvement): 0.14360760450363158\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.11681090593338013  to: 0.10163747072219849\n",
      "Training iteration: 574\n",
      "Validation loss (no improvement): 0.173680317401886\n",
      "Training iteration: 575\n",
      "Validation loss (no improvement): 0.18920279741287233\n",
      "Training iteration: 576\n",
      "Validation loss (no improvement): 0.1725725054740906\n",
      "Training iteration: 577\n",
      "Validation loss (no improvement): 0.11470493078231811\n",
      "Training iteration: 578\n",
      "Validation loss (no improvement): 0.1841752052307129\n",
      "Training iteration: 579\n",
      "Validation loss (no improvement): 0.1488507032394409\n",
      "Training iteration: 580\n",
      "Validation loss (no improvement): 0.13848778009414672\n",
      "Training iteration: 581\n",
      "Validation loss (no improvement): 0.10388736724853516\n",
      "Training iteration: 582\n",
      "Validation loss (no improvement): 0.13484929800033568\n",
      "Training iteration: 583\n",
      "Validation loss (no improvement): 0.1578904628753662\n",
      "Training iteration: 584\n",
      "Validation loss (no improvement): 0.21271328926086425\n",
      "Training iteration: 585\n",
      "Validation loss (no improvement): 0.14805355072021484\n",
      "Training iteration: 586\n",
      "Validation loss (no improvement): 0.13138092756271363\n",
      "Training iteration: 587\n",
      "Validation loss (no improvement): 0.16565603017807007\n",
      "Training iteration: 588\n",
      "Validation loss (no improvement): 0.16188472509384155\n",
      "Training iteration: 589\n",
      "Validation loss (no improvement): 0.17786843776702882\n",
      "Training iteration: 590\n",
      "Validation loss (no improvement): 0.14615401029586791\n",
      "Training iteration: 591\n",
      "Validation loss (no improvement): 0.23756136894226074\n",
      "Training iteration: 592\n",
      "Validation loss (no improvement): 0.22068026065826415\n",
      "Training iteration: 593\n",
      "Validation loss (no improvement): 0.21589882373809816\n",
      "Training iteration: 594\n",
      "Validation loss (no improvement): 0.229577374458313\n",
      "Training iteration: 595\n",
      "Validation loss (no improvement): 0.21469879150390625\n",
      "Training iteration: 596\n",
      "Validation loss (no improvement): 0.18939021825790406\n",
      "Training iteration: 597\n",
      "Validation loss (no improvement): 0.16797051429748536\n",
      "Training iteration: 598\n",
      "Validation loss (no improvement): 0.1922340750694275\n",
      "Training iteration: 599\n",
      "Validation loss (no improvement): 0.18428244590759277\n",
      "Training iteration: 600\n",
      "Validation loss (no improvement): 0.18705317974090577\n",
      "Training iteration: 601\n",
      "Validation loss (no improvement): 0.23550100326538087\n",
      "Training iteration: 602\n",
      "Validation loss (no improvement): 0.20542995929718016\n",
      "Training iteration: 603\n",
      "Validation loss (no improvement): 0.20943405628204345\n",
      "Training iteration: 604\n",
      "Validation loss (no improvement): 0.2119351625442505\n",
      "Training iteration: 605\n",
      "Validation loss (no improvement): 0.20614588260650635\n",
      "Training iteration: 606\n",
      "Validation loss (no improvement): 0.2015993356704712\n",
      "Training iteration: 607\n",
      "Validation loss (no improvement): 0.22402610778808593\n",
      "Training iteration: 608\n",
      "Validation loss (no improvement): 0.1641265034675598\n",
      "Training iteration: 609\n",
      "Validation loss (no improvement): 0.16189465522766114\n",
      "Training iteration: 610\n",
      "Validation loss (no improvement): 0.1627505302429199\n",
      "Training iteration: 611\n",
      "Validation loss (no improvement): 0.2177211284637451\n",
      "Training iteration: 612\n",
      "Validation loss (no improvement): 0.17454376220703124\n",
      "Training iteration: 613\n",
      "Validation loss (no improvement): 0.17734375\n",
      "Training iteration: 614\n",
      "Validation loss (no improvement): 0.15409226417541505\n",
      "Training iteration: 615\n",
      "Validation loss (no improvement): 0.1656159520149231\n",
      "Training iteration: 616\n",
      "Validation loss (no improvement): 0.172501802444458\n",
      "Training iteration: 617\n",
      "Validation loss (no improvement): 0.15099722146987915\n",
      "Training iteration: 618\n",
      "Validation loss (no improvement): 0.15210473537445068\n",
      "Training iteration: 619\n",
      "Validation loss (no improvement): 0.14791603088378907\n",
      "Training iteration: 620\n",
      "Validation loss (no improvement): 0.12782034873962403\n",
      "Training iteration: 621\n",
      "Validation loss (no improvement): 0.16686862707138062\n",
      "Training iteration: 622\n",
      "Validation loss (no improvement): 0.13324371576309205\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.10163747072219849  to: 0.09657132029533386\n",
      "Training iteration: 624\n",
      "Validation loss (no improvement): 0.18668341636657715\n",
      "Training iteration: 625\n",
      "Validation loss (no improvement): 0.12982503175735474\n",
      "Training iteration: 626\n",
      "Validation loss (no improvement): 0.14701755046844484\n",
      "Training iteration: 627\n",
      "Validation loss (no improvement): 0.18941302299499513\n",
      "Training iteration: 628\n",
      "Validation loss (no improvement): 0.1181403636932373\n",
      "Training iteration: 629\n",
      "Validation loss (no improvement): 0.10710573196411133\n",
      "Training iteration: 630\n",
      "Validation loss (no improvement): 0.16902921199798585\n",
      "Training iteration: 631\n",
      "Validation loss (no improvement): 0.15228883028030396\n",
      "Training iteration: 632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.17415785789489746\n",
      "Training iteration: 633\n",
      "Validation loss (no improvement): 0.1442735195159912\n",
      "Training iteration: 634\n",
      "Validation loss (no improvement): 0.16411733627319336\n",
      "Training iteration: 635\n",
      "Validation loss (no improvement): 0.10284581184387206\n",
      "Training iteration: 636\n",
      "Validation loss (no improvement): 0.19172580242156984\n",
      "Training iteration: 637\n",
      "Validation loss (no improvement): 0.11915445327758789\n",
      "Training iteration: 638\n",
      "Validation loss (no improvement): 0.13931612968444823\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.09657132029533386  to: 0.07250648736953735\n",
      "Training iteration: 640\n",
      "Validation loss (no improvement): 0.153597629070282\n",
      "Training iteration: 641\n",
      "Validation loss (no improvement): 0.11356823444366455\n",
      "Training iteration: 642\n",
      "Validation loss (no improvement): 0.14267325401306152\n",
      "Training iteration: 643\n",
      "Validation loss (no improvement): 0.15512490272521973\n",
      "Training iteration: 644\n",
      "Validation loss (no improvement): 0.12200524806976318\n",
      "Training iteration: 645\n",
      "Validation loss (no improvement): 0.10347645282745362\n",
      "Training iteration: 646\n",
      "Validation loss (no improvement): 0.11562083959579468\n",
      "Training iteration: 647\n",
      "Validation loss (no improvement): 0.17763981819152833\n",
      "Training iteration: 648\n",
      "Validation loss (no improvement): 0.11228375434875489\n",
      "Training iteration: 649\n",
      "Validation loss (no improvement): 0.14396636486053466\n",
      "Training iteration: 650\n",
      "Validation loss (no improvement): 0.12835097312927246\n",
      "Training iteration: 651\n",
      "Validation loss (no improvement): 0.1647335410118103\n",
      "Training iteration: 652\n",
      "Validation loss (no improvement): 0.11875705718994141\n",
      "Training iteration: 653\n",
      "Validation loss (no improvement): 0.10898160934448242\n",
      "Training iteration: 654\n",
      "Validation loss (no improvement): 0.16134910583496093\n",
      "Training iteration: 655\n",
      "Validation loss (no improvement): 0.10859441757202148\n",
      "Training iteration: 656\n",
      "Validation loss (no improvement): 0.11639419794082642\n",
      "Training iteration: 657\n",
      "Validation loss (no improvement): 0.15007443428039552\n",
      "Training iteration: 658\n",
      "Validation loss (no improvement): 0.16487990617752074\n",
      "Training iteration: 659\n",
      "Validation loss (no improvement): 0.13617461919784546\n",
      "Training iteration: 660\n",
      "Validation loss (no improvement): 0.11841609477996826\n",
      "Training iteration: 661\n",
      "Validation loss (no improvement): 0.15723471641540526\n",
      "Training iteration: 662\n",
      "Validation loss (no improvement): 0.13583425283432007\n",
      "Training iteration: 663\n",
      "Validation loss (no improvement): 0.15373506546020507\n",
      "Training iteration: 664\n",
      "Validation loss (no improvement): 0.16502586603164673\n",
      "Training iteration: 665\n",
      "Validation loss (no improvement): 0.11240594387054444\n",
      "Training iteration: 666\n",
      "Validation loss (no improvement): 0.13083596229553224\n",
      "Training iteration: 667\n",
      "Validation loss (no improvement): 0.11299272775650024\n",
      "Training iteration: 668\n",
      "Validation loss (no improvement): 0.11986386775970459\n",
      "Training iteration: 669\n",
      "Validation loss (no improvement): 0.1600209355354309\n",
      "Training iteration: 670\n",
      "Validation loss (no improvement): 0.1784055709838867\n",
      "Training iteration: 671\n",
      "Validation loss (no improvement): 0.16940019130706788\n",
      "Training iteration: 672\n",
      "Validation loss (no improvement): 0.11827514171600342\n",
      "Training iteration: 673\n",
      "Validation loss (no improvement): 0.13111652135849\n",
      "Training iteration: 674\n",
      "Validation loss (no improvement): 0.12592482566833496\n",
      "Training iteration: 675\n",
      "Validation loss (no improvement): 0.13137991428375245\n",
      "Training iteration: 676\n",
      "Validation loss (no improvement): 0.1289438247680664\n",
      "Training iteration: 677\n",
      "Validation loss (no improvement): 0.10614303350448609\n",
      "Training iteration: 678\n",
      "Validation loss (no improvement): 0.1200446605682373\n",
      "Training iteration: 679\n",
      "Validation loss (no improvement): 0.1573160171508789\n",
      "Training iteration: 680\n",
      "Validation loss (no improvement): 0.09469465017318726\n",
      "Training iteration: 681\n",
      "Validation loss (no improvement): 0.1257362484931946\n",
      "Training iteration: 682\n",
      "Validation loss (no improvement): 0.17031662464141845\n",
      "Training iteration: 683\n",
      "Validation loss (no improvement): 0.1744251251220703\n",
      "Training iteration: 684\n",
      "Validation loss (no improvement): 0.16619848012924193\n",
      "Training iteration: 685\n",
      "Validation loss (no improvement): 0.1312166452407837\n",
      "Training iteration: 686\n",
      "Validation loss (no improvement): 0.12853696346282958\n",
      "Training iteration: 687\n",
      "Validation loss (no improvement): 0.201278018951416\n",
      "Training iteration: 688\n",
      "Validation loss (no improvement): 0.13758084774017335\n",
      "Training iteration: 689\n",
      "Validation loss (no improvement): 0.11089601516723632\n",
      "Training iteration: 690\n",
      "Validation loss (no improvement): 0.0733087718486786\n",
      "Training iteration: 691\n",
      "Validation loss (no improvement): 0.11518088579177857\n",
      "Training iteration: 692\n",
      "Validation loss (no improvement): 0.1594140887260437\n",
      "Training iteration: 693\n",
      "Validation loss (no improvement): 0.14917455911636351\n",
      "Training iteration: 694\n",
      "Validation loss (no improvement): 0.16503021717071534\n",
      "Training iteration: 695\n",
      "Validation loss (no improvement): 0.1025848388671875\n",
      "Training iteration: 696\n",
      "Validation loss (no improvement): 0.10732622146606445\n",
      "Training iteration: 697\n",
      "Validation loss (no improvement): 0.11734925508499146\n",
      "Training iteration: 698\n",
      "Validation loss (no improvement): 0.11618796586990357\n",
      "Training iteration: 699\n",
      "Validation loss (no improvement): 0.15096287727355956\n",
      "Training iteration: 700\n",
      "Validation loss (no improvement): 0.10574183464050294\n",
      "Training iteration: 701\n",
      "Validation loss (no improvement): 0.1762739062309265\n",
      "Training iteration: 702\n",
      "Validation loss (no improvement): 0.1362614870071411\n",
      "Training iteration: 703\n",
      "Validation loss (no improvement): 0.15302776098251342\n",
      "Training iteration: 704\n",
      "Validation loss (no improvement): 0.15273773670196533\n",
      "Training iteration: 705\n",
      "Validation loss (no improvement): 0.13227046728134156\n",
      "Training iteration: 706\n",
      "Validation loss (no improvement): 0.14940367937088012\n",
      "Training iteration: 707\n",
      "Validation loss (no improvement): 0.1035271406173706\n",
      "Training iteration: 708\n",
      "Validation loss (no improvement): 0.13337379693984985\n",
      "Training iteration: 709\n",
      "Validation loss (no improvement): 0.09200614094734191\n",
      "Training iteration: 710\n",
      "Validation loss (no improvement): 0.11245298385620117\n",
      "Training iteration: 711\n",
      "Validation loss (no improvement): 0.10045157670974732\n",
      "Training iteration: 712\n",
      "Validation loss (no improvement): 0.12766423225402831\n",
      "Training iteration: 713\n",
      "Validation loss (no improvement): 0.1196295142173767\n",
      "Training iteration: 714\n",
      "Validation loss (no improvement): 0.12750893831253052\n",
      "Training iteration: 715\n",
      "Validation loss (no improvement): 0.10144486427307128\n",
      "Training iteration: 716\n",
      "Validation loss (no improvement): 0.1480270504951477\n",
      "Training iteration: 717\n",
      "Validation loss (no improvement): 0.14897431135177613\n",
      "Training iteration: 718\n",
      "Validation loss (no improvement): 0.1451658248901367\n",
      "Training iteration: 719\n",
      "Validation loss (no improvement): 0.10848203897476197\n",
      "Training iteration: 720\n",
      "Validation loss (no improvement): 0.14525456428527833\n",
      "Training iteration: 721\n",
      "Validation loss (no improvement): 0.12539491653442383\n",
      "Training iteration: 722\n",
      "Validation loss (no improvement): 0.1491428256034851\n",
      "Training iteration: 723\n",
      "Validation loss (no improvement): 0.12617844343185425\n",
      "Training iteration: 724\n",
      "Validation loss (no improvement): 0.12235865592956544\n",
      "Training iteration: 725\n",
      "Validation loss (no improvement): 0.1085471272468567\n",
      "Training iteration: 726\n",
      "Validation loss (no improvement): 0.09834814667701722\n",
      "Training iteration: 727\n",
      "Validation loss (no improvement): 0.12081449031829834\n",
      "Training iteration: 728\n",
      "Validation loss (no improvement): 0.14172096252441407\n",
      "Training iteration: 729\n",
      "Validation loss (no improvement): 0.12791565656661988\n",
      "Training iteration: 730\n",
      "Validation loss (no improvement): 0.15705676078796388\n",
      "Training iteration: 731\n",
      "Validation loss (no improvement): 0.13166873455047606\n",
      "Training iteration: 732\n",
      "Validation loss (no improvement): 0.12062212228775024\n",
      "Training iteration: 733\n",
      "Validation loss (no improvement): 0.16250115633010864\n",
      "Training iteration: 734\n",
      "Validation loss (no improvement): 0.11635069847106934\n",
      "Training iteration: 735\n",
      "Validation loss (no improvement): 0.15736602544784545\n",
      "Training iteration: 736\n",
      "Validation loss (no improvement): 0.21641764640808106\n",
      "Training iteration: 737\n",
      "Validation loss (no improvement): 0.1642824649810791\n",
      "Training iteration: 738\n",
      "Validation loss (no improvement): 0.09702606797218323\n",
      "Training iteration: 739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.14093316793441774\n"
     ]
    }
   ],
   "source": [
    "bayesian_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 30.1625244140625\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 30.1625244140625  to: 29.623876953125\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 29.623876953125  to: 29.10130920410156\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 29.10130920410156  to: 28.59993896484375\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 28.59993896484375  to: 28.106329345703124\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 28.106329345703124  to: 27.618914794921874\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 27.618914794921874  to: 27.143414306640626\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 27.143414306640626  to: 26.682830810546875\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 26.682830810546875  to: 26.2358642578125\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 26.2358642578125  to: 25.788571166992188\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 25.788571166992188  to: 25.3390380859375\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 25.3390380859375  to: 24.887362670898437\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 24.887362670898437  to: 24.4345703125\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 24.4345703125  to: 23.97790985107422\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 23.97790985107422  to: 23.516377258300782\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 23.516377258300782  to: 23.05023193359375\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 23.05023193359375  to: 22.579257202148437\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 22.579257202148437  to: 22.10382537841797\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 22.10382537841797  to: 21.621430969238283\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 21.621430969238283  to: 21.131982421875\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 21.131982421875  to: 20.635700988769532\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 20.635700988769532  to: 20.133663940429688\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 20.133663940429688  to: 19.6253662109375\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 19.6253662109375  to: 19.11185302734375\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 19.11185302734375  to: 18.594224548339845\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 18.594224548339845  to: 18.072964477539063\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 18.072964477539063  to: 17.54912872314453\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 17.54912872314453  to: 17.02447967529297\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 17.02447967529297  to: 16.500332641601563\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 16.500332641601563  to: 15.978005981445312\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 15.978005981445312  to: 15.459019470214844\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 15.459019470214844  to: 14.945112609863282\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 14.945112609863282  to: 14.43829803466797\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 14.43829803466797  to: 13.940571594238282\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 13.940571594238282  to: 13.453973388671875\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 13.453973388671875  to: 12.980622863769531\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 12.980622863769531  to: 12.52255859375\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 12.52255859375  to: 12.081806945800782\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 12.081806945800782  to: 11.660384368896484\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 11.660384368896484  to: 11.26021957397461\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 11.26021957397461  to: 10.883171081542969\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 10.883171081542969  to: 10.530708312988281\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 10.530708312988281  to: 10.204149627685547\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 10.204149627685547  to: 9.904509735107421\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 9.904509735107421  to: 9.632551574707032\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 9.632551574707032  to: 9.388610076904296\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 9.388610076904296  to: 9.172567749023438\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 9.172567749023438  to: 8.983898162841797\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 8.983898162841797  to: 8.821546173095703\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 8.821546173095703  to: 8.683967590332031\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 8.683967590332031  to: 8.569181823730469\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 8.569181823730469  to: 8.474843597412109\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 8.474843597412109  to: 8.398295593261718\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 8.398295593261718  to: 8.336619567871093\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 8.336619567871093  to: 8.286810302734375\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 8.286810302734375  to: 8.24591293334961\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 8.24591293334961  to: 8.21103286743164\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 8.21103286743164  to: 8.179474639892579\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 8.179474639892579  to: 8.14884033203125\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 8.14884033203125  to: 8.117034912109375\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 8.117034912109375  to: 8.082400512695312\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 8.082400512695312  to: 8.043802642822266\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 8.043802642822266  to: 8.000587463378906\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 8.000587463378906  to: 7.952357482910156\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 7.952357482910156  to: 7.899356842041016\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 7.899356842041016  to: 7.841973114013672\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 7.841973114013672  to: 7.781044769287109\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 7.781044769287109  to: 7.717478942871094\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 7.717478942871094  to: 7.652302551269531\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 7.652302551269531  to: 7.58660888671875\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 7.58660888671875  to: 7.521482849121094\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 7.521482849121094  to: 7.457820892333984\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 7.457820892333984  to: 7.396479797363281\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 7.396479797363281  to: 7.33851318359375\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 7.33851318359375  to: 7.284449768066406\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 7.284449768066406  to: 7.234779357910156\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 7.234779357910156  to: 7.189812469482422\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 7.189812469482422  to: 7.149369812011718\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 7.149369812011718  to: 7.113446044921875\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 7.113446044921875  to: 7.0818115234375\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 7.0818115234375  to: 7.0542442321777346\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 7.0542442321777346  to: 7.030545806884765\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 7.030545806884765  to: 7.010305786132813\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 7.010305786132813  to: 6.992964172363282\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 6.992964172363282  to: 6.977888488769532\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 6.977888488769532  to: 6.964567565917969\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 6.964567565917969  to: 6.952549743652344\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 6.952549743652344  to: 6.9413291931152346\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 6.9413291931152346  to: 6.930389404296875\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 6.930389404296875  to: 6.919267272949218\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 6.919267272949218  to: 6.90765609741211\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 6.90765609741211  to: 6.8951362609863285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 92\n",
      "Improved validation loss from: 6.8951362609863285  to: 6.881407165527344\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 6.881407165527344  to: 6.866193389892578\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 6.866193389892578  to: 6.849308013916016\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 6.849308013916016  to: 6.830638885498047\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 6.830638885498047  to: 6.810107421875\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 6.810107421875  to: 6.787571716308594\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 6.787571716308594  to: 6.762978363037109\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 6.762978363037109  to: 6.736314392089843\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 6.736314392089843  to: 6.707557678222656\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 6.707557678222656  to: 6.6767433166503904\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 6.6767433166503904  to: 6.6439872741699215\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 6.6439872741699215  to: 6.609413146972656\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 6.609413146972656  to: 6.573091888427735\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 6.573091888427735  to: 6.535030364990234\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 6.535030364990234  to: 6.495583343505859\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 6.495583343505859  to: 6.454718017578125\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 6.454718017578125  to: 6.4124900817871096\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 6.4124900817871096  to: 6.3693492889404295\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 6.3693492889404295  to: 6.325600433349609\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 6.325600433349609  to: 6.280954742431641\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 6.280954742431641  to: 6.23553581237793\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 6.23553581237793  to: 6.189711380004883\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 6.189711380004883  to: 6.1434776306152346\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 6.1434776306152346  to: 6.097210693359375\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 6.097210693359375  to: 6.050865554809571\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 6.050865554809571  to: 6.004631423950196\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 6.004631423950196  to: 5.958915328979492\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 5.958915328979492  to: 5.913686370849609\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 5.913686370849609  to: 5.869075775146484\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 5.869075775146484  to: 5.825341796875\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 5.825341796875  to: 5.782645416259766\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 5.782645416259766  to: 5.74084701538086\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 5.74084701538086  to: 5.700020599365234\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 5.700020599365234  to: 5.659952926635742\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 5.659952926635742  to: 5.620501708984375\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 5.620501708984375  to: 5.581641387939453\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 5.581641387939453  to: 5.543353271484375\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 5.543353271484375  to: 5.505524826049805\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 5.505524826049805  to: 5.468040084838867\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 5.468040084838867  to: 5.430806732177734\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 5.430806732177734  to: 5.393700027465821\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 5.393700027465821  to: 5.356784820556641\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 5.356784820556641  to: 5.319957733154297\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 5.319957733154297  to: 5.2829639434814455\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 5.2829639434814455  to: 5.2457435607910154\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 5.2457435607910154  to: 5.2081554412841795\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 5.2081554412841795  to: 5.170112228393554\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 5.170112228393554  to: 5.131915283203125\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 5.131915283203125  to: 5.093465805053711\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 5.093465805053711  to: 5.054665374755859\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 5.054665374755859  to: 5.015444564819336\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 5.015444564819336  to: 4.97572135925293\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 4.97572135925293  to: 4.935526275634766\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 4.935526275634766  to: 4.894771575927734\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 4.894771575927734  to: 4.8536529541015625\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 4.8536529541015625  to: 4.812097549438477\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 4.812097549438477  to: 4.77020263671875\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 4.77020263671875  to: 4.728268051147461\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 4.728268051147461  to: 4.686266326904297\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 4.686266326904297  to: 4.6443336486816404\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 4.6443336486816404  to: 4.602383422851562\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 4.602383422851562  to: 4.56060905456543\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 4.56060905456543  to: 4.5187938690185545\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 4.5187938690185545  to: 4.477200698852539\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 4.477200698852539  to: 4.4362022399902346\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 4.4362022399902346  to: 4.395318222045899\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 4.395318222045899  to: 4.354818725585938\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 4.354818725585938  to: 4.314504241943359\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 4.314504241943359  to: 4.274481964111328\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 4.274481964111328  to: 4.234763336181641\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 4.234763336181641  to: 4.195818328857422\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 4.195818328857422  to: 4.1578418731689455\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 4.1578418731689455  to: 4.120536041259766\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 4.120536041259766  to: 4.0838478088378904\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 4.0838478088378904  to: 4.047874069213867\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 4.047874069213867  to: 4.012786865234375\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 4.012786865234375  to: 3.9782970428466795\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 3.9782970428466795  to: 3.9447044372558593\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 3.9447044372558593  to: 3.9118194580078125\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 3.9118194580078125  to: 3.8793689727783205\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 3.8793689727783205  to: 3.847218704223633\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 3.847218704223633  to: 3.815660095214844\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 3.815660095214844  to: 3.7845916748046875\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 3.7845916748046875  to: 3.7537200927734373\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 3.7537200927734373  to: 3.7229030609130858\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 3.7229030609130858  to: 3.6920440673828123\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 3.6920440673828123  to: 3.661029815673828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 179\n",
      "Improved validation loss from: 3.661029815673828  to: 3.629930877685547\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 3.629930877685547  to: 3.5986522674560546\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 3.5986522674560546  to: 3.567148971557617\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 3.567148971557617  to: 3.5354957580566406\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 3.5354957580566406  to: 3.5038532257080077\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 3.5038532257080077  to: 3.472105026245117\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 3.472105026245117  to: 3.440154266357422\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 3.440154266357422  to: 3.4080951690673826\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 3.4080951690673826  to: 3.375732421875\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 3.375732421875  to: 3.3430126190185545\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 3.3430126190185545  to: 3.310131072998047\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 3.310131072998047  to: 3.277151107788086\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 3.277151107788086  to: 3.244150924682617\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 3.244150924682617  to: 3.2115345001220703\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 3.2115345001220703  to: 3.1792383193969727\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 3.1792383193969727  to: 3.147265625\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 3.147265625  to: 3.1154659271240233\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 3.1154659271240233  to: 3.0843719482421874\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 3.0843719482421874  to: 3.0537860870361326\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 3.0537860870361326  to: 3.023931884765625\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 3.023931884765625  to: 2.9946111679077148\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 2.9946111679077148  to: 2.9658117294311523\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 2.9658117294311523  to: 2.9373769760131836\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 2.9373769760131836  to: 2.909206771850586\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 2.909206771850586  to: 2.88114013671875\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 2.88114013671875  to: 2.8535745620727537\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 2.8535745620727537  to: 2.826481246948242\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 2.826481246948242  to: 2.7995737075805662\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 2.7995737075805662  to: 2.7727298736572266\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 2.7727298736572266  to: 2.7460199356079102\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 2.7460199356079102  to: 2.7194019317626954\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 2.7194019317626954  to: 2.6927648544311524\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 2.6927648544311524  to: 2.6660888671875\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 2.6660888671875  to: 2.6393829345703126\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 2.6393829345703126  to: 2.612485885620117\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 2.612485885620117  to: 2.5855276107788088\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 2.5855276107788088  to: 2.558573913574219\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 2.558573913574219  to: 2.5316822052001955\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 2.5316822052001955  to: 2.5050422668457033\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 2.5050422668457033  to: 2.4788324356079103\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 2.4788324356079103  to: 2.4529590606689453\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 2.4529590606689453  to: 2.4273839950561524\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 2.4273839950561524  to: 2.4021692276000977\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 2.4021692276000977  to: 2.377613830566406\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 2.377613830566406  to: 2.353532409667969\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 2.353532409667969  to: 2.3299217224121094\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 2.3299217224121094  to: 2.306601142883301\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 2.306601142883301  to: 2.2837013244628905\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 2.2837013244628905  to: 2.2613317489624025\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 2.2613317489624025  to: 2.2396608352661134\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 2.2396608352661134  to: 2.218609619140625\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 2.218609619140625  to: 2.1980569839477537\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 2.1980569839477537  to: 2.177993965148926\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 2.177993965148926  to: 2.1584415435791016\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 2.1584415435791016  to: 2.1391666412353514\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 2.1391666412353514  to: 2.1200305938720705\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 2.1200305938720705  to: 2.101039505004883\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 2.101039505004883  to: 2.0820865631103516\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 2.0820865631103516  to: 2.063014793395996\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 2.063014793395996  to: 2.0437721252441405\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 2.0437721252441405  to: 2.0245853424072267\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 2.0245853424072267  to: 2.0054969787597656\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 2.0054969787597656  to: 1.9866992950439453\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 1.9866992950439453  to: 1.9681304931640624\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 1.9681304931640624  to: 1.949826431274414\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 1.949826431274414  to: 1.9316381454467773\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 1.9316381454467773  to: 1.91348876953125\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 1.91348876953125  to: 1.8953983306884765\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 1.8953983306884765  to: 1.8775829315185546\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 1.8775829315185546  to: 1.8599597930908203\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 1.8599597930908203  to: 1.8424259185791017\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 1.8424259185791017  to: 1.8249195098876954\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 1.8249195098876954  to: 1.8073802947998048\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 1.8073802947998048  to: 1.7898782730102538\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 1.7898782730102538  to: 1.7724531173706055\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 1.7724531173706055  to: 1.7550647735595704\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 1.7550647735595704  to: 1.737600326538086\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 1.737600326538086  to: 1.7200950622558593\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 1.7200950622558593  to: 1.7025321960449218\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 1.7025321960449218  to: 1.6848831176757812\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 1.6848831176757812  to: 1.6672237396240235\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 1.6672237396240235  to: 1.6496479034423828\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 1.6496479034423828  to: 1.6322261810302734\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 1.6322261810302734  to: 1.61480770111084\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 1.61480770111084  to: 1.5974795341491699\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 1.5974795341491699  to: 1.580309772491455\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 1.580309772491455  to: 1.5633610725402831\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 1.5633610725402831  to: 1.5466852188110352\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 1.5466852188110352  to: 1.53030366897583\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 1.53030366897583  to: 1.5142376899719239\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 1.5142376899719239  to: 1.4985069274902343\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 1.4985069274902343  to: 1.4831069946289062\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 1.4831069946289062  to: 1.468023681640625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 272\n",
      "Improved validation loss from: 1.468023681640625  to: 1.4532646179199218\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 1.4532646179199218  to: 1.438787841796875\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 1.438787841796875  to: 1.4245447158813476\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 1.4245447158813476  to: 1.4104118347167969\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 1.4104118347167969  to: 1.3963048934936524\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 1.3963048934936524  to: 1.3821996688842773\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 1.3821996688842773  to: 1.368115234375\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 1.368115234375  to: 1.3539871215820312\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 1.3539871215820312  to: 1.339803123474121\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 1.339803123474121  to: 1.3256448745727538\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 1.3256448745727538  to: 1.3115942001342773\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 1.3115942001342773  to: 1.2977041244506835\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 1.2977041244506835  to: 1.284006690979004\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 1.284006690979004  to: 1.270550537109375\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 1.270550537109375  to: 1.2573721885681153\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 1.2573721885681153  to: 1.2445106506347656\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 1.2445106506347656  to: 1.2319887161254883\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 1.2319887161254883  to: 1.2197882652282714\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 1.2197882652282714  to: 1.2078412055969239\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 1.2078412055969239  to: 1.196131706237793\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 1.196131706237793  to: 1.184561061859131\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 1.184561061859131  to: 1.173124122619629\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 1.173124122619629  to: 1.1618332862854004\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 1.1618332862854004  to: 1.1506929397583008\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 1.1506929397583008  to: 1.1396779060363769\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 1.1396779060363769  to: 1.1287789344787598\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 1.1287789344787598  to: 1.1180018424987792\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 1.1180018424987792  to: 1.1073511123657227\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 1.1073511123657227  to: 1.0968236923217773\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 1.0968236923217773  to: 1.0863969802856446\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 1.0863969802856446  to: 1.0760727882385255\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 1.0760727882385255  to: 1.0658308029174806\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 1.0658308029174806  to: 1.0556777954101562\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 1.0556777954101562  to: 1.045626449584961\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 1.045626449584961  to: 1.0356902122497558\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 1.0356902122497558  to: 1.0258654594421386\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 1.0258654594421386  to: 1.016160774230957\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 1.016160774230957  to: 1.0065805435180664\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 1.0065805435180664  to: 0.9971287727355957\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.9971287727355957  to: 0.9878061294555665\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.9878061294555665  to: 0.9786077499389648\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.9786077499389648  to: 0.9695340156555176\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.9695340156555176  to: 0.9605798721313477\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.9605798721313477  to: 0.9517427444458008\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.9517427444458008  to: 0.9430160522460938\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.9430160522460938  to: 0.9343953132629395\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.9343953132629395  to: 0.9258767127990722\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.9258767127990722  to: 0.9174659729003907\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.9174659729003907  to: 0.909138298034668\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.909138298034668  to: 0.9008916854858399\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.9008916854858399  to: 0.8927289009094238\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.8927289009094238  to: 0.8846478462219238\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.8846478462219238  to: 0.8766481399536132\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.8766481399536132  to: 0.8687332153320313\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.8687332153320313  to: 0.8609130859375\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.8609130859375  to: 0.8531828880310058\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.8531828880310058  to: 0.8455411911010742\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.8455411911010742  to: 0.8379903793334961\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.8379903793334961  to: 0.8305338859558106\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.8305338859558106  to: 0.8231727600097656\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.8231727600097656  to: 0.815910530090332\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.815910530090332  to: 0.8087475776672364\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.8087475776672364  to: 0.8016829490661621\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.8016829490661621  to: 0.7947160243988037\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.7947160243988037  to: 0.787836217880249\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.787836217880249  to: 0.7810441493988037\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.7810441493988037  to: 0.7743404388427735\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.7743404388427735  to: 0.7677225589752197\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.7677225589752197  to: 0.7611873626708985\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.7611873626708985  to: 0.7547326564788819\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.7547326564788819  to: 0.7483566284179688\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.7483566284179688  to: 0.7420563697814941\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.7420563697814941  to: 0.73583083152771\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.73583083152771  to: 0.7296801567077636\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.7296801567077636  to: 0.7236050605773926\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.7236050605773926  to: 0.7176068782806396\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.7176068782806396  to: 0.7116846561431884\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.7116846561431884  to: 0.7058396816253663\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.7058396816253663  to: 0.7000468254089356\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.7000468254089356  to: 0.6943174839019776\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.6943174839019776  to: 0.6886630058288574\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.6886630058288574  to: 0.6830770969390869\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.6830770969390869  to: 0.6775561332702636\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.6775561332702636  to: 0.6721087455749511\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.6721087455749511  to: 0.6667346954345703\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.6667346954345703  to: 0.6614273071289063\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.6614273071289063  to: 0.6561903476715087\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.6561903476715087  to: 0.6510189056396485\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.6510189056396485  to: 0.6459233283996582\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.6459233283996582  to: 0.6409055233001709\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.6409055233001709  to: 0.6359682559967041\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.6359682559967041  to: 0.6311126708984375\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.6311126708984375  to: 0.626341438293457\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.626341438293457  to: 0.6216535568237305\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.6216535568237305  to: 0.617048978805542\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.617048978805542  to: 0.6125203132629394\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.6125203132629394  to: 0.6080615043640136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 369\n",
      "Improved validation loss from: 0.6080615043640136  to: 0.603673267364502\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.603673267364502  to: 0.5993503093719482\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.5993503093719482  to: 0.5950790405273437\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.5950790405273437  to: 0.5908362865447998\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.5908362865447998  to: 0.586641502380371\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.586641502380371  to: 0.5824892044067382\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.5824892044067382  to: 0.5783745288848877\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.5783745288848877  to: 0.5742960453033448\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.5742960453033448  to: 0.5702540397644043\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.5702540397644043  to: 0.5662501335144043\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.5662501335144043  to: 0.5622655868530273\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.5622655868530273  to: 0.5583189487457275\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.5583189487457275  to: 0.5544180870056152\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.5544180870056152  to: 0.5505684852600098\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.5505684852600098  to: 0.5467531681060791\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.5467531681060791  to: 0.542945384979248\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.542945384979248  to: 0.5391931056976318\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.5391931056976318  to: 0.5354951381683349\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.5354951381683349  to: 0.5318498134613037\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.5318498134613037  to: 0.5282549381256103\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.5282549381256103  to: 0.5247068881988526\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.5247068881988526  to: 0.5212015628814697\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.5212015628814697  to: 0.5177351951599121\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.5177351951599121  to: 0.5143084526062012\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.5143084526062012  to: 0.510922622680664\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.510922622680664  to: 0.5075778961181641\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.5075778961181641  to: 0.5042746543884278\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.5042746543884278  to: 0.5010148525238037\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.5010148525238037  to: 0.49779634475708007\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.49779634475708007  to: 0.49461946487426756\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.49461946487426756  to: 0.4914838790893555\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.4914838790893555  to: 0.48838791847229\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.48838791847229  to: 0.4853363990783691\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.4853363990783691  to: 0.4823297500610352\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.4823297500610352  to: 0.47937517166137694\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.47937517166137694  to: 0.4764579772949219\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.4764579772949219  to: 0.4735833168029785\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.4735833168029785  to: 0.4707134246826172\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.4707134246826172  to: 0.4678486824035645\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.4678486824035645  to: 0.4649918556213379\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.4649918556213379  to: 0.46214890480041504\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.46214890480041504  to: 0.45932540893554685\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.45932540893554685  to: 0.4565280914306641\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.4565280914306641  to: 0.45376291275024416\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.45376291275024416  to: 0.4510329246520996\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.4510329246520996  to: 0.44834046363830565\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.44834046363830565  to: 0.44570512771606446\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.44570512771606446  to: 0.44310483932495115\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.44310483932495115  to: 0.4404452800750732\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.4404452800750732  to: 0.43773784637451174\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.43773784637451174  to: 0.43500146865844724\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.43500146865844724  to: 0.43225831985473634\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.43225831985473634  to: 0.4295375347137451\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.4295375347137451  to: 0.4268647193908691\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.4268647193908691  to: 0.4242591381072998\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.4242591381072998  to: 0.42173161506652834\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.42173161506652834  to: 0.41928391456604003\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.41928391456604003  to: 0.41691021919250487\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.41691021919250487  to: 0.4145966529846191\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.4145966529846191  to: 0.41232614517211913\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.41232614517211913  to: 0.4100788116455078\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.4100788116455078  to: 0.4078362464904785\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.4078362464904785  to: 0.405590295791626\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.405590295791626  to: 0.4033339500427246\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.4033339500427246  to: 0.40106563568115233\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.40106563568115233  to: 0.3987880229949951\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.3987880229949951  to: 0.39650764465332033\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.39650764465332033  to: 0.3942334413528442\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.3942334413528442  to: 0.39197425842285155\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.39197425842285155  to: 0.38973891735076904\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.38973891735076904  to: 0.38753418922424315\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.38753418922424315  to: 0.38536293506622316\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.38536293506622316  to: 0.383225679397583\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.383225679397583  to: 0.38111960887908936\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.38111960887908936  to: 0.37903685569763185\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.37903685569763185  to: 0.37697253227233884\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.37697253227233884  to: 0.37491564750671386\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.37491564750671386  to: 0.3728585720062256\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.3728585720062256  to: 0.37079405784606934\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.37079405784606934  to: 0.3687194108963013\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.3687194108963013  to: 0.3666365146636963\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.3666365146636963  to: 0.36454880237579346\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.36454880237579346  to: 0.36246237754821775\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.36246237754821775  to: 0.36038448810577395\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.36038448810577395  to: 0.3583416700363159\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.3583416700363159  to: 0.35633895397186277\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.35633895397186277  to: 0.3543616771697998\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.3543616771697998  to: 0.3523963212966919\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.3523963212966919  to: 0.35043480396270754\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.35043480396270754  to: 0.3484940052032471\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.3484940052032471  to: 0.34653534889221194\n",
      "Training iteration: 460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.34653534889221194  to: 0.34456119537353513\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.34456119537353513  to: 0.3425777435302734\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.3425777435302734  to: 0.3405925273895264\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.3405925273895264  to: 0.3386151552200317\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.3386151552200317  to: 0.3366536617279053\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.3366536617279053  to: 0.3347157955169678\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.3347157955169678  to: 0.33280556201934813\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.33280556201934813  to: 0.3309242963790894\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.3309242963790894  to: 0.32907092571258545\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.32907092571258545  to: 0.3272410869598389\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.3272410869598389  to: 0.32543106079101564\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.32543106079101564  to: 0.3236340761184692\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.3236340761184692  to: 0.3218449831008911\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.3218449831008911  to: 0.32006020545959474\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.32006020545959474  to: 0.3182776212692261\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.3182776212692261  to: 0.3164961814880371\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.3164961814880371  to: 0.3147188663482666\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.3147188663482666  to: 0.3129758596420288\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.3129758596420288  to: 0.31126110553741454\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.31126110553741454  to: 0.30956597328186036\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.30956597328186036  to: 0.3078831434249878\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.3078831434249878  to: 0.30620546340942384\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.30620546340942384  to: 0.30454449653625487\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.30454449653625487  to: 0.30289974212646487\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.30289974212646487  to: 0.3012532711029053\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.3012532711029053  to: 0.2996085166931152\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.2996085166931152  to: 0.2980319023132324\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.2980319023132324  to: 0.29650638103485105\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.29650638103485105  to: 0.2950109958648682\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.2950109958648682  to: 0.29352636337280275\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.29352636337280275  to: 0.2920996189117432\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.2920996189117432  to: 0.2906725645065308\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.2906725645065308  to: 0.28921847343444823\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.28921847343444823  to: 0.2877397060394287\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.2877397060394287  to: 0.28624472618103025\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.28624472618103025  to: 0.284745454788208\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.284745454788208  to: 0.2832547426223755\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.2832547426223755  to: 0.28178431987762453\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.28178431987762453  to: 0.2803430318832397\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.2803430318832397  to: 0.2789355516433716\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.2789355516433716  to: 0.2775622844696045\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.2775622844696045  to: 0.2762197494506836\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.2762197494506836  to: 0.2749009370803833\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.2749009370803833  to: 0.27359776496887206\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.27359776496887206  to: 0.27230167388916016\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.27230167388916016  to: 0.27100586891174316\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.27100586891174316  to: 0.26970481872558594\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.26970481872558594  to: 0.2683978319168091\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.2683978319168091  to: 0.267085337638855\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.267085337638855  to: 0.26577134132385255\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.26577134132385255  to: 0.2644554615020752\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.2644554615020752  to: 0.26314678192138674\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.26314678192138674  to: 0.2618515729904175\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.2618515729904175  to: 0.26057379245758056\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.26057379245758056  to: 0.25931496620178224\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.25931496620178224  to: 0.2580769777297974\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.2580769777297974  to: 0.25685279369354247\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.25685279369354247  to: 0.2556434631347656\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.2556434631347656  to: 0.2544456243515015\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.2544456243515015  to: 0.2532565832138062\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.2532565832138062  to: 0.25207228660583497\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.25207228660583497  to: 0.2508908033370972\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.2508908033370972  to: 0.24968297481536866\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.24968297481536866  to: 0.2483680486679077\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.2483680486679077  to: 0.24698123931884766\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.24698123931884766  to: 0.2455827236175537\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.2455827236175537  to: 0.24425365924835205\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.24425365924835205  to: 0.2430142879486084\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.2430142879486084  to: 0.24185309410095215\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.24185309410095215  to: 0.24077372550964354\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.24077372550964354  to: 0.23977348804473878\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.23977348804473878  to: 0.23883702754974365\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.23883702754974365  to: 0.23794102668762207\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.23794102668762207  to: 0.23705811500549318\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.23705811500549318  to: 0.23616306781768798\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.23616306781768798  to: 0.23523447513580323\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.23523447513580323  to: 0.23426082134246826\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.23426082134246826  to: 0.23323967456817626\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.23323967456817626  to: 0.2321774959564209\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.2321774959564209  to: 0.23111224174499512\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.23111224174499512  to: 0.23005733489990235\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.23005733489990235  to: 0.2290254592895508\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.2290254592895508  to: 0.22802433967590333\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.22802433967590333  to: 0.2270585536956787\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.2270585536956787  to: 0.2261258602142334\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.2261258602142334  to: 0.22517733573913573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 546\n",
      "Improved validation loss from: 0.22517733573913573  to: 0.22421631813049317\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.22421631813049317  to: 0.22324910163879394\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.22324910163879394  to: 0.22228045463562013\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.22228045463562013  to: 0.22131712436676027\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.22131712436676027  to: 0.22036540508270264\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.22036540508270264  to: 0.2194150447845459\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.2194150447845459  to: 0.21847693920135497\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.21847693920135497  to: 0.21755998134613036\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.21755998134613036  to: 0.21667213439941407\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.21667213439941407  to: 0.21581733226776123\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.21581733226776123  to: 0.2150115489959717\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.2150115489959717  to: 0.21428916454315186\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.21428916454315186  to: 0.21361565589904785\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.21361565589904785  to: 0.21294121742248534\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.21294121742248534  to: 0.21223869323730468\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.21223869323730468  to: 0.21146535873413086\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.21146535873413086  to: 0.21052968502044678\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.21052968502044678  to: 0.20943090915679932\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.20943090915679932  to: 0.20821616649627686\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.20821616649627686  to: 0.2069322109222412\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.2069322109222412  to: 0.20565497875213623\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.20565497875213623  to: 0.2044515609741211\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.2044515609741211  to: 0.20338330268859864\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.20338330268859864  to: 0.20250754356384276\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.20250754356384276  to: 0.2017897129058838\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.2017897129058838  to: 0.20114810466766359\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.20114810466766359  to: 0.20053584575653077\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.20053584575653077  to: 0.19990842342376708\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.19990842342376708  to: 0.19923183917999268\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.19923183917999268  to: 0.19848744869232177\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.19848744869232177  to: 0.19769003391265869\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.19769003391265869  to: 0.1968623399734497\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.1968623399734497  to: 0.1960178017616272\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1960178017616272  to: 0.19517263174057006\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.19517263174057006  to: 0.19436304569244384\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.19436304569244384  to: 0.19357941150665284\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.19357941150665284  to: 0.19282238483428954\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.19282238483428954  to: 0.19208805561065673\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.19208805561065673  to: 0.19133498668670654\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.19133498668670654  to: 0.19056789875030516\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.19056789875030516  to: 0.18979523181915284\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.18979523181915284  to: 0.18904340267181396\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.18904340267181396  to: 0.18832972049713134\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.18832972049713134  to: 0.18765290975570678\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.18765290975570678  to: 0.18701488971710206\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.18701488971710206  to: 0.18639930486679077\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.18639930486679077  to: 0.18581022024154664\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.18581022024154664  to: 0.18522669076919557\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.18522669076919557  to: 0.18466991186141968\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.18466991186141968  to: 0.18410907983779906\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.18410907983779906  to: 0.18349215984344483\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.18349215984344483  to: 0.18283605575561523\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.18283605575561523  to: 0.18215415477752686\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.18215415477752686  to: 0.18145344257354737\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.18145344257354737  to: 0.18075586557388307\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.18075586557388307  to: 0.1801201581954956\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.1801201581954956  to: 0.1795371651649475\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.1795371651649475  to: 0.17897260189056396\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.17897260189056396  to: 0.17842111587524415\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.17842111587524415  to: 0.17783126831054688\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.17783126831054688  to: 0.17720768451690674\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.17720768451690674  to: 0.17655998468399048\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.17655998468399048  to: 0.17589231729507446\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.17589231729507446  to: 0.17522472143173218\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.17522472143173218  to: 0.1746072769165039\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.1746072769165039  to: 0.17405452728271484\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.17405452728271484  to: 0.17355842590332032\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.17355842590332032  to: 0.17307974100112916\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.17307974100112916  to: 0.17259471416473388\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.17259471416473388  to: 0.17211921215057374\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.17211921215057374  to: 0.17167835235595702\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.17167835235595702  to: 0.1712567090988159\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.1712567090988159  to: 0.17080106735229492\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.17080106735229492  to: 0.1702783226966858\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.1702783226966858  to: 0.16971285343170167\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.16971285343170167  to: 0.1691222906112671\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.1691222906112671  to: 0.16855453252792357\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.16855453252792357  to: 0.1680206298828125\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.1680206298828125  to: 0.16751632690429688\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.16751632690429688  to: 0.16702661514282227\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.16702661514282227  to: 0.1665553331375122\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.1665553331375122  to: 0.16612249612808228\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.16612249612808228  to: 0.16572450399398803\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.16572450399398803  to: 0.16534429788589478\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.16534429788589478  to: 0.16497585773468018\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.16497585773468018  to: 0.16461656093597413\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.16461656093597413  to: 0.16425946950912476\n",
      "Training iteration: 633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.16425946950912476  to: 0.16389844417572022\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.16389844417572022  to: 0.16352869272232057\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.16352869272232057  to: 0.16314761638641356\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.16314761638641356  to: 0.16275520324707032\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.16275520324707032  to: 0.16234023571014405\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.16234023571014405  to: 0.16192443370819093\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.16192443370819093  to: 0.16151412725448608\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.16151412725448608  to: 0.1611146569252014\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.1611146569252014  to: 0.16072864532470704\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.16072864532470704  to: 0.1603575110435486\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1603575110435486  to: 0.16001040935516359\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.16001040935516359  to: 0.1596805453300476\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.1596805453300476  to: 0.15935771465301513\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.15935771465301513  to: 0.1590340852737427\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.1590340852737427  to: 0.15870249271392822\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.15870249271392822  to: 0.1583590626716614\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.1583590626716614  to: 0.1580035924911499\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.1580035924911499  to: 0.15763801336288452\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.15763801336288452  to: 0.15726664066314697\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.15726664066314697  to: 0.15689369440078735\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.15689369440078735  to: 0.15652353763580323\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.15652353763580323  to: 0.1561584711074829\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.1561584711074829  to: 0.1558002710342407\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.1558002710342407  to: 0.15544842481613158\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.15544842481613158  to: 0.15510133504867554\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.15510133504867554  to: 0.15475701093673705\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.15475701093673705  to: 0.1544133424758911\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.1544133424758911  to: 0.15406856536865235\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.15406856536865235  to: 0.15372179746627807\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.15372179746627807  to: 0.1533736228942871\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.1533736228942871  to: 0.15302486419677735\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.15302486419677735  to: 0.15267689228057862\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.15267689228057862  to: 0.15233175754547118\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.15233175754547118  to: 0.15198981761932373\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.15198981761932373  to: 0.15165178775787352\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.15165178775787352  to: 0.15131678581237792\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.15131678581237792  to: 0.15098408460617066\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.15098408460617066  to: 0.15065252780914307\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.15065252780914307  to: 0.1503206253051758\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.1503206253051758  to: 0.14998703002929686\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.14998703002929686  to: 0.14965187311172484\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.14965187311172484  to: 0.14931532144546508\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.14931532144546508  to: 0.14897820949554444\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.14897820949554444  to: 0.14864267110824586\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.14864267110824586  to: 0.14830942153930665\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.14830942153930665  to: 0.14798022508621217\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.14798022508621217  to: 0.14765577316284179\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.14765577316284179  to: 0.14733576774597168\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.14733576774597168  to: 0.14701825380325317\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.14701825380325317  to: 0.14670461416244507\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.14670461416244507  to: 0.14639313220977784\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.14639313220977784  to: 0.14608254432678222\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.14608254432678222  to: 0.14577150344848633\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.14577150344848633  to: 0.14545979499816894\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.14545979499816894  to: 0.14515069723129273\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.14515069723129273  to: 0.14484243392944335\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.14484243392944335  to: 0.14453523159027098\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.14453523159027098  to: 0.14423015117645263\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.14423015117645263  to: 0.14392473697662353\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.14392473697662353  to: 0.14362016916275025\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.14362016916275025  to: 0.14331657886505128\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.14331657886505128  to: 0.1430156111717224\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.1430156111717224  to: 0.14271763563156128\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.14271763563156128  to: 0.14242632389068605\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.14242632389068605  to: 0.14213428497314454\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.14213428497314454  to: 0.14184262752532958\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.14184262752532958  to: 0.14155294895172119\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.14155294895172119  to: 0.14126670360565186\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.14126670360565186  to: 0.14098392724990844\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.14098392724990844  to: 0.1407039999961853\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.1407039999961853  to: 0.14042650461196898\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.14042650461196898  to: 0.14015045166015624\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.14015045166015624  to: 0.1398739695549011\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1398739695549011  to: 0.13959693908691406\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.13959693908691406  to: 0.13931846618652344\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.13931846618652344  to: 0.13903909921646118\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.13903909921646118  to: 0.13875943422317505\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.13875943422317505  to: 0.13848072290420532\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.13848072290420532  to: 0.13820356130599976\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.13820356130599976  to: 0.13792957067489625\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.13792957067489625  to: 0.137661874294281\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.137661874294281  to: 0.1373995065689087\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.1373995065689087  to: 0.137138831615448\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.137138831615448  to: 0.13687989711761475\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.13687989711761475  to: 0.13662188053131102\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.13662188053131102  to: 0.13636394739151\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.13636394739151  to: 0.13610632419586183\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.13610632419586183  to: 0.1358485460281372\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.1358485460281372  to: 0.13559077978134154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 722\n",
      "Improved validation loss from: 0.13559077978134154  to: 0.13533366918563844\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.13533366918563844  to: 0.1350774049758911\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.1350774049758911  to: 0.13482320308685303\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.13482320308685303  to: 0.13457019329071046\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.13457019329071046  to: 0.13432009220123292\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.13432009220123292  to: 0.1340712785720825\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1340712785720825  to: 0.13382457494735717\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.13382457494735717  to: 0.13357924222946166\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.13357924222946166  to: 0.13333473205566407\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.13333473205566407  to: 0.13309123516082763\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.13309123516082763  to: 0.13284790515899658\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.13284790515899658  to: 0.13260498046875\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.13260498046875  to: 0.1323629379272461\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.1323629379272461  to: 0.13212184906005858\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.13212184906005858  to: 0.13188157081604004\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.13188157081604004  to: 0.13164294958114625\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.13164294958114625  to: 0.13140599727630614\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.13140599727630614  to: 0.13117049932479857\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.13117049932479857  to: 0.130936598777771\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.130936598777771  to: 0.13070714473724365\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.13070714473724365  to: 0.1304849863052368\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.1304849863052368  to: 0.13026392459869385\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.13026392459869385  to: 0.1300437331199646\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.1300437331199646  to: 0.12982404232025146\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12982404232025146  to: 0.1296051859855652\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.1296051859855652  to: 0.12938662767410278\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12938662767410278  to: 0.12916891574859618\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12916891574859618  to: 0.12895185947418214\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12895185947418214  to: 0.12873612642288207\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12873612642288207  to: 0.12852132320404053\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12852132320404053  to: 0.12830812931060792\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12830812931060792  to: 0.12809656858444213\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12809656858444213  to: 0.1278864026069641\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.1278864026069641  to: 0.12767757177352906\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12767757177352906  to: 0.1274700164794922\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.1274700164794922  to: 0.12726277112960815\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12726277112960815  to: 0.12705638408660888\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12705638408660888  to: 0.12685033082962036\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12685033082962036  to: 0.12664538621902466\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12664538621902466  to: 0.12644119262695314\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12644119262695314  to: 0.12623733282089233\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12623733282089233  to: 0.12603423595428467\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12603423595428467  to: 0.12583229541778565\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12583229541778565  to: 0.12563129663467407\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12563129663467407  to: 0.12543082237243652\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12543082237243652  to: 0.1252300500869751\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.1252300500869751  to: 0.1250295877456665\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.1250295877456665  to: 0.12482941150665283\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12482941150665283  to: 0.12463083267211914\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12463083267211914  to: 0.12443406581878662\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12443406581878662  to: 0.12423827648162841\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12423827648162841  to: 0.12404371500015259\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12404371500015259  to: 0.12384980916976929\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12384980916976929  to: 0.1236571192741394\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.1236571192741394  to: 0.12346484661102294\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12346484661102294  to: 0.1232714056968689\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.1232714056968689  to: 0.12307648658752442\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12307648658752442  to: 0.12287935018539428\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.12287935018539428  to: 0.122680401802063\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.122680401802063  to: 0.12248094081878662\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12248094081878662  to: 0.12228175401687622\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12228175401687622  to: 0.12208405733108521\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12208405733108521  to: 0.12188864946365356\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12188864946365356  to: 0.1216958999633789\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.1216958999633789  to: 0.12150623798370361\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12150623798370361  to: 0.12131890058517455\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12131890058517455  to: 0.12113310098648071\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12113310098648071  to: 0.12094814777374267\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12094814777374267  to: 0.12076332569122314\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12076332569122314  to: 0.12057807445526122\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12057807445526122  to: 0.12039358615875244\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12039358615875244  to: 0.12020962238311768\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12020962238311768  to: 0.12002546787261963\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12002546787261963  to: 0.11984227895736695\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.11984227895736695  to: 0.11966052055358886\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.11966052055358886  to: 0.1194798231124878\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.1194798231124878  to: 0.11930111646652222\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.11930111646652222  to: 0.11912391185760499\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.11912391185760499  to: 0.11894820928573609\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.11894820928573609  to: 0.11877361536026002\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.11877361536026002  to: 0.11860003471374511\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.11860003471374511  to: 0.11842702627182007\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.11842702627182007  to: 0.11825422048568726\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.11825422048568726  to: 0.11808216571807861\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.11808216571807861  to: 0.11791054010391236\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.11791054010391236  to: 0.11773951053619384\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.11773951053619384  to: 0.11756926774978638\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.11756926774978638  to: 0.11739976406097412\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.11739976406097412  to: 0.11723139286041259\n",
      "Training iteration: 811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11723139286041259  to: 0.11706374883651734\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.11706374883651734  to: 0.1168968677520752\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.1168968677520752  to: 0.11673080921173096\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.11673080921173096  to: 0.11656512022018432\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.11656512022018432  to: 0.11640013456344604\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.11640013456344604  to: 0.11623548269271851\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.11623548269271851  to: 0.11607129573822021\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.11607129573822021  to: 0.11590831279754639\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.11590831279754639  to: 0.1157457709312439\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.1157457709312439  to: 0.11557669639587402\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.11557669639587402  to: 0.11540303230285645\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.11540303230285645  to: 0.11522833108901978\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.11522833108901978  to: 0.11505558490753173\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.11505558490753173  to: 0.11488673686981202\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.11488673686981202  to: 0.11472258567810059\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.11472258567810059  to: 0.11456472873687744\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.11456472873687744  to: 0.11441056728363037\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.11441056728363037  to: 0.1142566442489624\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.1142566442489624  to: 0.11410034894943237\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.11410034894943237  to: 0.11393979787826539\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.11393979787826539  to: 0.1137747049331665\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.1137747049331665  to: 0.1136064887046814\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.1136064887046814  to: 0.1134371042251587\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1134371042251587  to: 0.11326935291290283\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.11326935291290283  to: 0.11310532093048095\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.11310532093048095  to: 0.11294765472412109\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.11294765472412109  to: 0.1127972960472107\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.1127972960472107  to: 0.1126503586769104\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.1126503586769104  to: 0.11250510215759277\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.11250510215759277  to: 0.1123605489730835\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1123605489730835  to: 0.11221472024917603\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.11221472024917603  to: 0.1120673656463623\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.1120673656463623  to: 0.11191898584365845\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.11191898584365845  to: 0.11176927089691162\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.11176927089691162  to: 0.11161992549896241\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.11161992549896241  to: 0.11144311428070068\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.11144311428070068  to: 0.11124634742736816\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.11124634742736816  to: 0.11103948354721069\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.11103948354721069  to: 0.11086206436157227\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11086206436157227  to: 0.11071567535400391\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11071567535400391  to: 0.11059606075286865\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11059606075286865  to: 0.11049520969390869\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.11049520969390869  to: 0.11040354967117309\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.11040354967117309  to: 0.11031103134155273\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.11031103134155273  to: 0.11020951271057129\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.11020951271057129  to: 0.11009385585784912\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11009385585784912  to: 0.10996264219284058\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.10996264219284058  to: 0.10981810092926025\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.10981810092926025  to: 0.10966382026672364\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.10966382026672364  to: 0.10950639247894287\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.10950639247894287  to: 0.10935167074203492\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.10935167074203492  to: 0.10920474529266358\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.10920474529266358  to: 0.10906903743743897\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.10906903743743897  to: 0.10892055034637452\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.10892055034637452  to: 0.1087644338607788\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1087644338607788  to: 0.1086312174797058\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.1086312174797058  to: 0.10852019786834717\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.10852019786834717  to: 0.1084253430366516\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.1084253430366516  to: 0.10834085941314697\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.10834085941314697  to: 0.10825817584991455\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.10825817584991455  to: 0.10814679861068725\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.10814679861068725  to: 0.10802990198135376\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.10802990198135376  to: 0.10790685415267945\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.10790685415267945  to: 0.10777812004089356\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.10777812004089356  to: 0.10764548778533936\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.10764548778533936  to: 0.10751143693923951\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.10751143693923951  to: 0.10737842321395874\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.10737842321395874  to: 0.10724928379058837\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.10724928379058837  to: 0.10710422992706299\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.10710422992706299  to: 0.10697081089019775\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.10697081089019775  to: 0.10684990882873535\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.10684990882873535  to: 0.10674109458923339\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.10674109458923339  to: 0.10664150714874268\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.10664150714874268  to: 0.10652673244476318\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.10652673244476318  to: 0.10641686916351319\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.10641686916351319  to: 0.10630943775177001\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.10630943775177001  to: 0.1062017798423767\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.1062017798423767  to: 0.10609250068664551\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.10609250068664551  to: 0.1059615969657898\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.1059615969657898  to: 0.10583337545394897\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.10583337545394897  to: 0.10570967197418213\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.10570967197418213  to: 0.105591082572937\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.105591082572937  to: 0.10547769069671631\n",
      "Training iteration: 894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10547769069671631  to: 0.10536856651306152\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.10536856651306152  to: 0.10526224374771118\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.10526224374771118  to: 0.10513995885848999\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.10513995885848999  to: 0.1050046443939209\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.1050046443939209  to: 0.10486414432525634\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.10486414432525634  to: 0.10472332239151001\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.10472332239151001  to: 0.10458686351776122\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.10458686351776122  to: 0.10443640947341919\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.10443640947341919  to: 0.10427827835083008\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.10427827835083008  to: 0.10413357019424438\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.10413357019424438  to: 0.10400358438491822\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.10400358438491822  to: 0.10387259721755981\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.10387259721755981  to: 0.10374071598052978\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.10374071598052978  to: 0.10362955331802368\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.10362955331802368  to: 0.10352197885513306\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.10352197885513306  to: 0.10341514348983764\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.10341514348983764  to: 0.10330671072006226\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.10330671072006226  to: 0.10319507122039795\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.10319507122039795  to: 0.10307661294937134\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.10307661294937134  to: 0.10295341014862061\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.10295341014862061  to: 0.10284148454666138\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.10284148454666138  to: 0.10274094343185425\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.10274094343185425  to: 0.10262691974639893\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.10262691974639893  to: 0.10250284671783447\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.10250284671783447  to: 0.1023753523826599\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.1023753523826599  to: 0.10225008726119995\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.10225008726119995  to: 0.10213260650634766\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.10213260650634766  to: 0.10202568769454956\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.10202568769454956  to: 0.1019314169883728\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.1019314169883728  to: 0.10184853076934815\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.10184853076934815  to: 0.10177505016326904\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.10177505016326904  to: 0.10170750617980957\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.10170750617980957  to: 0.10164225101470947\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.10164225101470947  to: 0.10157577991485596\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.10157577991485596  to: 0.10150583982467651\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.10150583982467651  to: 0.10143115520477294\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.10143115520477294  to: 0.10135186910629272\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.10135186910629272  to: 0.10126938819885253\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.10126938819885253  to: 0.10118614435195923\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.10118614435195923  to: 0.10110430717468262\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.10110430717468262  to: 0.10102645158767701\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.10102645158767701  to: 0.10095419883728027\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.10095419883728027  to: 0.10088809728622436\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.10088809728622436  to: 0.10082811117172241\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.10082811117172241  to: 0.10077310800552368\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.10077310800552368  to: 0.10072126388549804\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.10072126388549804  to: 0.1006706714630127\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.1006706714630127  to: 0.10061910152435302\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.10061910152435302  to: 0.10056490898132324\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.10056490898132324  to: 0.10050705671310425\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.10050705671310425  to: 0.10044533014297485\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.10044533014297485  to: 0.1003800868988037\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.1003800868988037  to: 0.10031008720397949\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.10031008720397949  to: 0.10023750066757202\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.10023750066757202  to: 0.10016412734985351\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.10016412734985351  to: 0.10009251832962036\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.10009251832962036  to: 0.10002362728118896\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.10002362728118896  to: 0.09996034502983094\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.09996034502983094  to: 0.09990105628967286\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.09990105628967286  to: 0.09984377026557922\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.09984377026557922  to: 0.09978534579277039\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.09978534579277039  to: 0.09972472190856933\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.09972472190856933  to: 0.09966022372245789\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.09966022372245789  to: 0.09959200024604797\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.09959200024604797  to: 0.09952114224433899\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.09952114224433899  to: 0.09944874048233032\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.09944874048233032  to: 0.09937656521797181\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.09937656521797181  to: 0.09930534362792968\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.09930534362792968  to: 0.09923589825630189\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.09923589825630189  to: 0.09916772842407226\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.09916772842407226  to: 0.0991008460521698\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.0991008460521698  to: 0.09903456568717957\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.09903456568717957  to: 0.09896806478500367\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.09896806478500367  to: 0.09890130758285523\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.09890130758285523  to: 0.09883464574813842\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.09883464574813842  to: 0.09876804351806641\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.09876804351806641  to: 0.09870159029960632\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.09870159029960632  to: 0.09863580465316772\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.09863580465316772  to: 0.0985706627368927\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.0985706627368927  to: 0.09850550889968872\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.09850550889968872  to: 0.09844066500663758\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.09844066500663758  to: 0.09837543368339538\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.09837543368339538  to: 0.09830988645553589\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.09830988645553589  to: 0.09824395179748535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 978\n",
      "Improved validation loss from: 0.09824395179748535  to: 0.09817872047424317\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.09817872047424317  to: 0.0981136679649353\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.0981136679649353  to: 0.09804962277412414\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.09804962277412414  to: 0.09798589944839478\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.09798589944839478  to: 0.09792340993881225\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.09792340993881225  to: 0.0978616714477539\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.0978616714477539  to: 0.09780018925666809\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.09780018925666809  to: 0.09773882031440735\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.09773882031440735  to: 0.09767695665359497\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.09767695665359497  to: 0.09761451482772827\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.09761451482772827  to: 0.09755090475082398\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.09755090475082398  to: 0.09748691320419312\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.09748691320419312  to: 0.09742246866226197\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.09742246866226197  to: 0.09735807180404663\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.09735807180404663  to: 0.09729364514350891\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.09729364514350891  to: 0.09722997546195984\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.09722997546195984  to: 0.0971670150756836\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.0971670150756836  to: 0.09710419774055482\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.09710419774055482  to: 0.09704159498214722\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.09704159498214722  to: 0.09697934985160828\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.09697934985160828  to: 0.09691631197929382\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.09691631197929382  to: 0.0968529999256134\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.0968529999256134  to: 0.0967890739440918\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.0967890739440918  to: 0.09672515988349914\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.09672515988349914  to: 0.09666099548339843\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.09666099548339843  to: 0.09659751057624817\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.09659751057624817  to: 0.09653422236442566\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.09653422236442566  to: 0.09647130966186523\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.09647130966186523  to: 0.0964089035987854\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.0964089035987854  to: 0.09634685516357422\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.09634685516357422  to: 0.09628452062606811\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.09628452062606811  to: 0.09622215032577515\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.09622215032577515  to: 0.09617018699645996\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.09617018699645996  to: 0.09612088203430176\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.09612088203430176  to: 0.09607105255126953\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.09607105255126953  to: 0.09602108001708984\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.09602108001708984  to: 0.09597098231315612\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.09597098231315612  to: 0.09592102766036988\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.09592102766036988  to: 0.09587098360061645\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.09587098360061645  to: 0.09582082033157349\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.09582082033157349  to: 0.09577099084854127\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.09577099084854127  to: 0.09572077989578247\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.09572077989578247  to: 0.09566844701766967\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.09566844701766967  to: 0.09561095237731934\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.09561095237731934  to: 0.09554665684700012\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.09554665684700012  to: 0.09547475576400757\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.09547475576400757  to: 0.09539691209793091\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.09539691209793091  to: 0.09531480073928833\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.09531480073928833  to: 0.09523242115974426\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.09523242115974426  to: 0.09515458941459656\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.09515458941459656  to: 0.09508485794067383\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.09508485794067383  to: 0.09502439498901367\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.09502439498901367  to: 0.0949724018573761\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.0949724018573761  to: 0.09492555856704712\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.09492555856704712  to: 0.0948803722858429\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.0948803722858429  to: 0.09483348727226257\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.09483348727226257  to: 0.09478244781494141\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.09478244781494141  to: 0.09472696185111999\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.09472696185111999  to: 0.09466725587844849\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.09466725587844849  to: 0.09460304379463196\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.09460304379463196  to: 0.09453550577163697\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.09453550577163697  to: 0.09446523785591125\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.09446523785591125  to: 0.09439516067504883\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.09439516067504883  to: 0.0943278968334198\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.0943278968334198  to: 0.09426580667495728\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.09426580667495728  to: 0.09421022534370423\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.09421022534370423  to: 0.09416050910949707\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.09416050910949707  to: 0.09411260485649109\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.09411260485649109  to: 0.09406193494796752\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.09406193494796752  to: 0.0940050721168518\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.0940050721168518  to: 0.09394212961196899\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.09394212961196899  to: 0.09387395977973938\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.09387395977973938  to: 0.09380190968513488\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.09380190968513488  to: 0.09372979998588563\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.09372979998588563  to: 0.09366201162338257\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.09366201162338257  to: 0.09360079765319824\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.09360079765319824  to: 0.09354554414749146\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.09354554414749146  to: 0.09349300265312195\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.09349300265312195  to: 0.09344009160995484\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.09344009160995484  to: 0.0933847427368164\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.0933847427368164  to: 0.09332647323608398\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.09332647323608398  to: 0.09326512217521668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.09326512217521668  to: 0.09320019483566284\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.09320019483566284  to: 0.09313252568244934\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.09313252568244934  to: 0.0930639386177063\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.0930639386177063  to: 0.09299729466438293\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.09299729466438293  to: 0.0929338812828064\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.0929338812828064  to: 0.09287425875663757\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.09287425875663757  to: 0.09281724095344543\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.09281724095344543  to: 0.09275988340377808\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.09275988340377808  to: 0.09270095825195312\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.09270095825195312  to: 0.0926403522491455\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.0926403522491455  to: 0.09257926940917968\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.09257926940917968  to: 0.09251890182495118\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.09251890182495118  to: 0.09245802760124207\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.09245802760124207  to: 0.09239649772644043\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.09239649772644043  to: 0.09233301281929016\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.09233301281929016  to: 0.09226808547973633\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.09226808547973633  to: 0.09220388531684875\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.09220388531684875  to: 0.09214285016059875\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.09214285016059875  to: 0.09208492040634156\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.09208492040634156  to: 0.0920285701751709\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.0920285701751709  to: 0.09197184443473816\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.09197184443473816  to: 0.09191199541091918\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.09191199541091918  to: 0.09184901118278503\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.09184901118278503  to: 0.09178451299667359\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.09178451299667359  to: 0.09172146916389465\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.09172146916389465  to: 0.09166129231452942\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.09166129231452942  to: 0.0916037917137146\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.0916037917137146  to: 0.09154701232910156\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.09154701232910156  to: 0.09148914217948914\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.09148914217948914  to: 0.09142782092094422\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.09142782092094422  to: 0.09136335253715515\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.09136335253715515  to: 0.09129859209060669\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.09129859209060669  to: 0.0912347137928009\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.0912347137928009  to: 0.09117252230644227\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.09117252230644227  to: 0.09110922813415527\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.09110922813415527  to: 0.09104604721069336\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.09104604721069336  to: 0.09098394513130188\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.09098394513130188  to: 0.09092460870742798\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.09092460870742798  to: 0.0908679187297821\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.0908679187297821  to: 0.09081283807754517\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.09081283807754517  to: 0.09075702428817749\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.09075702428817749  to: 0.09070008993148804\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.09070008993148804  to: 0.09064247012138367\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.09064247012138367  to: 0.09058400392532348\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.09058400392532348  to: 0.09052497744560242\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.09052497744560242  to: 0.09046457409858703\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.09046457409858703  to: 0.09040402173995972\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.09040402173995972  to: 0.09034467935562134\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.09034467935562134  to: 0.09028633236885071\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.09028633236885071  to: 0.09022800326347351\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.09022800326347351  to: 0.09016810655593872\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.09016810655593872  to: 0.09010637402534485\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.09010637402534485  to: 0.09004324078559875\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.09004324078559875  to: 0.08997928500175476\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.08997928500175476  to: 0.0899140179157257\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.0899140179157257  to: 0.08984845876693726\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.08984845876693726  to: 0.08978255987167358\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.08978255987167358  to: 0.08971714973449707\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.08971714973449707  to: 0.08965336084365845\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.08965336084365845  to: 0.08959100842475891\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.08959100842475891  to: 0.08952856063842773\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.08952856063842773  to: 0.0894646942615509\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.0894646942615509  to: 0.08939892053604126\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.08939892053604126  to: 0.0893326759338379\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.0893326759338379  to: 0.08926755785942078\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.08926755785942078  to: 0.0892050564289093\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.0892050564289093  to: 0.08914520144462586\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.08914520144462586  to: 0.08908606767654419\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.08908606767654419  to: 0.0890256106853485\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.0890256106853485  to: 0.08896173238754272\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.08896173238754272  to: 0.08889397382736205\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.08889397382736205  to: 0.08882477879524231\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.08882477879524231  to: 0.08875705599784851\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.08875705599784851  to: 0.08869358301162719\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.08869358301162719  to: 0.08863547444343567\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.08863547444343567  to: 0.08858152627944946\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.08858152627944946  to: 0.08852869272232056\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.08852869272232056  to: 0.08847299814224244\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.08847299814224244  to: 0.08842297792434692\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.08842297792434692  to: 0.08836787939071655\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.08836787939071655  to: 0.08831003904342652\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.08831003904342652  to: 0.08825241327285767\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.08825241327285767  to: 0.08819884061813354\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.08819884061813354  to: 0.08815149068832398\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.08815149068832398  to: 0.0881100833415985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.0881100833415985  to: 0.08807135820388794\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.08807135820388794  to: 0.08802997469902038\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.08802997469902038  to: 0.08798006176948547\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.08798006176948547  to: 0.08791877627372742\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.08791877627372742  to: 0.08784724473953247\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.08784724473953247  to: 0.08777033686637878\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.08777033686637878  to: 0.08769402503967286\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.08769402503967286  to: 0.08762148022651672\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.08762148022651672  to: 0.0875544548034668\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.0875544548034668  to: 0.08749061822891235\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.08749061822891235  to: 0.08742772936820983\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.08742772936820983  to: 0.08736399412155152\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.08736399412155152  to: 0.08729819059371949\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.08729819059371949  to: 0.08722880482673645\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.08722880482673645  to: 0.08715711832046509\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.08715711832046509  to: 0.0870859146118164\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.0870859146118164  to: 0.08701926469802856\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.08701926469802856  to: 0.08695889711380005\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.08695889711380005  to: 0.08690590858459472\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.08690590858459472  to: 0.08685855865478516\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.08685855865478516  to: 0.0868141770362854\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.0868141770362854  to: 0.08677075505256653\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.08677075505256653  to: 0.08672629594802857\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.08672629594802857  to: 0.08667823076248168\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.08667823076248168  to: 0.08662417531013489\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.08662417531013489  to: 0.08656281232833862\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.08656281232833862  to: 0.08649578094482421\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.08649578094482421  to: 0.08642535209655762\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.08642535209655762  to: 0.08635576367378235\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.08635576367378235  to: 0.08629003763198853\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.08629003763198853  to: 0.08622983694076539\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.08622983694076539  to: 0.08617476224899293\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.08617476224899293  to: 0.08612245321273804\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.08612245321273804  to: 0.08607055544853211\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.08607055544853211  to: 0.08601630926132202\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.08601630926132202  to: 0.08595765233039857\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.08595765233039857  to: 0.08589503169059753\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.08589503169059753  to: 0.08582917451858521\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.08582917451858521  to: 0.08576107025146484\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.08576107025146484  to: 0.08569307327270508\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.08569307327270508  to: 0.08562755584716797\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.08562755584716797  to: 0.08556644320487976\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.08556644320487976  to: 0.08550858497619629\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.08550858497619629  to: 0.08545277714729309\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.08545277714729309  to: 0.08539716601371765\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.08539716601371765  to: 0.08534072637557984\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.08534072637557984  to: 0.0852813720703125\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.0852813720703125  to: 0.08521994352340698\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.08521994352340698  to: 0.0851571261882782\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.0851571261882782  to: 0.0850944995880127\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.0850944995880127  to: 0.08503267168998718\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.08503267168998718  to: 0.08497109413146972\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.08497109413146972  to: 0.08491023182868958\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.08491023182868958  to: 0.08485064506530762\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.08485064506530762  to: 0.08479166030883789\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.08479166030883789  to: 0.08473308682441712\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.08473308682441712  to: 0.08467574119567871\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.08467574119567871  to: 0.08461898565292358\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.08461898565292358  to: 0.08456152081489562\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.08456152081489562  to: 0.0845024585723877\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.0845024585723877  to: 0.08444195985794067\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.08444195985794067  to: 0.0843803882598877\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.0843803882598877  to: 0.08431801795959473\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.08431801795959473  to: 0.08425605893135071\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.08425605893135071  to: 0.08419722318649292\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.08419722318649292  to: 0.0841413676738739\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.0841413676738739  to: 0.08408788442611695\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.08408788442611695  to: 0.08403457403182983\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.08403457403182983  to: 0.08397880792617798\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.08397880792617798  to: 0.08391913175582885\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.08391913175582885  to: 0.083855801820755\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.083855801820755  to: 0.08379043340682983\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.08379043340682983  to: 0.08372529745101928\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.08372529745101928  to: 0.08366284370422364\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.08366284370422364  to: 0.08360296487808228\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.08360296487808228  to: 0.08354506492614747\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.08354506492614747  to: 0.0834882140159607\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.0834882140159607  to: 0.08343138694763183\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.08343138694763183  to: 0.08337360620498657\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.08337360620498657  to: 0.08331484794616699\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.08331484794616699  to: 0.0832547664642334\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.0832547664642334  to: 0.08319439888000488\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.08319439888000488  to: 0.0831346869468689\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.0831346869468689  to: 0.08307620882987976\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.08307620882987976  to: 0.08301897048950195\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.08301897048950195  to: 0.08296219110488892\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.08296219110488892  to: 0.08290536999702454\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.08290536999702454  to: 0.08284827470779418\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.08284827470779418  to: 0.08279150128364562\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.08279150128364562  to: 0.08273414373397828\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.08273414373397828  to: 0.08267685770988464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.08267685770988464  to: 0.08261910676956177\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.08261910676956177  to: 0.08256100416183472\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.08256100416183472  to: 0.08250241279602051\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.08250241279602051  to: 0.0824437141418457\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.0824437141418457  to: 0.08238492012023926\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.08238492012023926  to: 0.08232616186141968\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.08232616186141968  to: 0.08226763606071472\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.08226763606071472  to: 0.08220916986465454\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.08220916986465454  to: 0.08215106129646302\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.08215106129646302  to: 0.08209273219108582\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.08209273219108582  to: 0.08203460574150086\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.08203460574150086  to: 0.08197607994079589\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.08197607994079589  to: 0.08191778063774109\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.08191778063774109  to: 0.08185936212539673\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.08185936212539673  to: 0.08180130124092103\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.08180130124092103  to: 0.08174336552619935\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.08174336552619935  to: 0.08168567419052124\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.08168567419052124  to: 0.08162820935249329\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.08162820935249329  to: 0.08157057762145996\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.08157057762145996  to: 0.08151320219039918\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.08151320219039918  to: 0.08145594596862793\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.08145594596862793  to: 0.08139845132827758\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.08139845132827758  to: 0.08134132623672485\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.08134132623672485  to: 0.08128406405448914\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.08128406405448914  to: 0.08122695088386536\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.08122695088386536  to: 0.08116980791091918\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.08116980791091918  to: 0.0811126708984375\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.0811126708984375  to: 0.0810553252696991\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.0810553252696991  to: 0.08099840879440308\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.08099840879440308  to: 0.08094140887260437\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.08094140887260437  to: 0.08088434338569642\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.08088434338569642  to: 0.08082718849182129\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.08082718849182129  to: 0.08077008128166199\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.08077008128166199  to: 0.08071286082267762\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.08071286082267762  to: 0.08065556287765503\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.08065556287765503  to: 0.08059825897216796\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.08059825897216796  to: 0.08054121136665345\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.08054121136665345  to: 0.08048390150070191\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.08048390150070191  to: 0.08042667508125305\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.08042667508125305  to: 0.08036932945251465\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.08036932945251465  to: 0.08031207323074341\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.08031207323074341  to: 0.08025474548339843\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.08025474548339843  to: 0.08019728660583496\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.08019728660583496  to: 0.08014005422592163\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.08014005422592163  to: 0.08008267283439637\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.08008267283439637  to: 0.08002538681030273\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.08002538681030273  to: 0.07996799349784851\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.07996799349784851  to: 0.07991073727607727\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.07991073727607727  to: 0.07985354661941528\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.07985354661941528  to: 0.07979607582092285\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.07979607582092285  to: 0.07973688840866089\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.07973688840866089  to: 0.07967489957809448\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.07967489957809448  to: 0.07961017489433289\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.07961017489433289  to: 0.07954400777816772\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.07954400777816772  to: 0.0794796109199524\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.0794796109199524  to: 0.07941825985908509\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.07941825985908509  to: 0.07936011552810669\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.07936011552810669  to: 0.07930432558059693\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.07930432558059693  to: 0.07924960851669312\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.07924960851669312  to: 0.07919453382492066\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.07919453382492066  to: 0.07913859486579895\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.07913859486579895  to: 0.07908130288124085\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.07908130288124085  to: 0.07902368307113647\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.07902368307113647  to: 0.0789666473865509\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.0789666473865509  to: 0.07890976667404175\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.07890976667404175  to: 0.07885304093360901\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.07885304093360901  to: 0.07879574298858642\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.07879574298858642  to: 0.07873731851577759\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.07873731851577759  to: 0.07867716550827027\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.07867716550827027  to: 0.07861570119857789\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.07861570119857789  to: 0.07855451703071595\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.07855451703071595  to: 0.07849459052085876\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.07849459052085876  to: 0.07843729853630066\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.07843729853630066  to: 0.07838295698165894\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.07838295698165894  to: 0.07833002805709839\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.07833002805709839  to: 0.0782768726348877\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.0782768726348877  to: 0.0782209038734436\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.0782209038734436  to: 0.07816106081008911\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.07816106081008911  to: 0.07809800505638123\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.07809800505638123  to: 0.07803333401679993\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.07803333401679993  to: 0.07796935439109802\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.07796935439109802  to: 0.07790852785110473\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.07790852785110473  to: 0.07785059213638305\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.07785059213638305  to: 0.0777945101261139\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.0777945101261139  to: 0.07773934602737427\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.07773934602737427  to: 0.07768318057060242\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.07768318057060242  to: 0.07762560248374939\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.07762560248374939  to: 0.07756688594818115\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.07756688594818115  to: 0.07750729918479919\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.07750729918479919  to: 0.07744736075401307\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.07744736075401307  to: 0.07738704085350037\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.07738704085350037  to: 0.07732754945755005\n",
      "Training iteration: 1328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.07732754945755005  to: 0.07726920247077942\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.07726920247077942  to: 0.07721179723739624\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.07721179723739624  to: 0.07715517282485962\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.07715517282485962  to: 0.0770985722541809\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.0770985722541809  to: 0.0770413637161255\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.0770413637161255  to: 0.07698280215263367\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.07698280215263367  to: 0.07692312002182007\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.07692312002182007  to: 0.07686258554458618\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.07686258554458618  to: 0.07680121064186096\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.07680121064186096  to: 0.07674020528793335\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.07674020528793335  to: 0.07668064832687378\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.07668064832687378  to: 0.07662222981452942\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.07662222981452942  to: 0.076564359664917\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.076564359664917  to: 0.07650724053382874\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.07650724053382874  to: 0.07645017504692078\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.07645017504692078  to: 0.07639392018318177\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.07639392018318177  to: 0.07633762955665588\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.07633762955665588  to: 0.07628024220466614\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.07628024220466614  to: 0.07622105479240418\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.07622105479240418  to: 0.07616058588027955\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.07616058588027955  to: 0.07609909772872925\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.07609909772872925  to: 0.07603850960731506\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.07603850960731506  to: 0.0759804368019104\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.0759804368019104  to: 0.07592479586601257\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.07592479586601257  to: 0.07587007284164429\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.07587007284164429  to: 0.07581427693367004\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.07581427693367004  to: 0.07575656771659851\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.07575656771659851  to: 0.07569722533226013\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.07569722533226013  to: 0.07563799619674683\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.07563799619674683  to: 0.07557891011238098\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.07557891011238098  to: 0.07551957368850708\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.07551957368850708  to: 0.07546021938323974\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.07546021938323974  to: 0.07540169954299927\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.07540169954299927  to: 0.07534452080726624\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.07534452080726624  to: 0.07528803944587707\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.07528803944587707  to: 0.07523036599159241\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.07523036599159241  to: 0.07517120838165284\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.07517120838165284  to: 0.0751112937927246\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.0751112937927246  to: 0.0750521957874298\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.0750521957874298  to: 0.07499452233314514\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.07499452233314514  to: 0.0749371588230133\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.0749371588230133  to: 0.07487905621528626\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.07487905621528626  to: 0.07482128143310547\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.07482128143310547  to: 0.07476369738578796\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.07476369738578796  to: 0.07470560669898987\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.07470560669898987  to: 0.07464679479598998\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.07464679479598998  to: 0.07458750009536744\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.07458750009536744  to: 0.07452871799468994\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.07452871799468994  to: 0.07447107434272766\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.07447107434272766  to: 0.07441511154174804\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.07441511154174804  to: 0.07435937523841858\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.07435937523841858  to: 0.0743029236793518\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.0743029236793518  to: 0.07424452900886536\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.07424452900886536  to: 0.07418445348739625\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.07418445348739625  to: 0.07412360906600952\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.07412360906600952  to: 0.07406307458877563\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.07406307458877563  to: 0.07400503158569335\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.07400503158569335  to: 0.07394906878471375\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.07394906878471375  to: 0.07389408946037293\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.07389408946037293  to: 0.07383891344070434\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.07383891344070434  to: 0.07378246188163758\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.07378246188163758  to: 0.07372388243675232\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.07372388243675232  to: 0.07366369366645813\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.07366369366645813  to: 0.07360330820083619\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.07360330820083619  to: 0.07354332208633423\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.07354332208633423  to: 0.07348400950431824\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.07348400950431824  to: 0.07342544794082642\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.07342544794082642  to: 0.07336825132369995\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.07336825132369995  to: 0.07331179976463317\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.07331179976463317  to: 0.07325647473335266\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.07325647473335266  to: 0.07320077419281006\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.07320077419281006  to: 0.07314406633377075\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.07314406633377075  to: 0.07308632731437684\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.07308632731437684  to: 0.07302690744400024\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.07302690744400024  to: 0.0729670763015747\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.0729670763015747  to: 0.07290748953819275\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.07290748953819275  to: 0.07284963130950928\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.07284963130950928  to: 0.07279325723648071\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.07279325723648071  to: 0.0727375328540802\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.0727375328540802  to: 0.07268179655075073\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.07268179655075073  to: 0.07262588739395141\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.07262588739395141  to: 0.07256873846054077\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.07256873846054077  to: 0.07251061201095581\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.07251061201095581  to: 0.07245157361030578\n",
      "Training iteration: 1412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.07245157361030578  to: 0.07239328622817993\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.07239328622817993  to: 0.07233539819717408\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.07233539819717408  to: 0.07227846384048461\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.07227846384048461  to: 0.07222124338150024\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.07222124338150024  to: 0.0721640169620514\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.0721640169620514  to: 0.07210674285888671\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.07210674285888671  to: 0.07204939126968384\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.07204939126968384  to: 0.07199211716651917\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.07199211716651917  to: 0.07193413972854615\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.07193413972854615  to: 0.07187604904174805\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.07187604904174805  to: 0.07181830406188965\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.07181830406188965  to: 0.07176125049591064\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.07176125049591064  to: 0.07170458436012268\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.07170458436012268  to: 0.07164770364761353\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.07164770364761353  to: 0.07159062027931214\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.07159062027931214  to: 0.07153292298316956\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.07153292298316956  to: 0.07147510647773743\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.07147510647773743  to: 0.07141759991645813\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.07141759991645813  to: 0.07136040925979614\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.07136040925979614  to: 0.07130389809608459\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.07130389809608459  to: 0.07124727368354797\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.07124727368354797  to: 0.0711901068687439\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.0711901068687439  to: 0.07113357782363891\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.07113357782363891  to: 0.07107735872268676\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.07107735872268676  to: 0.07102111577987671\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.07102111577987671  to: 0.07096437811851501\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.07096437811851501  to: 0.07090665102005005\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.07090665102005005  to: 0.07084800004959106\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.07084800004959106  to: 0.07078925371170045\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.07078925371170045  to: 0.07073166966438293\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.07073166966438293  to: 0.07067612409591675\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.07067612409591675  to: 0.0706215739250183\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.0706215739250183  to: 0.07056728005409241\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.07056728005409241  to: 0.07051206827163696\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.07051206827163696  to: 0.07045506238937378\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.07045506238937378  to: 0.07039641737937927\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.07039641737937927  to: 0.07033702135086059\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.07033702135086059  to: 0.0702772855758667\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.0702772855758667  to: 0.07021831274032593\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.07021831274032593  to: 0.07016112804412841\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.07016112804412841  to: 0.07010554671287536\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.07010554671287536  to: 0.07005066871643066\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.07005066871643066  to: 0.06999517679214477\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.06999517679214477  to: 0.06993866562843323\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.06993866562843323  to: 0.06988075375556946\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.06988075375556946  to: 0.06982188224792481\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.06982188224792481  to: 0.06976319551467895\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.06976319551467895  to: 0.0697054922580719\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.0697054922580719  to: 0.0696488857269287\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.0696488857269287  to: 0.0695933997631073\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.0695933997631073  to: 0.06953842043876649\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.06953842043876649  to: 0.06948336958885193\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.06948336958885193  to: 0.06942747831344605\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.06942747831344605  to: 0.06937049031257629\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.06937049031257629  to: 0.06931286454200744\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.06931286454200744  to: 0.06925532817840577\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.06925532817840577  to: 0.06919820904731751\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.06919820904731751  to: 0.06914207935333253\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.06914207935333253  to: 0.06908680200576782\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.06908680200576782  to: 0.06903245449066162\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.06903245449066162  to: 0.06897769570350647\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.06897769570350647  to: 0.0689227283000946\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.0689227283000946  to: 0.06886694431304932\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.06886694431304932  to: 0.0688102662563324\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.0688102662563324  to: 0.06875313520431518\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.06875313520431518  to: 0.06869614720344544\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.06869614720344544  to: 0.06863990426063538\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.06863990426063538  to: 0.06858424544334411\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.06858424544334411  to: 0.06852904558181763\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.06852904558181763  to: 0.06847370862960815\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.06847370862960815  to: 0.06841824054718018\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.06841824054718018  to: 0.06836203336715699\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.06836203336715699  to: 0.0683057427406311\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.0683057427406311  to: 0.06824944615364074\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.06824944615364074  to: 0.06819322109222412\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.06819322109222412  to: 0.06813739538192749\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.06813739538192749  to: 0.06808165311813355\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.06808165311813355  to: 0.0680262565612793\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.0680262565612793  to: 0.0679704189300537\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.0679704189300537  to: 0.06791465282440186\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.06791465282440186  to: 0.06785857081413268\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.06785857081413268  to: 0.0678021788597107\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.0678021788597107  to: 0.06774579286575318\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.06774579286575318  to: 0.06768980026245117\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.06768980026245117  to: 0.06763408780097961\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.06763408780097961  to: 0.06757878065109253\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.06757878065109253  to: 0.06752391457557679\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.06752391457557679  to: 0.06746907234191894\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.06746907234191894  to: 0.0674140214920044\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.0674140214920044  to: 0.06735870838165284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.06735870838165284  to: 0.06730299592018127\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.06730299592018127  to: 0.06724708676338195\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.06724708676338195  to: 0.06719131469726562\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.06719131469726562  to: 0.06713548302650452\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.06713548302650452  to: 0.06707995533943176\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.06707995533943176  to: 0.06702475547790528\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.06702475547790528  to: 0.06696955561637878\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.06696955561637878  to: 0.06691431403160095\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.06691431403160095  to: 0.06685931086540223\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.06685931086540223  to: 0.06680426597595215\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.06680426597595215  to: 0.06674922108650208\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.06674922108650208  to: 0.06669424772262574\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.06669424772262574  to: 0.06663918495178223\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.06663918495178223  to: 0.06658410429954528\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.06658410429954528  to: 0.06652897596359253\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.06652897596359253  to: 0.06647365689277648\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.06647365689277648  to: 0.06641829013824463\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.06641829013824463  to: 0.06636294722557068\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.06636294722557068  to: 0.06630796194076538\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.06630796194076538  to: 0.06625277400016785\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.06625277400016785  to: 0.06619784235954285\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.06619784235954285  to: 0.066143000125885\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.066143000125885  to: 0.06608823537826539\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.06608823537826539  to: 0.06603336930274964\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.06603336930274964  to: 0.06597856283187867\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.06597856283187867  to: 0.06592376232147217\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.06592376232147217  to: 0.06586871743202209\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.06586871743202209  to: 0.0658138632774353\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.0658138632774353  to: 0.06575884819030761\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.06575884819030761  to: 0.0657041072845459\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.0657041072845459  to: 0.06564940810203553\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.06564940810203553  to: 0.0655947744846344\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.0655947744846344  to: 0.0655396580696106\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.0655396580696106  to: 0.06548453569412231\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.06548453569412231  to: 0.06542932987213135\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.06542932987213135  to: 0.065374094247818\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.065374094247818  to: 0.06531892418861389\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.06531892418861389  to: 0.06526389718055725\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.06526389718055725  to: 0.06520873308181763\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.06520873308181763  to: 0.06515384912490844\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.06515384912490844  to: 0.06509912014007568\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.06509912014007568  to: 0.0650442361831665\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.0650442361831665  to: 0.06498929262161254\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.06498929262161254  to: 0.06493450403213501\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.06493450403213501  to: 0.06487964987754821\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.06487964987754821  to: 0.06482474803924561\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.06482474803924561  to: 0.06476998925209046\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.06476998925209046  to: 0.06471518874168396\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.06471518874168396  to: 0.06466064453125\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.06466064453125  to: 0.06460611820220948\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.06460611820220948  to: 0.06455169916152954\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.06455169916152954  to: 0.06449716687202453\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.06449716687202453  to: 0.06444281339645386\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.06444281339645386  to: 0.06438846588134765\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.06438846588134765  to: 0.06433409452438354\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.06433409452438354  to: 0.06427978277206421\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.06427978277206421  to: 0.064225172996521\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.064225172996521  to: 0.06417080760002136\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.06417080760002136  to: 0.06411645412445069\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.06411645412445069  to: 0.06406208276748657\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.06406208276748657  to: 0.06400793194770812\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.06400793194770812  to: 0.06395373344421387\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.06395373344421387  to: 0.06389950513839722\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.06389950513839722  to: 0.06384523510932923\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.06384523510932923  to: 0.06379101872444153\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.06379101872444153  to: 0.0637368381023407\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.0637368381023407  to: 0.0636826992034912\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.0636826992034912  to: 0.06362866163253784\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.06362866163253784  to: 0.06357464790344239\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.06357464790344239  to: 0.06352076530456544\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.06352076530456544  to: 0.06346673965454101\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.06346673965454101  to: 0.06341273784637451\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.06341273784637451  to: 0.06335898637771606\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.06335898637771606  to: 0.0633051037788391\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.0633051037788391  to: 0.06325141191482545\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.06325141191482545  to: 0.0631975531578064\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.0631975531578064  to: 0.06314383745193482\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.06314383745193482  to: 0.06309014558792114\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.06309014558792114  to: 0.06303641200065613\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.06303641200065613  to: 0.06298283338546753\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.06298283338546753  to: 0.06292922496795654\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.06292922496795654  to: 0.06287572979927063\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.06287572979927063  to: 0.06282228231430054\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.06282228231430054  to: 0.06276888847351074\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.06276888847351074  to: 0.0627154290676117\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.0627154290676117  to: 0.06266204118728638\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.06266204118728638  to: 0.06260877847671509\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.06260877847671509  to: 0.06255530118942261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.06255530118942261  to: 0.06250194311141968\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.06250194311141968  to: 0.06244858503341675\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.06244858503341675  to: 0.06239534020423889\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.06239534020423889  to: 0.062342274188995364\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.062342274188995364  to: 0.06228903532028198\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.06228903532028198  to: 0.06223573684692383\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.06223573684692383  to: 0.06218279600143432\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.06218279600143432  to: 0.06212979555130005\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.06212979555130005  to: 0.06207687258720398\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.06207687258720398  to: 0.062024152278900145\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.062024152278900145  to: 0.061971384286880496\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.061971384286880496  to: 0.06191890835762024\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.06191890835762024  to: 0.06186617016792297\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.06186617016792297  to: 0.061813533306121826\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.061813533306121826  to: 0.06176084876060486\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.06176084876060486  to: 0.06170850992202759\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.06170850992202759  to: 0.06165591478347778\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.06165591478347778  to: 0.06160340905189514\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.06160340905189514  to: 0.061551105976104734\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.061551105976104734  to: 0.06149858236312866\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.06149858236312866  to: 0.061446362733840944\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.061446362733840944  to: 0.06139403581619263\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.06139403581619263  to: 0.06134175658226013\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.06134175658226013  to: 0.06128944158554077\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.06128944158554077  to: 0.061237215995788574\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.061237215995788574  to: 0.06118508577346802\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.06118508577346802  to: 0.06113287806510925\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.06113287806510925  to: 0.06108084321022034\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.06108084321022034  to: 0.06102858185768127\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.06102858185768127  to: 0.06097654104232788\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.06097654104232788  to: 0.060924625396728514\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.060924625396728514  to: 0.06087263822555542\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.06087263822555542  to: 0.0608207106590271\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.0608207106590271  to: 0.0607687771320343\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.0607687771320343  to: 0.060717010498046876\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.060717010498046876  to: 0.060665237903594973\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.060665237903594973  to: 0.0606134295463562\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.0606134295463562  to: 0.06056162118911743\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.06056162118911743  to: 0.06050993204116821\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.06050993204116821  to: 0.06045819520950317\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.06045819520950317  to: 0.06040652990341187\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.06040652990341187  to: 0.06035498976707458\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.06035498976707458  to: 0.06030322909355164\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.06030322909355164  to: 0.06025184392929077\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.06025184392929077  to: 0.060200202465057376\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.060200202465057376  to: 0.06014885306358338\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.06014885306358338  to: 0.060097455978393555\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.060097455978393555  to: 0.060045945644378665\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.060045945644378665  to: 0.05999468564987183\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.05999468564987183  to: 0.05994334816932678\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.05994334816932678  to: 0.05989197492599487\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.05989197492599487  to: 0.059840846061706546\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.059840846061706546  to: 0.059789490699768064\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.059789490699768064  to: 0.059738272428512575\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.059738272428512575  to: 0.05968729257583618\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.05968729257583618  to: 0.05963615775108337\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.05963615775108337  to: 0.059585177898406984\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.059585177898406984  to: 0.05953415632247925\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.05953415632247925  to: 0.05948302149772644\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.05948302149772644  to: 0.059432107210159305\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.059432107210159305  to: 0.05938118100166321\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.05938118100166321  to: 0.05933027267456055\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.05933027267456055  to: 0.05927940607070923\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.05927940607070923  to: 0.05922852754592896\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.05922852754592896  to: 0.059177863597869876\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.059177863597869876  to: 0.059127146005630495\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.059127146005630495  to: 0.059076505899429324\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.059076505899429324  to: 0.059025800228118895\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.059025800228118895  to: 0.058975183963775636\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.058975183963775636  to: 0.058924490213394166\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.058924490213394166  to: 0.058873933553695676\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.058873933553695676  to: 0.05882348418235779\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.05882348418235779  to: 0.05877312421798706\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.05877312421798706  to: 0.058722561597824095\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.058722561597824095  to: 0.05867222547531128\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.05867222547531128  to: 0.05862205028533936\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.05862205028533936  to: 0.05857164859771728\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.05857164859771728  to: 0.0585214376449585\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.0585214376449585  to: 0.0584711492061615\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.0584711492061615  to: 0.05842087864875793\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.05842087864875793  to: 0.0583707869052887\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.0583707869052887  to: 0.058320659399032596\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.058320659399032596  to: 0.058270621299743655\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.058270621299743655  to: 0.05822049975395203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.05822049975395203  to: 0.05817050337791443\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.05817050337791443  to: 0.05812055468559265\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.05812055468559265  to: 0.058070600032806396\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.058070600032806396  to: 0.05802082419395447\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.05802082419395447  to: 0.057971030473709106\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.057971030473709106  to: 0.05792112946510315\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.05792112946510315  to: 0.057871419191360476\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.057871419191360476  to: 0.05782178044319153\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.05782178044319153  to: 0.05777222514152527\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.05777222514152527  to: 0.05772243738174439\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.05772243738174439  to: 0.05767277479171753\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.05767277479171753  to: 0.0576231837272644\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.0576231837272644  to: 0.05757364630699158\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.05757364630699158  to: 0.05752424597740173\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.05752424597740173  to: 0.05747478008270264\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.05747478008270264  to: 0.05742533206939697\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.05742533206939697  to: 0.05737597346305847\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.05737597346305847  to: 0.05732683539390564\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.05732683539390564  to: 0.05727752447128296\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.05727752447128296  to: 0.05722818374633789\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.05722818374633789  to: 0.05717903971672058\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.05717903971672058  to: 0.05712980031967163\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.05712980031967163  to: 0.05708075761795044\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.05708075761795044  to: 0.05703170895576477\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.05703170895576477  to: 0.056982630491256715\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.056982630491256715  to: 0.05693356394767761\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.05693356394767761  to: 0.05688468217849731\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.05688468217849731  to: 0.056835830211639404\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.056835830211639404  to: 0.056786882877349856\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.056786882877349856  to: 0.056737983226776124\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.056737983226776124  to: 0.056689465045928956\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.056689465045928956  to: 0.056640458106994626\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.056640458106994626  to: 0.05659180283546448\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.05659180283546448  to: 0.05654318332672119\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.05654318332672119  to: 0.056494510173797606\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.056494510173797606  to: 0.05644610524177551\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.05644610524177551  to: 0.05639748573303223\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.05639748573303223  to: 0.05634889006614685\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.05634889006614685  to: 0.05630045533180237\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.05630045533180237  to: 0.05625216960906983\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.05625216960906983  to: 0.05620366334915161\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.05620366334915161  to: 0.056155288219451906\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.056155288219451906  to: 0.05610715746879578\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.05610715746879578  to: 0.05605899691581726\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.05605899691581726  to: 0.05601080060005188\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.05601080060005188  to: 0.05596257448196411\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.05596257448196411  to: 0.055914461612701416\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.055914461612701416  to: 0.05586642026901245\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.05586642026901245  to: 0.055818355083465575\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.055818355083465575  to: 0.05577033758163452\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.05577033758163452  to: 0.05572247505187988\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.05572247505187988  to: 0.05567435026168823\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.05567435026168823  to: 0.05562676191329956\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.05562676191329956  to: 0.055578958988189694\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.055578958988189694  to: 0.055531281232833865\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.055531281232833865  to: 0.05548352599143982\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.05548352599143982  to: 0.05543571710586548\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.05543571710586548  to: 0.055388104915618894\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.055388104915618894  to: 0.05534068942070007\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.05534068942070007  to: 0.055292928218841554\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.055292928218841554  to: 0.055245441198348996\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.055245441198348996  to: 0.055197876691818235\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.055197876691818235  to: 0.05515047311782837\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.05515047311782837  to: 0.055103206634521486\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.055103206634521486  to: 0.055055934190750125\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.055055934190750125  to: 0.05500859022140503\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.05500859022140503  to: 0.05496132969856262\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.05496132969856262  to: 0.05491418838500976\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.05491418838500976  to: 0.054867219924926755\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.054867219924926755  to: 0.05482001900672913\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.05482001900672913  to: 0.054772967100143434\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.054772967100143434  to: 0.05472586154937744\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.05472586154937744  to: 0.05467877984046936\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.05467877984046936  to: 0.054631787538528445\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.054631787538528445  to: 0.05458503365516663\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.05458503365516663  to: 0.0545382022857666\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.0545382022857666  to: 0.054491639137268066\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.054491639137268066  to: 0.054444903135299684\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.054444903135299684  to: 0.05439803600311279\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.05439803600311279  to: 0.0543514609336853\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.0543514609336853  to: 0.05430478453636169\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.05430478453636169  to: 0.05425807237625122\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.05425807237625122  to: 0.05421150922775268\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.05421150922775268  to: 0.05416510701179504\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.05416510701179504  to: 0.05411865711212158\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.05411865711212158  to: 0.054072332382202146\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.054072332382202146  to: 0.054025900363922116\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.054025900363922116  to: 0.053979885578155515\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.053979885578155515  to: 0.05393347144126892\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.05393347144126892  to: 0.05388737320899963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.05388737320899963  to: 0.05384091734886169\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.05384091734886169  to: 0.053795015811920165\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.053795015811920165  to: 0.053749024868011475\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.053749024868011475  to: 0.0537028968334198\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.0537028968334198  to: 0.053656911849975585\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.053656911849975585  to: 0.05361100435256958\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.05361100435256958  to: 0.05356520414352417\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.05356520414352417  to: 0.05351921319961548\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.05351921319961548  to: 0.05347352027893067\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.05347352027893067  to: 0.05342774987220764\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.05342774987220764  to: 0.053382200002670285\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.053382200002670285  to: 0.053336435556411745\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.053336435556411745  to: 0.053290766477584836\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.053290766477584836  to: 0.05324522256851196\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.05324522256851196  to: 0.053199630975723264\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.053199630975723264  to: 0.05315417647361755\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.05315417647361755  to: 0.05310887098312378\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.05310887098312378  to: 0.05306364297866821\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.05306364297866821  to: 0.05301825404167175\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.05301825404167175  to: 0.05297301411628723\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.05297301411628723  to: 0.0529277503490448\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.0529277503490448  to: 0.052882653474807736\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.052882653474807736  to: 0.052837586402893065\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.052837586402893065  to: 0.05279257893562317\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.05279257893562317  to: 0.05274730920791626\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.05274730920791626  to: 0.05270231962203979\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.05270231962203979  to: 0.05265730619430542\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.05265730619430542  to: 0.05261246562004089\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.05261246562004089  to: 0.052567654848098756\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.052567654848098756  to: 0.05252288579940796\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.05252288579940796  to: 0.05247827768325806\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.05247827768325806  to: 0.052433401346206665\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.052433401346206665  to: 0.0523887038230896\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.0523887038230896  to: 0.05234422087669373\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.05234422087669373  to: 0.05229966044425964\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.05229966044425964  to: 0.052255237102508546\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.052255237102508546  to: 0.05221077799797058\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.05221077799797058  to: 0.052166414260864255\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.052166414260864255  to: 0.052121973037719725\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.052121973037719725  to: 0.052077639102935794\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.052077639102935794  to: 0.05203351378440857\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.05203351378440857  to: 0.051989269256591794\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.051989269256591794  to: 0.05194498300552368\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.05194498300552368  to: 0.05190070867538452\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.05190070867538452  to: 0.051856815814971924\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.051856815814971924  to: 0.05181287527084351\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.05181287527084351  to: 0.051768910884857175\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.051768910884857175  to: 0.05172511339187622\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.05172511339187622  to: 0.05168120265007019\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.05168120265007019  to: 0.0516373336315155\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.0516373336315155  to: 0.051593607664108275\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.051593607664108275  to: 0.05154977440834045\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.05154977440834045  to: 0.05150607228279114\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.05150607228279114  to: 0.05146250128746033\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.05146250128746033  to: 0.05141892433166504\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.05141892433166504  to: 0.05137538313865662\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.05137538313865662  to: 0.05133195519447327\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.05133195519447327  to: 0.05128851532936096\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.05128851532936096  to: 0.051245260238647464\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.051245260238647464  to: 0.05120194554328918\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.05120194554328918  to: 0.05115852952003479\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.05115852952003479  to: 0.051115334033966064\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.051115334033966064  to: 0.05107212662696838\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.05107212662696838  to: 0.05102896094322205\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.05102896094322205  to: 0.05098576545715332\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.05098576545715332  to: 0.050942742824554445\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.050942742824554445  to: 0.050899827480316163\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.050899827480316163  to: 0.050856882333755495\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.050856882333755495  to: 0.05081415772438049\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.05081415772438049  to: 0.05077136158943176\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.05077136158943176  to: 0.05072856545448303\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.05072856545448303  to: 0.05068585872650146\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.05068585872650146  to: 0.05064312219619751\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.05064312219619751  to: 0.05060064196586609\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.05060064196586609  to: 0.05055786967277527\n",
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.05055786967277527  to: 0.050515228509902955\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.050515228509902955  to: 0.050472807884216306\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.050472807884216306  to: 0.050430333614349364\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.050430333614349364  to: 0.050388145446777347\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.050388145446777347  to: 0.05034592151641846\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.05034592151641846  to: 0.050303560495376584\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.050303560495376584  to: 0.050261521339416505\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.050261521339416505  to: 0.05021921396255493\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.05021921396255493  to: 0.05017712712287903\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.05017712712287903  to: 0.050134938955307004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.050134938955307004  to: 0.05009281635284424\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.05009281635284424  to: 0.05005084276199341\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.05005084276199341  to: 0.05000867247581482\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.05000867247581482  to: 0.049966859817504886\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.049966859817504886  to: 0.0499248206615448\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.0499248206615448  to: 0.04988326132297516\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.04988326132297516  to: 0.049841505289077756\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.049841505289077756  to: 0.049799776077270506\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.049799776077270506  to: 0.04975804686546326\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.04975804686546326  to: 0.049716433882713316\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.049716433882713316  to: 0.04967499673366547\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.04967499673366547  to: 0.049633520841598514\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.049633520841598514  to: 0.04959193766117096\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.04959193766117096  to: 0.049550586938858034\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.049550586938858034  to: 0.049509105086326596\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.049509105086326596  to: 0.04946787357330322\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.04946787357330322  to: 0.04942684769630432\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.04942684769630432  to: 0.04938555359840393\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.04938555359840393  to: 0.04934457838535309\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.04934457838535309  to: 0.049303650856018066\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.049303650856018066  to: 0.04926263689994812\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.04926263689994812  to: 0.04922148585319519\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.04922148585319519  to: 0.04918065965175629\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.04918065965175629  to: 0.04913977682590485\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.04913977682590485  to: 0.049099001288414004\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.049099001288414004  to: 0.04905825555324554\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.04905825555324554  to: 0.04901765286922455\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.04901765286922455  to: 0.04897696375846863\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.04897696375846863  to: 0.0489363968372345\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.0489363968372345  to: 0.048895740509033205\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.048895740509033205  to: 0.048855215311050415\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.048855215311050415  to: 0.04881495535373688\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.04881495535373688  to: 0.04877446293830871\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.04877446293830871  to: 0.04873438775539398\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.04873438775539398  to: 0.048694095015525816\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.048694095015525816  to: 0.04865378737449646\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.04865378737449646  to: 0.04861381053924561\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.04861381053924561  to: 0.048573607206344606\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.048573607206344606  to: 0.04853355288505554\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.04853355288505554  to: 0.0484935462474823\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.0484935462474823  to: 0.048453730344772336\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.048453730344772336  to: 0.04841374754905701\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.04841374754905701  to: 0.04837393760681152\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.04837393760681152  to: 0.04833418428897858\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.04833418428897858  to: 0.048294568061828615\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.048294568061828615  to: 0.04825492799282074\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.04825492799282074  to: 0.04821526408195496\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.04821526408195496  to: 0.04817555844783783\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.04817555844783783  to: 0.048136276006698606\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.048136276006698606  to: 0.04809679985046387\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.04809679985046387  to: 0.04805740416049957\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.04805740416049957  to: 0.048018264770507815\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.048018264770507815  to: 0.04797866940498352\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.04797866940498352  to: 0.047939592599868776\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.047939592599868776  to: 0.04789949357509613\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.04789949357509613  to: 0.047859391570091246\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.047859391570091246  to: 0.04781962037086487\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.04781962037086487  to: 0.04778043329715729\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.04778043329715729  to: 0.04774208068847656\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.04774208068847656  to: 0.04770467877388\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.04770467877388  to: 0.047667494416236876\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.047667494416236876  to: 0.047630110383033754\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.047630110383033754  to: 0.04759246408939362\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.04759246408939362  to: 0.047554033994674685\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.047554033994674685  to: 0.047515001893043515\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.047515001893043515  to: 0.047476339340209964\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.047476339340209964  to: 0.047438064217567445\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.047438064217567445  to: 0.047399896383285525\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.047399896383285525  to: 0.04736150801181793\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.04736150801181793  to: 0.0473227322101593\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.0473227322101593  to: 0.047283506393432616\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.047283506393432616  to: 0.04724380373954773\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.04724380373954773  to: 0.04720422625541687\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.04720422625541687  to: 0.04716487526893616\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.04716487526893616  to: 0.047125238180160525\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.047125238180160525  to: 0.04708591401576996\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.04708591401576996  to: 0.047047606110572814\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.047047606110572814  to: 0.04700993895530701\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.04700993895530701  to: 0.04697332382202148\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.04697332382202148  to: 0.04693740308284759\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.04693740308284759  to: 0.04690175950527191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.04690175950527191  to: 0.04686629176139832\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.04686629176139832  to: 0.04683058261871338\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.04683058261871338  to: 0.04679409861564636\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.04679409861564636  to: 0.046756571531295775\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.046756571531295775  to: 0.04671840071678161\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.04671840071678161  to: 0.04667885303497314\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.04667885303497314  to: 0.04663900434970856\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.04663900434970856  to: 0.046599572896957396\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.046599572896957396  to: 0.04656146466732025\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.04656146466732025  to: 0.046525639295578\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.046525639295578  to: 0.046491223573684695\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.046491223573684695  to: 0.04645681381225586\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.04645681381225586  to: 0.046420902013778687\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.046420902013778687  to: 0.04638291895389557\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.04638291895389557  to: 0.046343541145324706\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.046343541145324706  to: 0.04630386233329773\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.04630386233329773  to: 0.04626558423042297\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.04626558423042297  to: 0.04622932970523834\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.04622932970523834  to: 0.04619443416595459\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.04619443416595459  to: 0.04616057276725769\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.04616057276725769  to: 0.04612560868263245\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.04612560868263245  to: 0.04608898758888245\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.04608898758888245  to: 0.04605128169059754\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.04605128169059754  to: 0.046012979745864865\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.046012979745864865  to: 0.04597500860691071\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.04597500860691071  to: 0.045938807725906375\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.045938807725906375  to: 0.045904263854026794\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.045904263854026794  to: 0.04587070345878601\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.04587070345878601  to: 0.04583694338798523\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.04583694338798523  to: 0.04580143988132477\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.04580143988132477  to: 0.04576427340507507\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.04576427340507507  to: 0.045725947618484496\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.045725947618484496  to: 0.04568770825862885\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.04568770825862885  to: 0.04565021991729736\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.04565021991729736  to: 0.04561514854431152\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.04561514854431152  to: 0.04558170437812805\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.04558170437812805  to: 0.04554905891418457\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.04554905891418457  to: 0.04551529288291931\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.04551529288291931  to: 0.04547970294952393\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.04547970294952393  to: 0.0454426109790802\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.0454426109790802  to: 0.045405417680740356\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.045405417680740356  to: 0.045368915796279906\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.045368915796279906  to: 0.0453327476978302\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.0453327476978302  to: 0.04529723227024078\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.04529723227024078  to: 0.045262479782104494\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.045262479782104494  to: 0.0452291190624237\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.0452291190624237  to: 0.045196184515953065\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.045196184515953065  to: 0.04516211450099945\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.04516211450099945  to: 0.04512708187103272\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.04512708187103272  to: 0.045090803503990175\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.045090803503990175  to: 0.04505491852760315\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.04505491852760315  to: 0.04502001404762268\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.04502001404762268  to: 0.04498540461063385\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.04498540461063385  to: 0.04495059847831726\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.04495059847831726  to: 0.044916090369224546\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.044916090369224546  to: 0.044882583618164065\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.044882583618164065  to: 0.0448495626449585\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.0448495626449585  to: 0.04481554925441742\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.04481554925441742  to: 0.04478070139884949\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.04478070139884949  to: 0.04474535584449768\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.04474535584449768  to: 0.04471035897731781\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.04471035897731781  to: 0.04467645287513733\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.04467645287513733  to: 0.044642335176467894\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.044642335176467894  to: 0.04460820555686951\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.04460820555686951  to: 0.044574308395385745\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.044574308395385745  to: 0.044540303945541385\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.044540303945541385  to: 0.04450728297233582\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.04450728297233582  to: 0.04447480738162994\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.04447480738162994  to: 0.04444137513637543\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.04444137513637543  to: 0.044407224655151366\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.044407224655151366  to: 0.044372552633285524\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.044372552633285524  to: 0.044338721036911014\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.044338721036911014  to: 0.04430535435676575\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.04430535435676575  to: 0.04427222609519958\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.04427222609519958  to: 0.044238775968551636\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.044238775968551636  to: 0.04420501589775085\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.04420501589775085  to: 0.04417168498039246\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.04417168498039246  to: 0.04413913786411285\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.04413913786411285  to: 0.044106999039649965\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.044106999039649965  to: 0.04407433569431305\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.04407433569431305  to: 0.044040951132774356\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.044040951132774356  to: 0.04400728642940521\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.04400728642940521  to: 0.043973866105079654\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.043973866105079654  to: 0.04394071698188782\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.04394071698188782  to: 0.04390767216682434\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.04390767216682434  to: 0.04387572407722473\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.04387572407722473  to: 0.043843668699264524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.043843668699264524  to: 0.04381202161312103\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.04381202161312103  to: 0.04377964437007904\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.04377964437007904  to: 0.04374658465385437\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.04374658465385437  to: 0.04371303915977478\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.04371303915977478  to: 0.04367992281913757\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.04367992281913757  to: 0.043647783994674685\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.043647783994674685  to: 0.04361671805381775\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.04361671805381775  to: 0.04358550012111664\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.04358550012111664  to: 0.043553677201271054\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.043553677201271054  to: 0.043521460890769956\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.043521460890769956  to: 0.043489116430282596\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.043489116430282596  to: 0.043457627296447754\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.043457627296447754  to: 0.04342659115791321\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.04342659115791321  to: 0.043395590782165525\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.043395590782165525  to: 0.04336443841457367\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.04336443841457367  to: 0.04333282113075256\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.04333282113075256  to: 0.04330143928527832\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.04330143928527832  to: 0.04327021539211273\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.04327021539211273  to: 0.04324002861976624\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.04324002861976624  to: 0.04321045875549316\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.04321045875549316  to: 0.04318024516105652\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.04318024516105652  to: 0.04314926266670227\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.04314926266670227  to: 0.04311760365962982\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.04311760365962982  to: 0.043085876107215884\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.043085876107215884  to: 0.04305440783500671\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.04305440783500671  to: 0.04302436411380768\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.04302436411380768  to: 0.04299525618553161\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.04299525618553161  to: 0.042966055870056155\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.042966055870056155  to: 0.04293599724769592\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.04293599724769592  to: 0.042905157804489134\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.042905157804489134  to: 0.04287371039390564\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.04287371039390564  to: 0.04284264147281647\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.04284264147281647  to: 0.042812648415565493\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.042812648415565493  to: 0.04278369843959808\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.04278369843959808  to: 0.04275465607643127\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.04275465607643127  to: 0.042725196480751036\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.042725196480751036  to: 0.04269508421421051\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.04269508421421051  to: 0.04266464114189148\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.04266464114189148  to: 0.04263412356376648\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.04263412356376648  to: 0.042604082822799684\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.042604082822799684  to: 0.04257508814334869\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.04257508814334869  to: 0.04254468977451324\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.04254468977451324  to: 0.042512306571006776\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.042512306571006776  to: 0.04247924387454986\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.04247924387454986  to: 0.04244541227817535\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.04244541227817535  to: 0.042411217093467714\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.042411217093467714  to: 0.042377257347106935\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.042377257347106935  to: 0.04234406352043152\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.04234406352043152  to: 0.04231199622154236\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.04231199622154236  to: 0.0422802984714508\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.0422802984714508  to: 0.042248255014419554\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.042248255014419554  to: 0.04221642911434174\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.04221642911434174  to: 0.04218367636203766\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.04218367636203766  to: 0.042150235176086424\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.042150235176086424  to: 0.04211622774600983\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.04211622774600983  to: 0.04208267331123352\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.04208267331123352  to: 0.04204972684383392\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.04204972684383392  to: 0.0420182466506958\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.0420182466506958  to: 0.04198753833770752\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.04198753833770752  to: 0.041956338286399844\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.041956338286399844  to: 0.04192409515380859\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.04192409515380859  to: 0.04189098477363586\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.04189098477363586  to: 0.041857489943504335\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.041857489943504335  to: 0.04182385802268982\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.04182385802268982  to: 0.041791129112243655\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.041791129112243655  to: 0.041759330034255984\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.041759330034255984  to: 0.04172822535037994\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.04172822535037994  to: 0.041697487235069275\n",
      "Training iteration: 2085\n",
      "Improved validation loss from: 0.041697487235069275  to: 0.041667279601097104\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.041667279601097104  to: 0.041636142134666446\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.041636142134666446  to: 0.04160402715206146\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.04160402715206146  to: 0.041571575403213504\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.041571575403213504  to: 0.0415385901927948\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.0415385901927948  to: 0.0415053516626358\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.0415053516626358  to: 0.04147281646728516\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.04147281646728516  to: 0.041440758109092715\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.041440758109092715  to: 0.04140934348106384\n",
      "Training iteration: 2094\n",
      "Improved validation loss from: 0.04140934348106384  to: 0.04137868881225586\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.04137868881225586  to: 0.041348773241043094\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.041348773241043094  to: 0.04131845533847809\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.04131845533847809  to: 0.0412874847650528\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.0412874847650528  to: 0.04125598967075348\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.04125598967075348  to: 0.041223764419555664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.041223764419555664  to: 0.04119158685207367\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.04119158685207367  to: 0.041159945726394656\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.041159945726394656  to: 0.04112964570522308\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.04112964570522308  to: 0.04109942018985748\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.04109942018985748  to: 0.041069239377975464\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.041069239377975464  to: 0.04103826582431793\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.04103826582431793  to: 0.041007119417190555\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.041007119417190555  to: 0.04097543358802795\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.04097543358802795  to: 0.04094371795654297\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.04094371795654297  to: 0.040912455320358275\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.040912455320358275  to: 0.040881532430648806\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.040881532430648806  to: 0.04085131287574768\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.04085131287574768  to: 0.0408212810754776\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.0408212810754776  to: 0.040791860222816466\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.040791860222816466  to: 0.04076181948184967\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.04076181948184967  to: 0.040731081366539\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.040731081366539  to: 0.04069959223270416\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.04069959223270416  to: 0.040668123960494997\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.040668123960494997  to: 0.04063697457313538\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.04063697457313538  to: 0.04060644507408142\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.04060644507408142  to: 0.04057663083076477\n",
      "Training iteration: 2121\n",
      "Improved validation loss from: 0.04057663083076477  to: 0.04054728150367737\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.04054728150367737  to: 0.04051827490329742\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.04051827490329742  to: 0.040488576889038085\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.040488576889038085  to: 0.04045846462249756\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.04045846462249756  to: 0.04042826592922211\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.04042826592922211  to: 0.040397638082504274\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.040397638082504274  to: 0.04036720395088196\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.04036720395088196  to: 0.04033698439598084\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.04033698439598084  to: 0.040307196974754336\n",
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.040307196974754336  to: 0.04027787744998932\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.04027787744998932  to: 0.040248537063598634\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.040248537063598634  to: 0.04021919369697571\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.04021919369697571  to: 0.040189656615257266\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.040189656615257266  to: 0.04015992283821106\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.04015992283821106  to: 0.040130051970481875\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.040130051970481875  to: 0.04009995460510254\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.04009995460510254  to: 0.04007031321525574\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.04007031321525574  to: 0.04004097580909729\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.04004097580909729  to: 0.040011757612228395\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.040011757612228395  to: 0.03998267948627472\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.03998267948627472  to: 0.03995372056961059\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.03995372056961059  to: 0.03992477059364319\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.03992477059364319  to: 0.03989554941654205\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: 0.03989554941654205  to: 0.03986622393131256\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.03986622393131256  to: 0.03983697295188904\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.03983697295188904  to: 0.039807870984077454\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.039807870984077454  to: 0.03977894186973572\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.03977894186973572  to: 0.03975021243095398\n",
      "Training iteration: 2149\n",
      "Improved validation loss from: 0.03975021243095398  to: 0.03972160220146179\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.03972160220146179  to: 0.039692926406860354\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.039692926406860354  to: 0.039664119482040405\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.039664119482040405  to: 0.03963541090488434\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.03963541090488434  to: 0.039606761932373044\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.039606761932373044  to: 0.03957783579826355\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.03957783579826355  to: 0.03954928517341614\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.03954928517341614  to: 0.03952070474624634\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.03952070474624634  to: 0.03949236869812012\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.03949236869812012  to: 0.03946129381656647\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.03946129381656647  to: 0.03943168818950653\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.03943168818950653  to: 0.03940355181694031\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.03940355181694031  to: 0.039376085996627806\n",
      "Training iteration: 2162\n",
      "Improved validation loss from: 0.039376085996627806  to: 0.039347979426383975\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.039347979426383975  to: 0.03931877017021179\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.03931877017021179  to: 0.03928874135017395\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.03928874135017395  to: 0.03925883173942566\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.03925883173942566  to: 0.039230018854141235\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.039230018854141235  to: 0.039202290773391726\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.039202290773391726  to: 0.03917518556118012\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.03917518556118012  to: 0.03914778232574463\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.03914778232574463  to: 0.03911682665348053\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: 0.03911682665348053  to: 0.039086762070655826\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.039086762070655826  to: 0.03905796110630035\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.03905796110630035  to: 0.03903047442436218\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.03903047442436218  to: 0.03900321125984192\n",
      "Training iteration: 2175\n",
      "Improved validation loss from: 0.03900321125984192  to: 0.0389754056930542\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.0389754056930542  to: 0.038946697115898134\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.038946697115898134  to: 0.03891754150390625\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.03891754150390625  to: 0.038888826966285706\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.038888826966285706  to: 0.03886107802391052\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.03886107802391052  to: 0.03883420825004578\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.03883420825004578  to: 0.03880746960639954\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.03880746960639954  to: 0.03878042995929718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.03878042995929718  to: 0.038752737641334536\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.038752737641334536  to: 0.038724711537361144\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.038724711537361144  to: 0.03869434297084808\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.03869434297084808  to: 0.03866601288318634\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.03866601288318634  to: 0.03863993585109711\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.03863993585109711  to: 0.03861474096775055\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.03861474096775055  to: 0.038588514924049376\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.038588514924049376  to: 0.03856081068515778\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.03856081068515778  to: 0.03853160738945007\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.03853160738945007  to: 0.03850253224372864\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.03850253224372864  to: 0.03847494721412659\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.03847494721412659  to: 0.03844936788082123\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.03844936788082123  to: 0.03842442035675049\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.03842442035675049  to: 0.038396209478378296\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.038396209478378296  to: 0.03836812973022461\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.03836812973022461  to: 0.03834073543548584\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.03834073543548584  to: 0.03831384778022766\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.03831384778022766  to: 0.038287287950515746\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.038287287950515746  to: 0.038260525465011595\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.038260525465011595  to: 0.038233643770217894\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.038233643770217894  to: 0.03820669949054718\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.03820669949054718  to: 0.038180109858512876\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.038180109858512876  to: 0.03815388083457947\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.03815388083457947  to: 0.038125422596931455\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.038125422596931455  to: 0.038098499178886414\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.038098499178886414  to: 0.03807297945022583\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.03807297945022583  to: 0.03804773986339569\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.03804773986339569  to: 0.038021764159202574\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.038021764159202574  to: 0.037994807958602904\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.037994807958602904  to: 0.03796739876270294\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.03796739876270294  to: 0.03794044554233551\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.03794044554233551  to: 0.037914738059043884\n",
      "Training iteration: 2215\n",
      "Improved validation loss from: 0.037914738059043884  to: 0.03788749873638153\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.03788749873638153  to: 0.03786177039146423\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.03786177039146423  to: 0.03783702254295349\n",
      "Training iteration: 2218\n",
      "Improved validation loss from: 0.03783702254295349  to: 0.0378121018409729\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.0378121018409729  to: 0.03778595626354218\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.03778595626354218  to: 0.037758877873420714\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.037758877873420714  to: 0.03773181438446045\n",
      "Training iteration: 2222\n",
      "Improved validation loss from: 0.03773181438446045  to: 0.037705615162849426\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.037705615162849426  to: 0.0376808375120163\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.0376808375120163  to: 0.03765432238578796\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.03765432238578796  to: 0.03762899041175842\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.03762899041175842  to: 0.03760462701320648\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.03760462701320648  to: 0.0375796377658844\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.0375796377658844  to: 0.03755380809307098\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.03755380809307098  to: 0.037527349591255185\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.037527349591255185  to: 0.03750123083591461\n",
      "Training iteration: 2231\n",
      "Improved validation loss from: 0.03750123083591461  to: 0.03747570216655731\n",
      "Training iteration: 2232\n",
      "Improved validation loss from: 0.03747570216655731  to: 0.037448841333389285\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.037448841333389285  to: 0.037424135208129886\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.037424135208129886  to: 0.03740065097808838\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.03740065097808838  to: 0.03737687468528748\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.03737687468528748  to: 0.03735206425189972\n",
      "Training iteration: 2237\n",
      "Improved validation loss from: 0.03735206425189972  to: 0.037324252724647525\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.037324252724647525  to: 0.037294206023216245\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.037294206023216245  to: 0.03726260662078858\n",
      "Training iteration: 2240\n",
      "Improved validation loss from: 0.03726260662078858  to: 0.037234193086624144\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.037234193086624144  to: 0.03720828890800476\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.03720828890800476  to: 0.037182921171188356\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.037182921171188356  to: 0.0371557354927063\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.0371557354927063  to: 0.03712627291679382\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.03712627291679382  to: 0.037095746397972106\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.037095746397972106  to: 0.03706584870815277\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.03706584870815277  to: 0.037035626173019406\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.037035626173019406  to: 0.03700888752937317\n",
      "Training iteration: 2249\n",
      "Improved validation loss from: 0.03700888752937317  to: 0.036984381079673764\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.036984381079673764  to: 0.03695906400680542\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.03695906400680542  to: 0.03693147003650665\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.03693147003650665  to: 0.03690123856067658\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.03690123856067658  to: 0.036868155002593994\n",
      "Training iteration: 2254\n",
      "Improved validation loss from: 0.036868155002593994  to: 0.036838316917419435\n",
      "Training iteration: 2255\n",
      "Improved validation loss from: 0.036838316917419435  to: 0.03681260645389557\n",
      "Training iteration: 2256\n",
      "Improved validation loss from: 0.03681260645389557  to: 0.03678905665874481\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.03678905665874481  to: 0.036764496564865114\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.036764496564865114  to: 0.036736863851547244\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.036736863851547244  to: 0.036706548929214475\n",
      "Training iteration: 2260\n",
      "Improved validation loss from: 0.036706548929214475  to: 0.036673310399055484\n",
      "Training iteration: 2261\n",
      "Improved validation loss from: 0.036673310399055484  to: 0.03664410412311554\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.03664410412311554  to: 0.03661926090717316\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.03661926090717316  to: 0.03659647107124329\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.03659647107124329  to: 0.036572375893592836\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.036572375893592836  to: 0.03654438853263855\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.03654438853263855  to: 0.036511382460594176\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.036511382460594176  to: 0.03648001551628113\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: 0.03648001551628113  to: 0.03645302057266235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.03645302057266235  to: 0.03642961382865906\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.03642961382865906  to: 0.03640682101249695\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: 0.03640682101249695  to: 0.03638140559196472\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.03638140559196472  to: 0.036352801322937014\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.036352801322937014  to: 0.03632001876831055\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.03632001876831055  to: 0.036290010809898375\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.036290010809898375  to: 0.036264699697494504\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.036264699697494504  to: 0.0362425297498703\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.0362425297498703  to: 0.03621702790260315\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.03621702790260315  to: 0.03618959784507751\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.03618959784507751  to: 0.036160847544670104\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.036160847544670104  to: 0.0361319750547409\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: 0.0361319750547409  to: 0.03610453605651855\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.03610453605651855  to: 0.03607908189296723\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.03607908189296723  to: 0.0360547125339508\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.0360547125339508  to: 0.03603014647960663\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.03603014647960663  to: 0.036001601815223695\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.036001601815223695  to: 0.03597075641155243\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.03597075641155243  to: 0.03594313263893127\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.03594313263893127  to: 0.03591859638690949\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.03591859638690949  to: 0.03589491248130798\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.03589491248130798  to: 0.03586985468864441\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.03586985468864441  to: 0.03584256172180176\n",
      "Training iteration: 2292\n",
      "Improved validation loss from: 0.03584256172180176  to: 0.03581386208534241\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.03581386208534241  to: 0.03578611016273499\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.03578611016273499  to: 0.035760700702667236\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.035760700702667236  to: 0.035737261176109314\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.035737261176109314  to: 0.03571423590183258\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.03571423590183258  to: 0.035687202215194704\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.035687202215194704  to: 0.035657253861427304\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.035657253861427304  to: 0.03562981188297272\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.03562981188297272  to: 0.035605281591415405\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.035605281591415405  to: 0.0355821430683136\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.0355821430683136  to: 0.03555821776390076\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.03555821776390076  to: 0.035532256960868834\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.035532256960868834  to: 0.035504817962646484\n",
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.035504817962646484  to: 0.035477733612060545\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.035477733612060545  to: 0.03544992208480835\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.03544992208480835  to: 0.035425585508346555\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.035425585508346555  to: 0.035403507947921756\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.035403507947921756  to: 0.03537832200527191\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.03537832200527191  to: 0.035352295637130736\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.035352295637130736  to: 0.03532548546791077\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.03532548546791077  to: 0.03529904782772064\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.03529904782772064  to: 0.035273581743240356\n",
      "Training iteration: 2314\n",
      "Improved validation loss from: 0.035273581743240356  to: 0.03524932861328125\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.03524932861328125  to: 0.03522568643093109\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.03522568643093109  to: 0.03519933819770813\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.03519933819770813  to: 0.03517105579376221\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.03517105579376221  to: 0.035145577788352964\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.035145577788352964  to: 0.03512218296527862\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.03512218296527862  to: 0.03509904742240906\n",
      "Training iteration: 2321\n",
      "Improved validation loss from: 0.03509904742240906  to: 0.03507445752620697\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.03507445752620697  to: 0.03504827320575714\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.03504827320575714  to: 0.03502161204814911\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.03502161204814911  to: 0.03499636948108673\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.03499636948108673  to: 0.034973064064979555\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.034973064064979555  to: 0.03494851589202881\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.03494851589202881  to: 0.03492249250411987\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.03492249250411987  to: 0.03489832282066345\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.03489832282066345  to: 0.03487502932548523\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.03487502932548523  to: 0.034851089119911194\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.034851089119911194  to: 0.0348261296749115\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.0348261296749115  to: 0.034800609946250914\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.034800609946250914  to: 0.03477572798728943\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.03477572798728943  to: 0.03475209474563599\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.03475209474563599  to: 0.03472692668437958\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.03472692668437958  to: 0.03470120131969452\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.03470120131969452  to: 0.03467790186405182\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.03467790186405182  to: 0.034655600786209106\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.034655600786209106  to: 0.03463233113288879\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.03463233113288879  to: 0.034607458114624026\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.034607458114624026  to: 0.034581607580184935\n",
      "Training iteration: 2342\n",
      "Improved validation loss from: 0.034581607580184935  to: 0.03455655574798584\n",
      "Training iteration: 2343\n",
      "Improved validation loss from: 0.03455655574798584  to: 0.03453090786933899\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.03453090786933899  to: 0.034508296847343446\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.034508296847343446  to: 0.034487655758857726\n",
      "Training iteration: 2346\n",
      "Improved validation loss from: 0.034487655758857726  to: 0.03446364402770996\n",
      "Training iteration: 2347\n",
      "Improved validation loss from: 0.03446364402770996  to: 0.034439027309417725\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.034439027309417725  to: 0.034413987398147584\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.034413987398147584  to: 0.03438950181007385\n",
      "Training iteration: 2350\n",
      "Improved validation loss from: 0.03438950181007385  to: 0.03436603844165802\n",
      "Training iteration: 2351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03436603844165802  to: 0.03434108793735504\n",
      "Training iteration: 2352\n",
      "Improved validation loss from: 0.03434108793735504  to: 0.03431818783283234\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.03431818783283234  to: 0.034296488761901854\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.034296488761901854  to: 0.03427203595638275\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.03427203595638275  to: 0.03424785733222961\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.03424785733222961  to: 0.03422419428825378\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.03422419428825378  to: 0.03420094847679138\n",
      "Training iteration: 2358\n",
      "Improved validation loss from: 0.03420094847679138  to: 0.0341779887676239\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.0341779887676239  to: 0.034155231714248654\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.034155231714248654  to: 0.03412993252277374\n",
      "Training iteration: 2361\n",
      "Improved validation loss from: 0.03412993252277374  to: 0.03410396277904511\n",
      "Training iteration: 2362\n",
      "Improved validation loss from: 0.03410396277904511  to: 0.03408144116401672\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.03408144116401672  to: 0.03406117558479309\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.03406117558479309  to: 0.03404033184051514\n",
      "Training iteration: 2365\n",
      "Improved validation loss from: 0.03404033184051514  to: 0.03401713371276856\n",
      "Training iteration: 2366\n",
      "Improved validation loss from: 0.03401713371276856  to: 0.033991873264312744\n",
      "Training iteration: 2367\n",
      "Improved validation loss from: 0.033991873264312744  to: 0.03396657705307007\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.03396657705307007  to: 0.03394097685813904\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.03394097685813904  to: 0.03391968309879303\n",
      "Training iteration: 2370\n",
      "Improved validation loss from: 0.03391968309879303  to: 0.03389847874641418\n",
      "Training iteration: 2371\n",
      "Improved validation loss from: 0.03389847874641418  to: 0.03387853801250458\n",
      "Training iteration: 2372\n",
      "Improved validation loss from: 0.03387853801250458  to: 0.03385727107524872\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.03385727107524872  to: 0.03383345901966095\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.03383345901966095  to: 0.0338081032037735\n",
      "Training iteration: 2375\n",
      "Improved validation loss from: 0.0338081032037735  to: 0.03378112614154816\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.03378112614154816  to: 0.03375828564167023\n",
      "Training iteration: 2377\n",
      "Improved validation loss from: 0.03375828564167023  to: 0.03373941779136658\n",
      "Training iteration: 2378\n",
      "Improved validation loss from: 0.03373941779136658  to: 0.033721357583999634\n",
      "Training iteration: 2379\n",
      "Improved validation loss from: 0.033721357583999634  to: 0.03369844555854797\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.03369844555854797  to: 0.033671268820762636\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.033671268820762636  to: 0.033645883202552795\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.033645883202552795  to: 0.03362361490726471\n",
      "Training iteration: 2383\n",
      "Improved validation loss from: 0.03362361490726471  to: 0.03360389769077301\n",
      "Training iteration: 2384\n",
      "Improved validation loss from: 0.03360389769077301  to: 0.03358420431613922\n",
      "Training iteration: 2385\n",
      "Improved validation loss from: 0.03358420431613922  to: 0.033562475442886354\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.033562475442886354  to: 0.03353889286518097\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.03353889286518097  to: 0.03351523280143738\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.03351523280143738  to: 0.033490735292434695\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.033490735292434695  to: 0.033467423915863034\n",
      "Training iteration: 2390\n",
      "Improved validation loss from: 0.033467423915863034  to: 0.03344578146934509\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.03344578146934509  to: 0.033426955342292786\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.033426955342292786  to: 0.033407348394393924\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.033407348394393924  to: 0.033384770154953\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.033384770154953  to: 0.03335942327976227\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.03335942327976227  to: 0.03333463072776795\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.03333463072776795  to: 0.03331267237663269\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.03331267237663269  to: 0.03329416513442993\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.03329416513442993  to: 0.03327683508396149\n",
      "Training iteration: 2399\n",
      "Improved validation loss from: 0.03327683508396149  to: 0.033255210518836974\n",
      "Training iteration: 2400\n",
      "Improved validation loss from: 0.033255210518836974  to: 0.03322996497154236\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: 0.03322996497154236  to: 0.03320373594760895\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.03320373594760895  to: 0.0331820547580719\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.0331820547580719  to: 0.03316386640071869\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: 0.03316386640071869  to: 0.03314570784568786\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: 0.03314570784568786  to: 0.03312501013278961\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.03312501013278961  to: 0.03310143649578094\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.03310143649578094  to: 0.033077561855316163\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.033077561855316163  to: 0.03305579125881195\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.03305579125881195  to: 0.033037057518959044\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.033037057518959044  to: 0.03301718235015869\n",
      "Training iteration: 2411\n",
      "Improved validation loss from: 0.03301718235015869  to: 0.03299523591995239\n",
      "Training iteration: 2412\n",
      "Improved validation loss from: 0.03299523591995239  to: 0.032971882820129396\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.032971882820129396  to: 0.03295063376426697\n",
      "Training iteration: 2414\n",
      "Improved validation loss from: 0.03295063376426697  to: 0.032930925488471985\n",
      "Training iteration: 2415\n",
      "Improved validation loss from: 0.032930925488471985  to: 0.032911214232444766\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: 0.032911214232444766  to: 0.03289042413234711\n",
      "Training iteration: 2417\n",
      "Improved validation loss from: 0.03289042413234711  to: 0.032868567109107974\n",
      "Training iteration: 2418\n",
      "Improved validation loss from: 0.032868567109107974  to: 0.03284701406955719\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.03284701406955719  to: 0.03282404541969299\n",
      "Training iteration: 2420\n",
      "Improved validation loss from: 0.03282404541969299  to: 0.03280176222324371\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.03280176222324371  to: 0.03278333842754364\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.03278333842754364  to: 0.03276627957820892\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.03276627957820892  to: 0.03274742066860199\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.03274742066860199  to: 0.03272303342819214\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.03272303342819214  to: 0.03269882202148437\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: 0.03269882202148437  to: 0.03267759084701538\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.03267759084701538  to: 0.03265973925590515\n",
      "Training iteration: 2428\n",
      "Improved validation loss from: 0.03265973925590515  to: 0.03264055252075195\n",
      "Training iteration: 2429\n",
      "Improved validation loss from: 0.03264055252075195  to: 0.03261902332305908\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: 0.03261902332305908  to: 0.032598072290420534\n",
      "Training iteration: 2431\n",
      "Improved validation loss from: 0.032598072290420534  to: 0.03257775902748108\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: 0.03257775902748108  to: 0.03255780339241028\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.03255780339241028  to: 0.03253768980503082\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: 0.03253768980503082  to: 0.03251774907112122\n",
      "Training iteration: 2435\n",
      "Improved validation loss from: 0.03251774907112122  to: 0.032498040795326234\n",
      "Training iteration: 2436\n",
      "Improved validation loss from: 0.032498040795326234  to: 0.03247644007205963\n",
      "Training iteration: 2437\n",
      "Improved validation loss from: 0.03247644007205963  to: 0.03245431482791901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2438\n",
      "Improved validation loss from: 0.03245431482791901  to: 0.032433044910430905\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.032433044910430905  to: 0.032415324449539186\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.032415324449539186  to: 0.032398322224617006\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.032398322224617006  to: 0.03237897753715515\n",
      "Training iteration: 2442\n",
      "Improved validation loss from: 0.03237897753715515  to: 0.03235689401626587\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.03235689401626587  to: 0.03233433067798615\n",
      "Training iteration: 2444\n",
      "Improved validation loss from: 0.03233433067798615  to: 0.03231387734413147\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.03231387734413147  to: 0.032293984293937684\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.032293984293937684  to: 0.03227497637271881\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.03227497637271881  to: 0.03225850164890289\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.03225850164890289  to: 0.032241535186767575\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.032241535186767575  to: 0.032221898436546326\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.032221898436546326  to: 0.03219725489616394\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.03219725489616394  to: 0.032172161340713504\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.032172161340713504  to: 0.032152748107910155\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.032152748107910155  to: 0.03213831186294556\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.03213831186294556  to: 0.03212427794933319\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.03212427794933319  to: 0.032106328010559085\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.032106328010559085  to: 0.03208405375480652\n",
      "Training iteration: 2457\n",
      "Improved validation loss from: 0.03208405375480652  to: 0.032058095932006835\n",
      "Training iteration: 2458\n",
      "Improved validation loss from: 0.032058095932006835  to: 0.03203428685665131\n",
      "Training iteration: 2459\n",
      "Improved validation loss from: 0.03203428685665131  to: 0.03201798796653747\n",
      "Training iteration: 2460\n",
      "Improved validation loss from: 0.03201798796653747  to: 0.03200603127479553\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.03200603127479553  to: 0.03199219107627869\n",
      "Training iteration: 2462\n",
      "Improved validation loss from: 0.03199219107627869  to: 0.031969889998435974\n",
      "Training iteration: 2463\n",
      "Improved validation loss from: 0.031969889998435974  to: 0.03194490075111389\n",
      "Training iteration: 2464\n",
      "Improved validation loss from: 0.03194490075111389  to: 0.03192259669303894\n",
      "Training iteration: 2465\n",
      "Improved validation loss from: 0.03192259669303894  to: 0.03190271258354187\n",
      "Training iteration: 2466\n",
      "Improved validation loss from: 0.03190271258354187  to: 0.03188845813274384\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.03188845813274384  to: 0.03187270760536194\n",
      "Training iteration: 2468\n",
      "Improved validation loss from: 0.03187270760536194  to: 0.03185532689094543\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.03185532689094543  to: 0.03183503448963165\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.03183503448963165  to: 0.03181334137916565\n",
      "Training iteration: 2471\n",
      "Improved validation loss from: 0.03181334137916565  to: 0.031790485978126524\n",
      "Training iteration: 2472\n",
      "Improved validation loss from: 0.031790485978126524  to: 0.0317722350358963\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.0317722350358963  to: 0.03175804913043976\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.03175804913043976  to: 0.03174145817756653\n",
      "Training iteration: 2475\n",
      "Improved validation loss from: 0.03174145817756653  to: 0.031720992922782895\n",
      "Training iteration: 2476\n",
      "Improved validation loss from: 0.031720992922782895  to: 0.031700485944747926\n",
      "Training iteration: 2477\n",
      "Improved validation loss from: 0.031700485944747926  to: 0.031681209802627563\n",
      "Training iteration: 2478\n",
      "Improved validation loss from: 0.031681209802627563  to: 0.031663501262664796\n",
      "Training iteration: 2479\n",
      "Improved validation loss from: 0.031663501262664796  to: 0.03164657950401306\n",
      "Training iteration: 2480\n",
      "Improved validation loss from: 0.03164657950401306  to: 0.031629678606987\n",
      "Training iteration: 2481\n",
      "Improved validation loss from: 0.031629678606987  to: 0.031609296798706055\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.031609296798706055  to: 0.03158787786960602\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.03158787786960602  to: 0.03156774640083313\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.03156774640083313  to: 0.031552115082740785\n",
      "Training iteration: 2485\n",
      "Improved validation loss from: 0.031552115082740785  to: 0.03153795599937439\n",
      "Training iteration: 2486\n",
      "Improved validation loss from: 0.03153795599937439  to: 0.03152127861976624\n",
      "Training iteration: 2487\n",
      "Improved validation loss from: 0.03152127861976624  to: 0.031501290202140805\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.031501290202140805  to: 0.031479749083518985\n",
      "Training iteration: 2489\n",
      "Improved validation loss from: 0.031479749083518985  to: 0.03145788908004761\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.03145788908004761  to: 0.031438976526260376\n",
      "Training iteration: 2491\n",
      "Improved validation loss from: 0.031438976526260376  to: 0.03142600655555725\n",
      "Training iteration: 2492\n",
      "Improved validation loss from: 0.03142600655555725  to: 0.031414151191711426\n",
      "Training iteration: 2493\n",
      "Improved validation loss from: 0.031414151191711426  to: 0.03139843642711639\n",
      "Training iteration: 2494\n",
      "Improved validation loss from: 0.03139843642711639  to: 0.03137504160404205\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.03137504160404205  to: 0.03134906589984894\n",
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.03134906589984894  to: 0.031329646706581116\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.031329646706581116  to: 0.031317058205604556\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.031317058205604556  to: 0.031306672096252444\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.031306672096252444  to: 0.03129248023033142\n",
      "Training iteration: 2500\n",
      "Improved validation loss from: 0.03129248023033142  to: 0.031269881129264834\n",
      "Training iteration: 2501\n",
      "Improved validation loss from: 0.031269881129264834  to: 0.03124387562274933\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.03124387562274933  to: 0.031223979592323304\n",
      "Training iteration: 2503\n",
      "Improved validation loss from: 0.031223979592323304  to: 0.031211167573928833\n",
      "Training iteration: 2504\n",
      "Improved validation loss from: 0.031211167573928833  to: 0.031201130151748656\n",
      "Training iteration: 2505\n",
      "Improved validation loss from: 0.031201130151748656  to: 0.031184858083724974\n",
      "Training iteration: 2506\n",
      "Improved validation loss from: 0.031184858083724974  to: 0.03116183876991272\n",
      "Training iteration: 2507\n",
      "Improved validation loss from: 0.03116183876991272  to: 0.03113982081413269\n",
      "Training iteration: 2508\n",
      "Improved validation loss from: 0.03113982081413269  to: 0.03112240433692932\n",
      "Training iteration: 2509\n",
      "Improved validation loss from: 0.03112240433692932  to: 0.03110906183719635\n",
      "Training iteration: 2510\n",
      "Improved validation loss from: 0.03110906183719635  to: 0.03109648823738098\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.03109648823738098  to: 0.031078642606735228\n",
      "Training iteration: 2512\n",
      "Improved validation loss from: 0.031078642606735228  to: 0.031056177616119385\n",
      "Training iteration: 2513\n",
      "Improved validation loss from: 0.031056177616119385  to: 0.03103407025337219\n",
      "Training iteration: 2514\n",
      "Improved validation loss from: 0.03103407025337219  to: 0.031017932295799255\n",
      "Training iteration: 2515\n",
      "Improved validation loss from: 0.031017932295799255  to: 0.031006085872650146\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.031006085872650146  to: 0.03099364638328552\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.03099364638328552  to: 0.030976694822311402\n",
      "Training iteration: 2518\n",
      "Improved validation loss from: 0.030976694822311402  to: 0.030956259369850157\n",
      "Training iteration: 2519\n",
      "Improved validation loss from: 0.030956259369850157  to: 0.03093351423740387\n",
      "Training iteration: 2520\n",
      "Improved validation loss from: 0.03093351423740387  to: 0.030913758277893066\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.030913758277893066  to: 0.0309015691280365\n",
      "Training iteration: 2522\n",
      "Improved validation loss from: 0.0309015691280365  to: 0.030892261862754823\n",
      "Training iteration: 2523\n",
      "Improved validation loss from: 0.030892261862754823  to: 0.03087718188762665\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: 0.03087718188762665  to: 0.030854853987693786\n",
      "Training iteration: 2525\n",
      "Improved validation loss from: 0.030854853987693786  to: 0.03083288073539734\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.03083288073539734  to: 0.03081549108028412\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.03081549108028412  to: 0.03080267906188965\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: 0.03080267906188965  to: 0.03079114854335785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.03079114854335785  to: 0.030777227878570557\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.030777227878570557  to: 0.03075668215751648\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.03075668215751648  to: 0.030734148621559144\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: 0.030734148621559144  to: 0.03071485161781311\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.03071485161781311  to: 0.030700165033340453\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.030700165033340453  to: 0.030689752101898192\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.030689752101898192  to: 0.030677485466003417\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.030677485466003417  to: 0.03065953850746155\n",
      "Training iteration: 2537\n",
      "Improved validation loss from: 0.03065953850746155  to: 0.030637913942337038\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.030637913942337038  to: 0.030618005990982057\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: 0.030618005990982057  to: 0.030603280663490294\n",
      "Training iteration: 2540\n",
      "Improved validation loss from: 0.030603280663490294  to: 0.030590435862541197\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.030590435862541197  to: 0.030577057600021364\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.030577057600021364  to: 0.030560711026191713\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.030560711026191713  to: 0.03054388761520386\n",
      "Training iteration: 2544\n",
      "Improved validation loss from: 0.03054388761520386  to: 0.030527111887931824\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: 0.030527111887931824  to: 0.030511060357093812\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.030511060357093812  to: 0.03049587309360504\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.03049587309360504  to: 0.030479055643081666\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: 0.030479055643081666  to: 0.030461964011192322\n",
      "Training iteration: 2549\n",
      "Improved validation loss from: 0.030461964011192322  to: 0.030445510149002077\n",
      "Training iteration: 2550\n",
      "Improved validation loss from: 0.030445510149002077  to: 0.030432230234146117\n",
      "Training iteration: 2551\n",
      "Improved validation loss from: 0.030432230234146117  to: 0.030419406294822694\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: 0.030419406294822694  to: 0.03040395677089691\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: 0.03040395677089691  to: 0.03038622736930847\n",
      "Training iteration: 2554\n",
      "Improved validation loss from: 0.03038622736930847  to: 0.030365890264511107\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: 0.030365890264511107  to: 0.030347737669944762\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: 0.030347737669944762  to: 0.03033584952354431\n",
      "Training iteration: 2557\n",
      "Improved validation loss from: 0.03033584952354431  to: 0.030327069759368896\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: 0.030327069759368896  to: 0.03031272292137146\n",
      "Training iteration: 2559\n",
      "Improved validation loss from: 0.03031272292137146  to: 0.030294910073280334\n",
      "Training iteration: 2560\n",
      "Improved validation loss from: 0.030294910073280334  to: 0.0302732914686203\n",
      "Training iteration: 2561\n",
      "Improved validation loss from: 0.0302732914686203  to: 0.03025568425655365\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: 0.03025568425655365  to: 0.03024366796016693\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.03024366796016693  to: 0.030231598019599914\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.030231598019599914  to: 0.03021673560142517\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.03021673560142517  to: 0.03020133972167969\n",
      "Training iteration: 2566\n",
      "Improved validation loss from: 0.03020133972167969  to: 0.030185577273368836\n",
      "Training iteration: 2567\n",
      "Improved validation loss from: 0.030185577273368836  to: 0.030169814825057983\n",
      "Training iteration: 2568\n",
      "Improved validation loss from: 0.030169814825057983  to: 0.030155149102211\n",
      "Training iteration: 2569\n",
      "Improved validation loss from: 0.030155149102211  to: 0.030138838291168212\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.030138838291168212  to: 0.030122211575508116\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.030122211575508116  to: 0.030106830596923827\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: 0.030106830596923827  to: 0.03009488582611084\n",
      "Training iteration: 2573\n",
      "Improved validation loss from: 0.03009488582611084  to: 0.03008337914943695\n",
      "Training iteration: 2574\n",
      "Improved validation loss from: 0.03008337914943695  to: 0.030069193243980406\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.030069193243980406  to: 0.03004963994026184\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.03004963994026184  to: 0.030029100179672242\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: 0.030029100179672242  to: 0.030014809966087342\n",
      "Training iteration: 2578\n",
      "Improved validation loss from: 0.030014809966087342  to: 0.030005636811256408\n",
      "Training iteration: 2579\n",
      "Improved validation loss from: 0.030005636811256408  to: 0.029996460676193236\n",
      "Training iteration: 2580\n",
      "Improved validation loss from: 0.029996460676193236  to: 0.029980656504631043\n",
      "Training iteration: 2581\n",
      "Improved validation loss from: 0.029980656504631043  to: 0.02995973825454712\n",
      "Training iteration: 2582\n",
      "Improved validation loss from: 0.02995973825454712  to: 0.029942148923873903\n",
      "Training iteration: 2583\n",
      "Improved validation loss from: 0.029942148923873903  to: 0.029929926991462706\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.029929926991462706  to: 0.02991822361946106\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.02991822361946106  to: 0.029906594753265382\n",
      "Training iteration: 2586\n",
      "Improved validation loss from: 0.029906594753265382  to: 0.02989046275615692\n",
      "Training iteration: 2587\n",
      "Improved validation loss from: 0.02989046275615692  to: 0.02987409234046936\n",
      "Training iteration: 2588\n",
      "Improved validation loss from: 0.02987409234046936  to: 0.02985938489437103\n",
      "Training iteration: 2589\n",
      "Improved validation loss from: 0.02985938489437103  to: 0.029844218492507936\n",
      "Training iteration: 2590\n",
      "Improved validation loss from: 0.029844218492507936  to: 0.02982952296733856\n",
      "Training iteration: 2591\n",
      "Improved validation loss from: 0.02982952296733856  to: 0.02981759011745453\n",
      "Training iteration: 2592\n",
      "Improved validation loss from: 0.02981759011745453  to: 0.029805874824523924\n",
      "Training iteration: 2593\n",
      "Improved validation loss from: 0.029805874824523924  to: 0.029789772629737855\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.029789772629737855  to: 0.029773741960525513\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.029773741960525513  to: 0.029759517312049864\n",
      "Training iteration: 2596\n",
      "Improved validation loss from: 0.029759517312049864  to: 0.029744881391525268\n",
      "Training iteration: 2597\n",
      "Improved validation loss from: 0.029744881391525268  to: 0.02973041534423828\n",
      "Training iteration: 2598\n",
      "Improved validation loss from: 0.02973041534423828  to: 0.02971891164779663\n",
      "Training iteration: 2599\n",
      "Improved validation loss from: 0.02971891164779663  to: 0.029707640409469604\n",
      "Training iteration: 2600\n",
      "Improved validation loss from: 0.029707640409469604  to: 0.029691809415817262\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.029691809415817262  to: 0.029673227667808534\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.029673227667808534  to: 0.029658323526382445\n",
      "Training iteration: 2603\n",
      "Improved validation loss from: 0.029658323526382445  to: 0.029647716879844667\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.029647716879844667  to: 0.029638057947158812\n",
      "Training iteration: 2605\n",
      "Improved validation loss from: 0.029638057947158812  to: 0.029623639583587647\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.029623639583587647  to: 0.02960558533668518\n",
      "Training iteration: 2607\n",
      "Improved validation loss from: 0.02960558533668518  to: 0.02959044575691223\n",
      "Training iteration: 2608\n",
      "Improved validation loss from: 0.02959044575691223  to: 0.029579010605812073\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.029579010605812073  to: 0.029566773772239686\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.029566773772239686  to: 0.029552289843559267\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.029552289843559267  to: 0.02953893840312958\n",
      "Training iteration: 2612\n",
      "Improved validation loss from: 0.02953893840312958  to: 0.02952609658241272\n",
      "Training iteration: 2613\n",
      "Improved validation loss from: 0.02952609658241272  to: 0.029513463377952576\n",
      "Training iteration: 2614\n",
      "Improved validation loss from: 0.029513463377952576  to: 0.029498213529586793\n",
      "Training iteration: 2615\n",
      "Improved validation loss from: 0.029498213529586793  to: 0.029482141137123108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2616\n",
      "Improved validation loss from: 0.029482141137123108  to: 0.029467499256134032\n",
      "Training iteration: 2617\n",
      "Improved validation loss from: 0.029467499256134032  to: 0.029457235336303712\n",
      "Training iteration: 2618\n",
      "Improved validation loss from: 0.029457235336303712  to: 0.029447799921035765\n",
      "Training iteration: 2619\n",
      "Improved validation loss from: 0.029447799921035765  to: 0.029435637593269347\n",
      "Training iteration: 2620\n",
      "Improved validation loss from: 0.029435637593269347  to: 0.029420405626296997\n",
      "Training iteration: 2621\n",
      "Improved validation loss from: 0.029420405626296997  to: 0.029402151703834534\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.029402151703834534  to: 0.029386085271835328\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.029386085271835328  to: 0.02937440276145935\n",
      "Training iteration: 2624\n",
      "Improved validation loss from: 0.02937440276145935  to: 0.02936757206916809\n",
      "Training iteration: 2625\n",
      "Improved validation loss from: 0.02936757206916809  to: 0.029359382390975953\n",
      "Training iteration: 2626\n",
      "Improved validation loss from: 0.029359382390975953  to: 0.029342931509017945\n",
      "Training iteration: 2627\n",
      "Improved validation loss from: 0.029342931509017945  to: 0.029323333501815797\n",
      "Training iteration: 2628\n",
      "Improved validation loss from: 0.029323333501815797  to: 0.029301935434341432\n",
      "Training iteration: 2629\n",
      "Improved validation loss from: 0.029301935434341432  to: 0.029287713766098022\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.029287713766098022  to: 0.02927848994731903\n",
      "Training iteration: 2631\n",
      "Improved validation loss from: 0.02927848994731903  to: 0.02926495671272278\n",
      "Training iteration: 2632\n",
      "Improved validation loss from: 0.02926495671272278  to: 0.029245209693908692\n",
      "Training iteration: 2633\n",
      "Improved validation loss from: 0.029245209693908692  to: 0.0292226642370224\n",
      "Training iteration: 2634\n",
      "Improved validation loss from: 0.0292226642370224  to: 0.029204821586608885\n",
      "Training iteration: 2635\n",
      "Improved validation loss from: 0.029204821586608885  to: 0.029192191362380982\n",
      "Training iteration: 2636\n",
      "Improved validation loss from: 0.029192191362380982  to: 0.02918124794960022\n",
      "Training iteration: 2637\n",
      "Improved validation loss from: 0.02918124794960022  to: 0.029167357087135314\n",
      "Training iteration: 2638\n",
      "Improved validation loss from: 0.029167357087135314  to: 0.02914680540561676\n",
      "Training iteration: 2639\n",
      "Improved validation loss from: 0.02914680540561676  to: 0.029124683141708373\n",
      "Training iteration: 2640\n",
      "Improved validation loss from: 0.029124683141708373  to: 0.029106631875038147\n",
      "Training iteration: 2641\n",
      "Improved validation loss from: 0.029106631875038147  to: 0.029096055030822753\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.029096055030822753  to: 0.0290872186422348\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.0290872186422348  to: 0.029070848226547243\n",
      "Training iteration: 2644\n",
      "Improved validation loss from: 0.029070848226547243  to: 0.029047709703445435\n",
      "Training iteration: 2645\n",
      "Improved validation loss from: 0.029047709703445435  to: 0.029027184844017027\n",
      "Training iteration: 2646\n",
      "Improved validation loss from: 0.029027184844017027  to: 0.029013174772262573\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.029013174772262573  to: 0.029003721475601197\n",
      "Training iteration: 2648\n",
      "Improved validation loss from: 0.029003721475601197  to: 0.02899061143398285\n",
      "Training iteration: 2649\n",
      "Improved validation loss from: 0.02899061143398285  to: 0.028971996903419495\n",
      "Training iteration: 2650\n",
      "Improved validation loss from: 0.028971996903419495  to: 0.028950834274291994\n",
      "Training iteration: 2651\n",
      "Improved validation loss from: 0.028950834274291994  to: 0.028934139013290405\n",
      "Training iteration: 2652\n",
      "Improved validation loss from: 0.028934139013290405  to: 0.028922098875045776\n",
      "Training iteration: 2653\n",
      "Improved validation loss from: 0.028922098875045776  to: 0.028911155462265015\n",
      "Training iteration: 2654\n",
      "Improved validation loss from: 0.028911155462265015  to: 0.02889482080936432\n",
      "Training iteration: 2655\n",
      "Improved validation loss from: 0.02889482080936432  to: 0.028874531388282776\n",
      "Training iteration: 2656\n",
      "Improved validation loss from: 0.028874531388282776  to: 0.028857356309890746\n",
      "Training iteration: 2657\n",
      "Improved validation loss from: 0.028857356309890746  to: 0.02884492576122284\n",
      "Training iteration: 2658\n",
      "Improved validation loss from: 0.02884492576122284  to: 0.02883158326148987\n",
      "Training iteration: 2659\n",
      "Improved validation loss from: 0.02883158326148987  to: 0.02881535589694977\n",
      "Training iteration: 2660\n",
      "Improved validation loss from: 0.02881535589694977  to: 0.02879711389541626\n",
      "Training iteration: 2661\n",
      "Improved validation loss from: 0.02879711389541626  to: 0.028781262040138245\n",
      "Training iteration: 2662\n",
      "Improved validation loss from: 0.028781262040138245  to: 0.028767967224121095\n",
      "Training iteration: 2663\n",
      "Improved validation loss from: 0.028767967224121095  to: 0.028755146265029907\n",
      "Training iteration: 2664\n",
      "Improved validation loss from: 0.028755146265029907  to: 0.028741109371185302\n",
      "Training iteration: 2665\n",
      "Improved validation loss from: 0.028741109371185302  to: 0.02872277796268463\n",
      "Training iteration: 2666\n",
      "Improved validation loss from: 0.02872277796268463  to: 0.028703978657722472\n",
      "Training iteration: 2667\n",
      "Improved validation loss from: 0.028703978657722472  to: 0.028687849640846252\n",
      "Training iteration: 2668\n",
      "Improved validation loss from: 0.028687849640846252  to: 0.028674709796905517\n",
      "Training iteration: 2669\n",
      "Improved validation loss from: 0.028674709796905517  to: 0.028664448857307435\n",
      "Training iteration: 2670\n",
      "Improved validation loss from: 0.028664448857307435  to: 0.02865180969238281\n",
      "Training iteration: 2671\n",
      "Improved validation loss from: 0.02865180969238281  to: 0.02863488793373108\n",
      "Training iteration: 2672\n",
      "Improved validation loss from: 0.02863488793373108  to: 0.02861359417438507\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.02861359417438507  to: 0.02859426736831665\n",
      "Training iteration: 2674\n",
      "Improved validation loss from: 0.02859426736831665  to: 0.028583109378814697\n",
      "Training iteration: 2675\n",
      "Improved validation loss from: 0.028583109378814697  to: 0.028576210141181946\n",
      "Training iteration: 2676\n",
      "Improved validation loss from: 0.028576210141181946  to: 0.028563380241394043\n",
      "Training iteration: 2677\n",
      "Improved validation loss from: 0.028563380241394043  to: 0.02854340076446533\n",
      "Training iteration: 2678\n",
      "Improved validation loss from: 0.02854340076446533  to: 0.0285214900970459\n",
      "Training iteration: 2679\n",
      "Improved validation loss from: 0.0285214900970459  to: 0.02850634455680847\n",
      "Training iteration: 2680\n",
      "Improved validation loss from: 0.02850634455680847  to: 0.028497320413589478\n",
      "Training iteration: 2681\n",
      "Improved validation loss from: 0.028497320413589478  to: 0.02848876416683197\n",
      "Training iteration: 2682\n",
      "Improved validation loss from: 0.02848876416683197  to: 0.028475552797317505\n",
      "Training iteration: 2683\n",
      "Improved validation loss from: 0.028475552797317505  to: 0.028454768657684325\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.028454768657684325  to: 0.028433507680892943\n",
      "Training iteration: 2685\n",
      "Improved validation loss from: 0.028433507680892943  to: 0.028417783975601196\n",
      "Training iteration: 2686\n",
      "Improved validation loss from: 0.028417783975601196  to: 0.028407788276672362\n",
      "Training iteration: 2687\n",
      "Improved validation loss from: 0.028407788276672362  to: 0.028398123383522034\n",
      "Training iteration: 2688\n",
      "Improved validation loss from: 0.028398123383522034  to: 0.028385764360427855\n",
      "Training iteration: 2689\n",
      "Improved validation loss from: 0.028385764360427855  to: 0.028368455171585084\n",
      "Training iteration: 2690\n",
      "Improved validation loss from: 0.028368455171585084  to: 0.028348961472511293\n",
      "Training iteration: 2691\n",
      "Improved validation loss from: 0.028348961472511293  to: 0.028332597017288207\n",
      "Training iteration: 2692\n",
      "Improved validation loss from: 0.028332597017288207  to: 0.028321719169616698\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.028321719169616698  to: 0.028311008214950563\n",
      "Training iteration: 2694\n",
      "Improved validation loss from: 0.028311008214950563  to: 0.028297823667526246\n",
      "Training iteration: 2695\n",
      "Improved validation loss from: 0.028297823667526246  to: 0.028281480073928833\n",
      "Training iteration: 2696\n",
      "Improved validation loss from: 0.028281480073928833  to: 0.028264158964157106\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.028264158964157106  to: 0.02824811339378357\n",
      "Training iteration: 2698\n",
      "Improved validation loss from: 0.02824811339378357  to: 0.02823653817176819\n",
      "Training iteration: 2699\n",
      "Improved validation loss from: 0.02823653817176819  to: 0.0282261461019516\n",
      "Training iteration: 2700\n",
      "Improved validation loss from: 0.0282261461019516  to: 0.028213277459144592\n",
      "Training iteration: 2701\n",
      "Improved validation loss from: 0.028213277459144592  to: 0.02819758951663971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2702\n",
      "Improved validation loss from: 0.02819758951663971  to: 0.0281791090965271\n",
      "Training iteration: 2703\n",
      "Improved validation loss from: 0.0281791090965271  to: 0.02816283404827118\n",
      "Training iteration: 2704\n",
      "Improved validation loss from: 0.02816283404827118  to: 0.02815064787864685\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.02815064787864685  to: 0.028140178322792052\n",
      "Training iteration: 2706\n",
      "Improved validation loss from: 0.028140178322792052  to: 0.028129923343658447\n",
      "Training iteration: 2707\n",
      "Improved validation loss from: 0.028129923343658447  to: 0.02811596095561981\n",
      "Training iteration: 2708\n",
      "Improved validation loss from: 0.02811596095561981  to: 0.028098863363265992\n",
      "Training iteration: 2709\n",
      "Improved validation loss from: 0.028098863363265992  to: 0.028079858422279357\n",
      "Training iteration: 2710\n",
      "Improved validation loss from: 0.028079858422279357  to: 0.02806415855884552\n",
      "Training iteration: 2711\n",
      "Improved validation loss from: 0.02806415855884552  to: 0.028053483366966246\n",
      "Training iteration: 2712\n",
      "Improved validation loss from: 0.028053483366966246  to: 0.028046593070030212\n",
      "Training iteration: 2713\n",
      "Improved validation loss from: 0.028046593070030212  to: 0.02803657054901123\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.02803657054901123  to: 0.02801774740219116\n",
      "Training iteration: 2715\n",
      "Improved validation loss from: 0.02801774740219116  to: 0.02799839377403259\n",
      "Training iteration: 2716\n",
      "Improved validation loss from: 0.02799839377403259  to: 0.027981486916542054\n",
      "Training iteration: 2717\n",
      "Improved validation loss from: 0.027981486916542054  to: 0.027969983220100404\n",
      "Training iteration: 2718\n",
      "Improved validation loss from: 0.027969983220100404  to: 0.02796393632888794\n",
      "Training iteration: 2719\n",
      "Improved validation loss from: 0.02796393632888794  to: 0.02795587182044983\n",
      "Training iteration: 2720\n",
      "Improved validation loss from: 0.02795587182044983  to: 0.0279387891292572\n",
      "Training iteration: 2721\n",
      "Improved validation loss from: 0.0279387891292572  to: 0.02791692316532135\n",
      "Training iteration: 2722\n",
      "Improved validation loss from: 0.02791692316532135  to: 0.027898365259170534\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.027898365259170534  to: 0.027889564633369446\n",
      "Training iteration: 2724\n",
      "Improved validation loss from: 0.027889564633369446  to: 0.027885255217552186\n",
      "Training iteration: 2725\n",
      "Improved validation loss from: 0.027885255217552186  to: 0.027876654267311098\n",
      "Training iteration: 2726\n",
      "Improved validation loss from: 0.027876654267311098  to: 0.027857580780982973\n",
      "Training iteration: 2727\n",
      "Improved validation loss from: 0.027857580780982973  to: 0.02783507704734802\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.02783507704734802  to: 0.02781822681427002\n",
      "Training iteration: 2729\n",
      "Improved validation loss from: 0.02781822681427002  to: 0.02781185507774353\n",
      "Training iteration: 2730\n",
      "Improved validation loss from: 0.02781185507774353  to: 0.027806013822555542\n",
      "Training iteration: 2731\n",
      "Improved validation loss from: 0.027806013822555542  to: 0.02779599130153656\n",
      "Training iteration: 2732\n",
      "Improved validation loss from: 0.02779599130153656  to: 0.02777644097805023\n",
      "Training iteration: 2733\n",
      "Improved validation loss from: 0.02777644097805023  to: 0.02775699198246002\n",
      "Training iteration: 2734\n",
      "Improved validation loss from: 0.02775699198246002  to: 0.02774391770362854\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.02774391770362854  to: 0.02773465812206268\n",
      "Training iteration: 2736\n",
      "Improved validation loss from: 0.02773465812206268  to: 0.027725502848625183\n",
      "Training iteration: 2737\n",
      "Improved validation loss from: 0.027725502848625183  to: 0.027712732553482056\n",
      "Training iteration: 2738\n",
      "Improved validation loss from: 0.027712732553482056  to: 0.027696093916893004\n",
      "Training iteration: 2739\n",
      "Improved validation loss from: 0.027696093916893004  to: 0.02768113315105438\n",
      "Training iteration: 2740\n",
      "Improved validation loss from: 0.02768113315105438  to: 0.027669438719749452\n",
      "Training iteration: 2741\n",
      "Improved validation loss from: 0.027669438719749452  to: 0.027659523487091064\n",
      "Training iteration: 2742\n",
      "Improved validation loss from: 0.027659523487091064  to: 0.027648717164993286\n",
      "Training iteration: 2743\n",
      "Improved validation loss from: 0.027648717164993286  to: 0.02763332426548004\n",
      "Training iteration: 2744\n",
      "Improved validation loss from: 0.02763332426548004  to: 0.027617081999778748\n",
      "Training iteration: 2745\n",
      "Improved validation loss from: 0.027617081999778748  to: 0.02760344445705414\n",
      "Training iteration: 2746\n",
      "Improved validation loss from: 0.02760344445705414  to: 0.027592843770980834\n",
      "Training iteration: 2747\n",
      "Improved validation loss from: 0.027592843770980834  to: 0.027582803368568422\n",
      "Training iteration: 2748\n",
      "Improved validation loss from: 0.027582803368568422  to: 0.02757241725921631\n",
      "Training iteration: 2749\n",
      "Improved validation loss from: 0.02757241725921631  to: 0.027559128403663636\n",
      "Training iteration: 2750\n",
      "Improved validation loss from: 0.027559128403663636  to: 0.027544230222702026\n",
      "Training iteration: 2751\n",
      "Improved validation loss from: 0.027544230222702026  to: 0.02752789855003357\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.02752789855003357  to: 0.02751440405845642\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.02751440405845642  to: 0.02750464975833893\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.02750464975833893  to: 0.027498164772987367\n",
      "Training iteration: 2755\n",
      "Improved validation loss from: 0.027498164772987367  to: 0.027488940954208375\n",
      "Training iteration: 2756\n",
      "Improved validation loss from: 0.027488940954208375  to: 0.027471989393234253\n",
      "Training iteration: 2757\n",
      "Improved validation loss from: 0.027471989393234253  to: 0.027452343702316286\n",
      "Training iteration: 2758\n",
      "Improved validation loss from: 0.027452343702316286  to: 0.027439755201339722\n",
      "Training iteration: 2759\n",
      "Improved validation loss from: 0.027439755201339722  to: 0.027431395649909974\n",
      "Training iteration: 2760\n",
      "Improved validation loss from: 0.027431395649909974  to: 0.02742580771446228\n",
      "Training iteration: 2761\n",
      "Improved validation loss from: 0.02742580771446228  to: 0.027413803339004516\n",
      "Training iteration: 2762\n",
      "Improved validation loss from: 0.027413803339004516  to: 0.027395963668823242\n",
      "Training iteration: 2763\n",
      "Improved validation loss from: 0.027395963668823242  to: 0.027380484342575073\n",
      "Training iteration: 2764\n",
      "Improved validation loss from: 0.027380484342575073  to: 0.027370285987854005\n",
      "Training iteration: 2765\n",
      "Improved validation loss from: 0.027370285987854005  to: 0.02736063301563263\n",
      "Training iteration: 2766\n",
      "Improved validation loss from: 0.02736063301563263  to: 0.027349311113357543\n",
      "Training iteration: 2767\n",
      "Improved validation loss from: 0.027349311113357543  to: 0.027335456013679503\n",
      "Training iteration: 2768\n",
      "Improved validation loss from: 0.027335456013679503  to: 0.027323588728904724\n",
      "Training iteration: 2769\n",
      "Improved validation loss from: 0.027323588728904724  to: 0.027313029766082762\n",
      "Training iteration: 2770\n",
      "Improved validation loss from: 0.027313029766082762  to: 0.027300053834915163\n",
      "Training iteration: 2771\n",
      "Improved validation loss from: 0.027300053834915163  to: 0.027285832166671752\n",
      "Training iteration: 2772\n",
      "Improved validation loss from: 0.027285832166671752  to: 0.027275136113166808\n",
      "Training iteration: 2773\n",
      "Improved validation loss from: 0.027275136113166808  to: 0.02726409137248993\n",
      "Training iteration: 2774\n",
      "Improved validation loss from: 0.02726409137248993  to: 0.027252015471458436\n",
      "Training iteration: 2775\n",
      "Improved validation loss from: 0.027252015471458436  to: 0.027241337299346923\n",
      "Training iteration: 2776\n",
      "Improved validation loss from: 0.027241337299346923  to: 0.0272309809923172\n",
      "Training iteration: 2777\n",
      "Improved validation loss from: 0.0272309809923172  to: 0.02721704840660095\n",
      "Training iteration: 2778\n",
      "Improved validation loss from: 0.02721704840660095  to: 0.027201953530311584\n",
      "Training iteration: 2779\n",
      "Improved validation loss from: 0.027201953530311584  to: 0.027188915014266967\n",
      "Training iteration: 2780\n",
      "Improved validation loss from: 0.027188915014266967  to: 0.027181276679039003\n",
      "Training iteration: 2781\n",
      "Improved validation loss from: 0.027181276679039003  to: 0.027174469828605653\n",
      "Training iteration: 2782\n",
      "Improved validation loss from: 0.027174469828605653  to: 0.027161619067192076\n",
      "Training iteration: 2783\n",
      "Improved validation loss from: 0.027161619067192076  to: 0.027144473791122437\n",
      "Training iteration: 2784\n",
      "Improved validation loss from: 0.027144473791122437  to: 0.02713119387626648\n",
      "Training iteration: 2785\n",
      "Improved validation loss from: 0.02713119387626648  to: 0.027120524644851686\n",
      "Training iteration: 2786\n",
      "Improved validation loss from: 0.027120524644851686  to: 0.027113795280456543\n",
      "Training iteration: 2787\n",
      "Improved validation loss from: 0.027113795280456543  to: 0.02710341811180115\n",
      "Training iteration: 2788\n",
      "Improved validation loss from: 0.02710341811180115  to: 0.027088800072669984\n",
      "Training iteration: 2789\n",
      "Improved validation loss from: 0.027088800072669984  to: 0.02707310616970062\n",
      "Training iteration: 2790\n",
      "Improved validation loss from: 0.02707310616970062  to: 0.027062734961509703\n",
      "Training iteration: 2791\n",
      "Improved validation loss from: 0.027062734961509703  to: 0.027055925130844115\n",
      "Training iteration: 2792\n",
      "Improved validation loss from: 0.027055925130844115  to: 0.02704818844795227\n",
      "Training iteration: 2793\n",
      "Improved validation loss from: 0.02704818844795227  to: 0.027033865451812744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2794\n",
      "Improved validation loss from: 0.027033865451812744  to: 0.027017205953598022\n",
      "Training iteration: 2795\n",
      "Improved validation loss from: 0.027017205953598022  to: 0.027003544569015502\n",
      "Training iteration: 2796\n",
      "Improved validation loss from: 0.027003544569015502  to: 0.02699488401412964\n",
      "Training iteration: 2797\n",
      "Improved validation loss from: 0.02699488401412964  to: 0.026987770199775697\n",
      "Training iteration: 2798\n",
      "Improved validation loss from: 0.026987770199775697  to: 0.026979666948318482\n",
      "Training iteration: 2799\n",
      "Improved validation loss from: 0.026979666948318482  to: 0.02696726322174072\n",
      "Training iteration: 2800\n",
      "Improved validation loss from: 0.02696726322174072  to: 0.026952359080314636\n",
      "Training iteration: 2801\n",
      "Improved validation loss from: 0.026952359080314636  to: 0.026936858892440796\n",
      "Training iteration: 2802\n",
      "Improved validation loss from: 0.026936858892440796  to: 0.0269256591796875\n",
      "Training iteration: 2803\n",
      "Improved validation loss from: 0.0269256591796875  to: 0.02691870629787445\n",
      "Training iteration: 2804\n",
      "Improved validation loss from: 0.02691870629787445  to: 0.02691137492656708\n",
      "Training iteration: 2805\n",
      "Improved validation loss from: 0.02691137492656708  to: 0.026902183890342712\n",
      "Training iteration: 2806\n",
      "Improved validation loss from: 0.026902183890342712  to: 0.02688920497894287\n",
      "Training iteration: 2807\n",
      "Improved validation loss from: 0.02688920497894287  to: 0.026874986290931702\n",
      "Training iteration: 2808\n",
      "Improved validation loss from: 0.026874986290931702  to: 0.026860922574996948\n",
      "Training iteration: 2809\n",
      "Improved validation loss from: 0.026860922574996948  to: 0.026850694417953493\n",
      "Training iteration: 2810\n",
      "Improved validation loss from: 0.026850694417953493  to: 0.026843926310539244\n",
      "Training iteration: 2811\n",
      "Improved validation loss from: 0.026843926310539244  to: 0.026836374402046205\n",
      "Training iteration: 2812\n",
      "Improved validation loss from: 0.026836374402046205  to: 0.026824140548706056\n",
      "Training iteration: 2813\n",
      "Improved validation loss from: 0.026824140548706056  to: 0.0268106609582901\n",
      "Training iteration: 2814\n",
      "Improved validation loss from: 0.0268106609582901  to: 0.026797971129417418\n",
      "Training iteration: 2815\n",
      "Improved validation loss from: 0.026797971129417418  to: 0.026787668466567993\n",
      "Training iteration: 2816\n",
      "Improved validation loss from: 0.026787668466567993  to: 0.02677692174911499\n",
      "Training iteration: 2817\n",
      "Improved validation loss from: 0.02677692174911499  to: 0.02676653265953064\n",
      "Training iteration: 2818\n",
      "Improved validation loss from: 0.02676653265953064  to: 0.02675655484199524\n",
      "Training iteration: 2819\n",
      "Improved validation loss from: 0.02675655484199524  to: 0.02674647867679596\n",
      "Training iteration: 2820\n",
      "Improved validation loss from: 0.02674647867679596  to: 0.026735132932662962\n",
      "Training iteration: 2821\n",
      "Improved validation loss from: 0.026735132932662962  to: 0.026725035905838013\n",
      "Training iteration: 2822\n",
      "Improved validation loss from: 0.026725035905838013  to: 0.026714730262756347\n",
      "Training iteration: 2823\n",
      "Improved validation loss from: 0.026714730262756347  to: 0.026704111695289613\n",
      "Training iteration: 2824\n",
      "Improved validation loss from: 0.026704111695289613  to: 0.026691576838493346\n",
      "Training iteration: 2825\n",
      "Improved validation loss from: 0.026691576838493346  to: 0.02667999267578125\n",
      "Training iteration: 2826\n",
      "Improved validation loss from: 0.02667999267578125  to: 0.026670923829078673\n",
      "Training iteration: 2827\n",
      "Improved validation loss from: 0.026670923829078673  to: 0.02666270136833191\n",
      "Training iteration: 2828\n",
      "Improved validation loss from: 0.02666270136833191  to: 0.026655083894729613\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.026655083894729613  to: 0.026644736528396606\n",
      "Training iteration: 2830\n",
      "Improved validation loss from: 0.026644736528396606  to: 0.02662985324859619\n",
      "Training iteration: 2831\n",
      "Improved validation loss from: 0.02662985324859619  to: 0.026615312695503233\n",
      "Training iteration: 2832\n",
      "Improved validation loss from: 0.026615312695503233  to: 0.026608046889305115\n",
      "Training iteration: 2833\n",
      "Improved validation loss from: 0.026608046889305115  to: 0.026602211594581603\n",
      "Training iteration: 2834\n",
      "Improved validation loss from: 0.026602211594581603  to: 0.02659326195716858\n",
      "Training iteration: 2835\n",
      "Improved validation loss from: 0.02659326195716858  to: 0.026579928398132325\n",
      "Training iteration: 2836\n",
      "Improved validation loss from: 0.026579928398132325  to: 0.02656739354133606\n",
      "Training iteration: 2837\n",
      "Improved validation loss from: 0.02656739354133606  to: 0.02655780017375946\n",
      "Training iteration: 2838\n",
      "Improved validation loss from: 0.02655780017375946  to: 0.026547998189926147\n",
      "Training iteration: 2839\n",
      "Improved validation loss from: 0.026547998189926147  to: 0.026537925004959106\n",
      "Training iteration: 2840\n",
      "Improved validation loss from: 0.026537925004959106  to: 0.026527780294418334\n",
      "Training iteration: 2841\n",
      "Improved validation loss from: 0.026527780294418334  to: 0.02651999592781067\n",
      "Training iteration: 2842\n",
      "Improved validation loss from: 0.02651999592781067  to: 0.026512056589126587\n",
      "Training iteration: 2843\n",
      "Improved validation loss from: 0.026512056589126587  to: 0.026499733328819275\n",
      "Training iteration: 2844\n",
      "Improved validation loss from: 0.026499733328819275  to: 0.026485970616340636\n",
      "Training iteration: 2845\n",
      "Improved validation loss from: 0.026485970616340636  to: 0.026474538445472717\n",
      "Training iteration: 2846\n",
      "Improved validation loss from: 0.026474538445472717  to: 0.026466530561447144\n",
      "Training iteration: 2847\n",
      "Improved validation loss from: 0.026466530561447144  to: 0.026461872458457946\n",
      "Training iteration: 2848\n",
      "Improved validation loss from: 0.026461872458457946  to: 0.026454722881317137\n",
      "Training iteration: 2849\n",
      "Improved validation loss from: 0.026454722881317137  to: 0.026440033316612245\n",
      "Training iteration: 2850\n",
      "Improved validation loss from: 0.026440033316612245  to: 0.02642363905906677\n",
      "Training iteration: 2851\n",
      "Improved validation loss from: 0.02642363905906677  to: 0.026414749026298524\n",
      "Training iteration: 2852\n",
      "Improved validation loss from: 0.026414749026298524  to: 0.02640959620475769\n",
      "Training iteration: 2853\n",
      "Improved validation loss from: 0.02640959620475769  to: 0.02640320360660553\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.02640320360660553  to: 0.026392310857772827\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.026392310857772827  to: 0.026380586624145507\n",
      "Training iteration: 2856\n",
      "Improved validation loss from: 0.026380586624145507  to: 0.026370060443878175\n",
      "Training iteration: 2857\n",
      "Improved validation loss from: 0.026370060443878175  to: 0.02635912299156189\n",
      "Training iteration: 2858\n",
      "Improved validation loss from: 0.02635912299156189  to: 0.026349225640296937\n",
      "Training iteration: 2859\n",
      "Improved validation loss from: 0.026349225640296937  to: 0.026340579986572264\n",
      "Training iteration: 2860\n",
      "Improved validation loss from: 0.026340579986572264  to: 0.026334720849990844\n",
      "Training iteration: 2861\n",
      "Improved validation loss from: 0.026334720849990844  to: 0.02632516324520111\n",
      "Training iteration: 2862\n",
      "Improved validation loss from: 0.02632516324520111  to: 0.026311999559402464\n",
      "Training iteration: 2863\n",
      "Improved validation loss from: 0.026311999559402464  to: 0.02630155682563782\n",
      "Training iteration: 2864\n",
      "Improved validation loss from: 0.02630155682563782  to: 0.02629450261592865\n",
      "Training iteration: 2865\n",
      "Improved validation loss from: 0.02629450261592865  to: 0.026285713911056517\n",
      "Training iteration: 2866\n",
      "Improved validation loss from: 0.026285713911056517  to: 0.026275244355201722\n",
      "Training iteration: 2867\n",
      "Improved validation loss from: 0.026275244355201722  to: 0.026264289021492006\n",
      "Training iteration: 2868\n",
      "Improved validation loss from: 0.026264289021492006  to: 0.026254934072494508\n",
      "Training iteration: 2869\n",
      "Improved validation loss from: 0.026254934072494508  to: 0.026246121525764464\n",
      "Training iteration: 2870\n",
      "Improved validation loss from: 0.026246121525764464  to: 0.026239079236984254\n",
      "Training iteration: 2871\n",
      "Improved validation loss from: 0.026239079236984254  to: 0.02623089253902435\n",
      "Training iteration: 2872\n",
      "Improved validation loss from: 0.02623089253902435  to: 0.026220434904098512\n",
      "Training iteration: 2873\n",
      "Improved validation loss from: 0.026220434904098512  to: 0.026207274198532103\n",
      "Training iteration: 2874\n",
      "Improved validation loss from: 0.026207274198532103  to: 0.026196292042732237\n",
      "Training iteration: 2875\n",
      "Improved validation loss from: 0.026196292042732237  to: 0.02618968188762665\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.02618968188762665  to: 0.026184293627738952\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.026184293627738952  to: 0.02617579996585846\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.02617579996585846  to: 0.02616291642189026\n",
      "Training iteration: 2879\n",
      "Improved validation loss from: 0.02616291642189026  to: 0.02615157961845398\n",
      "Training iteration: 2880\n",
      "Improved validation loss from: 0.02615157961845398  to: 0.026143410801887514\n",
      "Training iteration: 2881\n",
      "Improved validation loss from: 0.026143410801887514  to: 0.026137572526931763\n",
      "Training iteration: 2882\n",
      "Improved validation loss from: 0.026137572526931763  to: 0.026128774881362914\n",
      "Training iteration: 2883\n",
      "Improved validation loss from: 0.026128774881362914  to: 0.026117998361587524\n",
      "Training iteration: 2884\n",
      "Improved validation loss from: 0.026117998361587524  to: 0.026107370853424072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2885\n",
      "Improved validation loss from: 0.026107370853424072  to: 0.02609871029853821\n",
      "Training iteration: 2886\n",
      "Improved validation loss from: 0.02609871029853821  to: 0.02609083354473114\n",
      "Training iteration: 2887\n",
      "Improved validation loss from: 0.02609083354473114  to: 0.02608448565006256\n",
      "Training iteration: 2888\n",
      "Improved validation loss from: 0.02608448565006256  to: 0.02607661783695221\n",
      "Training iteration: 2889\n",
      "Improved validation loss from: 0.02607661783695221  to: 0.026064082980155945\n",
      "Training iteration: 2890\n",
      "Improved validation loss from: 0.026064082980155945  to: 0.02605162262916565\n",
      "Training iteration: 2891\n",
      "Improved validation loss from: 0.02605162262916565  to: 0.02604313790798187\n",
      "Training iteration: 2892\n",
      "Improved validation loss from: 0.02604313790798187  to: 0.026040250062942506\n",
      "Training iteration: 2893\n",
      "Improved validation loss from: 0.026040250062942506  to: 0.026033970713615417\n",
      "Training iteration: 2894\n",
      "Improved validation loss from: 0.026033970713615417  to: 0.026021689176559448\n",
      "Training iteration: 2895\n",
      "Improved validation loss from: 0.026021689176559448  to: 0.02600981593132019\n",
      "Training iteration: 2896\n",
      "Improved validation loss from: 0.02600981593132019  to: 0.02600184977054596\n",
      "Training iteration: 2897\n",
      "Improved validation loss from: 0.02600184977054596  to: 0.025994709134101866\n",
      "Training iteration: 2898\n",
      "Improved validation loss from: 0.025994709134101866  to: 0.025987076759338378\n",
      "Training iteration: 2899\n",
      "Improved validation loss from: 0.025987076759338378  to: 0.025978213548660277\n",
      "Training iteration: 2900\n",
      "Improved validation loss from: 0.025978213548660277  to: 0.02596834599971771\n",
      "Training iteration: 2901\n",
      "Improved validation loss from: 0.02596834599971771  to: 0.02595825493335724\n",
      "Training iteration: 2902\n",
      "Improved validation loss from: 0.02595825493335724  to: 0.025948643684387207\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.025948643684387207  to: 0.025942298769950866\n",
      "Training iteration: 2904\n",
      "Improved validation loss from: 0.025942298769950866  to: 0.025936323404312133\n",
      "Training iteration: 2905\n",
      "Improved validation loss from: 0.025936323404312133  to: 0.025928360223770142\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: 0.025928360223770142  to: 0.02591640055179596\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.02591640055179596  to: 0.02590518295764923\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.02590518295764923  to: 0.025897660851478578\n",
      "Training iteration: 2909\n",
      "Improved validation loss from: 0.025897660851478578  to: 0.02589263916015625\n",
      "Training iteration: 2910\n",
      "Improved validation loss from: 0.02589263916015625  to: 0.02588580548763275\n",
      "Training iteration: 2911\n",
      "Improved validation loss from: 0.02588580548763275  to: 0.02587513029575348\n",
      "Training iteration: 2912\n",
      "Improved validation loss from: 0.02587513029575348  to: 0.025865504145622255\n",
      "Training iteration: 2913\n",
      "Improved validation loss from: 0.025865504145622255  to: 0.02585793733596802\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.02585793733596802  to: 0.025852030515670775\n",
      "Training iteration: 2915\n",
      "Improved validation loss from: 0.025852030515670775  to: 0.02584342360496521\n",
      "Training iteration: 2916\n",
      "Improved validation loss from: 0.02584342360496521  to: 0.025833123922348024\n",
      "Training iteration: 2917\n",
      "Improved validation loss from: 0.025833123922348024  to: 0.025823920965194702\n",
      "Training iteration: 2918\n",
      "Improved validation loss from: 0.025823920965194702  to: 0.025816524028778078\n",
      "Training iteration: 2919\n",
      "Improved validation loss from: 0.025816524028778078  to: 0.025809675455093384\n",
      "Training iteration: 2920\n",
      "Improved validation loss from: 0.025809675455093384  to: 0.025801113247871398\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.025801113247871398  to: 0.025790920853614806\n",
      "Training iteration: 2922\n",
      "Improved validation loss from: 0.025790920853614806  to: 0.02578299641609192\n",
      "Training iteration: 2923\n",
      "Improved validation loss from: 0.02578299641609192  to: 0.02577664852142334\n",
      "Training iteration: 2924\n",
      "Improved validation loss from: 0.02577664852142334  to: 0.02577017843723297\n",
      "Training iteration: 2925\n",
      "Improved validation loss from: 0.02577017843723297  to: 0.02576013207435608\n",
      "Training iteration: 2926\n",
      "Improved validation loss from: 0.02576013207435608  to: 0.025749507546424865\n",
      "Training iteration: 2927\n",
      "Improved validation loss from: 0.025749507546424865  to: 0.02574164867401123\n",
      "Training iteration: 2928\n",
      "Improved validation loss from: 0.02574164867401123  to: 0.02573625445365906\n",
      "Training iteration: 2929\n",
      "Improved validation loss from: 0.02573625445365906  to: 0.0257301390171051\n",
      "Training iteration: 2930\n",
      "Improved validation loss from: 0.0257301390171051  to: 0.025720825791358946\n",
      "Training iteration: 2931\n",
      "Improved validation loss from: 0.025720825791358946  to: 0.025712060928344726\n",
      "Training iteration: 2932\n",
      "Improved validation loss from: 0.025712060928344726  to: 0.025704437494277955\n",
      "Training iteration: 2933\n",
      "Improved validation loss from: 0.025704437494277955  to: 0.025695428252220154\n",
      "Training iteration: 2934\n",
      "Improved validation loss from: 0.025695428252220154  to: 0.025686734914779664\n",
      "Training iteration: 2935\n",
      "Improved validation loss from: 0.025686734914779664  to: 0.025679337978363036\n",
      "Training iteration: 2936\n",
      "Improved validation loss from: 0.025679337978363036  to: 0.02567535638809204\n",
      "Training iteration: 2937\n",
      "Improved validation loss from: 0.02567535638809204  to: 0.02566773593425751\n",
      "Training iteration: 2938\n",
      "Improved validation loss from: 0.02566773593425751  to: 0.025656494498252868\n",
      "Training iteration: 2939\n",
      "Improved validation loss from: 0.025656494498252868  to: 0.02564542889595032\n",
      "Training iteration: 2940\n",
      "Improved validation loss from: 0.02564542889595032  to: 0.025640267133712768\n",
      "Training iteration: 2941\n",
      "Improved validation loss from: 0.025640267133712768  to: 0.025638127326965333\n",
      "Training iteration: 2942\n",
      "Improved validation loss from: 0.025638127326965333  to: 0.02563069760799408\n",
      "Training iteration: 2943\n",
      "Improved validation loss from: 0.02563069760799408  to: 0.02561868727207184\n",
      "Training iteration: 2944\n",
      "Improved validation loss from: 0.02561868727207184  to: 0.0256072998046875\n",
      "Training iteration: 2945\n",
      "Improved validation loss from: 0.0256072998046875  to: 0.025600156188011168\n",
      "Training iteration: 2946\n",
      "Improved validation loss from: 0.025600156188011168  to: 0.02559588849544525\n",
      "Training iteration: 2947\n",
      "Improved validation loss from: 0.02559588849544525  to: 0.0255901038646698\n",
      "Training iteration: 2948\n",
      "Improved validation loss from: 0.0255901038646698  to: 0.02558276951313019\n",
      "Training iteration: 2949\n",
      "Improved validation loss from: 0.02558276951313019  to: 0.025573807954788207\n",
      "Training iteration: 2950\n",
      "Improved validation loss from: 0.025573807954788207  to: 0.025565457344055176\n",
      "Training iteration: 2951\n",
      "Improved validation loss from: 0.025565457344055176  to: 0.025556591153144837\n",
      "Training iteration: 2952\n",
      "Improved validation loss from: 0.025556591153144837  to: 0.025549611449241637\n",
      "Training iteration: 2953\n",
      "Improved validation loss from: 0.025549611449241637  to: 0.025544100999832155\n",
      "Training iteration: 2954\n",
      "Improved validation loss from: 0.025544100999832155  to: 0.025538098812103272\n",
      "Training iteration: 2955\n",
      "Improved validation loss from: 0.025538098812103272  to: 0.025529736280441286\n",
      "Training iteration: 2956\n",
      "Improved validation loss from: 0.025529736280441286  to: 0.025519877672195435\n",
      "Training iteration: 2957\n",
      "Improved validation loss from: 0.025519877672195435  to: 0.025510483980178834\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.025510483980178834  to: 0.025503042340278625\n",
      "Training iteration: 2959\n",
      "Improved validation loss from: 0.025503042340278625  to: 0.025498992204666136\n",
      "Training iteration: 2960\n",
      "Improved validation loss from: 0.025498992204666136  to: 0.025494349002838135\n",
      "Training iteration: 2961\n",
      "Improved validation loss from: 0.025494349002838135  to: 0.025486645102500916\n",
      "Training iteration: 2962\n",
      "Improved validation loss from: 0.025486645102500916  to: 0.025474968552589416\n",
      "Training iteration: 2963\n",
      "Improved validation loss from: 0.025474968552589416  to: 0.02546502649784088\n",
      "Training iteration: 2964\n",
      "Improved validation loss from: 0.02546502649784088  to: 0.02546021342277527\n",
      "Training iteration: 2965\n",
      "Improved validation loss from: 0.02546021342277527  to: 0.025457489490509033\n",
      "Training iteration: 2966\n",
      "Improved validation loss from: 0.025457489490509033  to: 0.025451576709747313\n",
      "Training iteration: 2967\n",
      "Improved validation loss from: 0.025451576709747313  to: 0.025441089272499086\n",
      "Training iteration: 2968\n",
      "Improved validation loss from: 0.025441089272499086  to: 0.025429335236549378\n",
      "Training iteration: 2969\n",
      "Improved validation loss from: 0.025429335236549378  to: 0.025423330068588258\n",
      "Training iteration: 2970\n",
      "Improved validation loss from: 0.025423330068588258  to: 0.025421193242073058\n",
      "Training iteration: 2971\n",
      "Improved validation loss from: 0.025421193242073058  to: 0.025414949655532836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2972\n",
      "Improved validation loss from: 0.025414949655532836  to: 0.02540428042411804\n",
      "Training iteration: 2973\n",
      "Improved validation loss from: 0.02540428042411804  to: 0.025393813848495483\n",
      "Training iteration: 2974\n",
      "Improved validation loss from: 0.025393813848495483  to: 0.025389665365219118\n",
      "Training iteration: 2975\n",
      "Improved validation loss from: 0.025389665365219118  to: 0.02538580894470215\n",
      "Training iteration: 2976\n",
      "Improved validation loss from: 0.02538580894470215  to: 0.02537875771522522\n",
      "Training iteration: 2977\n",
      "Improved validation loss from: 0.02537875771522522  to: 0.025368773937225343\n",
      "Training iteration: 2978\n",
      "Improved validation loss from: 0.025368773937225343  to: 0.025359320640563964\n",
      "Training iteration: 2979\n",
      "Improved validation loss from: 0.025359320640563964  to: 0.025355267524719238\n",
      "Training iteration: 2980\n",
      "Improved validation loss from: 0.025355267524719238  to: 0.02535311281681061\n",
      "Training iteration: 2981\n",
      "Improved validation loss from: 0.02535311281681061  to: 0.025345379114151002\n",
      "Training iteration: 2982\n",
      "Improved validation loss from: 0.025345379114151002  to: 0.025333875417709352\n",
      "Training iteration: 2983\n",
      "Improved validation loss from: 0.025333875417709352  to: 0.025324234366416933\n",
      "Training iteration: 2984\n",
      "Improved validation loss from: 0.025324234366416933  to: 0.025319191813468932\n",
      "Training iteration: 2985\n",
      "Improved validation loss from: 0.025319191813468932  to: 0.025316137075424194\n",
      "Training iteration: 2986\n",
      "Improved validation loss from: 0.025316137075424194  to: 0.025310248136520386\n",
      "Training iteration: 2987\n",
      "Improved validation loss from: 0.025310248136520386  to: 0.025302711129188537\n",
      "Training iteration: 2988\n",
      "Improved validation loss from: 0.025302711129188537  to: 0.02529444694519043\n",
      "Training iteration: 2989\n",
      "Improved validation loss from: 0.02529444694519043  to: 0.025284957885742188\n",
      "Training iteration: 2990\n",
      "Improved validation loss from: 0.025284957885742188  to: 0.02527783215045929\n",
      "Training iteration: 2991\n",
      "Improved validation loss from: 0.02527783215045929  to: 0.025273531675338745\n",
      "Training iteration: 2992\n",
      "Improved validation loss from: 0.025273531675338745  to: 0.025269326567649842\n",
      "Training iteration: 2993\n",
      "Improved validation loss from: 0.025269326567649842  to: 0.025262057781219482\n",
      "Training iteration: 2994\n",
      "Improved validation loss from: 0.025262057781219482  to: 0.02525497376918793\n",
      "Training iteration: 2995\n",
      "Improved validation loss from: 0.02525497376918793  to: 0.025245743989944457\n",
      "Training iteration: 2996\n",
      "Improved validation loss from: 0.025245743989944457  to: 0.025239911675453187\n",
      "Training iteration: 2997\n",
      "Improved validation loss from: 0.025239911675453187  to: 0.025234240293502807\n",
      "Training iteration: 2998\n",
      "Improved validation loss from: 0.025234240293502807  to: 0.025228220224380492\n",
      "Training iteration: 2999\n",
      "Improved validation loss from: 0.025228220224380492  to: 0.025221508741378785\n",
      "Training iteration: 3000\n",
      "Improved validation loss from: 0.025221508741378785  to: 0.025214213132858276\n",
      "Training iteration: 3001\n",
      "Improved validation loss from: 0.025214213132858276  to: 0.02520681023597717\n",
      "Training iteration: 3002\n",
      "Improved validation loss from: 0.02520681023597717  to: 0.025202247500419616\n",
      "Training iteration: 3003\n",
      "Improved validation loss from: 0.025202247500419616  to: 0.025197991728782655\n",
      "Training iteration: 3004\n",
      "Improved validation loss from: 0.025197991728782655  to: 0.025189331173896788\n",
      "Training iteration: 3005\n",
      "Improved validation loss from: 0.025189331173896788  to: 0.025179758667945862\n",
      "Training iteration: 3006\n",
      "Improved validation loss from: 0.025179758667945862  to: 0.025173169374465943\n",
      "Training iteration: 3007\n",
      "Improved validation loss from: 0.025173169374465943  to: 0.0251695454120636\n",
      "Training iteration: 3008\n",
      "Improved validation loss from: 0.0251695454120636  to: 0.025165274739265442\n",
      "Training iteration: 3009\n",
      "Improved validation loss from: 0.025165274739265442  to: 0.025157254934310914\n",
      "Training iteration: 3010\n",
      "Improved validation loss from: 0.025157254934310914  to: 0.025147074460983278\n",
      "Training iteration: 3011\n",
      "Improved validation loss from: 0.025147074460983278  to: 0.025140935182571413\n",
      "Training iteration: 3012\n",
      "Improved validation loss from: 0.025140935182571413  to: 0.02513830065727234\n",
      "Training iteration: 3013\n",
      "Improved validation loss from: 0.02513830065727234  to: 0.025135451555252077\n",
      "Training iteration: 3014\n",
      "Improved validation loss from: 0.025135451555252077  to: 0.025127106904983522\n",
      "Training iteration: 3015\n",
      "Improved validation loss from: 0.025127106904983522  to: 0.025117003917694093\n",
      "Training iteration: 3016\n",
      "Improved validation loss from: 0.025117003917694093  to: 0.025110039114952087\n",
      "Training iteration: 3017\n",
      "Improved validation loss from: 0.025110039114952087  to: 0.025106918811798096\n",
      "Training iteration: 3018\n",
      "Improved validation loss from: 0.025106918811798096  to: 0.025103530287742613\n",
      "Training iteration: 3019\n",
      "Improved validation loss from: 0.025103530287742613  to: 0.025096315145492553\n",
      "Training iteration: 3020\n",
      "Improved validation loss from: 0.025096315145492553  to: 0.025086313486099243\n",
      "Training iteration: 3021\n",
      "Improved validation loss from: 0.025086313486099243  to: 0.02507736086845398\n",
      "Training iteration: 3022\n",
      "Improved validation loss from: 0.02507736086845398  to: 0.02507180571556091\n",
      "Training iteration: 3023\n",
      "Improved validation loss from: 0.02507180571556091  to: 0.02506779134273529\n",
      "Training iteration: 3024\n",
      "Improved validation loss from: 0.02506779134273529  to: 0.025064697861671446\n",
      "Training iteration: 3025\n",
      "Improved validation loss from: 0.025064697861671446  to: 0.025058692693710326\n",
      "Training iteration: 3026\n",
      "Improved validation loss from: 0.025058692693710326  to: 0.025050625205039978\n",
      "Training iteration: 3027\n",
      "Improved validation loss from: 0.025050625205039978  to: 0.0250413179397583\n",
      "Training iteration: 3028\n",
      "Improved validation loss from: 0.0250413179397583  to: 0.0250355064868927\n",
      "Training iteration: 3029\n",
      "Improved validation loss from: 0.0250355064868927  to: 0.02503354549407959\n",
      "Training iteration: 3030\n",
      "Improved validation loss from: 0.02503354549407959  to: 0.025030866265296936\n",
      "Training iteration: 3031\n",
      "Improved validation loss from: 0.025030866265296936  to: 0.02502371370792389\n",
      "Training iteration: 3032\n",
      "Improved validation loss from: 0.02502371370792389  to: 0.025013580918312073\n",
      "Training iteration: 3033\n",
      "Improved validation loss from: 0.025013580918312073  to: 0.02500479817390442\n",
      "Training iteration: 3034\n",
      "Improved validation loss from: 0.02500479817390442  to: 0.02499968707561493\n",
      "Training iteration: 3035\n",
      "Improved validation loss from: 0.02499968707561493  to: 0.0249965101480484\n",
      "Training iteration: 3036\n",
      "Improved validation loss from: 0.0249965101480484  to: 0.024993868172168733\n",
      "Training iteration: 3037\n",
      "Improved validation loss from: 0.024993868172168733  to: 0.024987931549549102\n",
      "Training iteration: 3038\n",
      "Improved validation loss from: 0.024987931549549102  to: 0.024976782500743866\n",
      "Training iteration: 3039\n",
      "Improved validation loss from: 0.024976782500743866  to: 0.024967126548290253\n",
      "Training iteration: 3040\n",
      "Improved validation loss from: 0.024967126548290253  to: 0.02496340274810791\n",
      "Training iteration: 3041\n",
      "Improved validation loss from: 0.02496340274810791  to: 0.024962830543518066\n",
      "Training iteration: 3042\n",
      "Improved validation loss from: 0.024962830543518066  to: 0.024959126114845277\n",
      "Training iteration: 3043\n",
      "Improved validation loss from: 0.024959126114845277  to: 0.024952831864356994\n",
      "Training iteration: 3044\n",
      "Improved validation loss from: 0.024952831864356994  to: 0.02494232654571533\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: 0.02494232654571533  to: 0.02493349611759186\n",
      "Training iteration: 3046\n",
      "Improved validation loss from: 0.02493349611759186  to: 0.02492956668138504\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.024930696189403533\n",
      "Training iteration: 3048\n",
      "Improved validation loss from: 0.02492956668138504  to: 0.024926483631134033\n",
      "Training iteration: 3049\n",
      "Improved validation loss from: 0.024926483631134033  to: 0.0249159574508667\n",
      "Training iteration: 3050\n",
      "Improved validation loss from: 0.0249159574508667  to: 0.024905307590961455\n",
      "Training iteration: 3051\n",
      "Improved validation loss from: 0.024905307590961455  to: 0.024902625381946562\n",
      "Training iteration: 3052\n",
      "Improved validation loss from: 0.024902625381946562  to: 0.024901227653026582\n",
      "Training iteration: 3053\n",
      "Improved validation loss from: 0.024901227653026582  to: 0.02489645928144455\n",
      "Training iteration: 3054\n",
      "Improved validation loss from: 0.02489645928144455  to: 0.024887588620185853\n",
      "Training iteration: 3055\n",
      "Improved validation loss from: 0.024887588620185853  to: 0.024878697097301485\n",
      "Training iteration: 3056\n",
      "Improved validation loss from: 0.024878697097301485  to: 0.024873287975788118\n",
      "Training iteration: 3057\n",
      "Improved validation loss from: 0.024873287975788118  to: 0.024873022735118867\n",
      "Training iteration: 3058\n",
      "Improved validation loss from: 0.024873022735118867  to: 0.024868826568126678\n",
      "Training iteration: 3059\n",
      "Improved validation loss from: 0.024868826568126678  to: 0.02485935240983963\n",
      "Training iteration: 3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02485935240983963  to: 0.024852514266967773\n",
      "Training iteration: 3061\n",
      "Improved validation loss from: 0.024852514266967773  to: 0.024847495555877685\n",
      "Training iteration: 3062\n",
      "Improved validation loss from: 0.024847495555877685  to: 0.024843978881835937\n",
      "Training iteration: 3063\n",
      "Improved validation loss from: 0.024843978881835937  to: 0.02483993023633957\n",
      "Training iteration: 3064\n",
      "Improved validation loss from: 0.02483993023633957  to: 0.024833691120147706\n",
      "Training iteration: 3065\n",
      "Improved validation loss from: 0.024833691120147706  to: 0.02482616901397705\n",
      "Training iteration: 3066\n",
      "Improved validation loss from: 0.02482616901397705  to: 0.02481934130191803\n",
      "Training iteration: 3067\n",
      "Improved validation loss from: 0.02481934130191803  to: 0.024816890060901643\n",
      "Training iteration: 3068\n",
      "Improved validation loss from: 0.024816890060901643  to: 0.02481544464826584\n",
      "Training iteration: 3069\n",
      "Improved validation loss from: 0.02481544464826584  to: 0.024808232486248017\n",
      "Training iteration: 3070\n",
      "Improved validation loss from: 0.024808232486248017  to: 0.024798682332038878\n",
      "Training iteration: 3071\n",
      "Improved validation loss from: 0.024798682332038878  to: 0.024792246520519257\n",
      "Training iteration: 3072\n",
      "Improved validation loss from: 0.024792246520519257  to: 0.024789872765541076\n",
      "Training iteration: 3073\n",
      "Improved validation loss from: 0.024789872765541076  to: 0.02478756904602051\n",
      "Training iteration: 3074\n",
      "Improved validation loss from: 0.02478756904602051  to: 0.024781425297260285\n",
      "Training iteration: 3075\n",
      "Improved validation loss from: 0.024781425297260285  to: 0.024772214889526366\n",
      "Training iteration: 3076\n",
      "Improved validation loss from: 0.024772214889526366  to: 0.02476428300142288\n",
      "Training iteration: 3077\n",
      "Improved validation loss from: 0.02476428300142288  to: 0.02476266324520111\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.024762873351573945\n",
      "Training iteration: 3079\n",
      "Improved validation loss from: 0.02476266324520111  to: 0.02475622147321701\n",
      "Training iteration: 3080\n",
      "Improved validation loss from: 0.02475622147321701  to: 0.0247457891702652\n",
      "Training iteration: 3081\n",
      "Improved validation loss from: 0.0247457891702652  to: 0.0247382253408432\n",
      "Training iteration: 3082\n",
      "Improved validation loss from: 0.0247382253408432  to: 0.024736237525939942\n",
      "Training iteration: 3083\n",
      "Improved validation loss from: 0.024736237525939942  to: 0.024735574424266816\n",
      "Training iteration: 3084\n",
      "Improved validation loss from: 0.024735574424266816  to: 0.024730613827705382\n",
      "Training iteration: 3085\n",
      "Improved validation loss from: 0.024730613827705382  to: 0.024721142649650574\n",
      "Training iteration: 3086\n",
      "Improved validation loss from: 0.024721142649650574  to: 0.024712231755256654\n",
      "Training iteration: 3087\n",
      "Improved validation loss from: 0.024712231755256654  to: 0.02471020966768265\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.024711127579212188\n",
      "Training iteration: 3089\n",
      "Improved validation loss from: 0.02471020966768265  to: 0.024705585837364197\n",
      "Training iteration: 3090\n",
      "Improved validation loss from: 0.024705585837364197  to: 0.02469543516635895\n",
      "Training iteration: 3091\n",
      "Improved validation loss from: 0.02469543516635895  to: 0.024687406420707703\n",
      "Training iteration: 3092\n",
      "Improved validation loss from: 0.024687406420707703  to: 0.024684938788414\n",
      "Training iteration: 3093\n",
      "Improved validation loss from: 0.024684938788414  to: 0.02468440532684326\n",
      "Training iteration: 3094\n",
      "Improved validation loss from: 0.02468440532684326  to: 0.024680104851722718\n",
      "Training iteration: 3095\n",
      "Improved validation loss from: 0.024680104851722718  to: 0.024671168625354768\n",
      "Training iteration: 3096\n",
      "Improved validation loss from: 0.024671168625354768  to: 0.02466208040714264\n",
      "Training iteration: 3097\n",
      "Improved validation loss from: 0.02466208040714264  to: 0.024657221138477327\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.024658098816871643\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.024657630920410158\n",
      "Training iteration: 3100\n",
      "Improved validation loss from: 0.024657221138477327  to: 0.02464883029460907\n",
      "Training iteration: 3101\n",
      "Improved validation loss from: 0.02464883029460907  to: 0.024637703597545624\n",
      "Training iteration: 3102\n",
      "Improved validation loss from: 0.024637703597545624  to: 0.02463229149580002\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.02463286817073822\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.024632596969604494\n",
      "Training iteration: 3105\n",
      "Improved validation loss from: 0.02463229149580002  to: 0.02462609261274338\n",
      "Training iteration: 3106\n",
      "Improved validation loss from: 0.02462609261274338  to: 0.024615421891212463\n",
      "Training iteration: 3107\n",
      "Improved validation loss from: 0.024615421891212463  to: 0.02460753172636032\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.02460799664258957\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.024610193073749544\n",
      "Training iteration: 3110\n",
      "Improved validation loss from: 0.02460753172636032  to: 0.024603605270385742\n",
      "Training iteration: 3111\n",
      "Improved validation loss from: 0.024603605270385742  to: 0.024591967463493347\n",
      "Training iteration: 3112\n",
      "Improved validation loss from: 0.024591967463493347  to: 0.02458431273698807\n",
      "Training iteration: 3113\n",
      "Improved validation loss from: 0.02458431273698807  to: 0.024583785235881804\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.024584741890430452\n",
      "Training iteration: 3115\n",
      "Improved validation loss from: 0.024583785235881804  to: 0.024580101668834686\n",
      "Training iteration: 3116\n",
      "Improved validation loss from: 0.024580101668834686  to: 0.02456996440887451\n",
      "Training iteration: 3117\n",
      "Improved validation loss from: 0.02456996440887451  to: 0.02456078827381134\n",
      "Training iteration: 3118\n",
      "Improved validation loss from: 0.02456078827381134  to: 0.02455734759569168\n",
      "Training iteration: 3119\n",
      "Improved validation loss from: 0.02455734759569168  to: 0.024557001888751984\n",
      "Training iteration: 3120\n",
      "Improved validation loss from: 0.024557001888751984  to: 0.024556806683540343\n",
      "Training iteration: 3121\n",
      "Improved validation loss from: 0.024556806683540343  to: 0.02455163300037384\n",
      "Training iteration: 3122\n",
      "Improved validation loss from: 0.02455163300037384  to: 0.024540796875953674\n",
      "Training iteration: 3123\n",
      "Improved validation loss from: 0.024540796875953674  to: 0.024533072113990785\n",
      "Training iteration: 3124\n",
      "Improved validation loss from: 0.024533072113990785  to: 0.024532535672187807\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.02453422099351883\n",
      "Training iteration: 3126\n",
      "Improved validation loss from: 0.024532535672187807  to: 0.02453055828809738\n",
      "Training iteration: 3127\n",
      "Improved validation loss from: 0.02453055828809738  to: 0.024520882964134218\n",
      "Training iteration: 3128\n",
      "Improved validation loss from: 0.024520882964134218  to: 0.024511590600013733\n",
      "Training iteration: 3129\n",
      "Improved validation loss from: 0.024511590600013733  to: 0.024507752060890196\n",
      "Training iteration: 3130\n",
      "Improved validation loss from: 0.024507752060890196  to: 0.024507613480091096\n",
      "Training iteration: 3131\n",
      "Improved validation loss from: 0.024507613480091096  to: 0.024505309760570526\n",
      "Training iteration: 3132\n",
      "Improved validation loss from: 0.024505309760570526  to: 0.024500446021556856\n",
      "Training iteration: 3133\n",
      "Improved validation loss from: 0.024500446021556856  to: 0.0244938924908638\n",
      "Training iteration: 3134\n",
      "Improved validation loss from: 0.0244938924908638  to: 0.024486260116100313\n",
      "Training iteration: 3135\n",
      "Improved validation loss from: 0.024486260116100313  to: 0.02448221743106842\n",
      "Training iteration: 3136\n",
      "Improved validation loss from: 0.02448221743106842  to: 0.024481502175331116\n",
      "Training iteration: 3137\n",
      "Improved validation loss from: 0.024481502175331116  to: 0.02447998821735382\n",
      "Training iteration: 3138\n",
      "Improved validation loss from: 0.02447998821735382  to: 0.024474509060382843\n",
      "Training iteration: 3139\n",
      "Improved validation loss from: 0.024474509060382843  to: 0.024466629326343536\n",
      "Training iteration: 3140\n",
      "Improved validation loss from: 0.024466629326343536  to: 0.02446046322584152\n",
      "Training iteration: 3141\n",
      "Improved validation loss from: 0.02446046322584152  to: 0.024457506835460663\n",
      "Training iteration: 3142\n",
      "Improved validation loss from: 0.024457506835460663  to: 0.02445543706417084\n",
      "Training iteration: 3143\n",
      "Improved validation loss from: 0.02445543706417084  to: 0.02445116937160492\n",
      "Training iteration: 3144\n",
      "Improved validation loss from: 0.02445116937160492  to: 0.02444709837436676\n",
      "Training iteration: 3145\n",
      "Improved validation loss from: 0.02444709837436676  to: 0.024443292617797853\n",
      "Training iteration: 3146\n",
      "Improved validation loss from: 0.024443292617797853  to: 0.024437156319618226\n",
      "Training iteration: 3147\n",
      "Improved validation loss from: 0.024437156319618226  to: 0.024431970715522767\n",
      "Training iteration: 3148\n",
      "Improved validation loss from: 0.024431970715522767  to: 0.024429258704185487\n",
      "Training iteration: 3149\n",
      "Improved validation loss from: 0.024429258704185487  to: 0.02442753314971924\n",
      "Training iteration: 3150\n",
      "Improved validation loss from: 0.02442753314971924  to: 0.024423742294311525\n",
      "Training iteration: 3151\n",
      "Improved validation loss from: 0.024423742294311525  to: 0.02441757172346115\n",
      "Training iteration: 3152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02441757172346115  to: 0.024411149322986603\n",
      "Training iteration: 3153\n",
      "Improved validation loss from: 0.024411149322986603  to: 0.024406623840332032\n",
      "Training iteration: 3154\n",
      "Improved validation loss from: 0.024406623840332032  to: 0.02440381944179535\n",
      "Training iteration: 3155\n",
      "Improved validation loss from: 0.02440381944179535  to: 0.024400463700294493\n",
      "Training iteration: 3156\n",
      "Improved validation loss from: 0.024400463700294493  to: 0.024395187199115754\n",
      "Training iteration: 3157\n",
      "Improved validation loss from: 0.024395187199115754  to: 0.024391886591911317\n",
      "Training iteration: 3158\n",
      "Improved validation loss from: 0.024391886591911317  to: 0.02438697814941406\n",
      "Training iteration: 3159\n",
      "Improved validation loss from: 0.02438697814941406  to: 0.024381670355796813\n",
      "Training iteration: 3160\n",
      "Improved validation loss from: 0.024381670355796813  to: 0.02437780350446701\n",
      "Training iteration: 3161\n",
      "Improved validation loss from: 0.02437780350446701  to: 0.02437779903411865\n",
      "Training iteration: 3162\n",
      "Improved validation loss from: 0.02437779903411865  to: 0.02437458336353302\n",
      "Training iteration: 3163\n",
      "Improved validation loss from: 0.02437458336353302  to: 0.02436784505844116\n",
      "Training iteration: 3164\n",
      "Improved validation loss from: 0.02436784505844116  to: 0.02436186373233795\n",
      "Training iteration: 3165\n",
      "Improved validation loss from: 0.02436186373233795  to: 0.02435886412858963\n",
      "Training iteration: 3166\n",
      "Improved validation loss from: 0.02435886412858963  to: 0.024357469379901887\n",
      "Training iteration: 3167\n",
      "Improved validation loss from: 0.024357469379901887  to: 0.024353976547718047\n",
      "Training iteration: 3168\n",
      "Improved validation loss from: 0.024353976547718047  to: 0.024347662925720215\n",
      "Training iteration: 3169\n",
      "Improved validation loss from: 0.024347662925720215  to: 0.024341145157814027\n",
      "Training iteration: 3170\n",
      "Improved validation loss from: 0.024341145157814027  to: 0.024339623749256134\n",
      "Training iteration: 3171\n",
      "Improved validation loss from: 0.024339623749256134  to: 0.02433728873729706\n",
      "Training iteration: 3172\n",
      "Improved validation loss from: 0.02433728873729706  to: 0.02433217465877533\n",
      "Training iteration: 3173\n",
      "Improved validation loss from: 0.02433217465877533  to: 0.024326291680336\n",
      "Training iteration: 3174\n",
      "Improved validation loss from: 0.024326291680336  to: 0.024322132766246795\n",
      "Training iteration: 3175\n",
      "Improved validation loss from: 0.024322132766246795  to: 0.024319958686828614\n",
      "Training iteration: 3176\n",
      "Improved validation loss from: 0.024319958686828614  to: 0.024317240715026854\n",
      "Training iteration: 3177\n",
      "Improved validation loss from: 0.024317240715026854  to: 0.024315507709980012\n",
      "Training iteration: 3178\n",
      "Improved validation loss from: 0.024315507709980012  to: 0.024309535324573518\n",
      "Training iteration: 3179\n",
      "Improved validation loss from: 0.024309535324573518  to: 0.024302959442138672\n",
      "Training iteration: 3180\n",
      "Improved validation loss from: 0.024302959442138672  to: 0.024299418926239012\n",
      "Training iteration: 3181\n",
      "Improved validation loss from: 0.024299418926239012  to: 0.024298553168773652\n",
      "Training iteration: 3182\n",
      "Improved validation loss from: 0.024298553168773652  to: 0.02429656982421875\n",
      "Training iteration: 3183\n",
      "Improved validation loss from: 0.02429656982421875  to: 0.02429117262363434\n",
      "Training iteration: 3184\n",
      "Improved validation loss from: 0.02429117262363434  to: 0.02428436726331711\n",
      "Training iteration: 3185\n",
      "Improved validation loss from: 0.02428436726331711  to: 0.024282357096672057\n",
      "Training iteration: 3186\n",
      "Improved validation loss from: 0.024282357096672057  to: 0.02428007423877716\n",
      "Training iteration: 3187\n",
      "Improved validation loss from: 0.02428007423877716  to: 0.024276068806648253\n",
      "Training iteration: 3188\n",
      "Improved validation loss from: 0.024276068806648253  to: 0.024271078407764435\n",
      "Training iteration: 3189\n",
      "Improved validation loss from: 0.024271078407764435  to: 0.024266763031482695\n",
      "Training iteration: 3190\n",
      "Improved validation loss from: 0.024266763031482695  to: 0.024263787269592284\n",
      "Training iteration: 3191\n",
      "Improved validation loss from: 0.024263787269592284  to: 0.02426077574491501\n",
      "Training iteration: 3192\n",
      "Improved validation loss from: 0.02426077574491501  to: 0.024256595969200136\n",
      "Training iteration: 3193\n",
      "Improved validation loss from: 0.024256595969200136  to: 0.0242544025182724\n",
      "Training iteration: 3194\n",
      "Improved validation loss from: 0.0242544025182724  to: 0.024249930679798127\n",
      "Training iteration: 3195\n",
      "Improved validation loss from: 0.024249930679798127  to: 0.02424482852220535\n",
      "Training iteration: 3196\n",
      "Improved validation loss from: 0.02424482852220535  to: 0.024241252243518828\n",
      "Training iteration: 3197\n",
      "Improved validation loss from: 0.024241252243518828  to: 0.024239273369312288\n",
      "Training iteration: 3198\n",
      "Improved validation loss from: 0.024239273369312288  to: 0.024236507713794708\n",
      "Training iteration: 3199\n",
      "Improved validation loss from: 0.024236507713794708  to: 0.024231848120689393\n",
      "Training iteration: 3200\n",
      "Improved validation loss from: 0.024231848120689393  to: 0.024226465821266176\n",
      "Training iteration: 3201\n",
      "Improved validation loss from: 0.024226465821266176  to: 0.024222549796104432\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.02422269880771637\n",
      "Training iteration: 3203\n",
      "Improved validation loss from: 0.024222549796104432  to: 0.02421985864639282\n",
      "Training iteration: 3204\n",
      "Improved validation loss from: 0.02421985864639282  to: 0.02421351969242096\n",
      "Training iteration: 3205\n",
      "Improved validation loss from: 0.02421351969242096  to: 0.024207858741283415\n",
      "Training iteration: 3206\n",
      "Improved validation loss from: 0.024207858741283415  to: 0.024205306172370912\n",
      "Training iteration: 3207\n",
      "Improved validation loss from: 0.024205306172370912  to: 0.024204400181770325\n",
      "Training iteration: 3208\n",
      "Improved validation loss from: 0.024204400181770325  to: 0.024201640486717226\n",
      "Training iteration: 3209\n",
      "Improved validation loss from: 0.024201640486717226  to: 0.02419608533382416\n",
      "Training iteration: 3210\n",
      "Improved validation loss from: 0.02419608533382416  to: 0.02419312745332718\n",
      "Training iteration: 3211\n",
      "Improved validation loss from: 0.02419312745332718  to: 0.024189487099647522\n",
      "Training iteration: 3212\n",
      "Improved validation loss from: 0.024189487099647522  to: 0.024185732007026672\n",
      "Training iteration: 3213\n",
      "Improved validation loss from: 0.024185732007026672  to: 0.024182558059692383\n",
      "Training iteration: 3214\n",
      "Improved validation loss from: 0.024182558059692383  to: 0.024179640412330627\n",
      "Training iteration: 3215\n",
      "Improved validation loss from: 0.024179640412330627  to: 0.024176347255706786\n",
      "Training iteration: 3216\n",
      "Improved validation loss from: 0.024176347255706786  to: 0.02417239248752594\n",
      "Training iteration: 3217\n",
      "Improved validation loss from: 0.02417239248752594  to: 0.02416825294494629\n",
      "Training iteration: 3218\n",
      "Improved validation loss from: 0.02416825294494629  to: 0.02416457384824753\n",
      "Training iteration: 3219\n",
      "Improved validation loss from: 0.02416457384824753  to: 0.02416134774684906\n",
      "Training iteration: 3220\n",
      "Improved validation loss from: 0.02416134774684906  to: 0.024158143997192384\n",
      "Training iteration: 3221\n",
      "Improved validation loss from: 0.024158143997192384  to: 0.024157004058361055\n",
      "Training iteration: 3222\n",
      "Improved validation loss from: 0.024157004058361055  to: 0.024152712523937227\n",
      "Training iteration: 3223\n",
      "Improved validation loss from: 0.024152712523937227  to: 0.024147221446037294\n",
      "Training iteration: 3224\n",
      "Improved validation loss from: 0.024147221446037294  to: 0.02414356768131256\n",
      "Training iteration: 3225\n",
      "Improved validation loss from: 0.02414356768131256  to: 0.024142447113990783\n",
      "Training iteration: 3226\n",
      "Improved validation loss from: 0.024142447113990783  to: 0.02414090186357498\n",
      "Training iteration: 3227\n",
      "Improved validation loss from: 0.02414090186357498  to: 0.024136781692504883\n",
      "Training iteration: 3228\n",
      "Improved validation loss from: 0.024136781692504883  to: 0.024131175875663758\n",
      "Training iteration: 3229\n",
      "Improved validation loss from: 0.024131175875663758  to: 0.024126751720905303\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.024127545952796935\n",
      "Training iteration: 3231\n",
      "Improved validation loss from: 0.024126751720905303  to: 0.024125552177429198\n",
      "Training iteration: 3232\n",
      "Improved validation loss from: 0.024125552177429198  to: 0.02411985844373703\n",
      "Training iteration: 3233\n",
      "Improved validation loss from: 0.02411985844373703  to: 0.024114279448986052\n",
      "Training iteration: 3234\n",
      "Improved validation loss from: 0.024114279448986052  to: 0.024111613631248474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3235\n",
      "Improved validation loss from: 0.024111613631248474  to: 0.024110957980155945\n",
      "Training iteration: 3236\n",
      "Improved validation loss from: 0.024110957980155945  to: 0.0241085484623909\n",
      "Training iteration: 3237\n",
      "Improved validation loss from: 0.0241085484623909  to: 0.024103593826293946\n",
      "Training iteration: 3238\n",
      "Improved validation loss from: 0.024103593826293946  to: 0.024098269641399384\n",
      "Training iteration: 3239\n",
      "Improved validation loss from: 0.024098269641399384  to: 0.02409510165452957\n",
      "Training iteration: 3240\n",
      "Improved validation loss from: 0.02409510165452957  to: 0.024093739688396454\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.024094180762767793\n",
      "Training iteration: 3242\n",
      "Improved validation loss from: 0.024093739688396454  to: 0.024089395999908447\n",
      "Training iteration: 3243\n",
      "Improved validation loss from: 0.024089395999908447  to: 0.024082517623901366\n",
      "Training iteration: 3244\n",
      "Improved validation loss from: 0.024082517623901366  to: 0.02407887428998947\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.024079203605651855\n",
      "Training iteration: 3246\n",
      "Improved validation loss from: 0.02407887428998947  to: 0.024078865349292756\n",
      "Training iteration: 3247\n",
      "Improved validation loss from: 0.024078865349292756  to: 0.0240744024515152\n",
      "Training iteration: 3248\n",
      "Improved validation loss from: 0.0240744024515152  to: 0.024067720770835875\n",
      "Training iteration: 3249\n",
      "Improved validation loss from: 0.024067720770835875  to: 0.02406337708234787\n",
      "Training iteration: 3250\n",
      "Improved validation loss from: 0.02406337708234787  to: 0.024062471091747285\n",
      "Training iteration: 3251\n",
      "Improved validation loss from: 0.024062471091747285  to: 0.02406179904937744\n",
      "Training iteration: 3252\n",
      "Improved validation loss from: 0.02406179904937744  to: 0.024057979881763458\n",
      "Training iteration: 3253\n",
      "Improved validation loss from: 0.024057979881763458  to: 0.024054770171642304\n",
      "Training iteration: 3254\n",
      "Improved validation loss from: 0.024054770171642304  to: 0.02404992878437042\n",
      "Training iteration: 3255\n",
      "Improved validation loss from: 0.02404992878437042  to: 0.0240463450551033\n",
      "Training iteration: 3256\n",
      "Improved validation loss from: 0.0240463450551033  to: 0.024044986069202422\n",
      "Training iteration: 3257\n",
      "Improved validation loss from: 0.024044986069202422  to: 0.024043984711170197\n",
      "Training iteration: 3258\n",
      "Improved validation loss from: 0.024043984711170197  to: 0.024040909111499788\n",
      "Training iteration: 3259\n",
      "Improved validation loss from: 0.024040909111499788  to: 0.024036160111427306\n",
      "Training iteration: 3260\n",
      "Improved validation loss from: 0.024036160111427306  to: 0.024031862616539\n",
      "Training iteration: 3261\n",
      "Improved validation loss from: 0.024031862616539  to: 0.024029593169689178\n",
      "Training iteration: 3262\n",
      "Improved validation loss from: 0.024029593169689178  to: 0.0240279883146286\n",
      "Training iteration: 3263\n",
      "Improved validation loss from: 0.0240279883146286  to: 0.024025166034698488\n",
      "Training iteration: 3264\n",
      "Improved validation loss from: 0.024025166034698488  to: 0.024020877480506898\n",
      "Training iteration: 3265\n",
      "Improved validation loss from: 0.024020877480506898  to: 0.02401689291000366\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.024016931653022766\n",
      "Training iteration: 3267\n",
      "Improved validation loss from: 0.02401689291000366  to: 0.024014630913734437\n",
      "Training iteration: 3268\n",
      "Improved validation loss from: 0.024014630913734437  to: 0.024009862542152406\n",
      "Training iteration: 3269\n",
      "Improved validation loss from: 0.024009862542152406  to: 0.024005909264087678\n",
      "Training iteration: 3270\n",
      "Improved validation loss from: 0.024005909264087678  to: 0.0240044042468071\n",
      "Training iteration: 3271\n",
      "Improved validation loss from: 0.0240044042468071  to: 0.024003605544567107\n",
      "Training iteration: 3272\n",
      "Improved validation loss from: 0.024003605544567107  to: 0.024000707268714904\n",
      "Training iteration: 3273\n",
      "Improved validation loss from: 0.024000707268714904  to: 0.023995885252952577\n",
      "Training iteration: 3274\n",
      "Improved validation loss from: 0.023995885252952577  to: 0.02399158924818039\n",
      "Training iteration: 3275\n",
      "Improved validation loss from: 0.02399158924818039  to: 0.023989419639110564\n",
      "Training iteration: 3276\n",
      "Improved validation loss from: 0.023989419639110564  to: 0.023988182842731475\n",
      "Training iteration: 3277\n",
      "Improved validation loss from: 0.023988182842731475  to: 0.023985560238361358\n",
      "Training iteration: 3278\n",
      "Improved validation loss from: 0.023985560238361358  to: 0.023981285095214844\n",
      "Training iteration: 3279\n",
      "Improved validation loss from: 0.023981285095214844  to: 0.02397707402706146\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.023977331817150116\n",
      "Training iteration: 3281\n",
      "Improved validation loss from: 0.02397707402706146  to: 0.023975634574890138\n",
      "Training iteration: 3282\n",
      "Improved validation loss from: 0.023975634574890138  to: 0.02397136241197586\n",
      "Training iteration: 3283\n",
      "Improved validation loss from: 0.02397136241197586  to: 0.023967376351356505\n",
      "Training iteration: 3284\n",
      "Improved validation loss from: 0.023967376351356505  to: 0.023965613543987276\n",
      "Training iteration: 3285\n",
      "Improved validation loss from: 0.023965613543987276  to: 0.023964738845825194\n",
      "Training iteration: 3286\n",
      "Improved validation loss from: 0.023964738845825194  to: 0.02396221607923508\n",
      "Training iteration: 3287\n",
      "Improved validation loss from: 0.02396221607923508  to: 0.02395779639482498\n",
      "Training iteration: 3288\n",
      "Improved validation loss from: 0.02395779639482498  to: 0.023953719437122344\n",
      "Training iteration: 3289\n",
      "Improved validation loss from: 0.023953719437122344  to: 0.023951688408851625\n",
      "Training iteration: 3290\n",
      "Improved validation loss from: 0.023951688408851625  to: 0.023950353264808655\n",
      "Training iteration: 3291\n",
      "Improved validation loss from: 0.023950353264808655  to: 0.02394769638776779\n",
      "Training iteration: 3292\n",
      "Improved validation loss from: 0.02394769638776779  to: 0.023943467438220976\n",
      "Training iteration: 3293\n",
      "Improved validation loss from: 0.023943467438220976  to: 0.023939529061317445\n",
      "Training iteration: 3294\n",
      "Improved validation loss from: 0.023939529061317445  to: 0.023937225341796875\n",
      "Training iteration: 3295\n",
      "Improved validation loss from: 0.023937225341796875  to: 0.02393605262041092\n",
      "Training iteration: 3296\n",
      "Improved validation loss from: 0.02393605262041092  to: 0.023933891952037812\n",
      "Training iteration: 3297\n",
      "Improved validation loss from: 0.023933891952037812  to: 0.023933109641075135\n",
      "Training iteration: 3298\n",
      "Improved validation loss from: 0.023933109641075135  to: 0.023929068446159364\n",
      "Training iteration: 3299\n",
      "Improved validation loss from: 0.023929068446159364  to: 0.023924627900123598\n",
      "Training iteration: 3300\n",
      "Improved validation loss from: 0.023924627900123598  to: 0.023922738432884217\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.023922772705554964\n",
      "Training iteration: 3302\n",
      "Improved validation loss from: 0.023922738432884217  to: 0.02392132729291916\n",
      "Training iteration: 3303\n",
      "Improved validation loss from: 0.02392132729291916  to: 0.02391723096370697\n",
      "Training iteration: 3304\n",
      "Improved validation loss from: 0.02391723096370697  to: 0.0239123672246933\n",
      "Training iteration: 3305\n",
      "Improved validation loss from: 0.0239123672246933  to: 0.02390964925289154\n",
      "Training iteration: 3306\n",
      "Improved validation loss from: 0.02390964925289154  to: 0.023908670246601104\n",
      "Training iteration: 3307\n",
      "Improved validation loss from: 0.023908670246601104  to: 0.023906974494457243\n",
      "Training iteration: 3308\n",
      "Improved validation loss from: 0.023906974494457243  to: 0.023903171718120574\n",
      "Training iteration: 3309\n",
      "Improved validation loss from: 0.023903171718120574  to: 0.02389892637729645\n",
      "Training iteration: 3310\n",
      "Improved validation loss from: 0.02389892637729645  to: 0.023896268010139464\n",
      "Training iteration: 3311\n",
      "Improved validation loss from: 0.023896268010139464  to: 0.023895001411437987\n",
      "Training iteration: 3312\n",
      "Improved validation loss from: 0.023895001411437987  to: 0.023893292248249053\n",
      "Training iteration: 3313\n",
      "Improved validation loss from: 0.023893292248249053  to: 0.023890236020088197\n",
      "Training iteration: 3314\n",
      "Improved validation loss from: 0.023890236020088197  to: 0.02388689070940018\n",
      "Training iteration: 3315\n",
      "Improved validation loss from: 0.02388689070940018  to: 0.023884554207324982\n",
      "Training iteration: 3316\n",
      "Improved validation loss from: 0.023884554207324982  to: 0.023883035778999327\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.023884010314941407\n",
      "Training iteration: 3318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.023883035778999327  to: 0.023880735039711\n",
      "Training iteration: 3319\n",
      "Improved validation loss from: 0.023880735039711  to: 0.023875418305397033\n",
      "Training iteration: 3320\n",
      "Improved validation loss from: 0.023875418305397033  to: 0.023872657120227812\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.023873062431812288\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.023872676491737365\n",
      "Training iteration: 3323\n",
      "Improved validation loss from: 0.023872657120227812  to: 0.023868688941001893\n",
      "Training iteration: 3324\n",
      "Improved validation loss from: 0.023868688941001893  to: 0.023863323032855988\n",
      "Training iteration: 3325\n",
      "Improved validation loss from: 0.023863323032855988  to: 0.023860196769237518\n",
      "Training iteration: 3326\n",
      "Improved validation loss from: 0.023860196769237518  to: 0.023859719932079314\n",
      "Training iteration: 3327\n",
      "Improved validation loss from: 0.023859719932079314  to: 0.02385879456996918\n",
      "Training iteration: 3328\n",
      "Improved validation loss from: 0.02385879456996918  to: 0.023855295777320863\n",
      "Training iteration: 3329\n",
      "Improved validation loss from: 0.023855295777320863  to: 0.02385062426328659\n",
      "Training iteration: 3330\n",
      "Improved validation loss from: 0.02385062426328659  to: 0.023847874999046326\n",
      "Training iteration: 3331\n",
      "Improved validation loss from: 0.023847874999046326  to: 0.023847293853759766\n",
      "Training iteration: 3332\n",
      "Improved validation loss from: 0.023847293853759766  to: 0.023846368491649627\n",
      "Training iteration: 3333\n",
      "Improved validation loss from: 0.023846368491649627  to: 0.023843415081501007\n",
      "Training iteration: 3334\n",
      "Improved validation loss from: 0.023843415081501007  to: 0.023839464783668517\n",
      "Training iteration: 3335\n",
      "Improved validation loss from: 0.023839464783668517  to: 0.023837061226367952\n",
      "Training iteration: 3336\n",
      "Improved validation loss from: 0.023837061226367952  to: 0.023836274445056916\n",
      "Training iteration: 3337\n",
      "Improved validation loss from: 0.023836274445056916  to: 0.023835158348083495\n",
      "Training iteration: 3338\n",
      "Improved validation loss from: 0.023835158348083495  to: 0.023832285404205324\n",
      "Training iteration: 3339\n",
      "Improved validation loss from: 0.023832285404205324  to: 0.02382860630750656\n",
      "Training iteration: 3340\n",
      "Improved validation loss from: 0.02382860630750656  to: 0.023826202750205992\n",
      "Training iteration: 3341\n",
      "Improved validation loss from: 0.023826202750205992  to: 0.023824837803840638\n",
      "Training iteration: 3342\n",
      "Improved validation loss from: 0.023824837803840638  to: 0.023823222517967223\n",
      "Training iteration: 3343\n",
      "Improved validation loss from: 0.023823222517967223  to: 0.023820415139198303\n",
      "Training iteration: 3344\n",
      "Improved validation loss from: 0.023820415139198303  to: 0.023819880187511445\n",
      "Training iteration: 3345\n",
      "Improved validation loss from: 0.023819880187511445  to: 0.023816990852355956\n",
      "Training iteration: 3346\n",
      "Improved validation loss from: 0.023816990852355956  to: 0.023813319206237794\n",
      "Training iteration: 3347\n",
      "Improved validation loss from: 0.023813319206237794  to: 0.02381151169538498\n",
      "Training iteration: 3348\n",
      "Improved validation loss from: 0.02381151169538498  to: 0.02381100654602051\n",
      "Training iteration: 3349\n",
      "Improved validation loss from: 0.02381100654602051  to: 0.023809643089771272\n",
      "Training iteration: 3350\n",
      "Improved validation loss from: 0.023809643089771272  to: 0.023806414008140563\n",
      "Training iteration: 3351\n",
      "Improved validation loss from: 0.023806414008140563  to: 0.023802682757377625\n",
      "Training iteration: 3352\n",
      "Improved validation loss from: 0.023802682757377625  to: 0.02380049228668213\n",
      "Training iteration: 3353\n",
      "Improved validation loss from: 0.02380049228668213  to: 0.023799748718738557\n",
      "Training iteration: 3354\n",
      "Improved validation loss from: 0.023799748718738557  to: 0.023798170685768127\n",
      "Training iteration: 3355\n",
      "Improved validation loss from: 0.023798170685768127  to: 0.023795071244239806\n",
      "Training iteration: 3356\n",
      "Improved validation loss from: 0.023795071244239806  to: 0.02379160374403\n",
      "Training iteration: 3357\n",
      "Improved validation loss from: 0.02379160374403  to: 0.023789462447166444\n",
      "Training iteration: 3358\n",
      "Improved validation loss from: 0.023789462447166444  to: 0.023788419365882874\n",
      "Training iteration: 3359\n",
      "Improved validation loss from: 0.023788419365882874  to: 0.023786905407905578\n",
      "Training iteration: 3360\n",
      "Improved validation loss from: 0.023786905407905578  to: 0.023784263432025908\n",
      "Training iteration: 3361\n",
      "Improved validation loss from: 0.023784263432025908  to: 0.023781207203865052\n",
      "Training iteration: 3362\n",
      "Improved validation loss from: 0.023781207203865052  to: 0.02377924919128418\n",
      "Training iteration: 3363\n",
      "Improved validation loss from: 0.02377924919128418  to: 0.023777878284454344\n",
      "Training iteration: 3364\n",
      "Improved validation loss from: 0.023777878284454344  to: 0.02377605140209198\n",
      "Training iteration: 3365\n",
      "Improved validation loss from: 0.02377605140209198  to: 0.023773522675037385\n",
      "Training iteration: 3366\n",
      "Improved validation loss from: 0.023773522675037385  to: 0.023770752549171447\n",
      "Training iteration: 3367\n",
      "Improved validation loss from: 0.023770752549171447  to: 0.023768797516822815\n",
      "Training iteration: 3368\n",
      "Improved validation loss from: 0.023768797516822815  to: 0.023767173290252686\n",
      "Training iteration: 3369\n",
      "Improved validation loss from: 0.023767173290252686  to: 0.023764999210834505\n",
      "Training iteration: 3370\n",
      "Improved validation loss from: 0.023764999210834505  to: 0.023762492835521697\n",
      "Training iteration: 3371\n",
      "Improved validation loss from: 0.023762492835521697  to: 0.023760151863098145\n",
      "Training iteration: 3372\n",
      "Improved validation loss from: 0.023760151863098145  to: 0.023758235573768615\n",
      "Training iteration: 3373\n",
      "Improved validation loss from: 0.023758235573768615  to: 0.023756781220436098\n",
      "Training iteration: 3374\n",
      "Improved validation loss from: 0.023756781220436098  to: 0.02375471144914627\n",
      "Training iteration: 3375\n",
      "Improved validation loss from: 0.02375471144914627  to: 0.02375214099884033\n",
      "Training iteration: 3376\n",
      "Improved validation loss from: 0.02375214099884033  to: 0.02374981939792633\n",
      "Training iteration: 3377\n",
      "Improved validation loss from: 0.02374981939792633  to: 0.02374812066555023\n",
      "Training iteration: 3378\n",
      "Improved validation loss from: 0.02374812066555023  to: 0.023746781051158905\n",
      "Training iteration: 3379\n",
      "Improved validation loss from: 0.023746781051158905  to: 0.023745064437389374\n",
      "Training iteration: 3380\n",
      "Improved validation loss from: 0.023745064437389374  to: 0.023742809891700745\n",
      "Training iteration: 3381\n",
      "Improved validation loss from: 0.023742809891700745  to: 0.02374071329832077\n",
      "Training iteration: 3382\n",
      "Improved validation loss from: 0.02374071329832077  to: 0.02373901307582855\n",
      "Training iteration: 3383\n",
      "Improved validation loss from: 0.02373901307582855  to: 0.023737421631813048\n",
      "Training iteration: 3384\n",
      "Improved validation loss from: 0.023737421631813048  to: 0.023735484480857848\n",
      "Training iteration: 3385\n",
      "Improved validation loss from: 0.023735484480857848  to: 0.023733393847942354\n",
      "Training iteration: 3386\n",
      "Improved validation loss from: 0.023733393847942354  to: 0.023731498420238493\n",
      "Training iteration: 3387\n",
      "Improved validation loss from: 0.023731498420238493  to: 0.02372986376285553\n",
      "Training iteration: 3388\n",
      "Improved validation loss from: 0.02372986376285553  to: 0.02372823506593704\n",
      "Training iteration: 3389\n",
      "Improved validation loss from: 0.02372823506593704  to: 0.02372620105743408\n",
      "Training iteration: 3390\n",
      "Improved validation loss from: 0.02372620105743408  to: 0.023724284768104554\n",
      "Training iteration: 3391\n",
      "Improved validation loss from: 0.023724284768104554  to: 0.023722362518310548\n",
      "Training iteration: 3392\n",
      "Improved validation loss from: 0.023722362518310548  to: 0.02372071295976639\n",
      "Training iteration: 3393\n",
      "Improved validation loss from: 0.02372071295976639  to: 0.023719124495983124\n",
      "Training iteration: 3394\n",
      "Improved validation loss from: 0.023719124495983124  to: 0.023717407882213593\n",
      "Training iteration: 3395\n",
      "Improved validation loss from: 0.023717407882213593  to: 0.02371537685394287\n",
      "Training iteration: 3396\n",
      "Improved validation loss from: 0.02371537685394287  to: 0.023713564872741698\n",
      "Training iteration: 3397\n",
      "Improved validation loss from: 0.023713564872741698  to: 0.02371189296245575\n",
      "Training iteration: 3398\n",
      "Improved validation loss from: 0.02371189296245575  to: 0.02371023893356323\n",
      "Training iteration: 3399\n",
      "Improved validation loss from: 0.02371023893356323  to: 0.02370848208665848\n",
      "Training iteration: 3400\n",
      "Improved validation loss from: 0.02370848208665848  to: 0.023706631362438203\n",
      "Training iteration: 3401\n",
      "Improved validation loss from: 0.023706631362438203  to: 0.023704838752746583\n",
      "Training iteration: 3402\n",
      "Improved validation loss from: 0.023704838752746583  to: 0.02370312511920929\n",
      "Training iteration: 3403\n",
      "Improved validation loss from: 0.02370312511920929  to: 0.023701319098472597\n",
      "Training iteration: 3404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.023701319098472597  to: 0.023699569702148437\n",
      "Training iteration: 3405\n",
      "Improved validation loss from: 0.023699569702148437  to: 0.02369779646396637\n",
      "Training iteration: 3406\n",
      "Improved validation loss from: 0.02369779646396637  to: 0.02369609326124191\n",
      "Training iteration: 3407\n",
      "Improved validation loss from: 0.02369609326124191  to: 0.023694339394569396\n",
      "Training iteration: 3408\n",
      "Improved validation loss from: 0.023694339394569396  to: 0.023692452907562257\n",
      "Training iteration: 3409\n",
      "Improved validation loss from: 0.023692452907562257  to: 0.023690705001354218\n",
      "Training iteration: 3410\n",
      "Improved validation loss from: 0.023690705001354218  to: 0.023688873648643492\n",
      "Training iteration: 3411\n",
      "Improved validation loss from: 0.023688873648643492  to: 0.023687119781970977\n",
      "Training iteration: 3412\n",
      "Improved validation loss from: 0.023687119781970977  to: 0.023685565590858458\n",
      "Training iteration: 3413\n",
      "Improved validation loss from: 0.023685565590858458  to: 0.023683719336986542\n",
      "Training iteration: 3414\n",
      "Improved validation loss from: 0.023683719336986542  to: 0.023681959509849547\n",
      "Training iteration: 3415\n",
      "Improved validation loss from: 0.023681959509849547  to: 0.02368004769086838\n",
      "Training iteration: 3416\n",
      "Improved validation loss from: 0.02368004769086838  to: 0.02367834597826004\n",
      "Training iteration: 3417\n",
      "Improved validation loss from: 0.02367834597826004  to: 0.02367662489414215\n",
      "Training iteration: 3418\n",
      "Improved validation loss from: 0.02367662489414215  to: 0.023674897849559784\n",
      "Training iteration: 3419\n",
      "Improved validation loss from: 0.023674897849559784  to: 0.0236731082201004\n",
      "Training iteration: 3420\n",
      "Improved validation loss from: 0.0236731082201004  to: 0.02367132157087326\n",
      "Training iteration: 3421\n",
      "Improved validation loss from: 0.02367132157087326  to: 0.023669688403606413\n",
      "Training iteration: 3422\n",
      "Improved validation loss from: 0.023669688403606413  to: 0.02366797477006912\n",
      "Training iteration: 3423\n",
      "Improved validation loss from: 0.02366797477006912  to: 0.023666217923164368\n",
      "Training iteration: 3424\n",
      "Improved validation loss from: 0.023666217923164368  to: 0.023664398491382597\n",
      "Training iteration: 3425\n",
      "Improved validation loss from: 0.023664398491382597  to: 0.023662646114826203\n",
      "Training iteration: 3426\n",
      "Improved validation loss from: 0.023662646114826203  to: 0.023661021888256074\n",
      "Training iteration: 3427\n",
      "Improved validation loss from: 0.023661021888256074  to: 0.023659396171569824\n",
      "Training iteration: 3428\n",
      "Improved validation loss from: 0.023659396171569824  to: 0.0236576646566391\n",
      "Training iteration: 3429\n",
      "Improved validation loss from: 0.0236576646566391  to: 0.02365586757659912\n",
      "Training iteration: 3430\n",
      "Improved validation loss from: 0.02365586757659912  to: 0.02365400791168213\n",
      "Training iteration: 3431\n",
      "Improved validation loss from: 0.02365400791168213  to: 0.023652353882789613\n",
      "Training iteration: 3432\n",
      "Improved validation loss from: 0.023652353882789613  to: 0.02365075796842575\n",
      "Training iteration: 3433\n",
      "Improved validation loss from: 0.02365075796842575  to: 0.023649224638938905\n",
      "Training iteration: 3434\n",
      "Improved validation loss from: 0.023649224638938905  to: 0.023647579550743102\n",
      "Training iteration: 3435\n",
      "Improved validation loss from: 0.023647579550743102  to: 0.023645648360252382\n",
      "Training iteration: 3436\n",
      "Improved validation loss from: 0.023645648360252382  to: 0.0236439511179924\n",
      "Training iteration: 3437\n",
      "Improved validation loss from: 0.0236439511179924  to: 0.023642334342002868\n",
      "Training iteration: 3438\n",
      "Improved validation loss from: 0.023642334342002868  to: 0.02364085167646408\n",
      "Training iteration: 3439\n",
      "Improved validation loss from: 0.02364085167646408  to: 0.023639145493507385\n",
      "Training iteration: 3440\n",
      "Improved validation loss from: 0.023639145493507385  to: 0.023637452721595766\n",
      "Training iteration: 3441\n",
      "Improved validation loss from: 0.023637452721595766  to: 0.023635706305503844\n",
      "Training iteration: 3442\n",
      "Improved validation loss from: 0.023635706305503844  to: 0.023634083569049835\n",
      "Training iteration: 3443\n",
      "Improved validation loss from: 0.023634083569049835  to: 0.02363244295120239\n",
      "Training iteration: 3444\n",
      "Improved validation loss from: 0.02363244295120239  to: 0.02363092601299286\n",
      "Training iteration: 3445\n",
      "Improved validation loss from: 0.02363092601299286  to: 0.023629407584667205\n",
      "Training iteration: 3446\n",
      "Improved validation loss from: 0.023629407584667205  to: 0.02362765520811081\n",
      "Training iteration: 3447\n",
      "Improved validation loss from: 0.02362765520811081  to: 0.023626017570495605\n",
      "Training iteration: 3448\n",
      "Improved validation loss from: 0.023626017570495605  to: 0.023624399304389955\n",
      "Training iteration: 3449\n",
      "Improved validation loss from: 0.023624399304389955  to: 0.02362288534641266\n",
      "Training iteration: 3450\n",
      "Improved validation loss from: 0.02362288534641266  to: 0.023621268570423126\n",
      "Training iteration: 3451\n",
      "Improved validation loss from: 0.023621268570423126  to: 0.023619771003723145\n",
      "Training iteration: 3452\n",
      "Improved validation loss from: 0.023619771003723145  to: 0.023618245124816896\n",
      "Training iteration: 3453\n",
      "Improved validation loss from: 0.023618245124816896  to: 0.023616614937782287\n",
      "Training iteration: 3454\n",
      "Improved validation loss from: 0.023616614937782287  to: 0.023615007102489472\n",
      "Training iteration: 3455\n",
      "Improved validation loss from: 0.023615007102489472  to: 0.023613381385803222\n",
      "Training iteration: 3456\n",
      "Improved validation loss from: 0.023613381385803222  to: 0.023611918091773987\n",
      "Training iteration: 3457\n",
      "Improved validation loss from: 0.023611918091773987  to: 0.023610429465770723\n",
      "Training iteration: 3458\n",
      "Improved validation loss from: 0.023610429465770723  to: 0.02360895574092865\n",
      "Training iteration: 3459\n",
      "Improved validation loss from: 0.02360895574092865  to: 0.023607416450977324\n",
      "Training iteration: 3460\n",
      "Improved validation loss from: 0.023607416450977324  to: 0.023605886101722717\n",
      "Training iteration: 3461\n",
      "Improved validation loss from: 0.023605886101722717  to: 0.023604384064674376\n",
      "Training iteration: 3462\n",
      "Improved validation loss from: 0.023604384064674376  to: 0.023602673411369325\n",
      "Training iteration: 3463\n",
      "Improved validation loss from: 0.023602673411369325  to: 0.023601117730140685\n",
      "Training iteration: 3464\n",
      "Improved validation loss from: 0.023601117730140685  to: 0.0235999196767807\n",
      "Training iteration: 3465\n",
      "Improved validation loss from: 0.0235999196767807  to: 0.023598608374595643\n",
      "Training iteration: 3466\n",
      "Improved validation loss from: 0.023598608374595643  to: 0.023597025871276857\n",
      "Training iteration: 3467\n",
      "Improved validation loss from: 0.023597025871276857  to: 0.023595348000526428\n",
      "Training iteration: 3468\n",
      "Improved validation loss from: 0.023595348000526428  to: 0.023593755066394807\n",
      "Training iteration: 3469\n",
      "Improved validation loss from: 0.023593755066394807  to: 0.023592348396778106\n",
      "Training iteration: 3470\n",
      "Improved validation loss from: 0.023592348396778106  to: 0.023591014742851257\n",
      "Training iteration: 3471\n",
      "Improved validation loss from: 0.023591014742851257  to: 0.023589649796485902\n",
      "Training iteration: 3472\n",
      "Improved validation loss from: 0.023589649796485902  to: 0.023588287830352783\n",
      "Training iteration: 3473\n",
      "Improved validation loss from: 0.023588287830352783  to: 0.023586659133434294\n",
      "Training iteration: 3474\n",
      "Improved validation loss from: 0.023586659133434294  to: 0.023585064709186553\n",
      "Training iteration: 3475\n",
      "Improved validation loss from: 0.023585064709186553  to: 0.023583650588989258\n",
      "Training iteration: 3476\n",
      "Improved validation loss from: 0.023583650588989258  to: 0.023582379519939422\n",
      "Training iteration: 3477\n",
      "Improved validation loss from: 0.023582379519939422  to: 0.02358107566833496\n",
      "Training iteration: 3478\n",
      "Improved validation loss from: 0.02358107566833496  to: 0.023579545319080353\n",
      "Training iteration: 3479\n",
      "Improved validation loss from: 0.023579545319080353  to: 0.023578062653541565\n",
      "Training iteration: 3480\n",
      "Improved validation loss from: 0.023578062653541565  to: 0.023576505482196808\n",
      "Training iteration: 3481\n",
      "Improved validation loss from: 0.023576505482196808  to: 0.02357519567012787\n",
      "Training iteration: 3482\n",
      "Improved validation loss from: 0.02357519567012787  to: 0.023573879897594453\n",
      "Training iteration: 3483\n",
      "Improved validation loss from: 0.023573879897594453  to: 0.023572584986686705\n",
      "Training iteration: 3484\n",
      "Improved validation loss from: 0.023572584986686705  to: 0.02357117235660553\n",
      "Training iteration: 3485\n",
      "Improved validation loss from: 0.02357117235660553  to: 0.023569671809673308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3486\n",
      "Improved validation loss from: 0.023569671809673308  to: 0.02356817275285721\n",
      "Training iteration: 3487\n",
      "Improved validation loss from: 0.02356817275285721  to: 0.023566846549510957\n",
      "Training iteration: 3488\n",
      "Improved validation loss from: 0.023566846549510957  to: 0.023565630614757537\n",
      "Training iteration: 3489\n",
      "Improved validation loss from: 0.023565630614757537  to: 0.023564335703849793\n",
      "Training iteration: 3490\n",
      "Improved validation loss from: 0.023564335703849793  to: 0.0235629603266716\n",
      "Training iteration: 3491\n",
      "Improved validation loss from: 0.0235629603266716  to: 0.023561401665210722\n",
      "Training iteration: 3492\n",
      "Improved validation loss from: 0.023561401665210722  to: 0.023560003936290742\n",
      "Training iteration: 3493\n",
      "Improved validation loss from: 0.023560003936290742  to: 0.023558716475963592\n",
      "Training iteration: 3494\n",
      "Improved validation loss from: 0.023558716475963592  to: 0.023557595908641815\n",
      "Training iteration: 3495\n",
      "Improved validation loss from: 0.023557595908641815  to: 0.023556323349475862\n",
      "Training iteration: 3496\n",
      "Improved validation loss from: 0.023556323349475862  to: 0.023554912209510802\n",
      "Training iteration: 3497\n",
      "Improved validation loss from: 0.023554912209510802  to: 0.023553423583507538\n",
      "Training iteration: 3498\n",
      "Improved validation loss from: 0.023553423583507538  to: 0.023552021384239195\n",
      "Training iteration: 3499\n",
      "Improved validation loss from: 0.023552021384239195  to: 0.023550716042518616\n",
      "Training iteration: 3500\n",
      "Improved validation loss from: 0.023550716042518616  to: 0.023549571633338928\n",
      "Training iteration: 3501\n",
      "Improved validation loss from: 0.023549571633338928  to: 0.023548403382301332\n",
      "Training iteration: 3502\n",
      "Improved validation loss from: 0.023548403382301332  to: 0.023547101020812988\n",
      "Training iteration: 3503\n",
      "Improved validation loss from: 0.023547101020812988  to: 0.023545649647712708\n",
      "Training iteration: 3504\n",
      "Improved validation loss from: 0.023545649647712708  to: 0.02354423552751541\n",
      "Training iteration: 3505\n",
      "Improved validation loss from: 0.02354423552751541  to: 0.02354303151369095\n",
      "Training iteration: 3506\n",
      "Improved validation loss from: 0.02354303151369095  to: 0.02354181706905365\n",
      "Training iteration: 3507\n",
      "Improved validation loss from: 0.02354181706905365  to: 0.02354067862033844\n",
      "Training iteration: 3508\n",
      "Improved validation loss from: 0.02354067862033844  to: 0.02353932857513428\n",
      "Training iteration: 3509\n",
      "Improved validation loss from: 0.02353932857513428  to: 0.023538002371788026\n",
      "Training iteration: 3510\n",
      "Improved validation loss from: 0.023538002371788026  to: 0.02353673428297043\n",
      "Training iteration: 3511\n",
      "Improved validation loss from: 0.02353673428297043  to: 0.02353549897670746\n",
      "Training iteration: 3512\n",
      "Improved validation loss from: 0.02353549897670746  to: 0.023534306883811952\n",
      "Training iteration: 3513\n",
      "Improved validation loss from: 0.023534306883811952  to: 0.02353312522172928\n",
      "Training iteration: 3514\n",
      "Improved validation loss from: 0.02353312522172928  to: 0.0235319048166275\n",
      "Training iteration: 3515\n",
      "Improved validation loss from: 0.0235319048166275  to: 0.023530371487140656\n",
      "Training iteration: 3516\n",
      "Improved validation loss from: 0.023530371487140656  to: 0.02352910488843918\n",
      "Training iteration: 3517\n",
      "Improved validation loss from: 0.02352910488843918  to: 0.023527976870536805\n",
      "Training iteration: 3518\n",
      "Improved validation loss from: 0.023527976870536805  to: 0.02352692633867264\n",
      "Training iteration: 3519\n",
      "Improved validation loss from: 0.02352692633867264  to: 0.023525729775428772\n",
      "Training iteration: 3520\n",
      "Improved validation loss from: 0.023525729775428772  to: 0.023524375259876253\n",
      "Training iteration: 3521\n",
      "Improved validation loss from: 0.023524375259876253  to: 0.02352299690246582\n",
      "Training iteration: 3522\n",
      "Improved validation loss from: 0.02352299690246582  to: 0.023521895706653594\n",
      "Training iteration: 3523\n",
      "Improved validation loss from: 0.023521895706653594  to: 0.023520660400390626\n",
      "Training iteration: 3524\n",
      "Improved validation loss from: 0.023520660400390626  to: 0.023519590497016907\n",
      "Training iteration: 3525\n",
      "Improved validation loss from: 0.023519590497016907  to: 0.02351844310760498\n",
      "Training iteration: 3526\n",
      "Improved validation loss from: 0.02351844310760498  to: 0.023517151176929475\n",
      "Training iteration: 3527\n",
      "Improved validation loss from: 0.023517151176929475  to: 0.023516058921813965\n",
      "Training iteration: 3528\n",
      "Improved validation loss from: 0.023516058921813965  to: 0.02351485788822174\n",
      "Training iteration: 3529\n",
      "Improved validation loss from: 0.02351485788822174  to: 0.023513583838939665\n",
      "Training iteration: 3530\n",
      "Improved validation loss from: 0.023513583838939665  to: 0.023512358963489532\n",
      "Training iteration: 3531\n",
      "Improved validation loss from: 0.023512358963489532  to: 0.023511186242103577\n",
      "Training iteration: 3532\n",
      "Improved validation loss from: 0.023511186242103577  to: 0.023510079085826873\n",
      "Training iteration: 3533\n",
      "Improved validation loss from: 0.023510079085826873  to: 0.023509106040000914\n",
      "Training iteration: 3534\n",
      "Improved validation loss from: 0.023509106040000914  to: 0.023507861793041228\n",
      "Training iteration: 3535\n",
      "Improved validation loss from: 0.023507861793041228  to: 0.0235066220164299\n",
      "Training iteration: 3536\n",
      "Improved validation loss from: 0.0235066220164299  to: 0.023505418002605437\n",
      "Training iteration: 3537\n",
      "Improved validation loss from: 0.023505418002605437  to: 0.023504313826560975\n",
      "Training iteration: 3538\n",
      "Improved validation loss from: 0.023504313826560975  to: 0.02350328415632248\n",
      "Training iteration: 3539\n",
      "Improved validation loss from: 0.02350328415632248  to: 0.02350225895643234\n",
      "Training iteration: 3540\n",
      "Improved validation loss from: 0.02350225895643234  to: 0.023501157760620117\n",
      "Training iteration: 3541\n",
      "Improved validation loss from: 0.023501157760620117  to: 0.023499949276447295\n",
      "Training iteration: 3542\n",
      "Improved validation loss from: 0.023499949276447295  to: 0.023498788475990295\n",
      "Training iteration: 3543\n",
      "Improved validation loss from: 0.023498788475990295  to: 0.023497553169727327\n",
      "Training iteration: 3544\n",
      "Improved validation loss from: 0.023497553169727327  to: 0.023496489226818084\n",
      "Training iteration: 3545\n",
      "Improved validation loss from: 0.023496489226818084  to: 0.023495443165302277\n",
      "Training iteration: 3546\n",
      "Improved validation loss from: 0.023495443165302277  to: 0.02349437475204468\n",
      "Training iteration: 3547\n",
      "Improved validation loss from: 0.02349437475204468  to: 0.023493269085884096\n",
      "Training iteration: 3548\n",
      "Improved validation loss from: 0.023493269085884096  to: 0.023492126166820525\n",
      "Training iteration: 3549\n",
      "Improved validation loss from: 0.023492126166820525  to: 0.023491087555885314\n",
      "Training iteration: 3550\n",
      "Improved validation loss from: 0.023491087555885314  to: 0.023489972949028014\n",
      "Training iteration: 3551\n",
      "Improved validation loss from: 0.023489972949028014  to: 0.023489005863666534\n",
      "Training iteration: 3552\n",
      "Improved validation loss from: 0.023489005863666534  to: 0.023487982153892518\n",
      "Training iteration: 3553\n",
      "Improved validation loss from: 0.023487982153892518  to: 0.023486872017383576\n",
      "Training iteration: 3554\n",
      "Improved validation loss from: 0.023486872017383576  to: 0.02348562777042389\n",
      "Training iteration: 3555\n",
      "Improved validation loss from: 0.02348562777042389  to: 0.023484604060649873\n",
      "Training iteration: 3556\n",
      "Improved validation loss from: 0.023484604060649873  to: 0.023483553528785707\n",
      "Training iteration: 3557\n",
      "Improved validation loss from: 0.023483553528785707  to: 0.023482617735862733\n",
      "Training iteration: 3558\n",
      "Improved validation loss from: 0.023482617735862733  to: 0.02348148077726364\n",
      "Training iteration: 3559\n",
      "Improved validation loss from: 0.02348148077726364  to: 0.02348036766052246\n",
      "Training iteration: 3560\n",
      "Improved validation loss from: 0.02348036766052246  to: 0.02347920387983322\n",
      "Training iteration: 3561\n",
      "Improved validation loss from: 0.02347920387983322  to: 0.023478189110755922\n",
      "Training iteration: 3562\n",
      "Improved validation loss from: 0.023478189110755922  to: 0.023477208614349366\n",
      "Training iteration: 3563\n",
      "Improved validation loss from: 0.023477208614349366  to: 0.02347617894411087\n",
      "Training iteration: 3564\n",
      "Improved validation loss from: 0.02347617894411087  to: 0.023475134372711183\n",
      "Training iteration: 3565\n",
      "Improved validation loss from: 0.023475134372711183  to: 0.023473963141441345\n",
      "Training iteration: 3566\n",
      "Improved validation loss from: 0.023473963141441345  to: 0.023473019897937774\n",
      "Training iteration: 3567\n",
      "Improved validation loss from: 0.023473019897937774  to: 0.023472103476524352\n",
      "Training iteration: 3568\n",
      "Improved validation loss from: 0.023472103476524352  to: 0.023471167683601378\n",
      "Training iteration: 3569\n",
      "Improved validation loss from: 0.023471167683601378  to: 0.023470011353492738\n",
      "Training iteration: 3570\n",
      "Improved validation loss from: 0.023470011353492738  to: 0.02346894294023514\n",
      "Training iteration: 3571\n",
      "Improved validation loss from: 0.02346894294023514  to: 0.023467950522899628\n",
      "Training iteration: 3572\n",
      "Improved validation loss from: 0.023467950522899628  to: 0.02346702516078949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3573\n",
      "Improved validation loss from: 0.02346702516078949  to: 0.023466050624847412\n",
      "Training iteration: 3574\n",
      "Improved validation loss from: 0.023466050624847412  to: 0.023465009033679964\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.023465009033679964  to: 0.023464007675647734\n",
      "Training iteration: 3576\n",
      "Improved validation loss from: 0.023464007675647734  to: 0.023463091254234313\n",
      "Training iteration: 3577\n",
      "Improved validation loss from: 0.023463091254234313  to: 0.023462191224098206\n",
      "Training iteration: 3578\n",
      "Improved validation loss from: 0.023462191224098206  to: 0.023461155593395233\n",
      "Training iteration: 3579\n",
      "Improved validation loss from: 0.023461155593395233  to: 0.023460082709789276\n",
      "Training iteration: 3580\n",
      "Improved validation loss from: 0.023460082709789276  to: 0.023459091782569885\n",
      "Training iteration: 3581\n",
      "Improved validation loss from: 0.023459091782569885  to: 0.02345818728208542\n",
      "Training iteration: 3582\n",
      "Improved validation loss from: 0.02345818728208542  to: 0.023457355797290802\n",
      "Training iteration: 3583\n",
      "Improved validation loss from: 0.023457355797290802  to: 0.023456308245658874\n",
      "Training iteration: 3584\n",
      "Improved validation loss from: 0.023456308245658874  to: 0.023455262184143066\n",
      "Training iteration: 3585\n",
      "Improved validation loss from: 0.023455262184143066  to: 0.023454275727272034\n",
      "Training iteration: 3586\n",
      "Improved validation loss from: 0.023454275727272034  to: 0.023453423380851747\n",
      "Training iteration: 3587\n",
      "Improved validation loss from: 0.023453423380851747  to: 0.02345268726348877\n",
      "Training iteration: 3588\n",
      "Improved validation loss from: 0.02345268726348877  to: 0.02345169335603714\n",
      "Training iteration: 3589\n",
      "Improved validation loss from: 0.02345169335603714  to: 0.023450593650341033\n",
      "Training iteration: 3590\n",
      "Improved validation loss from: 0.023450593650341033  to: 0.023449477553367615\n",
      "Training iteration: 3591\n",
      "Improved validation loss from: 0.023449477553367615  to: 0.023448543250560762\n",
      "Training iteration: 3592\n",
      "Improved validation loss from: 0.023448543250560762  to: 0.023447784781455993\n",
      "Training iteration: 3593\n",
      "Improved validation loss from: 0.023447784781455993  to: 0.02344697415828705\n",
      "Training iteration: 3594\n",
      "Improved validation loss from: 0.02344697415828705  to: 0.02344605028629303\n",
      "Training iteration: 3595\n",
      "Improved validation loss from: 0.02344605028629303  to: 0.023444993793964385\n",
      "Training iteration: 3596\n",
      "Improved validation loss from: 0.023444993793964385  to: 0.023444096744060516\n",
      "Training iteration: 3597\n",
      "Improved validation loss from: 0.023444096744060516  to: 0.023443107306957246\n",
      "Training iteration: 3598\n",
      "Improved validation loss from: 0.023443107306957246  to: 0.023442280292510987\n",
      "Training iteration: 3599\n",
      "Improved validation loss from: 0.023442280292510987  to: 0.023441359400749207\n",
      "Training iteration: 3600\n",
      "Improved validation loss from: 0.023441359400749207  to: 0.023440460860729217\n",
      "Training iteration: 3601\n",
      "Improved validation loss from: 0.023440460860729217  to: 0.02343955487012863\n",
      "Training iteration: 3602\n",
      "Improved validation loss from: 0.02343955487012863  to: 0.023438672721385955\n",
      "Training iteration: 3603\n",
      "Improved validation loss from: 0.023438672721385955  to: 0.023437757790088654\n",
      "Training iteration: 3604\n",
      "Improved validation loss from: 0.023437757790088654  to: 0.023436954617500304\n",
      "Training iteration: 3605\n",
      "Improved validation loss from: 0.023436954617500304  to: 0.023436093330383302\n",
      "Training iteration: 3606\n",
      "Improved validation loss from: 0.023436093330383302  to: 0.02343514859676361\n",
      "Training iteration: 3607\n",
      "Improved validation loss from: 0.02343514859676361  to: 0.02343420535326004\n",
      "Training iteration: 3608\n",
      "Improved validation loss from: 0.02343420535326004  to: 0.02343330830335617\n",
      "Training iteration: 3609\n",
      "Improved validation loss from: 0.02343330830335617  to: 0.02343248873949051\n",
      "Training iteration: 3610\n",
      "Improved validation loss from: 0.02343248873949051  to: 0.023431703448295593\n",
      "Training iteration: 3611\n",
      "Improved validation loss from: 0.023431703448295593  to: 0.023430924117565154\n",
      "Training iteration: 3612\n",
      "Improved validation loss from: 0.023430924117565154  to: 0.02343002110719681\n",
      "Training iteration: 3613\n",
      "Improved validation loss from: 0.02343002110719681  to: 0.02342901974916458\n",
      "Training iteration: 3614\n",
      "Improved validation loss from: 0.02342901974916458  to: 0.02342802584171295\n",
      "Training iteration: 3615\n",
      "Improved validation loss from: 0.02342802584171295  to: 0.023427371680736542\n",
      "Training iteration: 3616\n",
      "Improved validation loss from: 0.023427371680736542  to: 0.023426762223243712\n",
      "Training iteration: 3617\n",
      "Improved validation loss from: 0.023426762223243712  to: 0.023425951600074768\n",
      "Training iteration: 3618\n",
      "Improved validation loss from: 0.023425951600074768  to: 0.0234248548746109\n",
      "Training iteration: 3619\n",
      "Improved validation loss from: 0.0234248548746109  to: 0.023423781991004942\n",
      "Training iteration: 3620\n",
      "Improved validation loss from: 0.023423781991004942  to: 0.023423171043395995\n",
      "Training iteration: 3621\n",
      "Improved validation loss from: 0.023423171043395995  to: 0.02342236042022705\n",
      "Training iteration: 3622\n",
      "Improved validation loss from: 0.02342236042022705  to: 0.0234216645359993\n",
      "Training iteration: 3623\n",
      "Improved validation loss from: 0.0234216645359993  to: 0.023420839011669158\n",
      "Training iteration: 3624\n",
      "Improved validation loss from: 0.023420839011669158  to: 0.02341996431350708\n",
      "Training iteration: 3625\n",
      "Improved validation loss from: 0.02341996431350708  to: 0.02341907024383545\n",
      "Training iteration: 3626\n",
      "Improved validation loss from: 0.02341907024383545  to: 0.023418298363685607\n",
      "Training iteration: 3627\n",
      "Improved validation loss from: 0.023418298363685607  to: 0.023417477309703828\n",
      "Training iteration: 3628\n",
      "Improved validation loss from: 0.023417477309703828  to: 0.02341686189174652\n",
      "Training iteration: 3629\n",
      "Improved validation loss from: 0.02341686189174652  to: 0.02341609001159668\n",
      "Training iteration: 3630\n",
      "Improved validation loss from: 0.02341609001159668  to: 0.023415212333202363\n",
      "Training iteration: 3631\n",
      "Improved validation loss from: 0.023415212333202363  to: 0.02341422587633133\n",
      "Training iteration: 3632\n",
      "Improved validation loss from: 0.02341422587633133  to: 0.023413495719432832\n",
      "Training iteration: 3633\n",
      "Improved validation loss from: 0.023413495719432832  to: 0.023412635922431944\n",
      "Training iteration: 3634\n",
      "Improved validation loss from: 0.023412635922431944  to: 0.023412027955055238\n",
      "Training iteration: 3635\n",
      "Improved validation loss from: 0.023412027955055238  to: 0.023411229252815247\n",
      "Training iteration: 3636\n",
      "Improved validation loss from: 0.023411229252815247  to: 0.023410396277904512\n",
      "Training iteration: 3637\n",
      "Improved validation loss from: 0.023410396277904512  to: 0.023409652709960937\n",
      "Training iteration: 3638\n",
      "Improved validation loss from: 0.023409652709960937  to: 0.023408722877502442\n",
      "Training iteration: 3639\n",
      "Improved validation loss from: 0.023408722877502442  to: 0.023408094048500062\n",
      "Training iteration: 3640\n",
      "Improved validation loss from: 0.023408094048500062  to: 0.023407533764839172\n",
      "Training iteration: 3641\n",
      "Improved validation loss from: 0.023407533764839172  to: 0.02340671718120575\n",
      "Training iteration: 3642\n",
      "Improved validation loss from: 0.02340671718120575  to: 0.023405656218528748\n",
      "Training iteration: 3643\n",
      "Improved validation loss from: 0.023405656218528748  to: 0.02340492755174637\n",
      "Training iteration: 3644\n",
      "Improved validation loss from: 0.02340492755174637  to: 0.023404304683208466\n",
      "Training iteration: 3645\n",
      "Improved validation loss from: 0.023404304683208466  to: 0.023403744399547576\n",
      "Training iteration: 3646\n",
      "Improved validation loss from: 0.023403744399547576  to: 0.023402956128120423\n",
      "Training iteration: 3647\n",
      "Improved validation loss from: 0.023402956128120423  to: 0.023402056097984313\n",
      "Training iteration: 3648\n",
      "Improved validation loss from: 0.023402056097984313  to: 0.023401212692260743\n",
      "Training iteration: 3649\n",
      "Improved validation loss from: 0.023401212692260743  to: 0.023400631546974183\n",
      "Training iteration: 3650\n",
      "Improved validation loss from: 0.023400631546974183  to: 0.023399856686592103\n",
      "Training iteration: 3651\n",
      "Improved validation loss from: 0.023399856686592103  to: 0.023399266600608825\n",
      "Training iteration: 3652\n",
      "Improved validation loss from: 0.023399266600608825  to: 0.02339859902858734\n",
      "Training iteration: 3653\n",
      "Improved validation loss from: 0.02339859902858734  to: 0.023397724330425262\n",
      "Training iteration: 3654\n",
      "Improved validation loss from: 0.023397724330425262  to: 0.023396892845630644\n",
      "Training iteration: 3655\n",
      "Improved validation loss from: 0.023396892845630644  to: 0.02339615374803543\n",
      "Training iteration: 3656\n",
      "Improved validation loss from: 0.02339615374803543  to: 0.023395653069019317\n",
      "Training iteration: 3657\n",
      "Improved validation loss from: 0.023395653069019317  to: 0.02339501827955246\n",
      "Training iteration: 3658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02339501827955246  to: 0.02339424192905426\n",
      "Training iteration: 3659\n",
      "Improved validation loss from: 0.02339424192905426  to: 0.023393359780311585\n",
      "Training iteration: 3660\n",
      "Improved validation loss from: 0.023393359780311585  to: 0.02339266091585159\n",
      "Training iteration: 3661\n",
      "Improved validation loss from: 0.02339266091585159  to: 0.023392114043235778\n",
      "Training iteration: 3662\n",
      "Improved validation loss from: 0.023392114043235778  to: 0.02339143753051758\n",
      "Training iteration: 3663\n",
      "Improved validation loss from: 0.02339143753051758  to: 0.023390693962574004\n",
      "Training iteration: 3664\n",
      "Improved validation loss from: 0.023390693962574004  to: 0.023390021920204163\n",
      "Training iteration: 3665\n",
      "Improved validation loss from: 0.023390021920204163  to: 0.023389196395874022\n",
      "Training iteration: 3666\n",
      "Improved validation loss from: 0.023389196395874022  to: 0.023388543725013734\n",
      "Training iteration: 3667\n",
      "Improved validation loss from: 0.023388543725013734  to: 0.023387880623340608\n",
      "Training iteration: 3668\n",
      "Improved validation loss from: 0.023387880623340608  to: 0.023387250304222108\n",
      "Training iteration: 3669\n",
      "Improved validation loss from: 0.023387250304222108  to: 0.023386716842651367\n",
      "Training iteration: 3670\n",
      "Improved validation loss from: 0.023386716842651367  to: 0.023385925590991972\n",
      "Training iteration: 3671\n",
      "Improved validation loss from: 0.023385925590991972  to: 0.023385122418403625\n",
      "Training iteration: 3672\n",
      "Improved validation loss from: 0.023385122418403625  to: 0.02338441163301468\n",
      "Training iteration: 3673\n",
      "Improved validation loss from: 0.02338441163301468  to: 0.023383812606334688\n",
      "Training iteration: 3674\n",
      "Improved validation loss from: 0.023383812606334688  to: 0.023383398354053498\n",
      "Training iteration: 3675\n",
      "Improved validation loss from: 0.023383398354053498  to: 0.023382660746574403\n",
      "Training iteration: 3676\n",
      "Improved validation loss from: 0.023382660746574403  to: 0.023381800949573518\n",
      "Training iteration: 3677\n",
      "Improved validation loss from: 0.023381800949573518  to: 0.02338104248046875\n",
      "Training iteration: 3678\n",
      "Improved validation loss from: 0.02338104248046875  to: 0.0233805850148201\n",
      "Training iteration: 3679\n",
      "Improved validation loss from: 0.0233805850148201  to: 0.023379948735237122\n",
      "Training iteration: 3680\n",
      "Improved validation loss from: 0.023379948735237122  to: 0.023379513621330263\n",
      "Training iteration: 3681\n",
      "Improved validation loss from: 0.023379513621330263  to: 0.023378734290599824\n",
      "Training iteration: 3682\n",
      "Improved validation loss from: 0.023378734290599824  to: 0.02337794005870819\n",
      "Training iteration: 3683\n",
      "Improved validation loss from: 0.02337794005870819  to: 0.023377323150634767\n",
      "Training iteration: 3684\n",
      "Improved validation loss from: 0.023377323150634767  to: 0.02337670773267746\n",
      "Training iteration: 3685\n",
      "Improved validation loss from: 0.02337670773267746  to: 0.02337619811296463\n",
      "Training iteration: 3686\n",
      "Improved validation loss from: 0.02337619811296463  to: 0.02337566912174225\n",
      "Training iteration: 3687\n",
      "Improved validation loss from: 0.02337566912174225  to: 0.023374874889850617\n",
      "Training iteration: 3688\n",
      "Improved validation loss from: 0.023374874889850617  to: 0.023374147713184357\n",
      "Training iteration: 3689\n",
      "Improved validation loss from: 0.023374147713184357  to: 0.023373541235923768\n",
      "Training iteration: 3690\n",
      "Improved validation loss from: 0.023373541235923768  to: 0.02337305247783661\n",
      "Training iteration: 3691\n",
      "Improved validation loss from: 0.02337305247783661  to: 0.02337251156568527\n",
      "Training iteration: 3692\n",
      "Improved validation loss from: 0.02337251156568527  to: 0.023371827602386475\n",
      "Training iteration: 3693\n",
      "Improved validation loss from: 0.023371827602386475  to: 0.02337113320827484\n",
      "Training iteration: 3694\n",
      "Improved validation loss from: 0.02337113320827484  to: 0.023370346426963805\n",
      "Training iteration: 3695\n",
      "Improved validation loss from: 0.023370346426963805  to: 0.02336985170841217\n",
      "Training iteration: 3696\n",
      "Improved validation loss from: 0.02336985170841217  to: 0.023369474709033965\n",
      "Training iteration: 3697\n",
      "Improved validation loss from: 0.023369474709033965  to: 0.023368942737579345\n",
      "Training iteration: 3698\n",
      "Improved validation loss from: 0.023368942737579345  to: 0.02336820065975189\n",
      "Training iteration: 3699\n",
      "Improved validation loss from: 0.02336820065975189  to: 0.023367562890052797\n",
      "Training iteration: 3700\n",
      "Improved validation loss from: 0.023367562890052797  to: 0.023366861045360565\n",
      "Training iteration: 3701\n",
      "Improved validation loss from: 0.023366861045360565  to: 0.02336634397506714\n",
      "Training iteration: 3702\n",
      "Improved validation loss from: 0.02336634397506714  to: 0.02336585968732834\n",
      "Training iteration: 3703\n",
      "Improved validation loss from: 0.02336585968732834  to: 0.0233653262257576\n",
      "Training iteration: 3704\n",
      "Improved validation loss from: 0.0233653262257576  to: 0.02336471825838089\n",
      "Training iteration: 3705\n",
      "Improved validation loss from: 0.02336471825838089  to: 0.023364052176475525\n",
      "Training iteration: 3706\n",
      "Improved validation loss from: 0.023364052176475525  to: 0.023363474011421203\n",
      "Training iteration: 3707\n",
      "Improved validation loss from: 0.023363474011421203  to: 0.02336297333240509\n",
      "Training iteration: 3708\n",
      "Improved validation loss from: 0.02336297333240509  to: 0.023362484574317933\n",
      "Training iteration: 3709\n",
      "Improved validation loss from: 0.023362484574317933  to: 0.023361973464488983\n",
      "Training iteration: 3710\n",
      "Improved validation loss from: 0.023361973464488983  to: 0.02336115837097168\n",
      "Training iteration: 3711\n",
      "Improved validation loss from: 0.02336115837097168  to: 0.02336055338382721\n",
      "Training iteration: 3712\n",
      "Improved validation loss from: 0.02336055338382721  to: 0.023360128700733184\n",
      "Training iteration: 3713\n",
      "Improved validation loss from: 0.023360128700733184  to: 0.023359569907188415\n",
      "Training iteration: 3714\n",
      "Improved validation loss from: 0.023359569907188415  to: 0.02335912436246872\n",
      "Training iteration: 3715\n",
      "Improved validation loss from: 0.02335912436246872  to: 0.023358480632305147\n",
      "Training iteration: 3716\n",
      "Improved validation loss from: 0.023358480632305147  to: 0.023357847332954408\n",
      "Training iteration: 3717\n",
      "Improved validation loss from: 0.023357847332954408  to: 0.02335749417543411\n",
      "Training iteration: 3718\n",
      "Improved validation loss from: 0.02335749417543411  to: 0.023357348144054414\n",
      "Training iteration: 3719\n",
      "Improved validation loss from: 0.023357348144054414  to: 0.023356787860393524\n",
      "Training iteration: 3720\n",
      "Improved validation loss from: 0.023356787860393524  to: 0.023355841636657715\n",
      "Training iteration: 3721\n",
      "Improved validation loss from: 0.023355841636657715  to: 0.023354823887348174\n",
      "Training iteration: 3722\n",
      "Improved validation loss from: 0.023354823887348174  to: 0.023354068398475647\n",
      "Training iteration: 3723\n",
      "Improved validation loss from: 0.023354068398475647  to: 0.023353520035743713\n",
      "Training iteration: 3724\n",
      "Improved validation loss from: 0.023353520035743713  to: 0.02335290014743805\n",
      "Training iteration: 3725\n",
      "Improved validation loss from: 0.02335290014743805  to: 0.02335198372602463\n",
      "Training iteration: 3726\n",
      "Improved validation loss from: 0.02335198372602463  to: 0.023350910842418672\n",
      "Training iteration: 3727\n",
      "Improved validation loss from: 0.023350910842418672  to: 0.023349933326244354\n",
      "Training iteration: 3728\n",
      "Improved validation loss from: 0.023349933326244354  to: 0.023349137604236604\n",
      "Training iteration: 3729\n",
      "Improved validation loss from: 0.023349137604236604  to: 0.023348721861839294\n",
      "Training iteration: 3730\n",
      "Improved validation loss from: 0.023348721861839294  to: 0.02334802895784378\n",
      "Training iteration: 3731\n",
      "Improved validation loss from: 0.02334802895784378  to: 0.023347234725952147\n",
      "Training iteration: 3732\n",
      "Improved validation loss from: 0.023347234725952147  to: 0.023346385359764098\n",
      "Training iteration: 3733\n",
      "Improved validation loss from: 0.023346385359764098  to: 0.023345637321472167\n",
      "Training iteration: 3734\n",
      "Improved validation loss from: 0.023345637321472167  to: 0.023345184326171876\n",
      "Training iteration: 3735\n",
      "Improved validation loss from: 0.023345184326171876  to: 0.02334476411342621\n",
      "Training iteration: 3736\n",
      "Improved validation loss from: 0.02334476411342621  to: 0.023344166576862335\n",
      "Training iteration: 3737\n",
      "Improved validation loss from: 0.023344166576862335  to: 0.023343487083911894\n",
      "Training iteration: 3738\n",
      "Improved validation loss from: 0.023343487083911894  to: 0.023342642188072204\n",
      "Training iteration: 3739\n",
      "Improved validation loss from: 0.023342642188072204  to: 0.023342022299766542\n",
      "Training iteration: 3740\n",
      "Improved validation loss from: 0.023342022299766542  to: 0.02334151566028595\n",
      "Training iteration: 3741\n",
      "Improved validation loss from: 0.02334151566028595  to: 0.02334116995334625\n",
      "Training iteration: 3742\n",
      "Improved validation loss from: 0.02334116995334625  to: 0.023340873420238495\n",
      "Training iteration: 3743\n",
      "Improved validation loss from: 0.023340873420238495  to: 0.02334023416042328\n",
      "Training iteration: 3744\n",
      "Improved validation loss from: 0.02334023416042328  to: 0.023339399695396425\n",
      "Training iteration: 3745\n",
      "Improved validation loss from: 0.023339399695396425  to: 0.02333870381116867\n",
      "Training iteration: 3746\n",
      "Improved validation loss from: 0.02333870381116867  to: 0.02333833873271942\n",
      "Training iteration: 3747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02333833873271942  to: 0.023338159918785094\n",
      "Training iteration: 3748\n",
      "Improved validation loss from: 0.023338159918785094  to: 0.02333764135837555\n",
      "Training iteration: 3749\n",
      "Improved validation loss from: 0.02333764135837555  to: 0.023337061703205108\n",
      "Training iteration: 3750\n",
      "Improved validation loss from: 0.023337061703205108  to: 0.02333637773990631\n",
      "Training iteration: 3751\n",
      "Improved validation loss from: 0.02333637773990631  to: 0.02333574593067169\n",
      "Training iteration: 3752\n",
      "Improved validation loss from: 0.02333574593067169  to: 0.023335365951061247\n",
      "Training iteration: 3753\n",
      "Improved validation loss from: 0.023335365951061247  to: 0.02333497554063797\n",
      "Training iteration: 3754\n",
      "Improved validation loss from: 0.02333497554063797  to: 0.02333444058895111\n",
      "Training iteration: 3755\n",
      "Improved validation loss from: 0.02333444058895111  to: 0.023333629965782164\n",
      "Training iteration: 3756\n",
      "Improved validation loss from: 0.023333629965782164  to: 0.023332934081554412\n",
      "Training iteration: 3757\n",
      "Improved validation loss from: 0.023332934081554412  to: 0.023332569003105163\n",
      "Training iteration: 3758\n",
      "Improved validation loss from: 0.023332569003105163  to: 0.023332126438617706\n",
      "Training iteration: 3759\n",
      "Improved validation loss from: 0.023332126438617706  to: 0.02333155870437622\n",
      "Training iteration: 3760\n",
      "Improved validation loss from: 0.02333155870437622  to: 0.023330757021903993\n",
      "Training iteration: 3761\n",
      "Improved validation loss from: 0.023330757021903993  to: 0.023330113291740416\n",
      "Training iteration: 3762\n",
      "Validation loss (no improvement): 0.023330512642860412\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.023330263793468475\n",
      "Training iteration: 3764\n",
      "Improved validation loss from: 0.023330113291740416  to: 0.023329421877861023\n",
      "Training iteration: 3765\n",
      "Improved validation loss from: 0.023329421877861023  to: 0.023328447341918947\n",
      "Training iteration: 3766\n",
      "Improved validation loss from: 0.023328447341918947  to: 0.023328037559986116\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.023328232765197753\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.023328296840190887\n",
      "Training iteration: 3769\n",
      "Improved validation loss from: 0.023328037559986116  to: 0.023327693343162537\n",
      "Training iteration: 3770\n",
      "Improved validation loss from: 0.023327693343162537  to: 0.023326651751995088\n",
      "Training iteration: 3771\n",
      "Improved validation loss from: 0.023326651751995088  to: 0.02332574427127838\n",
      "Training iteration: 3772\n",
      "Improved validation loss from: 0.02332574427127838  to: 0.02332550287246704\n",
      "Training iteration: 3773\n",
      "Improved validation loss from: 0.02332550287246704  to: 0.023325440287590028\n",
      "Training iteration: 3774\n",
      "Improved validation loss from: 0.023325440287590028  to: 0.023325014114379882\n",
      "Training iteration: 3775\n",
      "Improved validation loss from: 0.023325014114379882  to: 0.023323988914489745\n",
      "Training iteration: 3776\n",
      "Improved validation loss from: 0.023323988914489745  to: 0.02332303524017334\n",
      "Training iteration: 3777\n",
      "Improved validation loss from: 0.02332303524017334  to: 0.023322391510009765\n",
      "Training iteration: 3778\n",
      "Improved validation loss from: 0.023322391510009765  to: 0.02332218885421753\n",
      "Training iteration: 3779\n",
      "Improved validation loss from: 0.02332218885421753  to: 0.023321899771690368\n",
      "Training iteration: 3780\n",
      "Improved validation loss from: 0.023321899771690368  to: 0.02332121431827545\n",
      "Training iteration: 3781\n",
      "Improved validation loss from: 0.02332121431827545  to: 0.023320241272449492\n",
      "Training iteration: 3782\n",
      "Improved validation loss from: 0.023320241272449492  to: 0.023319587111473083\n",
      "Training iteration: 3783\n",
      "Improved validation loss from: 0.023319587111473083  to: 0.023319201171398164\n",
      "Training iteration: 3784\n",
      "Improved validation loss from: 0.023319201171398164  to: 0.023319005966186523\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.023319177329540253\n",
      "Training iteration: 3786\n",
      "Improved validation loss from: 0.023319005966186523  to: 0.023318667709827424\n",
      "Training iteration: 3787\n",
      "Improved validation loss from: 0.023318667709827424  to: 0.023317638039588928\n",
      "Training iteration: 3788\n",
      "Improved validation loss from: 0.023317638039588928  to: 0.02331697940826416\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.023316991329193116\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.023317289352416993\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.02331707775592804\n",
      "Training iteration: 3792\n",
      "Improved validation loss from: 0.02331697940826416  to: 0.023316292464733122\n",
      "Training iteration: 3793\n",
      "Improved validation loss from: 0.023316292464733122  to: 0.023315365612506866\n",
      "Training iteration: 3794\n",
      "Improved validation loss from: 0.023315365612506866  to: 0.02331487238407135\n",
      "Training iteration: 3795\n",
      "Improved validation loss from: 0.02331487238407135  to: 0.02331485003232956\n",
      "Training iteration: 3796\n",
      "Improved validation loss from: 0.02331485003232956  to: 0.023314718902111054\n",
      "Training iteration: 3797\n",
      "Improved validation loss from: 0.023314718902111054  to: 0.023314189910888673\n",
      "Training iteration: 3798\n",
      "Improved validation loss from: 0.023314189910888673  to: 0.023313260078430174\n",
      "Training iteration: 3799\n",
      "Improved validation loss from: 0.023313260078430174  to: 0.023312500119209288\n",
      "Training iteration: 3800\n",
      "Improved validation loss from: 0.023312500119209288  to: 0.02331196367740631\n",
      "Training iteration: 3801\n",
      "Improved validation loss from: 0.02331196367740631  to: 0.023311583697795867\n",
      "Training iteration: 3802\n",
      "Improved validation loss from: 0.023311583697795867  to: 0.023311218619346617\n",
      "Training iteration: 3803\n",
      "Improved validation loss from: 0.023311218619346617  to: 0.02331072837114334\n",
      "Training iteration: 3804\n",
      "Improved validation loss from: 0.02331072837114334  to: 0.023310060799121856\n",
      "Training iteration: 3805\n",
      "Improved validation loss from: 0.023310060799121856  to: 0.02330939769744873\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.023309743404388426\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.023309537768363954\n",
      "Training iteration: 3808\n",
      "Improved validation loss from: 0.02330939769744873  to: 0.023308810591697694\n",
      "Training iteration: 3809\n",
      "Improved validation loss from: 0.023308810591697694  to: 0.02330805957317352\n",
      "Training iteration: 3810\n",
      "Improved validation loss from: 0.02330805957317352  to: 0.023307828605175017\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.023308023810386658\n",
      "Training iteration: 3812\n",
      "Validation loss (no improvement): 0.02330792844295502\n",
      "Training iteration: 3813\n",
      "Improved validation loss from: 0.023307828605175017  to: 0.023307356238365173\n",
      "Training iteration: 3814\n",
      "Improved validation loss from: 0.023307356238365173  to: 0.023306560516357423\n",
      "Training iteration: 3815\n",
      "Improved validation loss from: 0.023306560516357423  to: 0.02330595552921295\n",
      "Training iteration: 3816\n",
      "Improved validation loss from: 0.02330595552921295  to: 0.0233059361577034\n",
      "Training iteration: 3817\n",
      "Improved validation loss from: 0.0233059361577034  to: 0.02330566942691803\n",
      "Training iteration: 3818\n",
      "Improved validation loss from: 0.02330566942691803  to: 0.023305058479309082\n",
      "Training iteration: 3819\n",
      "Improved validation loss from: 0.023305058479309082  to: 0.023304232954978944\n",
      "Training iteration: 3820\n",
      "Improved validation loss from: 0.023304232954978944  to: 0.023303619027137755\n",
      "Training iteration: 3821\n",
      "Improved validation loss from: 0.023303619027137755  to: 0.023303166031837463\n",
      "Training iteration: 3822\n",
      "Improved validation loss from: 0.023303166031837463  to: 0.023302960395812988\n",
      "Training iteration: 3823\n",
      "Improved validation loss from: 0.023302960395812988  to: 0.023302534222602846\n",
      "Training iteration: 3824\n",
      "Improved validation loss from: 0.023302534222602846  to: 0.023301827907562255\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.02330208718776703\n",
      "Training iteration: 3826\n",
      "Improved validation loss from: 0.023301827907562255  to: 0.023301753401756286\n",
      "Training iteration: 3827\n",
      "Improved validation loss from: 0.023301753401756286  to: 0.023301033675670622\n",
      "Training iteration: 3828\n",
      "Improved validation loss from: 0.023301033675670622  to: 0.02330070734024048\n",
      "Training iteration: 3829\n",
      "Improved validation loss from: 0.02330070734024048  to: 0.023300547897815705\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.023300671577453615\n",
      "Training iteration: 3831\n",
      "Improved validation loss from: 0.023300547897815705  to: 0.023300322890281677\n",
      "Training iteration: 3832\n",
      "Improved validation loss from: 0.023300322890281677  to: 0.02329978495836258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3833\n",
      "Improved validation loss from: 0.02329978495836258  to: 0.023299284279346466\n",
      "Training iteration: 3834\n",
      "Improved validation loss from: 0.023299284279346466  to: 0.023298850655555724\n",
      "Training iteration: 3835\n",
      "Improved validation loss from: 0.023298850655555724  to: 0.023298415541648864\n",
      "Training iteration: 3836\n",
      "Improved validation loss from: 0.023298415541648864  to: 0.023298120498657225\n",
      "Training iteration: 3837\n",
      "Improved validation loss from: 0.023298120498657225  to: 0.023297688364982604\n",
      "Training iteration: 3838\n",
      "Improved validation loss from: 0.023297688364982604  to: 0.02329724282026291\n",
      "Training iteration: 3839\n",
      "Improved validation loss from: 0.02329724282026291  to: 0.023296603560447694\n",
      "Training iteration: 3840\n",
      "Improved validation loss from: 0.023296603560447694  to: 0.023296085000038148\n",
      "Training iteration: 3841\n",
      "Improved validation loss from: 0.023296085000038148  to: 0.023295772075653077\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.023296210169792175\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.023295819759368896\n",
      "Training iteration: 3844\n",
      "Improved validation loss from: 0.023295772075653077  to: 0.023294925689697266\n",
      "Training iteration: 3845\n",
      "Improved validation loss from: 0.023294925689697266  to: 0.02329426258802414\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.02329430878162384\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.023294639587402344\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.023294591903686525\n",
      "Training iteration: 3849\n",
      "Improved validation loss from: 0.02329426258802414  to: 0.023293986916542053\n",
      "Training iteration: 3850\n",
      "Improved validation loss from: 0.023293986916542053  to: 0.023293085396289825\n",
      "Training iteration: 3851\n",
      "Improved validation loss from: 0.023293085396289825  to: 0.023292645812034607\n",
      "Training iteration: 3852\n",
      "Improved validation loss from: 0.023292645812034607  to: 0.023292580246925355\n",
      "Training iteration: 3853\n",
      "Improved validation loss from: 0.023292580246925355  to: 0.023292556405067444\n",
      "Training iteration: 3854\n",
      "Improved validation loss from: 0.023292556405067444  to: 0.023291973769664763\n",
      "Training iteration: 3855\n",
      "Improved validation loss from: 0.023291973769664763  to: 0.023290972411632537\n",
      "Training iteration: 3856\n",
      "Improved validation loss from: 0.023290972411632537  to: 0.023290272057056426\n",
      "Training iteration: 3857\n",
      "Improved validation loss from: 0.023290272057056426  to: 0.02329005002975464\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.02329099625349045\n",
      "Training iteration: 3859\n",
      "Validation loss (no improvement): 0.023290714621543883\n",
      "Training iteration: 3860\n",
      "Improved validation loss from: 0.02329005002975464  to: 0.023289585113525392\n",
      "Training iteration: 3861\n",
      "Improved validation loss from: 0.023289585113525392  to: 0.02328874170780182\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.02328888475894928\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.023289647698402405\n",
      "Training iteration: 3864\n",
      "Validation loss (no improvement): 0.02328966110944748\n",
      "Training iteration: 3865\n",
      "Improved validation loss from: 0.02328874170780182  to: 0.023288735747337343\n",
      "Training iteration: 3866\n",
      "Improved validation loss from: 0.023288735747337343  to: 0.023287728428840637\n",
      "Training iteration: 3867\n",
      "Improved validation loss from: 0.023287728428840637  to: 0.02328721582889557\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.023287463188171386\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.023287621140480042\n",
      "Training iteration: 3870\n",
      "Improved validation loss from: 0.02328721582889557  to: 0.023287005722522736\n",
      "Training iteration: 3871\n",
      "Improved validation loss from: 0.023287005722522736  to: 0.02328595668077469\n",
      "Training iteration: 3872\n",
      "Improved validation loss from: 0.02328595668077469  to: 0.023285050690174103\n",
      "Training iteration: 3873\n",
      "Improved validation loss from: 0.023285050690174103  to: 0.023284873366355895\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.023285993933677675\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.023285874724388124\n",
      "Training iteration: 3876\n",
      "Improved validation loss from: 0.023284873366355895  to: 0.023284764587879182\n",
      "Training iteration: 3877\n",
      "Improved validation loss from: 0.023284764587879182  to: 0.023283782601356506\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.02328389436006546\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.023284654319286346\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.023284974694252013\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.023284046351909636\n",
      "Training iteration: 3882\n",
      "Improved validation loss from: 0.023283782601356506  to: 0.02328290194272995\n",
      "Training iteration: 3883\n",
      "Improved validation loss from: 0.02328290194272995  to: 0.023282356560230255\n",
      "Training iteration: 3884\n",
      "Validation loss (no improvement): 0.023282687366008758\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.023282694816589355\n",
      "Training iteration: 3886\n",
      "Improved validation loss from: 0.023282356560230255  to: 0.023282167315483094\n",
      "Training iteration: 3887\n",
      "Improved validation loss from: 0.023282167315483094  to: 0.023281149566173553\n",
      "Training iteration: 3888\n",
      "Improved validation loss from: 0.023281149566173553  to: 0.023280486464500427\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.02328125238418579\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.02328154593706131\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.02328074425458908\n",
      "Training iteration: 3892\n",
      "Improved validation loss from: 0.023280486464500427  to: 0.023279865086078644\n",
      "Training iteration: 3893\n",
      "Improved validation loss from: 0.023279865086078644  to: 0.023279643058776854\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.023280176520347595\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.02328062504529953\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.023280100524425508\n",
      "Training iteration: 3897\n",
      "Improved validation loss from: 0.023279643058776854  to: 0.023278966546058655\n",
      "Training iteration: 3898\n",
      "Improved validation loss from: 0.023278966546058655  to: 0.02327832728624344\n",
      "Training iteration: 3899\n",
      "Improved validation loss from: 0.02327832728624344  to: 0.023278312385082246\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.023278680443763734\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.0232783168554306\n",
      "Training iteration: 3902\n",
      "Improved validation loss from: 0.023278312385082246  to: 0.023277243971824645\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.023277382552623748\n",
      "Training iteration: 3904\n",
      "Improved validation loss from: 0.023277243971824645  to: 0.023277199268341063\n",
      "Training iteration: 3905\n",
      "Improved validation loss from: 0.023277199268341063  to: 0.02327698767185211\n",
      "Training iteration: 3906\n",
      "Improved validation loss from: 0.02327698767185211  to: 0.02327693998813629\n",
      "Training iteration: 3907\n",
      "Improved validation loss from: 0.02327693998813629  to: 0.02327670305967331\n",
      "Training iteration: 3908\n",
      "Improved validation loss from: 0.02327670305967331  to: 0.023276571929454804\n",
      "Training iteration: 3909\n",
      "Improved validation loss from: 0.023276571929454804  to: 0.023276421427726745\n",
      "Training iteration: 3910\n",
      "Improved validation loss from: 0.023276421427726745  to: 0.02327621877193451\n",
      "Training iteration: 3911\n",
      "Improved validation loss from: 0.02327621877193451  to: 0.023275916278362275\n",
      "Training iteration: 3912\n",
      "Improved validation loss from: 0.023275916278362275  to: 0.023275649547576903\n",
      "Training iteration: 3913\n",
      "Improved validation loss from: 0.023275649547576903  to: 0.023275187611579894\n",
      "Training iteration: 3914\n",
      "Improved validation loss from: 0.023275187611579894  to: 0.023274616897106172\n",
      "Training iteration: 3915\n",
      "Improved validation loss from: 0.023274616897106172  to: 0.023274211585521697\n",
      "Training iteration: 3916\n",
      "Improved validation loss from: 0.023274211585521697  to: 0.02327398508787155\n",
      "Training iteration: 3917\n",
      "Improved validation loss from: 0.02327398508787155  to: 0.023273682594299315\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.02327422648668289\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.023273739218711852\n",
      "Training iteration: 3920\n",
      "Improved validation loss from: 0.023273682594299315  to: 0.023272943496704102\n",
      "Training iteration: 3921\n",
      "Improved validation loss from: 0.023272943496704102  to: 0.02327267676591873\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.023272962868213655\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.02327342927455902\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.023273186385631563\n",
      "Training iteration: 3925\n",
      "Improved validation loss from: 0.02327267676591873  to: 0.023272447288036346\n",
      "Training iteration: 3926\n",
      "Improved validation loss from: 0.023272447288036346  to: 0.023271915316581727\n",
      "Training iteration: 3927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.023271915316581727  to: 0.02327175885438919\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.023271866142749786\n",
      "Training iteration: 3929\n",
      "Improved validation loss from: 0.02327175885438919  to: 0.023271575570106506\n",
      "Training iteration: 3930\n",
      "Improved validation loss from: 0.023271575570106506  to: 0.023270857334136964\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.02327117621898651\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.023270979523658752\n",
      "Training iteration: 3933\n",
      "Improved validation loss from: 0.023270857334136964  to: 0.023270463943481444\n",
      "Training iteration: 3934\n",
      "Improved validation loss from: 0.023270463943481444  to: 0.02327027767896652\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.023270413279533386\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.023270508646965025\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.02327042371034622\n",
      "Training iteration: 3938\n",
      "Improved validation loss from: 0.02327027767896652  to: 0.023269934952259062\n",
      "Training iteration: 3939\n",
      "Improved validation loss from: 0.023269934952259062  to: 0.023269462585449218\n",
      "Training iteration: 3940\n",
      "Improved validation loss from: 0.023269462585449218  to: 0.02326916754245758\n",
      "Training iteration: 3941\n",
      "Improved validation loss from: 0.02326916754245758  to: 0.023269084095954896\n",
      "Training iteration: 3942\n",
      "Improved validation loss from: 0.023269084095954896  to: 0.02326890528202057\n",
      "Training iteration: 3943\n",
      "Improved validation loss from: 0.02326890528202057  to: 0.023268285393714904\n",
      "Training iteration: 3944\n",
      "Improved validation loss from: 0.023268285393714904  to: 0.02326769083738327\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.023268303275108336\n",
      "Training iteration: 3946\n",
      "Validation loss (no improvement): 0.023268285393714904\n",
      "Training iteration: 3947\n",
      "Improved validation loss from: 0.02326769083738327  to: 0.023267586529254914\n",
      "Training iteration: 3948\n",
      "Improved validation loss from: 0.023267586529254914  to: 0.023266975581645966\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.02326708287000656\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.02326764166355133\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.023267874121665956\n",
      "Training iteration: 3952\n",
      "Validation loss (no improvement): 0.02326720952987671\n",
      "Training iteration: 3953\n",
      "Improved validation loss from: 0.023266975581645966  to: 0.023266363143920898\n",
      "Training iteration: 3954\n",
      "Improved validation loss from: 0.023266363143920898  to: 0.02326623648405075\n",
      "Training iteration: 3955\n",
      "Validation loss (no improvement): 0.023266367614269257\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.023266315460205078\n",
      "Training iteration: 3957\n",
      "Improved validation loss from: 0.02326623648405075  to: 0.023265676200389863\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.023265881836414336\n",
      "Training iteration: 3959\n",
      "Improved validation loss from: 0.023265676200389863  to: 0.023265549540519716\n",
      "Training iteration: 3960\n",
      "Improved validation loss from: 0.023265549540519716  to: 0.023265095055103303\n",
      "Training iteration: 3961\n",
      "Improved validation loss from: 0.023265095055103303  to: 0.02326505184173584\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.023265257477760315\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.023265354335308075\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.023265111446380615\n",
      "Training iteration: 3965\n",
      "Improved validation loss from: 0.02326505184173584  to: 0.023264817893505096\n",
      "Training iteration: 3966\n",
      "Improved validation loss from: 0.023264817893505096  to: 0.0232647180557251\n",
      "Training iteration: 3967\n",
      "Improved validation loss from: 0.0232647180557251  to: 0.02326461523771286\n",
      "Training iteration: 3968\n",
      "Improved validation loss from: 0.02326461523771286  to: 0.02326427400112152\n",
      "Training iteration: 3969\n",
      "Improved validation loss from: 0.02326427400112152  to: 0.023263673484325408\n",
      "Training iteration: 3970\n",
      "Improved validation loss from: 0.023263673484325408  to: 0.023263125121593474\n",
      "Training iteration: 3971\n",
      "Validation loss (no improvement): 0.023263683915138243\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.0232635498046875\n",
      "Training iteration: 3973\n",
      "Improved validation loss from: 0.023263125121593474  to: 0.023262616991996766\n",
      "Training iteration: 3974\n",
      "Improved validation loss from: 0.023262616991996766  to: 0.023262004554271697\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.02326206713914871\n",
      "Training iteration: 3976\n",
      "Validation loss (no improvement): 0.023262563347816467\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.02326247692108154\n",
      "Training iteration: 3978\n",
      "Improved validation loss from: 0.023262004554271697  to: 0.02326176464557648\n",
      "Training iteration: 3979\n",
      "Improved validation loss from: 0.02326176464557648  to: 0.023260864615440368\n",
      "Training iteration: 3980\n",
      "Improved validation loss from: 0.023260864615440368  to: 0.023260653018951416\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.023260895907878876\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.023260799050331116\n",
      "Training iteration: 3983\n",
      "Improved validation loss from: 0.023260653018951416  to: 0.023260195553302766\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.023260211944580077\n",
      "Training iteration: 3985\n",
      "Improved validation loss from: 0.023260195553302766  to: 0.023259644210338593\n",
      "Training iteration: 3986\n",
      "Improved validation loss from: 0.023259644210338593  to: 0.023259040713310242\n",
      "Training iteration: 3987\n",
      "Improved validation loss from: 0.023259040713310242  to: 0.023259024322032928\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.023259249329566956\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.023259063065052033\n",
      "Training iteration: 3990\n",
      "Improved validation loss from: 0.023259024322032928  to: 0.02325868159532547\n",
      "Training iteration: 3991\n",
      "Improved validation loss from: 0.02325868159532547  to: 0.02325795143842697\n",
      "Training iteration: 3992\n",
      "Improved validation loss from: 0.02325795143842697  to: 0.02325763404369354\n",
      "Training iteration: 3993\n",
      "Improved validation loss from: 0.02325763404369354  to: 0.023257513344287873\n",
      "Training iteration: 3994\n",
      "Improved validation loss from: 0.023257513344287873  to: 0.023257264494895936\n",
      "Training iteration: 3995\n",
      "Improved validation loss from: 0.023257264494895936  to: 0.023256583511829375\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.02325698882341385\n",
      "Training iteration: 3997\n",
      "Improved validation loss from: 0.023256583511829375  to: 0.023256464302539824\n",
      "Training iteration: 3998\n",
      "Improved validation loss from: 0.023256464302539824  to: 0.023255595564842226\n",
      "Training iteration: 3999\n",
      "Improved validation loss from: 0.023255595564842226  to: 0.023255130648612975\n"
     ]
    }
   ],
   "source": [
    "baseline_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 28.821942138671876\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 28.821942138671876  to: 28.519378662109375\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 28.519378662109375  to: 27.98516845703125\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 27.98516845703125  to: 27.394744873046875\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 27.394744873046875  to: 26.969500732421874\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 26.969500732421874  to: 26.237139892578124\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 26.237139892578124  to: 25.94035949707031\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 25.94035949707031  to: 24.9039306640625\n",
      "Training iteration: 8\n",
      "Validation loss (no improvement): 24.911312866210938\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 24.9039306640625  to: 24.1631103515625\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 24.1631103515625  to: 23.673674011230467\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 23.673674011230467  to: 23.16960906982422\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 23.16960906982422  to: 23.085234069824217\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 23.085234069824217  to: 22.131440734863283\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 22.131440734863283  to: 21.850801086425783\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 21.850801086425783  to: 20.79394989013672\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 20.79394989013672  to: 20.313519287109376\n",
      "Training iteration: 17\n",
      "Validation loss (no improvement): 21.085537719726563\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 20.313519287109376  to: 19.452354431152344\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 19.452354431152344  to: 18.782377624511717\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 18.782377624511717  to: 18.069419860839844\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 18.069419860839844  to: 17.99737548828125\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 17.99737548828125  to: 17.24127960205078\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 17.24127960205078  to: 16.545513916015626\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 16.545513916015626  to: 15.963410949707031\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 15.963410949707031  to: 15.753567504882813\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 15.753567504882813  to: 14.885957336425781\n",
      "Training iteration: 27\n",
      "Validation loss (no improvement): 15.48975830078125\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 14.885957336425781  to: 14.47989501953125\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 14.47989501953125  to: 14.048007202148437\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 14.048007202148437  to: 13.266531372070313\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 13.266531372070313  to: 13.123941040039062\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 13.123941040039062  to: 12.791129302978515\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 12.791129302978515  to: 11.396578216552735\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 11.396578216552735  to: 11.290969085693359\n",
      "Training iteration: 35\n",
      "Validation loss (no improvement): 11.41476821899414\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 11.290969085693359  to: 9.847892761230469\n",
      "Training iteration: 37\n",
      "Validation loss (no improvement): 10.762343597412109\n",
      "Training iteration: 38\n",
      "Validation loss (no improvement): 10.696256256103515\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 9.847892761230469  to: 9.396568298339844\n",
      "Training iteration: 40\n",
      "Validation loss (no improvement): 10.248701477050782\n",
      "Training iteration: 41\n",
      "Validation loss (no improvement): 9.599110412597657\n",
      "Training iteration: 42\n",
      "Validation loss (no improvement): 11.17515869140625\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 9.396568298339844  to: 8.274673461914062\n",
      "Training iteration: 44\n",
      "Validation loss (no improvement): 8.942047119140625\n",
      "Training iteration: 45\n",
      "Validation loss (no improvement): 9.401190185546875\n",
      "Training iteration: 46\n",
      "Validation loss (no improvement): 9.211869049072266\n",
      "Training iteration: 47\n",
      "Validation loss (no improvement): 10.034303283691406\n",
      "Training iteration: 48\n",
      "Validation loss (no improvement): 8.537459564208984\n",
      "Training iteration: 49\n",
      "Validation loss (no improvement): 9.351805877685546\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 8.274673461914062  to: 8.15747528076172\n",
      "Training iteration: 51\n",
      "Validation loss (no improvement): 8.930552673339843\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 8.15747528076172  to: 7.7604530334472654\n",
      "Training iteration: 53\n",
      "Validation loss (no improvement): 8.817485046386718\n",
      "Training iteration: 54\n",
      "Validation loss (no improvement): 8.650398254394531\n",
      "Training iteration: 55\n",
      "Validation loss (no improvement): 9.235011291503906\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 7.7604530334472654  to: 7.537884521484375\n",
      "Training iteration: 57\n",
      "Validation loss (no improvement): 8.005812072753907\n",
      "Training iteration: 58\n",
      "Validation loss (no improvement): 8.137540435791015\n",
      "Training iteration: 59\n",
      "Validation loss (no improvement): 7.793553161621094\n",
      "Training iteration: 60\n",
      "Validation loss (no improvement): 8.469566345214844\n",
      "Training iteration: 61\n",
      "Validation loss (no improvement): 8.734727478027343\n",
      "Training iteration: 62\n",
      "Validation loss (no improvement): 7.616818237304687\n",
      "Training iteration: 63\n",
      "Validation loss (no improvement): 7.913035583496094\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 7.537884521484375  to: 7.483365631103515\n",
      "Training iteration: 65\n",
      "Validation loss (no improvement): 8.079720306396485\n",
      "Training iteration: 66\n",
      "Validation loss (no improvement): 7.54425048828125\n",
      "Training iteration: 67\n",
      "Validation loss (no improvement): 8.093192291259765\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 7.483365631103515  to: 6.456586456298828\n",
      "Training iteration: 69\n",
      "Validation loss (no improvement): 7.64453353881836\n",
      "Training iteration: 70\n",
      "Validation loss (no improvement): 7.897068023681641\n",
      "Training iteration: 71\n",
      "Validation loss (no improvement): 6.736085510253906\n",
      "Training iteration: 72\n",
      "Validation loss (no improvement): 8.238008117675781\n",
      "Training iteration: 73\n",
      "Validation loss (no improvement): 7.656524658203125\n",
      "Training iteration: 74\n",
      "Validation loss (no improvement): 6.544192504882813\n",
      "Training iteration: 75\n",
      "Validation loss (no improvement): 6.655324554443359\n",
      "Training iteration: 76\n",
      "Validation loss (no improvement): 6.821575927734375\n",
      "Training iteration: 77\n",
      "Validation loss (no improvement): 7.233839416503907\n",
      "Training iteration: 78\n",
      "Validation loss (no improvement): 7.011513519287109\n",
      "Training iteration: 79\n",
      "Validation loss (no improvement): 7.614439392089844\n",
      "Training iteration: 80\n",
      "Validation loss (no improvement): 6.581654357910156\n",
      "Training iteration: 81\n",
      "Validation loss (no improvement): 7.8639686584472654\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 6.456586456298828  to: 6.416371917724609\n",
      "Training iteration: 83\n",
      "Validation loss (no improvement): 6.691016387939453\n",
      "Training iteration: 84\n",
      "Validation loss (no improvement): 8.08892364501953\n",
      "Training iteration: 85\n",
      "Validation loss (no improvement): 6.461316680908203\n",
      "Training iteration: 86\n",
      "Validation loss (no improvement): 6.9538414001464846\n",
      "Training iteration: 87\n",
      "Validation loss (no improvement): 6.746314239501953\n",
      "Training iteration: 88\n",
      "Validation loss (no improvement): 6.982490539550781\n",
      "Training iteration: 89\n",
      "Validation loss (no improvement): 7.54951171875\n",
      "Training iteration: 90\n",
      "Validation loss (no improvement): 8.257073974609375\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 6.416371917724609  to: 6.048003005981445\n",
      "Training iteration: 92\n",
      "Validation loss (no improvement): 6.199699020385742\n",
      "Training iteration: 93\n",
      "Validation loss (no improvement): 6.226361083984375\n",
      "Training iteration: 94\n",
      "Validation loss (no improvement): 6.9540855407714846\n",
      "Training iteration: 95\n",
      "Validation loss (no improvement): 7.578769683837891\n",
      "Training iteration: 96\n",
      "Validation loss (no improvement): 7.204740142822265\n",
      "Training iteration: 97\n",
      "Validation loss (no improvement): 6.0786285400390625\n",
      "Training iteration: 98\n",
      "Validation loss (no improvement): 7.300761413574219\n",
      "Training iteration: 99\n",
      "Validation loss (no improvement): 7.33392333984375\n",
      "Training iteration: 100\n",
      "Validation loss (no improvement): 8.086176300048828\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 6.048003005981445  to: 5.851753997802734\n",
      "Training iteration: 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 6.943199157714844\n",
      "Training iteration: 103\n",
      "Validation loss (no improvement): 6.345285797119141\n",
      "Training iteration: 104\n",
      "Validation loss (no improvement): 6.302814483642578\n",
      "Training iteration: 105\n",
      "Validation loss (no improvement): 7.391024017333985\n",
      "Training iteration: 106\n",
      "Validation loss (no improvement): 6.231621551513672\n",
      "Training iteration: 107\n",
      "Validation loss (no improvement): 6.422181701660156\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 5.851753997802734  to: 5.812520599365234\n",
      "Training iteration: 109\n",
      "Validation loss (no improvement): 7.06481704711914\n",
      "Training iteration: 110\n",
      "Validation loss (no improvement): 6.675209045410156\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 5.812520599365234  to: 5.788079071044922\n",
      "Training iteration: 112\n",
      "Validation loss (no improvement): 5.902627563476562\n",
      "Training iteration: 113\n",
      "Validation loss (no improvement): 6.3190757751464846\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 5.788079071044922  to: 5.6626842498779295\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 5.6626842498779295  to: 5.403557586669922\n",
      "Training iteration: 116\n",
      "Validation loss (no improvement): 5.50928840637207\n",
      "Training iteration: 117\n",
      "Validation loss (no improvement): 6.449250030517578\n",
      "Training iteration: 118\n",
      "Validation loss (no improvement): 6.178485107421875\n",
      "Training iteration: 119\n",
      "Validation loss (no improvement): 7.508660125732422\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 5.403557586669922  to: 5.29333267211914\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 5.29333267211914  to: 5.002700424194336\n",
      "Training iteration: 122\n",
      "Validation loss (no improvement): 5.2643379211425785\n",
      "Training iteration: 123\n",
      "Validation loss (no improvement): 5.583020782470703\n",
      "Training iteration: 124\n",
      "Validation loss (no improvement): 5.333483123779297\n",
      "Training iteration: 125\n",
      "Validation loss (no improvement): 5.3066356658935545\n",
      "Training iteration: 126\n",
      "Validation loss (no improvement): 5.089889144897461\n",
      "Training iteration: 127\n",
      "Validation loss (no improvement): 5.57234001159668\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 5.002700424194336  to: 4.768766403198242\n",
      "Training iteration: 129\n",
      "Validation loss (no improvement): 4.944623947143555\n",
      "Training iteration: 130\n",
      "Validation loss (no improvement): 5.428728103637695\n",
      "Training iteration: 131\n",
      "Validation loss (no improvement): 5.074687576293945\n",
      "Training iteration: 132\n",
      "Validation loss (no improvement): 5.522454071044922\n",
      "Training iteration: 133\n",
      "Validation loss (no improvement): 5.143369674682617\n",
      "Training iteration: 134\n",
      "Validation loss (no improvement): 5.515398406982422\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 4.768766403198242  to: 4.681874847412109\n",
      "Training iteration: 136\n",
      "Validation loss (no improvement): 5.50477409362793\n",
      "Training iteration: 137\n",
      "Validation loss (no improvement): 6.18602409362793\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 4.681874847412109  to: 4.156300735473633\n",
      "Training iteration: 139\n",
      "Validation loss (no improvement): 4.8271221160888675\n",
      "Training iteration: 140\n",
      "Validation loss (no improvement): 4.2436058044433596\n",
      "Training iteration: 141\n",
      "Validation loss (no improvement): 5.506195068359375\n",
      "Training iteration: 142\n",
      "Validation loss (no improvement): 4.820767211914062\n",
      "Training iteration: 143\n",
      "Validation loss (no improvement): 4.186074829101562\n",
      "Training iteration: 144\n",
      "Validation loss (no improvement): 4.551238250732422\n",
      "Training iteration: 145\n",
      "Validation loss (no improvement): 4.317507553100586\n",
      "Training iteration: 146\n",
      "Validation loss (no improvement): 5.1128486633300785\n",
      "Training iteration: 147\n",
      "Validation loss (no improvement): 4.2265888214111325\n",
      "Training iteration: 148\n",
      "Validation loss (no improvement): 4.550355911254883\n",
      "Training iteration: 149\n",
      "Validation loss (no improvement): 4.465727233886719\n",
      "Training iteration: 150\n",
      "Validation loss (no improvement): 4.216435241699219\n",
      "Training iteration: 151\n",
      "Validation loss (no improvement): 4.399718475341797\n",
      "Training iteration: 152\n",
      "Validation loss (no improvement): 4.733248138427735\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 4.156300735473633  to: 3.9415122985839846\n",
      "Training iteration: 154\n",
      "Validation loss (no improvement): 5.673052978515625\n",
      "Training iteration: 155\n",
      "Validation loss (no improvement): 4.008063888549804\n",
      "Training iteration: 156\n",
      "Validation loss (no improvement): 4.238132476806641\n",
      "Training iteration: 157\n",
      "Validation loss (no improvement): 5.192467117309571\n",
      "Training iteration: 158\n",
      "Validation loss (no improvement): 4.369443893432617\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 3.9415122985839846  to: 3.918334197998047\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 3.918334197998047  to: 3.5804237365722655\n",
      "Training iteration: 161\n",
      "Validation loss (no improvement): 5.1882568359375\n",
      "Training iteration: 162\n",
      "Validation loss (no improvement): 4.731147766113281\n",
      "Training iteration: 163\n",
      "Validation loss (no improvement): 3.8243183135986327\n",
      "Training iteration: 164\n",
      "Validation loss (no improvement): 3.7409809112548826\n",
      "Training iteration: 165\n",
      "Validation loss (no improvement): 3.667622375488281\n",
      "Training iteration: 166\n",
      "Validation loss (no improvement): 5.163387298583984\n",
      "Training iteration: 167\n",
      "Validation loss (no improvement): 4.960079193115234\n",
      "Training iteration: 168\n",
      "Validation loss (no improvement): 3.8842453002929687\n",
      "Training iteration: 169\n",
      "Validation loss (no improvement): 3.749428558349609\n",
      "Training iteration: 170\n",
      "Validation loss (no improvement): 3.9214252471923827\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 3.5804237365722655  to: 3.537273406982422\n",
      "Training iteration: 172\n",
      "Validation loss (no improvement): 3.959288787841797\n",
      "Training iteration: 173\n",
      "Validation loss (no improvement): 3.9302024841308594\n",
      "Training iteration: 174\n",
      "Validation loss (no improvement): 4.019652938842773\n",
      "Training iteration: 175\n",
      "Validation loss (no improvement): 4.326805496215821\n",
      "Training iteration: 176\n",
      "Validation loss (no improvement): 4.263815307617188\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 3.537273406982422  to: 3.4033763885498045\n",
      "Training iteration: 178\n",
      "Validation loss (no improvement): 5.1003662109375\n",
      "Training iteration: 179\n",
      "Validation loss (no improvement): 4.385693740844727\n",
      "Training iteration: 180\n",
      "Validation loss (no improvement): 3.510276031494141\n",
      "Training iteration: 181\n",
      "Validation loss (no improvement): 4.068062591552734\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 3.4033763885498045  to: 3.199200439453125\n",
      "Training iteration: 183\n",
      "Validation loss (no improvement): 3.2119850158691405\n",
      "Training iteration: 184\n",
      "Validation loss (no improvement): 4.2244712829589846\n",
      "Training iteration: 185\n",
      "Validation loss (no improvement): 3.50494384765625\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 3.199200439453125  to: 3.179815101623535\n",
      "Training iteration: 187\n",
      "Validation loss (no improvement): 3.3527381896972654\n",
      "Training iteration: 188\n",
      "Validation loss (no improvement): 3.5371803283691405\n",
      "Training iteration: 189\n",
      "Validation loss (no improvement): 3.4602897644042967\n",
      "Training iteration: 190\n",
      "Validation loss (no improvement): 3.2045799255371095\n",
      "Training iteration: 191\n",
      "Validation loss (no improvement): 3.350193405151367\n",
      "Training iteration: 192\n",
      "Validation loss (no improvement): 3.42791862487793\n",
      "Training iteration: 193\n",
      "Validation loss (no improvement): 3.443904495239258\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 3.179815101623535  to: 3.160756301879883\n",
      "Training iteration: 195\n",
      "Validation loss (no improvement): 3.3236427307128906\n",
      "Training iteration: 196\n",
      "Validation loss (no improvement): 3.1781747817993162\n",
      "Training iteration: 197\n",
      "Validation loss (no improvement): 3.2330863952636717\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 3.160756301879883  to: 3.0481380462646483\n",
      "Training iteration: 199\n",
      "Validation loss (no improvement): 3.2458709716796874\n",
      "Training iteration: 200\n",
      "Validation loss (no improvement): 3.693106842041016\n",
      "Training iteration: 201\n",
      "Validation loss (no improvement): 3.5385562896728517\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 3.0481380462646483  to: 2.7052684783935548\n",
      "Training iteration: 203\n",
      "Validation loss (no improvement): 2.893549156188965\n",
      "Training iteration: 204\n",
      "Validation loss (no improvement): 3.0640033721923827\n",
      "Training iteration: 205\n",
      "Validation loss (no improvement): 3.623738479614258\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 2.7052684783935548  to: 2.6664852142333983\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 2.6664852142333983  to: 2.5213129043579103\n",
      "Training iteration: 208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 2.5213129043579103  to: 2.3473888397216798\n",
      "Training iteration: 209\n",
      "Validation loss (no improvement): 2.471222496032715\n",
      "Training iteration: 210\n",
      "Validation loss (no improvement): 2.564987564086914\n",
      "Training iteration: 211\n",
      "Validation loss (no improvement): 2.601723861694336\n",
      "Training iteration: 212\n",
      "Validation loss (no improvement): 2.98599910736084\n",
      "Training iteration: 213\n",
      "Validation loss (no improvement): 3.2168201446533202\n",
      "Training iteration: 214\n",
      "Validation loss (no improvement): 3.1014476776123048\n",
      "Training iteration: 215\n",
      "Validation loss (no improvement): 3.074501609802246\n",
      "Training iteration: 216\n",
      "Validation loss (no improvement): 2.83719539642334\n",
      "Training iteration: 217\n",
      "Validation loss (no improvement): 2.9747711181640626\n",
      "Training iteration: 218\n",
      "Validation loss (no improvement): 2.4045299530029296\n",
      "Training iteration: 219\n",
      "Validation loss (no improvement): 2.6216506958007812\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 2.3473888397216798  to: 2.258737564086914\n",
      "Training iteration: 221\n",
      "Validation loss (no improvement): 3.72625617980957\n",
      "Training iteration: 222\n",
      "Validation loss (no improvement): 2.371498680114746\n",
      "Training iteration: 223\n",
      "Validation loss (no improvement): 4.554781723022461\n",
      "Training iteration: 224\n",
      "Validation loss (no improvement): 2.7909265518188477\n",
      "Training iteration: 225\n",
      "Validation loss (no improvement): 3.033626365661621\n",
      "Training iteration: 226\n",
      "Validation loss (no improvement): 3.0240398406982423\n",
      "Training iteration: 227\n",
      "Validation loss (no improvement): 3.139237976074219\n",
      "Training iteration: 228\n",
      "Validation loss (no improvement): 3.1943836212158203\n",
      "Training iteration: 229\n",
      "Validation loss (no improvement): 2.779217529296875\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 2.258737564086914  to: 1.6554950714111327\n",
      "Training iteration: 231\n",
      "Validation loss (no improvement): 2.2842029571533202\n",
      "Training iteration: 232\n",
      "Validation loss (no improvement): 2.2657188415527343\n",
      "Training iteration: 233\n",
      "Validation loss (no improvement): 4.150251007080078\n",
      "Training iteration: 234\n",
      "Validation loss (no improvement): 1.7927242279052735\n",
      "Training iteration: 235\n",
      "Validation loss (no improvement): 1.807501220703125\n",
      "Training iteration: 236\n",
      "Validation loss (no improvement): 2.019828224182129\n",
      "Training iteration: 237\n",
      "Validation loss (no improvement): 2.126454734802246\n",
      "Training iteration: 238\n",
      "Validation loss (no improvement): 3.0642343521118165\n",
      "Training iteration: 239\n",
      "Validation loss (no improvement): 2.373278999328613\n",
      "Training iteration: 240\n",
      "Validation loss (no improvement): 2.263998794555664\n",
      "Training iteration: 241\n",
      "Validation loss (no improvement): 2.8313507080078124\n",
      "Training iteration: 242\n",
      "Validation loss (no improvement): 2.3056631088256836\n",
      "Training iteration: 243\n",
      "Validation loss (no improvement): 2.6200551986694336\n",
      "Training iteration: 244\n",
      "Validation loss (no improvement): 2.539501190185547\n",
      "Training iteration: 245\n",
      "Validation loss (no improvement): 1.9979135513305664\n",
      "Training iteration: 246\n",
      "Validation loss (no improvement): 2.306474304199219\n",
      "Training iteration: 247\n",
      "Validation loss (no improvement): 2.1774127960205076\n",
      "Training iteration: 248\n",
      "Validation loss (no improvement): 1.6865810394287108\n",
      "Training iteration: 249\n",
      "Validation loss (no improvement): 1.721002197265625\n",
      "Training iteration: 250\n",
      "Validation loss (no improvement): 2.629915237426758\n",
      "Training iteration: 251\n",
      "Validation loss (no improvement): 2.18591423034668\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 1.6554950714111327  to: 1.4645662307739258\n",
      "Training iteration: 253\n",
      "Validation loss (no improvement): 2.0773427963256834\n",
      "Training iteration: 254\n",
      "Validation loss (no improvement): 2.522119140625\n",
      "Training iteration: 255\n",
      "Validation loss (no improvement): 2.539340591430664\n",
      "Training iteration: 256\n",
      "Validation loss (no improvement): 2.6666742324829102\n",
      "Training iteration: 257\n",
      "Validation loss (no improvement): 2.934610939025879\n",
      "Training iteration: 258\n",
      "Validation loss (no improvement): 1.4915664672851563\n",
      "Training iteration: 259\n",
      "Validation loss (no improvement): 1.9584833145141602\n",
      "Training iteration: 260\n",
      "Validation loss (no improvement): 1.5981651306152345\n",
      "Training iteration: 261\n",
      "Validation loss (no improvement): 2.3047435760498045\n",
      "Training iteration: 262\n",
      "Validation loss (no improvement): 4.249573516845703\n",
      "Training iteration: 263\n",
      "Validation loss (no improvement): 1.646251106262207\n",
      "Training iteration: 264\n",
      "Validation loss (no improvement): 2.4615570068359376\n",
      "Training iteration: 265\n",
      "Validation loss (no improvement): 1.8266866683959961\n",
      "Training iteration: 266\n",
      "Validation loss (no improvement): 2.749419593811035\n",
      "Training iteration: 267\n",
      "Validation loss (no improvement): 1.847188377380371\n",
      "Training iteration: 268\n",
      "Validation loss (no improvement): 2.7042896270751955\n",
      "Training iteration: 269\n",
      "Validation loss (no improvement): 1.9792285919189454\n",
      "Training iteration: 270\n",
      "Validation loss (no improvement): 2.3709386825561523\n",
      "Training iteration: 271\n",
      "Validation loss (no improvement): 1.5376217842102051\n",
      "Training iteration: 272\n",
      "Validation loss (no improvement): 1.8343196868896485\n",
      "Training iteration: 273\n",
      "Validation loss (no improvement): 2.6244842529296877\n",
      "Training iteration: 274\n",
      "Validation loss (no improvement): 1.8116985321044923\n",
      "Training iteration: 275\n",
      "Validation loss (no improvement): 2.742100143432617\n",
      "Training iteration: 276\n",
      "Validation loss (no improvement): 1.5821413040161132\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 1.4645662307739258  to: 1.3963567733764648\n",
      "Training iteration: 278\n",
      "Validation loss (no improvement): 1.9840896606445313\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 1.3963567733764648  to: 1.2370585441589355\n",
      "Training iteration: 280\n",
      "Validation loss (no improvement): 1.5697303771972657\n",
      "Training iteration: 281\n",
      "Validation loss (no improvement): 2.6139026641845704\n",
      "Training iteration: 282\n",
      "Validation loss (no improvement): 1.368439292907715\n",
      "Training iteration: 283\n",
      "Validation loss (no improvement): 3.0467859268188477\n",
      "Training iteration: 284\n",
      "Validation loss (no improvement): 1.4538915634155274\n",
      "Training iteration: 285\n",
      "Validation loss (no improvement): 1.6619989395141601\n",
      "Training iteration: 286\n",
      "Validation loss (no improvement): 1.7418901443481445\n",
      "Training iteration: 287\n",
      "Validation loss (no improvement): 1.8260921478271483\n",
      "Training iteration: 288\n",
      "Validation loss (no improvement): 1.297104835510254\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 1.2370585441589355  to: 1.215260887145996\n",
      "Training iteration: 290\n",
      "Validation loss (no improvement): 1.4065096855163575\n",
      "Training iteration: 291\n",
      "Validation loss (no improvement): 1.8898174285888671\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 1.215260887145996  to: 1.0681355476379395\n",
      "Training iteration: 293\n",
      "Validation loss (no improvement): 2.1100391387939452\n",
      "Training iteration: 294\n",
      "Validation loss (no improvement): 1.1498868942260743\n",
      "Training iteration: 295\n",
      "Validation loss (no improvement): 1.5018041610717774\n",
      "Training iteration: 296\n",
      "Validation loss (no improvement): 1.806084442138672\n",
      "Training iteration: 297\n",
      "Validation loss (no improvement): 2.1489038467407227\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 1.0681355476379395  to: 1.014856719970703\n",
      "Training iteration: 299\n",
      "Validation loss (no improvement): 1.4716262817382812\n",
      "Training iteration: 300\n",
      "Validation loss (no improvement): 2.0433303833007814\n",
      "Training iteration: 301\n",
      "Validation loss (no improvement): 2.3987159729003906\n",
      "Training iteration: 302\n",
      "Validation loss (no improvement): 1.1874464988708495\n",
      "Training iteration: 303\n",
      "Validation loss (no improvement): 2.1380054473876955\n",
      "Training iteration: 304\n",
      "Validation loss (no improvement): 1.2262165069580078\n",
      "Training iteration: 305\n",
      "Validation loss (no improvement): 1.1606828689575195\n",
      "Training iteration: 306\n",
      "Validation loss (no improvement): 1.226778221130371\n",
      "Training iteration: 307\n",
      "Validation loss (no improvement): 1.3402630805969238\n",
      "Training iteration: 308\n",
      "Validation loss (no improvement): 1.2778834342956542\n",
      "Training iteration: 309\n",
      "Validation loss (no improvement): 2.634735107421875\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 1.014856719970703  to: 1.012669277191162\n",
      "Training iteration: 311\n",
      "Validation loss (no improvement): 3.4467803955078127\n",
      "Training iteration: 312\n",
      "Validation loss (no improvement): 1.6131389617919922\n",
      "Training iteration: 313\n",
      "Validation loss (no improvement): 1.196474266052246\n",
      "Training iteration: 314\n",
      "Validation loss (no improvement): 2.3433834075927735\n",
      "Training iteration: 315\n",
      "Validation loss (no improvement): 2.080440902709961\n",
      "Training iteration: 316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 1.3166722297668456\n",
      "Training iteration: 317\n",
      "Validation loss (no improvement): 1.459147071838379\n",
      "Training iteration: 318\n",
      "Validation loss (no improvement): 1.2634595870971679\n",
      "Training iteration: 319\n",
      "Validation loss (no improvement): 1.1143908500671387\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 1.012669277191162  to: 0.9934304237365723\n",
      "Training iteration: 321\n",
      "Validation loss (no improvement): 1.1352401733398438\n",
      "Training iteration: 322\n",
      "Validation loss (no improvement): 1.5203867912292481\n",
      "Training iteration: 323\n",
      "Validation loss (no improvement): 1.8355291366577149\n",
      "Training iteration: 324\n",
      "Validation loss (no improvement): 1.2151972770690918\n",
      "Training iteration: 325\n",
      "Validation loss (no improvement): 1.4512141227722168\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.9934304237365723  to: 0.9079060554504395\n",
      "Training iteration: 327\n",
      "Validation loss (no improvement): 1.2903749465942382\n",
      "Training iteration: 328\n",
      "Validation loss (no improvement): 1.4900335311889648\n",
      "Training iteration: 329\n",
      "Validation loss (no improvement): 0.9700411796569824\n",
      "Training iteration: 330\n",
      "Validation loss (no improvement): 1.6290924072265625\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.9079060554504395  to: 0.6521855354309082\n",
      "Training iteration: 332\n",
      "Validation loss (no improvement): 1.1927157402038575\n",
      "Training iteration: 333\n",
      "Validation loss (no improvement): 1.9863315582275392\n",
      "Training iteration: 334\n",
      "Validation loss (no improvement): 1.2667000770568848\n",
      "Training iteration: 335\n",
      "Validation loss (no improvement): 1.3899456977844238\n",
      "Training iteration: 336\n",
      "Validation loss (no improvement): 1.0423474311828613\n",
      "Training iteration: 337\n",
      "Validation loss (no improvement): 1.1129377365112305\n",
      "Training iteration: 338\n",
      "Validation loss (no improvement): 1.3261945724487305\n",
      "Training iteration: 339\n",
      "Validation loss (no improvement): 0.9959979057312012\n",
      "Training iteration: 340\n",
      "Validation loss (no improvement): 1.277028465270996\n",
      "Training iteration: 341\n",
      "Validation loss (no improvement): 1.4536094665527344\n",
      "Training iteration: 342\n",
      "Validation loss (no improvement): 1.1038456916809083\n",
      "Training iteration: 343\n",
      "Validation loss (no improvement): 1.0372289657592773\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.6521855354309082  to: 0.6485659599304199\n",
      "Training iteration: 345\n",
      "Validation loss (no improvement): 1.0596841812133788\n",
      "Training iteration: 346\n",
      "Validation loss (no improvement): 1.0234237670898438\n",
      "Training iteration: 347\n",
      "Validation loss (no improvement): 1.3142168998718262\n",
      "Training iteration: 348\n",
      "Validation loss (no improvement): 1.7822790145874023\n",
      "Training iteration: 349\n",
      "Validation loss (no improvement): 2.124489974975586\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.6485659599304199  to: 0.6301666259765625\n",
      "Training iteration: 351\n",
      "Validation loss (no improvement): 0.792332935333252\n",
      "Training iteration: 352\n",
      "Validation loss (no improvement): 1.0060747146606446\n",
      "Training iteration: 353\n",
      "Validation loss (no improvement): 0.7499402523040771\n",
      "Training iteration: 354\n",
      "Validation loss (no improvement): 1.3604022026062013\n",
      "Training iteration: 355\n",
      "Validation loss (no improvement): 0.736913537979126\n",
      "Training iteration: 356\n",
      "Validation loss (no improvement): 1.0974899291992188\n",
      "Training iteration: 357\n",
      "Validation loss (no improvement): 1.2846689224243164\n",
      "Training iteration: 358\n",
      "Validation loss (no improvement): 1.4896191596984862\n",
      "Training iteration: 359\n",
      "Validation loss (no improvement): 1.5348530769348145\n",
      "Training iteration: 360\n",
      "Validation loss (no improvement): 1.8534931182861327\n",
      "Training iteration: 361\n",
      "Validation loss (no improvement): 1.148152732849121\n",
      "Training iteration: 362\n",
      "Validation loss (no improvement): 1.0427703857421875\n",
      "Training iteration: 363\n",
      "Validation loss (no improvement): 0.9291212081909179\n",
      "Training iteration: 364\n",
      "Validation loss (no improvement): 1.235832405090332\n",
      "Training iteration: 365\n",
      "Validation loss (no improvement): 0.8788614273071289\n",
      "Training iteration: 366\n",
      "Validation loss (no improvement): 1.2833440780639649\n",
      "Training iteration: 367\n",
      "Validation loss (no improvement): 0.9865854263305665\n",
      "Training iteration: 368\n",
      "Validation loss (no improvement): 1.0901182174682618\n",
      "Training iteration: 369\n",
      "Validation loss (no improvement): 2.430207061767578\n",
      "Training iteration: 370\n",
      "Validation loss (no improvement): 1.3260434150695801\n",
      "Training iteration: 371\n",
      "Validation loss (no improvement): 1.0245065689086914\n",
      "Training iteration: 372\n",
      "Validation loss (no improvement): 1.2033933639526366\n",
      "Training iteration: 373\n",
      "Validation loss (no improvement): 1.1698953628540039\n",
      "Training iteration: 374\n",
      "Validation loss (no improvement): 0.9804117202758789\n",
      "Training iteration: 375\n",
      "Validation loss (no improvement): 0.7906369209289551\n",
      "Training iteration: 376\n",
      "Validation loss (no improvement): 0.7700580596923828\n",
      "Training iteration: 377\n",
      "Validation loss (no improvement): 0.6694470405578613\n",
      "Training iteration: 378\n",
      "Validation loss (no improvement): 1.285136604309082\n",
      "Training iteration: 379\n",
      "Validation loss (no improvement): 0.9697367668151855\n",
      "Training iteration: 380\n",
      "Validation loss (no improvement): 0.7466893672943116\n",
      "Training iteration: 381\n",
      "Validation loss (no improvement): 0.8791988372802735\n",
      "Training iteration: 382\n",
      "Validation loss (no improvement): 1.212278461456299\n",
      "Training iteration: 383\n",
      "Validation loss (no improvement): 0.9506540298461914\n",
      "Training iteration: 384\n",
      "Validation loss (no improvement): 1.1441609382629394\n",
      "Training iteration: 385\n",
      "Validation loss (no improvement): 0.8365297317504883\n",
      "Training iteration: 386\n",
      "Validation loss (no improvement): 1.008980655670166\n",
      "Training iteration: 387\n",
      "Validation loss (no improvement): 1.4169568061828612\n",
      "Training iteration: 388\n",
      "Validation loss (no improvement): 1.885256576538086\n",
      "Training iteration: 389\n",
      "Validation loss (no improvement): 2.187508201599121\n",
      "Training iteration: 390\n",
      "Validation loss (no improvement): 1.0414874076843261\n",
      "Training iteration: 391\n",
      "Validation loss (no improvement): 1.1634862899780274\n",
      "Training iteration: 392\n",
      "Validation loss (no improvement): 0.6421134948730469\n",
      "Training iteration: 393\n",
      "Validation loss (no improvement): 1.1345135688781738\n",
      "Training iteration: 394\n",
      "Validation loss (no improvement): 0.9033106803894043\n",
      "Training iteration: 395\n",
      "Validation loss (no improvement): 0.8933855056762695\n",
      "Training iteration: 396\n",
      "Validation loss (no improvement): 0.7522009372711181\n",
      "Training iteration: 397\n",
      "Validation loss (no improvement): 0.7597837924957276\n",
      "Training iteration: 398\n",
      "Validation loss (no improvement): 0.6471876621246337\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.6301666259765625  to: 0.571322250366211\n",
      "Training iteration: 400\n",
      "Validation loss (no improvement): 1.1869799613952636\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.571322250366211  to: 0.4388479709625244\n",
      "Training iteration: 402\n",
      "Validation loss (no improvement): 0.5003706455230713\n",
      "Training iteration: 403\n",
      "Validation loss (no improvement): 0.6213185310363769\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.4388479709625244  to: 0.3692575454711914\n",
      "Training iteration: 405\n",
      "Validation loss (no improvement): 0.6425581455230713\n",
      "Training iteration: 406\n",
      "Validation loss (no improvement): 0.41114606857299807\n",
      "Training iteration: 407\n",
      "Validation loss (no improvement): 0.9501139640808105\n",
      "Training iteration: 408\n",
      "Validation loss (no improvement): 0.6140519142150879\n",
      "Training iteration: 409\n",
      "Validation loss (no improvement): 0.5501688957214356\n",
      "Training iteration: 410\n",
      "Validation loss (no improvement): 3.0564367294311525\n",
      "Training iteration: 411\n",
      "Validation loss (no improvement): 0.4310896396636963\n",
      "Training iteration: 412\n",
      "Validation loss (no improvement): 0.44928884506225586\n",
      "Training iteration: 413\n",
      "Validation loss (no improvement): 0.5845484733581543\n",
      "Training iteration: 414\n",
      "Validation loss (no improvement): 1.197645664215088\n",
      "Training iteration: 415\n",
      "Validation loss (no improvement): 0.8016995429992676\n",
      "Training iteration: 416\n",
      "Validation loss (no improvement): 0.8050172805786133\n",
      "Training iteration: 417\n",
      "Validation loss (no improvement): 1.3740163803100587\n",
      "Training iteration: 418\n",
      "Validation loss (no improvement): 2.109903335571289\n",
      "Training iteration: 419\n",
      "Validation loss (no improvement): 1.8630100250244142\n",
      "Training iteration: 420\n",
      "Validation loss (no improvement): 1.2219514846801758\n",
      "Training iteration: 421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.5794817924499511\n",
      "Training iteration: 422\n",
      "Validation loss (no improvement): 0.8111385345458985\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.3692575454711914  to: 0.2541207790374756\n",
      "Training iteration: 424\n",
      "Validation loss (no improvement): 1.3232213973999023\n",
      "Training iteration: 425\n",
      "Validation loss (no improvement): 0.3942588806152344\n",
      "Training iteration: 426\n",
      "Validation loss (no improvement): 0.31093294620513917\n",
      "Training iteration: 427\n",
      "Validation loss (no improvement): 1.3617986679077148\n",
      "Training iteration: 428\n",
      "Validation loss (no improvement): 0.851325798034668\n",
      "Training iteration: 429\n",
      "Validation loss (no improvement): 0.578320837020874\n",
      "Training iteration: 430\n",
      "Validation loss (no improvement): 0.39869384765625\n",
      "Training iteration: 431\n",
      "Validation loss (no improvement): 0.6103028297424317\n",
      "Training iteration: 432\n",
      "Validation loss (no improvement): 0.48382139205932617\n",
      "Training iteration: 433\n",
      "Validation loss (no improvement): 0.5739736080169677\n",
      "Training iteration: 434\n",
      "Validation loss (no improvement): 0.41385436058044434\n",
      "Training iteration: 435\n",
      "Validation loss (no improvement): 1.0719734191894532\n",
      "Training iteration: 436\n",
      "Validation loss (no improvement): 1.325343132019043\n",
      "Training iteration: 437\n",
      "Validation loss (no improvement): 0.4246496677398682\n",
      "Training iteration: 438\n",
      "Validation loss (no improvement): 0.59805908203125\n",
      "Training iteration: 439\n",
      "Validation loss (no improvement): 1.4407338142395019\n",
      "Training iteration: 440\n",
      "Validation loss (no improvement): 0.7624459743499756\n",
      "Training iteration: 441\n",
      "Validation loss (no improvement): 0.4024816036224365\n",
      "Training iteration: 442\n",
      "Validation loss (no improvement): 0.46108293533325195\n",
      "Training iteration: 443\n",
      "Validation loss (no improvement): 0.7169679164886474\n",
      "Training iteration: 444\n",
      "Validation loss (no improvement): 0.6101507186889649\n",
      "Training iteration: 445\n",
      "Validation loss (no improvement): 0.39605667591094973\n",
      "Training iteration: 446\n",
      "Validation loss (no improvement): 0.7432209014892578\n",
      "Training iteration: 447\n",
      "Validation loss (no improvement): 0.8785699844360352\n",
      "Training iteration: 448\n",
      "Validation loss (no improvement): 0.3788935899734497\n",
      "Training iteration: 449\n",
      "Validation loss (no improvement): 0.5264401435852051\n",
      "Training iteration: 450\n",
      "Validation loss (no improvement): 1.1408464431762695\n",
      "Training iteration: 451\n",
      "Validation loss (no improvement): 0.29800992012023925\n",
      "Training iteration: 452\n",
      "Validation loss (no improvement): 0.7369509696960449\n",
      "Training iteration: 453\n",
      "Validation loss (no improvement): 0.4157845497131348\n",
      "Training iteration: 454\n",
      "Validation loss (no improvement): 0.4669914722442627\n",
      "Training iteration: 455\n",
      "Validation loss (no improvement): 1.4179125785827638\n",
      "Training iteration: 456\n",
      "Validation loss (no improvement): 1.8768768310546875\n",
      "Training iteration: 457\n",
      "Validation loss (no improvement): 0.5563897132873535\n",
      "Training iteration: 458\n",
      "Validation loss (no improvement): 0.3530793428421021\n",
      "Training iteration: 459\n",
      "Validation loss (no improvement): 0.4480120658874512\n",
      "Training iteration: 460\n",
      "Validation loss (no improvement): 0.7183282375335693\n",
      "Training iteration: 461\n",
      "Validation loss (no improvement): 0.688745641708374\n",
      "Training iteration: 462\n",
      "Validation loss (no improvement): 0.495603084564209\n",
      "Training iteration: 463\n",
      "Validation loss (no improvement): 1.2707090377807617\n",
      "Training iteration: 464\n",
      "Validation loss (no improvement): 0.35367114543914796\n",
      "Training iteration: 465\n",
      "Validation loss (no improvement): 0.2816074132919312\n",
      "Training iteration: 466\n",
      "Validation loss (no improvement): 0.9623767852783203\n",
      "Training iteration: 467\n",
      "Validation loss (no improvement): 0.6989835262298584\n",
      "Training iteration: 468\n",
      "Validation loss (no improvement): 0.8556467056274414\n",
      "Training iteration: 469\n",
      "Validation loss (no improvement): 1.3192957878112792\n",
      "Training iteration: 470\n",
      "Validation loss (no improvement): 0.4562983989715576\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.2541207790374756  to: 0.22588400840759276\n",
      "Training iteration: 472\n",
      "Validation loss (no improvement): 0.5209826469421387\n",
      "Training iteration: 473\n",
      "Validation loss (no improvement): 0.640634298324585\n",
      "Training iteration: 474\n",
      "Validation loss (no improvement): 0.9423930168151855\n",
      "Training iteration: 475\n",
      "Validation loss (no improvement): 1.031346321105957\n",
      "Training iteration: 476\n",
      "Validation loss (no improvement): 0.7768511772155762\n",
      "Training iteration: 477\n",
      "Validation loss (no improvement): 0.5389064311981201\n",
      "Training iteration: 478\n",
      "Validation loss (no improvement): 0.9007259368896484\n",
      "Training iteration: 479\n",
      "Validation loss (no improvement): 0.7209263801574707\n",
      "Training iteration: 480\n",
      "Validation loss (no improvement): 0.5747112274169922\n",
      "Training iteration: 481\n",
      "Validation loss (no improvement): 1.140221118927002\n",
      "Training iteration: 482\n",
      "Validation loss (no improvement): 0.3104207992553711\n",
      "Training iteration: 483\n",
      "Validation loss (no improvement): 0.7552220344543457\n",
      "Training iteration: 484\n",
      "Validation loss (no improvement): 3.0956085205078123\n",
      "Training iteration: 485\n",
      "Validation loss (no improvement): 0.3038326740264893\n",
      "Training iteration: 486\n",
      "Validation loss (no improvement): 0.9147482872009277\n",
      "Training iteration: 487\n",
      "Validation loss (no improvement): 0.6295897006988526\n",
      "Training iteration: 488\n",
      "Validation loss (no improvement): 0.4954505443572998\n",
      "Training iteration: 489\n",
      "Validation loss (no improvement): 0.7823513984680176\n",
      "Training iteration: 490\n",
      "Validation loss (no improvement): 0.32768294811248777\n",
      "Training iteration: 491\n",
      "Validation loss (no improvement): 0.5807436466217041\n",
      "Training iteration: 492\n",
      "Validation loss (no improvement): 0.3688767194747925\n",
      "Training iteration: 493\n",
      "Validation loss (no improvement): 0.6020219326019287\n",
      "Training iteration: 494\n",
      "Validation loss (no improvement): 0.4832638740539551\n",
      "Training iteration: 495\n",
      "Validation loss (no improvement): 0.6184032440185547\n",
      "Training iteration: 496\n",
      "Validation loss (no improvement): 0.48879270553588866\n",
      "Training iteration: 497\n",
      "Validation loss (no improvement): 0.31329429149627686\n",
      "Training iteration: 498\n",
      "Validation loss (no improvement): 0.42252178192138673\n",
      "Training iteration: 499\n",
      "Validation loss (no improvement): 0.6685361385345459\n",
      "Training iteration: 500\n",
      "Validation loss (no improvement): 0.3982697010040283\n",
      "Training iteration: 501\n",
      "Validation loss (no improvement): 0.4254880905151367\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.22588400840759276  to: 0.2129124164581299\n",
      "Training iteration: 503\n",
      "Validation loss (no improvement): 0.5627807140350342\n",
      "Training iteration: 504\n",
      "Validation loss (no improvement): 0.6171310424804688\n",
      "Training iteration: 505\n",
      "Validation loss (no improvement): 0.42508821487426757\n",
      "Training iteration: 506\n",
      "Validation loss (no improvement): 0.27390124797821047\n",
      "Training iteration: 507\n",
      "Validation loss (no improvement): 0.6082391262054443\n",
      "Training iteration: 508\n",
      "Validation loss (no improvement): 1.028749942779541\n",
      "Training iteration: 509\n",
      "Validation loss (no improvement): 0.5101403713226318\n",
      "Training iteration: 510\n",
      "Validation loss (no improvement): 0.3250064134597778\n",
      "Training iteration: 511\n",
      "Validation loss (no improvement): 0.2574606418609619\n",
      "Training iteration: 512\n",
      "Validation loss (no improvement): 0.3258072376251221\n",
      "Training iteration: 513\n",
      "Validation loss (no improvement): 0.5683230400085449\n",
      "Training iteration: 514\n",
      "Validation loss (no improvement): 0.24859929084777832\n",
      "Training iteration: 515\n",
      "Validation loss (no improvement): 0.3740671634674072\n",
      "Training iteration: 516\n",
      "Validation loss (no improvement): 0.4375697135925293\n",
      "Training iteration: 517\n",
      "Validation loss (no improvement): 0.23987345695495604\n",
      "Training iteration: 518\n",
      "Validation loss (no improvement): 0.3083081007003784\n",
      "Training iteration: 519\n",
      "Validation loss (no improvement): 0.35990707874298095\n",
      "Training iteration: 520\n",
      "Validation loss (no improvement): 0.7791363716125488\n",
      "Training iteration: 521\n",
      "Validation loss (no improvement): 0.43020024299621584\n",
      "Training iteration: 522\n",
      "Validation loss (no improvement): 1.0377245903015138\n",
      "Training iteration: 523\n",
      "Validation loss (no improvement): 0.276377010345459\n",
      "Training iteration: 524\n",
      "Validation loss (no improvement): 0.37052154541015625\n",
      "Training iteration: 525\n",
      "Validation loss (no improvement): 0.5793967247009277\n",
      "Training iteration: 526\n",
      "Validation loss (no improvement): 0.5089672088623047\n",
      "Training iteration: 527\n",
      "Validation loss (no improvement): 0.5475444316864013\n",
      "Training iteration: 528\n",
      "Validation loss (no improvement): 0.3476444721221924\n",
      "Training iteration: 529\n",
      "Validation loss (no improvement): 0.28545308113098145\n",
      "Training iteration: 530\n",
      "Validation loss (no improvement): 1.6713459014892578\n",
      "Training iteration: 531\n",
      "Validation loss (no improvement): 0.5821650505065918\n",
      "Training iteration: 532\n",
      "Validation loss (no improvement): 0.6676197052001953\n",
      "Training iteration: 533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.6908303737640381\n",
      "Training iteration: 534\n",
      "Validation loss (no improvement): 0.37586159706115724\n",
      "Training iteration: 535\n",
      "Validation loss (no improvement): 0.6687998294830322\n",
      "Training iteration: 536\n",
      "Validation loss (no improvement): 0.587420654296875\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.2129124164581299  to: 0.20216593742370606\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.20216593742370606  to: 0.16484473943710326\n",
      "Training iteration: 539\n",
      "Validation loss (no improvement): 0.3451990604400635\n",
      "Training iteration: 540\n",
      "Validation loss (no improvement): 0.27340245246887207\n",
      "Training iteration: 541\n",
      "Validation loss (no improvement): 1.1209939956665038\n",
      "Training iteration: 542\n",
      "Validation loss (no improvement): 2.1920204162597656\n",
      "Training iteration: 543\n",
      "Validation loss (no improvement): 0.7480122566223144\n",
      "Training iteration: 544\n",
      "Validation loss (no improvement): 0.2961052417755127\n",
      "Training iteration: 545\n",
      "Validation loss (no improvement): 1.7232593536376952\n",
      "Training iteration: 546\n",
      "Validation loss (no improvement): 0.2167111873626709\n",
      "Training iteration: 547\n",
      "Validation loss (no improvement): 0.47526941299438474\n",
      "Training iteration: 548\n",
      "Validation loss (no improvement): 0.26868400573730467\n",
      "Training iteration: 549\n",
      "Validation loss (no improvement): 0.7206708431243897\n",
      "Training iteration: 550\n",
      "Validation loss (no improvement): 0.5596298217773438\n",
      "Training iteration: 551\n",
      "Validation loss (no improvement): 0.5217707633972168\n",
      "Training iteration: 552\n",
      "Validation loss (no improvement): 0.402728796005249\n",
      "Training iteration: 553\n",
      "Validation loss (no improvement): 0.2337486743927002\n",
      "Training iteration: 554\n",
      "Validation loss (no improvement): 0.33621788024902344\n",
      "Training iteration: 555\n",
      "Validation loss (no improvement): 0.7955495357513428\n",
      "Training iteration: 556\n",
      "Validation loss (no improvement): 0.29757113456726075\n",
      "Training iteration: 557\n",
      "Validation loss (no improvement): 0.6646533966064453\n",
      "Training iteration: 558\n",
      "Validation loss (no improvement): 0.2803430795669556\n",
      "Training iteration: 559\n",
      "Validation loss (no improvement): 0.43190717697143555\n",
      "Training iteration: 560\n",
      "Validation loss (no improvement): 0.5161070823669434\n",
      "Training iteration: 561\n",
      "Validation loss (no improvement): 0.7182990074157715\n",
      "Training iteration: 562\n",
      "Validation loss (no improvement): 0.6395644664764404\n",
      "Training iteration: 563\n",
      "Validation loss (no improvement): 0.7813111305236816\n",
      "Training iteration: 564\n",
      "Validation loss (no improvement): 0.23531055450439453\n",
      "Training iteration: 565\n",
      "Validation loss (no improvement): 1.637664222717285\n",
      "Training iteration: 566\n",
      "Validation loss (no improvement): 1.3015022277832031\n",
      "Training iteration: 567\n",
      "Validation loss (no improvement): 0.5949084281921386\n",
      "Training iteration: 568\n",
      "Validation loss (no improvement): 0.5722481727600097\n",
      "Training iteration: 569\n",
      "Validation loss (no improvement): 0.9458231925964355\n",
      "Training iteration: 570\n",
      "Validation loss (no improvement): 0.29220287799835204\n",
      "Training iteration: 571\n",
      "Validation loss (no improvement): 1.1114768981933594\n",
      "Training iteration: 572\n",
      "Validation loss (no improvement): 0.4832189083099365\n",
      "Training iteration: 573\n",
      "Validation loss (no improvement): 0.6186898231506348\n",
      "Training iteration: 574\n",
      "Validation loss (no improvement): 0.2506263732910156\n",
      "Training iteration: 575\n",
      "Validation loss (no improvement): 0.3614846706390381\n",
      "Training iteration: 576\n",
      "Validation loss (no improvement): 0.3183371305465698\n",
      "Training iteration: 577\n",
      "Validation loss (no improvement): 0.3285315752029419\n",
      "Training iteration: 578\n",
      "Validation loss (no improvement): 1.2571995735168457\n",
      "Training iteration: 579\n",
      "Validation loss (no improvement): 0.19101617336273194\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.16484473943710326  to: 0.14760081768035888\n",
      "Training iteration: 581\n",
      "Validation loss (no improvement): 0.2860316038131714\n",
      "Training iteration: 582\n",
      "Validation loss (no improvement): 0.4165985107421875\n",
      "Training iteration: 583\n",
      "Validation loss (no improvement): 0.26876990795135497\n",
      "Training iteration: 584\n",
      "Validation loss (no improvement): 0.9575874328613281\n",
      "Training iteration: 585\n",
      "Validation loss (no improvement): 1.4248347282409668\n",
      "Training iteration: 586\n",
      "Validation loss (no improvement): 0.5666510581970214\n",
      "Training iteration: 587\n",
      "Validation loss (no improvement): 0.3693950653076172\n",
      "Training iteration: 588\n",
      "Validation loss (no improvement): 0.27581374645233153\n",
      "Training iteration: 589\n",
      "Validation loss (no improvement): 0.17623674869537354\n",
      "Training iteration: 590\n",
      "Validation loss (no improvement): 0.35381903648376467\n",
      "Training iteration: 591\n",
      "Validation loss (no improvement): 0.2515442371368408\n",
      "Training iteration: 592\n",
      "Validation loss (no improvement): 0.4025848388671875\n",
      "Training iteration: 593\n",
      "Validation loss (no improvement): 0.9953360557556152\n",
      "Training iteration: 594\n",
      "Validation loss (no improvement): 0.5816512107849121\n",
      "Training iteration: 595\n",
      "Validation loss (no improvement): 1.0324033737182616\n",
      "Training iteration: 596\n",
      "Validation loss (no improvement): 0.48415746688842776\n",
      "Training iteration: 597\n",
      "Validation loss (no improvement): 0.3525432586669922\n",
      "Training iteration: 598\n",
      "Validation loss (no improvement): 1.61875\n",
      "Training iteration: 599\n",
      "Validation loss (no improvement): 0.3154356241226196\n",
      "Training iteration: 600\n",
      "Validation loss (no improvement): 0.16052839756011963\n",
      "Training iteration: 601\n",
      "Validation loss (no improvement): 0.43066110610961916\n",
      "Training iteration: 602\n",
      "Validation loss (no improvement): 0.30351762771606444\n",
      "Training iteration: 603\n",
      "Validation loss (no improvement): 0.24544548988342285\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.14760081768035888  to: 0.11922719478607177\n",
      "Training iteration: 605\n",
      "Validation loss (no improvement): 0.2906379461288452\n",
      "Training iteration: 606\n",
      "Validation loss (no improvement): 0.4537468910217285\n",
      "Training iteration: 607\n",
      "Validation loss (no improvement): 0.4988499641418457\n",
      "Training iteration: 608\n",
      "Validation loss (no improvement): 0.903775691986084\n",
      "Training iteration: 609\n",
      "Validation loss (no improvement): 0.8120903015136719\n",
      "Training iteration: 610\n",
      "Validation loss (no improvement): 0.38876259326934814\n",
      "Training iteration: 611\n",
      "Validation loss (no improvement): 0.5216189384460449\n",
      "Training iteration: 612\n",
      "Validation loss (no improvement): 1.0301488876342773\n",
      "Training iteration: 613\n",
      "Validation loss (no improvement): 1.0323453903198243\n",
      "Training iteration: 614\n",
      "Validation loss (no improvement): 0.24710488319396973\n",
      "Training iteration: 615\n",
      "Validation loss (no improvement): 0.27423415184020994\n",
      "Training iteration: 616\n",
      "Validation loss (no improvement): 0.24098351001739501\n",
      "Training iteration: 617\n",
      "Validation loss (no improvement): 1.1721097946166992\n",
      "Training iteration: 618\n",
      "Validation loss (no improvement): 0.6743839263916016\n",
      "Training iteration: 619\n",
      "Validation loss (no improvement): 0.3004429340362549\n",
      "Training iteration: 620\n",
      "Validation loss (no improvement): 0.18837130069732666\n",
      "Training iteration: 621\n",
      "Validation loss (no improvement): 0.9401549339294434\n",
      "Training iteration: 622\n",
      "Validation loss (no improvement): 0.5226784706115722\n",
      "Training iteration: 623\n",
      "Validation loss (no improvement): 0.5610658645629882\n",
      "Training iteration: 624\n",
      "Validation loss (no improvement): 0.16134790182113648\n",
      "Training iteration: 625\n",
      "Validation loss (no improvement): 0.6090137004852295\n",
      "Training iteration: 626\n",
      "Validation loss (no improvement): 1.0842083930969237\n",
      "Training iteration: 627\n",
      "Validation loss (no improvement): 0.6259041786193847\n",
      "Training iteration: 628\n",
      "Validation loss (no improvement): 0.2796884298324585\n",
      "Training iteration: 629\n",
      "Validation loss (no improvement): 0.3900617599487305\n",
      "Training iteration: 630\n",
      "Validation loss (no improvement): 0.43578948974609377\n",
      "Training iteration: 631\n",
      "Validation loss (no improvement): 0.3743699550628662\n",
      "Training iteration: 632\n",
      "Validation loss (no improvement): 0.22599222660064697\n",
      "Training iteration: 633\n",
      "Validation loss (no improvement): 0.2962629795074463\n",
      "Training iteration: 634\n",
      "Validation loss (no improvement): 0.35783920288085935\n",
      "Training iteration: 635\n",
      "Validation loss (no improvement): 0.3696474552154541\n",
      "Training iteration: 636\n",
      "Validation loss (no improvement): 0.15601880550384523\n",
      "Training iteration: 637\n",
      "Validation loss (no improvement): 0.17621549367904663\n",
      "Training iteration: 638\n",
      "Validation loss (no improvement): 0.32852485179901125\n",
      "Training iteration: 639\n",
      "Validation loss (no improvement): 0.8102544784545899\n",
      "Training iteration: 640\n",
      "Validation loss (no improvement): 0.23108036518096925\n",
      "Training iteration: 641\n",
      "Validation loss (no improvement): 0.29461886882781985\n",
      "Training iteration: 642\n",
      "Validation loss (no improvement): 0.9387803077697754\n",
      "Training iteration: 643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.25999817848205564\n",
      "Training iteration: 644\n",
      "Validation loss (no improvement): 0.4017855167388916\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.11922719478607177  to: 0.11795775890350342\n",
      "Training iteration: 646\n",
      "Validation loss (no improvement): 0.5747461318969727\n",
      "Training iteration: 647\n",
      "Validation loss (no improvement): 0.399299693107605\n",
      "Training iteration: 648\n",
      "Validation loss (no improvement): 0.17702629566192626\n",
      "Training iteration: 649\n",
      "Validation loss (no improvement): 0.3619250774383545\n",
      "Training iteration: 650\n",
      "Validation loss (no improvement): 0.736646032333374\n",
      "Training iteration: 651\n",
      "Validation loss (no improvement): 0.44795899391174315\n",
      "Training iteration: 652\n",
      "Validation loss (no improvement): 0.25417797565460204\n",
      "Training iteration: 653\n",
      "Validation loss (no improvement): 0.1273464798927307\n",
      "Training iteration: 654\n",
      "Validation loss (no improvement): 0.5521043300628662\n",
      "Training iteration: 655\n",
      "Validation loss (no improvement): 1.0052128791809083\n",
      "Training iteration: 656\n",
      "Validation loss (no improvement): 0.5321642875671386\n",
      "Training iteration: 657\n",
      "Validation loss (no improvement): 2.2215042114257812\n",
      "Training iteration: 658\n",
      "Validation loss (no improvement): 0.8518224716186523\n",
      "Training iteration: 659\n",
      "Validation loss (no improvement): 0.2371772289276123\n",
      "Training iteration: 660\n",
      "Validation loss (no improvement): 0.18847362995147704\n",
      "Training iteration: 661\n",
      "Validation loss (no improvement): 0.32391197681427003\n",
      "Training iteration: 662\n",
      "Validation loss (no improvement): 0.40047054290771483\n",
      "Training iteration: 663\n",
      "Validation loss (no improvement): 0.4404426097869873\n",
      "Training iteration: 664\n",
      "Validation loss (no improvement): 1.1875219345092773\n",
      "Training iteration: 665\n",
      "Validation loss (no improvement): 0.4812758445739746\n",
      "Training iteration: 666\n",
      "Validation loss (no improvement): 0.23988449573516846\n",
      "Training iteration: 667\n",
      "Validation loss (no improvement): 0.2863684415817261\n",
      "Training iteration: 668\n",
      "Validation loss (no improvement): 0.3423096418380737\n",
      "Training iteration: 669\n",
      "Validation loss (no improvement): 0.649660587310791\n",
      "Training iteration: 670\n",
      "Validation loss (no improvement): 0.23441617488861083\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.11795775890350342  to: 0.10642600059509277\n",
      "Training iteration: 672\n",
      "Validation loss (no improvement): 0.15643218755722046\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.10642600059509277  to: 0.09629864692687988\n",
      "Training iteration: 674\n",
      "Validation loss (no improvement): 0.22564592361450195\n",
      "Training iteration: 675\n",
      "Validation loss (no improvement): 0.8110349655151368\n",
      "Training iteration: 676\n",
      "Validation loss (no improvement): 1.8070734024047852\n",
      "Training iteration: 677\n",
      "Validation loss (no improvement): 0.30588927268981936\n",
      "Training iteration: 678\n",
      "Validation loss (no improvement): 0.8875852584838867\n",
      "Training iteration: 679\n",
      "Validation loss (no improvement): 0.7245431423187256\n",
      "Training iteration: 680\n",
      "Validation loss (no improvement): 0.26976785659790037\n",
      "Training iteration: 681\n",
      "Validation loss (no improvement): 0.5363675117492676\n",
      "Training iteration: 682\n",
      "Validation loss (no improvement): 0.22696945667266846\n",
      "Training iteration: 683\n",
      "Validation loss (no improvement): 0.1493792414665222\n",
      "Training iteration: 684\n",
      "Validation loss (no improvement): 0.19776408672332763\n",
      "Training iteration: 685\n",
      "Validation loss (no improvement): 0.2780420780181885\n",
      "Training iteration: 686\n",
      "Validation loss (no improvement): 0.6217679023742676\n",
      "Training iteration: 687\n",
      "Validation loss (no improvement): 0.3165857791900635\n",
      "Training iteration: 688\n",
      "Validation loss (no improvement): 0.3008828639984131\n",
      "Training iteration: 689\n",
      "Validation loss (no improvement): 0.2782533407211304\n",
      "Training iteration: 690\n",
      "Validation loss (no improvement): 0.5827208995819092\n",
      "Training iteration: 691\n",
      "Validation loss (no improvement): 1.0003270149230956\n",
      "Training iteration: 692\n",
      "Validation loss (no improvement): 0.5240156650543213\n",
      "Training iteration: 693\n",
      "Validation loss (no improvement): 0.7332078456878662\n",
      "Training iteration: 694\n",
      "Validation loss (no improvement): 0.5734623908996582\n",
      "Training iteration: 695\n",
      "Validation loss (no improvement): 0.26872284412384034\n",
      "Training iteration: 696\n",
      "Validation loss (no improvement): 0.3219528913497925\n",
      "Training iteration: 697\n",
      "Validation loss (no improvement): 0.7009558200836181\n",
      "Training iteration: 698\n",
      "Validation loss (no improvement): 0.17626157999038697\n",
      "Training iteration: 699\n",
      "Validation loss (no improvement): 1.2022273063659668\n",
      "Training iteration: 700\n",
      "Validation loss (no improvement): 0.35732262134552\n",
      "Training iteration: 701\n",
      "Validation loss (no improvement): 0.7261441230773926\n",
      "Training iteration: 702\n",
      "Validation loss (no improvement): 0.9303140640258789\n",
      "Training iteration: 703\n",
      "Validation loss (no improvement): 0.3140244007110596\n",
      "Training iteration: 704\n",
      "Validation loss (no improvement): 0.7654763221740722\n",
      "Training iteration: 705\n",
      "Validation loss (no improvement): 0.6248201847076416\n",
      "Training iteration: 706\n",
      "Validation loss (no improvement): 0.3864284515380859\n",
      "Training iteration: 707\n",
      "Validation loss (no improvement): 0.3000924587249756\n",
      "Training iteration: 708\n",
      "Validation loss (no improvement): 0.5210665702819824\n",
      "Training iteration: 709\n",
      "Validation loss (no improvement): 0.6956189155578614\n",
      "Training iteration: 710\n",
      "Validation loss (no improvement): 0.18138614892959595\n",
      "Training iteration: 711\n",
      "Validation loss (no improvement): 0.21004652976989746\n",
      "Training iteration: 712\n",
      "Validation loss (no improvement): 0.5741806983947754\n",
      "Training iteration: 713\n",
      "Validation loss (no improvement): 0.3819227695465088\n",
      "Training iteration: 714\n",
      "Validation loss (no improvement): 0.4688724517822266\n",
      "Training iteration: 715\n",
      "Validation loss (no improvement): 0.15428706407546997\n",
      "Training iteration: 716\n",
      "Validation loss (no improvement): 0.4639925956726074\n",
      "Training iteration: 717\n",
      "Validation loss (no improvement): 0.8734273910522461\n",
      "Training iteration: 718\n",
      "Validation loss (no improvement): 0.21611371040344238\n",
      "Training iteration: 719\n",
      "Validation loss (no improvement): 0.9727897644042969\n",
      "Training iteration: 720\n",
      "Validation loss (no improvement): 0.1855145812034607\n",
      "Training iteration: 721\n",
      "Validation loss (no improvement): 0.3428084135055542\n",
      "Training iteration: 722\n",
      "Validation loss (no improvement): 0.19226585626602172\n",
      "Training iteration: 723\n",
      "Validation loss (no improvement): 0.5225381851196289\n",
      "Training iteration: 724\n",
      "Validation loss (no improvement): 0.12797811031341552\n",
      "Training iteration: 725\n",
      "Validation loss (no improvement): 0.2713104009628296\n",
      "Training iteration: 726\n",
      "Validation loss (no improvement): 0.6721338748931884\n",
      "Training iteration: 727\n",
      "Validation loss (no improvement): 0.296290922164917\n",
      "Training iteration: 728\n",
      "Validation loss (no improvement): 0.20940814018249512\n",
      "Training iteration: 729\n",
      "Validation loss (no improvement): 0.4910109043121338\n",
      "Training iteration: 730\n",
      "Validation loss (no improvement): 0.2022631883621216\n",
      "Training iteration: 731\n",
      "Validation loss (no improvement): 0.22324800491333008\n",
      "Training iteration: 732\n",
      "Validation loss (no improvement): 0.16404826641082765\n",
      "Training iteration: 733\n",
      "Validation loss (no improvement): 0.1828693389892578\n",
      "Training iteration: 734\n",
      "Validation loss (no improvement): 0.4002979755401611\n",
      "Training iteration: 735\n",
      "Validation loss (no improvement): 0.32454681396484375\n",
      "Training iteration: 736\n",
      "Validation loss (no improvement): 0.29021918773651123\n",
      "Training iteration: 737\n",
      "Validation loss (no improvement): 0.13536452054977416\n",
      "Training iteration: 738\n",
      "Validation loss (no improvement): 0.20673272609710694\n",
      "Training iteration: 739\n",
      "Validation loss (no improvement): 0.13513624668121338\n",
      "Training iteration: 740\n",
      "Validation loss (no improvement): 0.22051148414611815\n",
      "Training iteration: 741\n",
      "Validation loss (no improvement): 1.1242788314819336\n",
      "Training iteration: 742\n",
      "Validation loss (no improvement): 0.3161412477493286\n",
      "Training iteration: 743\n",
      "Validation loss (no improvement): 0.49036035537719724\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.09629864692687988  to: 0.09587792158126832\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.09587792158126832  to: 0.08029088973999024\n",
      "Training iteration: 746\n",
      "Validation loss (no improvement): 0.4189902305603027\n",
      "Training iteration: 747\n",
      "Validation loss (no improvement): 0.21984496116638183\n",
      "Training iteration: 748\n",
      "Validation loss (no improvement): 0.3014592409133911\n",
      "Training iteration: 749\n",
      "Validation loss (no improvement): 0.14907686710357665\n",
      "Training iteration: 750\n",
      "Validation loss (no improvement): 0.5200341224670411\n",
      "Training iteration: 751\n",
      "Validation loss (no improvement): 0.4412232398986816\n",
      "Training iteration: 752\n",
      "Validation loss (no improvement): 0.5304577350616455\n",
      "Training iteration: 753\n",
      "Validation loss (no improvement): 0.5308396339416503\n",
      "Training iteration: 754\n",
      "Validation loss (no improvement): 0.3370378494262695\n",
      "Training iteration: 755\n",
      "Validation loss (no improvement): 0.16445844173431395\n",
      "Training iteration: 756\n",
      "Validation loss (no improvement): 1.6331266403198241\n",
      "Training iteration: 757\n",
      "Validation loss (no improvement): 0.18429877758026122\n",
      "Training iteration: 758\n",
      "Validation loss (no improvement): 0.2960529804229736\n",
      "Training iteration: 759\n",
      "Validation loss (no improvement): 1.0234349250793457\n",
      "Training iteration: 760\n",
      "Validation loss (no improvement): 0.2041165828704834\n",
      "Training iteration: 761\n",
      "Validation loss (no improvement): 0.2634120941162109\n",
      "Training iteration: 762\n",
      "Validation loss (no improvement): 0.12415925264358521\n",
      "Training iteration: 763\n",
      "Validation loss (no improvement): 0.736810302734375\n",
      "Training iteration: 764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 1.0279186248779297\n",
      "Training iteration: 765\n",
      "Validation loss (no improvement): 0.4229579925537109\n",
      "Training iteration: 766\n",
      "Validation loss (no improvement): 1.0255083084106444\n",
      "Training iteration: 767\n",
      "Validation loss (no improvement): 0.6628496646881104\n",
      "Training iteration: 768\n",
      "Validation loss (no improvement): 0.5601374626159668\n",
      "Training iteration: 769\n",
      "Validation loss (no improvement): 0.08069158792495727\n",
      "Training iteration: 770\n",
      "Validation loss (no improvement): 0.1839604616165161\n",
      "Training iteration: 771\n",
      "Validation loss (no improvement): 0.14077602624893187\n",
      "Training iteration: 772\n",
      "Validation loss (no improvement): 0.2661810159683228\n",
      "Training iteration: 773\n",
      "Validation loss (no improvement): 0.924609375\n",
      "Training iteration: 774\n",
      "Validation loss (no improvement): 0.22104876041412352\n",
      "Training iteration: 775\n",
      "Validation loss (no improvement): 0.18637908697128297\n",
      "Training iteration: 776\n",
      "Validation loss (no improvement): 0.35948493480682375\n",
      "Training iteration: 777\n",
      "Validation loss (no improvement): 0.4086594581604004\n",
      "Training iteration: 778\n",
      "Validation loss (no improvement): 0.33941586017608644\n",
      "Training iteration: 779\n",
      "Validation loss (no improvement): 0.21093463897705078\n",
      "Training iteration: 780\n",
      "Validation loss (no improvement): 0.4647508144378662\n",
      "Training iteration: 781\n",
      "Validation loss (no improvement): 0.825009536743164\n",
      "Training iteration: 782\n",
      "Validation loss (no improvement): 0.2900595426559448\n",
      "Training iteration: 783\n",
      "Validation loss (no improvement): 0.6625734806060791\n",
      "Training iteration: 784\n",
      "Validation loss (no improvement): 0.18064601421356202\n",
      "Training iteration: 785\n",
      "Validation loss (no improvement): 0.3875780820846558\n",
      "Training iteration: 786\n",
      "Validation loss (no improvement): 0.4696536064147949\n",
      "Training iteration: 787\n",
      "Validation loss (no improvement): 0.5258258819580078\n",
      "Training iteration: 788\n",
      "Validation loss (no improvement): 1.6372552871704102\n",
      "Training iteration: 789\n",
      "Validation loss (no improvement): 0.2484370708465576\n",
      "Training iteration: 790\n",
      "Validation loss (no improvement): 0.24175114631652833\n",
      "Training iteration: 791\n",
      "Validation loss (no improvement): 0.16781995296478272\n",
      "Training iteration: 792\n",
      "Validation loss (no improvement): 0.18674857616424562\n",
      "Training iteration: 793\n",
      "Validation loss (no improvement): 1.0471844673156738\n",
      "Training iteration: 794\n",
      "Validation loss (no improvement): 0.34398531913757324\n",
      "Training iteration: 795\n",
      "Validation loss (no improvement): 0.7212687492370605\n",
      "Training iteration: 796\n",
      "Validation loss (no improvement): 0.2003260374069214\n",
      "Training iteration: 797\n",
      "Validation loss (no improvement): 0.41948442459106444\n",
      "Training iteration: 798\n",
      "Validation loss (no improvement): 0.8996598243713378\n",
      "Training iteration: 799\n",
      "Validation loss (no improvement): 0.5548287391662597\n",
      "Training iteration: 800\n",
      "Validation loss (no improvement): 1.438501739501953\n",
      "Training iteration: 801\n",
      "Validation loss (no improvement): 0.8217181205749512\n",
      "Training iteration: 802\n",
      "Validation loss (no improvement): 0.12315937280654907\n",
      "Training iteration: 803\n",
      "Validation loss (no improvement): 0.30227274894714357\n",
      "Training iteration: 804\n",
      "Validation loss (no improvement): 0.15688328742980956\n",
      "Training iteration: 805\n",
      "Validation loss (no improvement): 0.4530996799468994\n",
      "Training iteration: 806\n",
      "Validation loss (no improvement): 0.2553190946578979\n",
      "Training iteration: 807\n",
      "Validation loss (no improvement): 0.1662648558616638\n",
      "Training iteration: 808\n",
      "Validation loss (no improvement): 0.08480091094970703\n",
      "Training iteration: 809\n",
      "Validation loss (no improvement): 0.33925366401672363\n",
      "Training iteration: 810\n",
      "Validation loss (no improvement): 0.333125376701355\n",
      "Training iteration: 811\n",
      "Validation loss (no improvement): 0.5696780204772949\n",
      "Training iteration: 812\n",
      "Validation loss (no improvement): 0.5320477962493897\n",
      "Training iteration: 813\n",
      "Validation loss (no improvement): 1.628554916381836\n",
      "Training iteration: 814\n",
      "Validation loss (no improvement): 0.23609695434570313\n",
      "Training iteration: 815\n",
      "Validation loss (no improvement): 0.20883193016052246\n",
      "Training iteration: 816\n",
      "Validation loss (no improvement): 0.3579680919647217\n",
      "Training iteration: 817\n",
      "Validation loss (no improvement): 0.35300095081329347\n",
      "Training iteration: 818\n",
      "Validation loss (no improvement): 0.21632742881774902\n",
      "Training iteration: 819\n",
      "Validation loss (no improvement): 0.1289205551147461\n",
      "Training iteration: 820\n",
      "Validation loss (no improvement): 0.6935858726501465\n",
      "Training iteration: 821\n",
      "Validation loss (no improvement): 0.44007058143615724\n",
      "Training iteration: 822\n",
      "Validation loss (no improvement): 1.1639328002929688\n",
      "Training iteration: 823\n",
      "Validation loss (no improvement): 0.19902391433715821\n",
      "Training iteration: 824\n",
      "Validation loss (no improvement): 0.6573542594909668\n",
      "Training iteration: 825\n",
      "Validation loss (no improvement): 0.18175933361053467\n",
      "Training iteration: 826\n",
      "Validation loss (no improvement): 0.3067796230316162\n",
      "Training iteration: 827\n",
      "Validation loss (no improvement): 0.8118602752685546\n",
      "Training iteration: 828\n",
      "Validation loss (no improvement): 0.40439658164978026\n",
      "Training iteration: 829\n",
      "Validation loss (no improvement): 0.47322492599487304\n",
      "Training iteration: 830\n",
      "Validation loss (no improvement): 0.1367795467376709\n",
      "Training iteration: 831\n",
      "Validation loss (no improvement): 0.6988868713378906\n",
      "Training iteration: 832\n",
      "Validation loss (no improvement): 0.3487967252731323\n",
      "Training iteration: 833\n",
      "Validation loss (no improvement): 0.5677260398864746\n",
      "Training iteration: 834\n",
      "Validation loss (no improvement): 0.2925878524780273\n",
      "Training iteration: 835\n",
      "Validation loss (no improvement): 0.7359054565429688\n",
      "Training iteration: 836\n",
      "Validation loss (no improvement): 0.23025336265563964\n",
      "Training iteration: 837\n",
      "Validation loss (no improvement): 1.4200238227844237\n",
      "Training iteration: 838\n",
      "Validation loss (no improvement): 1.7534793853759765\n",
      "Training iteration: 839\n",
      "Validation loss (no improvement): 0.21018662452697753\n",
      "Training iteration: 840\n",
      "Validation loss (no improvement): 0.22308025360107422\n",
      "Training iteration: 841\n",
      "Validation loss (no improvement): 1.1052831649780273\n",
      "Training iteration: 842\n",
      "Validation loss (no improvement): 0.39858901500701904\n",
      "Training iteration: 843\n",
      "Validation loss (no improvement): 0.48532953262329104\n",
      "Training iteration: 844\n",
      "Validation loss (no improvement): 0.19548851251602173\n",
      "Training iteration: 845\n",
      "Validation loss (no improvement): 0.790432357788086\n"
     ]
    }
   ],
   "source": [
    "dropout_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 8.543504333496093\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 8.543504333496093  to: 6.167788696289063\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 6.167788696289063  to: 4.5039619445800785\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 4.5039619445800785  to: 3.3495086669921874\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 3.3495086669921874  to: 2.5355951309204103\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 2.5355951309204103  to: 1.9437358856201172\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 1.9437358856201172  to: 1.5109571456909179\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 1.5109571456909179  to: 1.1925321578979493\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.1925321578979493  to: 0.9565099716186524\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 0.9565099716186524  to: 0.7804024696350098\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 0.7804024696350098  to: 0.6479053497314453\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.6479053497314453  to: 0.5472270488739014\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.5472270488739014  to: 0.4700127601623535\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.4700127601623535  to: 0.4102922916412354\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.4102922916412354  to: 0.36374797821044924\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.36374797821044924  to: 0.3271580934524536\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.3271580934524536  to: 0.2981067180633545\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.2981067180633545  to: 0.2748166561126709\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.2748166561126709  to: 0.2559672832489014\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.2559672832489014  to: 0.24058222770690918\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.24058222770690918  to: 0.22794466018676757\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.22794466018676757  to: 0.21748054027557373\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.21748054027557373  to: 0.2087696075439453\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.2087696075439453  to: 0.20145742893218993\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.20145742893218993  to: 0.19529151916503906\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.19529151916503906  to: 0.19006465673446654\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.19006465673446654  to: 0.1856072425842285\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.1856072425842285  to: 0.18178927898406982\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.18178927898406982  to: 0.17850427627563475\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.17850427627563475  to: 0.1756646752357483\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.1756646752357483  to: 0.17319368124008178\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.17319368124008178  to: 0.17103493213653564\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.17103493213653564  to: 0.16914066076278686\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.16914066076278686  to: 0.16747090816497803\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.16747090816497803  to: 0.16599595546722412\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.16599595546722412  to: 0.1646851897239685\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.1646851897239685  to: 0.1635136604309082\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.1635136604309082  to: 0.16245880126953124\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.16245880126953124  to: 0.1615020751953125\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.1615020751953125  to: 0.16063506603240968\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.16063506603240968  to: 0.15984604358673096\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.15984604358673096  to: 0.15912487506866455\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.15912487506866455  to: 0.1584629535675049\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1584629535675049  to: 0.15785322189331055\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.15785322189331055  to: 0.15728845596313476\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.15728845596313476  to: 0.15676368474960328\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.15676368474960328  to: 0.156274151802063\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.156274151802063  to: 0.15581581592559815\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.15581581592559815  to: 0.15538183450698853\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.15538183450698853  to: 0.15497164726257323\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.15497164726257323  to: 0.15458211898803711\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.15458211898803711  to: 0.1542109489440918\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.1542109489440918  to: 0.15385665893554687\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.15385665893554687  to: 0.15351823568344117\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.15351823568344117  to: 0.15319414138793946\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.15319414138793946  to: 0.15288290977478028\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.15288290977478028  to: 0.15258337259292604\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.15258337259292604  to: 0.15229425430297852\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.15229425430297852  to: 0.15201432704925538\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.15201432704925538  to: 0.15174338817596436\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.15174338817596436  to: 0.15148022174835205\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.15148022174835205  to: 0.15122333765029908\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.15122333765029908  to: 0.15097261667251588\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.15097261667251588  to: 0.15072805881500245\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.15072805881500245  to: 0.15048907995223998\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.15048907995223998  to: 0.1502552628517151\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.1502552628517151  to: 0.1500261902809143\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.1500261902809143  to: 0.14980151653289794\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.14980151653289794  to: 0.1495809316635132\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.1495809316635132  to: 0.14936418533325196\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.14936418533325196  to: 0.14915109872817994\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14915109872817994  to: 0.14894142150878906\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.14894142150878906  to: 0.14873499870300294\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.14873499870300294  to: 0.14853168725967408\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.14853168725967408  to: 0.1483312726020813\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.1483312726020813  to: 0.1481335997581482\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.1481335997581482  to: 0.14793760776519777\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.14793760776519777  to: 0.14774415493011475\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.14774415493011475  to: 0.14755319356918334\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.14755319356918334  to: 0.14736456871032716\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.14736456871032716  to: 0.14717823266983032\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14717823266983032  to: 0.14699405431747437\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.14699405431747437  to: 0.1468118906021118\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1468118906021118  to: 0.14663171768188477\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.14663171768188477  to: 0.1464533567428589\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.1464533567428589  to: 0.1462768793106079\n",
      "Training iteration: 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1462768793106079  to: 0.14610209465026855\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.14610209465026855  to: 0.14592907428741456\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14592907428741456  to: 0.145757794380188\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.145757794380188  to: 0.14558818340301513\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.14558818340301513  to: 0.14542024135589598\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.14542024135589598  to: 0.1452528715133667\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1452528715133667  to: 0.14508657455444335\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14508657455444335  to: 0.14492180347442626\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.14492180347442626  to: 0.14475857019424437\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14475857019424437  to: 0.14459681510925293\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.14459681510925293  to: 0.1444365382194519\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.1444365382194519  to: 0.1442776918411255\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.1442776918411255  to: 0.14412018060684204\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14412018060684204  to: 0.14396359920501708\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14396359920501708  to: 0.14380837678909303\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14380837678909303  to: 0.143654465675354\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.143654465675354  to: 0.14350180625915526\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14350180625915526  to: 0.1433501124382019\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.1433501124382019  to: 0.1431996703147888\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.1431996703147888  to: 0.14305050373077394\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14305050373077394  to: 0.14290256500244142\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14290256500244142  to: 0.14275586605072021\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.14275586605072021  to: 0.14261038303375245\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14261038303375245  to: 0.14246610403060914\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14246610403060914  to: 0.14232299327850342\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14232299327850342  to: 0.14218080043792725\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14218080043792725  to: 0.1420397400856018\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.1420397400856018  to: 0.14190002679824829\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14190002679824829  to: 0.14176198244094848\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14176198244094848  to: 0.1416250467300415\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.1416250467300415  to: 0.14148918390274048\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14148918390274048  to: 0.14135439395904542\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14135439395904542  to: 0.1412206768989563\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.1412206768989563  to: 0.14108798503875733\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14108798503875733  to: 0.14095633029937743\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14095633029937743  to: 0.14082568883895874\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14082568883895874  to: 0.14069592952728271\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14069592952728271  to: 0.14056671857833863\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14056671857833863  to: 0.14043850898742677\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14043850898742677  to: 0.1403112530708313\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.1403112530708313  to: 0.14018497467041016\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14018497467041016  to: 0.14005963802337645\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14005963802337645  to: 0.13993515968322753\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.13993515968322753  to: 0.13981159925460815\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.13981159925460815  to: 0.1396884560585022\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.1396884560585022  to: 0.139566171169281\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.139566171169281  to: 0.1394448161125183\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.1394448161125183  to: 0.13932397365570068\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.13932397365570068  to: 0.13920385837554933\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.13920385837554933  to: 0.13908461332321168\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.13908461332321168  to: 0.1389662504196167\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.1389662504196167  to: 0.13884880542755126\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.13884880542755126  to: 0.13873250484466554\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.13873250484466554  to: 0.13861736059188842\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.13861736059188842  to: 0.1385030746459961\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.1385030746459961  to: 0.13838959932327272\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.13838959932327272  to: 0.13827688694000245\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.13827688694000245  to: 0.13816500902175904\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.13816500902175904  to: 0.13805395364761353\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.13805395364761353  to: 0.13794370889663696\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.13794370889663696  to: 0.1378342866897583\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1378342866897583  to: 0.13772618770599365\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.13772618770599365  to: 0.13761910200119018\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.13761910200119018  to: 0.13751281499862672\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.13751281499862672  to: 0.13740726709365844\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.13740726709365844  to: 0.1373024821281433\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.1373024821281433  to: 0.13719848394393921\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.13719848394393921  to: 0.13709524869918824\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.13709524869918824  to: 0.13699276447296144\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.13699276447296144  to: 0.13689104318618775\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.13689104318618775  to: 0.13679003715515137\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.13679003715515137  to: 0.1366897463798523\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.1366897463798523  to: 0.13659021854400635\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.13659021854400635  to: 0.13649139404296876\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.13649139404296876  to: 0.13639328479766846\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.13639328479766846  to: 0.13629591464996338\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.13629591464996338  to: 0.1361992597579956\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.1361992597579956  to: 0.136103355884552\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.136103355884552  to: 0.1360081434249878\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1360081434249878  to: 0.1359136462211609\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.1359136462211609  to: 0.13581985235214233\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.13581985235214233  to: 0.13572672605514527\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.13572672605514527  to: 0.13563419580459596\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.13563419580459596  to: 0.13554214239120482\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.13554214239120482  to: 0.13545076847076415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 171\n",
      "Improved validation loss from: 0.13545076847076415  to: 0.13536012172698975\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.13536012172698975  to: 0.1352703332901001\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.1352703332901001  to: 0.1351812481880188\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.1351812481880188  to: 0.1350931406021118\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.1350931406021118  to: 0.13500568866729737\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.13500568866729737  to: 0.1349189043045044\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.1349189043045044  to: 0.134832763671875\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.134832763671875  to: 0.1347472548484802\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.1347472548484802  to: 0.13466222286224366\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.13466222286224366  to: 0.13457772731781006\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.13457772731781006  to: 0.13449389934539796\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.13449389934539796  to: 0.13441070318222045\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.13441070318222045  to: 0.13432815074920654\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.13432815074920654  to: 0.13424642086029054\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.13424642086029054  to: 0.13416558504104614\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.13416558504104614  to: 0.1340853452682495\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.1340853452682495  to: 0.13400576114654542\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13400576114654542  to: 0.13392679691314696\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13392679691314696  to: 0.13384872674942017\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.13384872674942017  to: 0.13377132415771484\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13377132415771484  to: 0.13369452953338623\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13369452953338623  to: 0.13361839056015015\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13361839056015015  to: 0.13354287147521973\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13354287147521973  to: 0.13346798419952394\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13346798419952394  to: 0.13339370489120483\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13339370489120483  to: 0.13332006931304932\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13332006931304932  to: 0.1332470417022705\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.1332470417022705  to: 0.1331746459007263\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1331746459007263  to: 0.1331028699874878\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.1331028699874878  to: 0.13303143978118898\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13303143978118898  to: 0.13296066522598265\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13296066522598265  to: 0.13289053440093995\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13289053440093995  to: 0.13282104730606079\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.13282104730606079  to: 0.13275221586227418\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13275221586227418  to: 0.13268401622772216\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13268401622772216  to: 0.1326164960861206\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.1326164960861206  to: 0.13254963159561156\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13254963159561156  to: 0.1324834108352661\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.1324834108352661  to: 0.13241784572601317\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.13241784572601317  to: 0.1323529601097107\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.1323529601097107  to: 0.13228873014450074\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.13228873014450074  to: 0.1322251796722412\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.1322251796722412  to: 0.13216228485107423\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13216228485107423  to: 0.13210008144378663\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.13210008144378663  to: 0.13203856945037842\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13203856945037842  to: 0.1319777250289917\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.1319777250289917  to: 0.13191759586334229\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.13191759586334229  to: 0.1318581819534302\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1318581819534302  to: 0.1317994236946106\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.1317994236946106  to: 0.13174139261245726\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13174139261245726  to: 0.13168405294418334\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13168405294418334  to: 0.13162745237350465\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13162745237350465  to: 0.1315717339515686\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1315717339515686  to: 0.1315167188644409\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.1315167188644409  to: 0.13146241903305053\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13146241903305053  to: 0.13140878677368165\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13140878677368165  to: 0.13135581016540526\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13135581016540526  to: 0.1313035249710083\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.1313035249710083  to: 0.13125197887420653\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13125197887420653  to: 0.1312011480331421\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.1312011480331421  to: 0.1311510443687439\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.1311510443687439  to: 0.13110167980194093\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.13110167980194093  to: 0.13105306625366211\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13105306625366211  to: 0.1310051679611206\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1310051679611206  to: 0.13095804452896118\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.13095804452896118  to: 0.13091163635253905\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13091163635253905  to: 0.1308659791946411\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1308659791946411  to: 0.13082107305526733\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13082107305526733  to: 0.13077691793441773\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.13077691793441773  to: 0.13073351383209228\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13073351383209228  to: 0.130690860748291\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.130690860748291  to: 0.130648934841156\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.130648934841156  to: 0.1306077718734741\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.1306077718734741  to: 0.13056737184524536\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13056737184524536  to: 0.13052771091461182\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.13052771091461182  to: 0.13048880100250243\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13048880100250243  to: 0.13045060634613037\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.13045060634613037  to: 0.13041322231292723\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13041322231292723  to: 0.13037660121917724\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13037660121917724  to: 0.13034069538116455\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13034069538116455  to: 0.13030554056167604\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.13030554056167604  to: 0.13027111291885377\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13027111291885377  to: 0.13023741245269777\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13023741245269777  to: 0.13020442724227904\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13020442724227904  to: 0.13017218112945556\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13017218112945556  to: 0.13014065027236937\n",
      "Training iteration: 257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13014065027236937  to: 0.13010982275009156\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13010982275009156  to: 0.13007969856262208\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13007969856262208  to: 0.13005028963088988\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13005028963088988  to: 0.1300215721130371\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.1300215721130371  to: 0.12999355792999268\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.12999355792999268  to: 0.12996619939804077\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.12996619939804077  to: 0.12993953227996827\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.12993953227996827  to: 0.12991353273391723\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.12991353273391723  to: 0.12988818883895875\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.12988818883895875  to: 0.12986352443695068\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.12986352443695068  to: 0.12983949184417726\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.12983949184417726  to: 0.12981611490249634\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.12981611490249634  to: 0.12979333400726317\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.12979333400726317  to: 0.1297710657119751\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.1297710657119751  to: 0.12974939346313477\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.12974939346313477  to: 0.12972828149795532\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.12972828149795532  to: 0.12970755100250245\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.12970755100250245  to: 0.12968740463256836\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.12968740463256836  to: 0.12966779470443726\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.12966779470443726  to: 0.12964874505996704\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.12964874505996704  to: 0.12963021993637086\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.12963021993637086  to: 0.12961212396621705\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.12961212396621705  to: 0.12959449291229247\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.12959449291229247  to: 0.12957736253738403\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.12957736253738403  to: 0.12956072092056276\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.12956072092056276  to: 0.12954453229904175\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.12954453229904175  to: 0.12952879667282105\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.12952879667282105  to: 0.12951351404190065\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.12951351404190065  to: 0.12949858903884887\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.12949858903884887  to: 0.1294838547706604\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1294838547706604  to: 0.12946949005126954\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.12946949005126954  to: 0.1294555425643921\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.1294555425643921  to: 0.12944190502166747\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.12944190502166747  to: 0.12942864894866943\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.12942864894866943  to: 0.12941570281982423\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.12941570281982423  to: 0.12940307855606079\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.12940307855606079  to: 0.12939069271087647\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.12939069271087647  to: 0.12937841415405274\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.12937841415405274  to: 0.1293663740158081\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.1293663740158081  to: 0.1293545961380005\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.1293545961380005  to: 0.12934305667877197\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.12934305667877197  to: 0.12933170795440674\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.12933170795440674  to: 0.12932056188583374\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.12932056188583374  to: 0.12930963039398194\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.12930963039398194  to: 0.1292988419532776\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.1292988419532776  to: 0.12928823232650757\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.12928823232650757  to: 0.12927777767181398\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.12927777767181398  to: 0.1292674422264099\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.1292674422264099  to: 0.12925726175308228\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.12925726175308228  to: 0.12924716472625733\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.12924716472625733  to: 0.12923717498779297\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.12923717498779297  to: 0.12922728061676025\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.12922728061676025  to: 0.12921730279922486\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.12921730279922486  to: 0.1292073130607605\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.1292073130607605  to: 0.12919739484786988\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.12919739484786988  to: 0.12918750047683716\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.12918750047683716  to: 0.12917764186859132\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.12917764186859132  to: 0.12916780710220338\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.12916780710220338  to: 0.12915796041488647\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.12915796041488647  to: 0.1291481375694275\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.1291481375694275  to: 0.12913830280303956\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.12913830280303956  to: 0.1291284441947937\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.1291284441947937  to: 0.1291185975074768\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1291185975074768  to: 0.129108726978302\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.129108726978302  to: 0.1290988326072693\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1290988326072693  to: 0.1290889263153076\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.1290889263153076  to: 0.12907896041870118\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.12907896041870118  to: 0.12906897068023682\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.12906897068023682  to: 0.12905892133712768\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.12905892133712768  to: 0.12904881238937377\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.12904881238937377  to: 0.12903865575790405\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.12903865575790405  to: 0.12902839183807374\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.12902839183807374  to: 0.12901772260665895\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.12901772260665895  to: 0.1290067672729492\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.1290067672729492  to: 0.12899569272994996\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.12899569272994996  to: 0.12898454666137696\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.12898454666137696  to: 0.12897332906723022\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.12897332906723022  to: 0.1289620280265808\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.1289620280265808  to: 0.12895063161849976\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.12895063161849976  to: 0.12893915176391602\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.12893915176391602  to: 0.12892757654190062\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.12892757654190062  to: 0.1289158821105957\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.1289158821105957  to: 0.128904128074646\n",
      "Training iteration: 340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.128904128074646  to: 0.1288922667503357\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.1288922667503357  to: 0.12888033390045167\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.12888033390045167  to: 0.12886825799942017\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.12886825799942017  to: 0.1288560748100281\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.1288560748100281  to: 0.1288437843322754\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.1288437843322754  to: 0.1288313627243042\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.1288313627243042  to: 0.1288188338279724\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.1288188338279724  to: 0.12880618572235109\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.12880618572235109  to: 0.12879340648651122\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.12879340648651122  to: 0.12878050804138183\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.12878050804138183  to: 0.12876747846603392\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.12876747846603392  to: 0.12875431776046753\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.12875431776046753  to: 0.12874103784561158\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.12874103784561158  to: 0.1287276029586792\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.1287276029586792  to: 0.12871408462524414\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.12871408462524414  to: 0.12870041131973267\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.12870041131973267  to: 0.12868661880493165\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.12868661880493165  to: 0.1286726713180542\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1286726713180542  to: 0.1286584496498108\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.1286584496498108  to: 0.12864391803741454\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.12864391803741454  to: 0.12862926721572876\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.12862926721572876  to: 0.12861452102661133\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.12861452102661133  to: 0.12859961986541749\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.12859961986541749  to: 0.12858461141586303\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.12858461141586303  to: 0.12856943607330323\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.12856943607330323  to: 0.1285541296005249\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.1285541296005249  to: 0.12853869199752807\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.12853869199752807  to: 0.12852318286895753\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.12852318286895753  to: 0.12850781679153442\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.12850781679153442  to: 0.12849231958389282\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.12849231958389282  to: 0.12847671508789063\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.12847671508789063  to: 0.12846100330352783\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.12846100330352783  to: 0.1284451961517334\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.1284451961517334  to: 0.1284293055534363\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.1284293055534363  to: 0.12841331958770752\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.12841331958770752  to: 0.1283972978591919\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.1283972978591919  to: 0.1283812165260315\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1283812165260315  to: 0.12836507558822632\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.12836507558822632  to: 0.12834887504577636\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.12834887504577636  to: 0.12833263874053955\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.12833263874053955  to: 0.128316330909729\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.128316330909729  to: 0.12830004692077637\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.12830004692077637  to: 0.1282838225364685\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.1282838225364685  to: 0.128267502784729\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.128267502784729  to: 0.12825112342834472\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.12825112342834472  to: 0.1282346725463867\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.1282346725463867  to: 0.12821816205978392\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.12821816205978392  to: 0.12820156812667846\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.12820156812667846  to: 0.12818490266799926\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.12818490266799926  to: 0.1281681776046753\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1281681776046753  to: 0.1281513452529907\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.1281513452529907  to: 0.12813444137573243\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.12813444137573243  to: 0.12811744213104248\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12811744213104248  to: 0.12810035943984985\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12810035943984985  to: 0.12808319330215454\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.12808319330215454  to: 0.12806590795516967\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.12806590795516967  to: 0.1280485510826111\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.1280485510826111  to: 0.1280311107635498\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1280311107635498  to: 0.12801353931427\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.12801353931427  to: 0.12799588441848755\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.12799588441848755  to: 0.12797813415527343\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12797813415527343  to: 0.12796027660369874\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.12796027660369874  to: 0.12794233560562135\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.12794233560562135  to: 0.12792428731918334\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.12792428731918334  to: 0.12790616750717163\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.12790616750717163  to: 0.1278879761695862\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.1278879761695862  to: 0.12786970138549805\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.12786970138549805  to: 0.12785135507583617\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.12785135507583617  to: 0.12783291339874267\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.12783291339874267  to: 0.1278144121170044\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.1278144121170044  to: 0.12779581546783447\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.12779581546783447  to: 0.12777713537216187\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.12777713537216187  to: 0.12775828838348388\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.12775828838348388  to: 0.1277393341064453\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1277393341064453  to: 0.12772033214569092\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.12772033214569092  to: 0.1277013063430786\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.1277013063430786  to: 0.12768218517303467\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.12768218517303467  to: 0.12766298055648803\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.12766298055648803  to: 0.12764365673065187\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.12764365673065187  to: 0.1276242733001709\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.1276242733001709  to: 0.12760480642318725\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.12760480642318725  to: 0.1275852918624878\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1275852918624878  to: 0.12756569385528566\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.12756569385528566  to: 0.12754602432250978\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.12754602432250978  to: 0.12752630710601806\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.12752630710601806  to: 0.12750651836395263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 426\n",
      "Improved validation loss from: 0.12750651836395263  to: 0.12748669385910033\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.12748669385910033  to: 0.1274668335914612\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1274668335914612  to: 0.1274469256401062\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1274469256401062  to: 0.1274269700050354\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1274269700050354  to: 0.12740694284439086\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.12740694284439086  to: 0.12738686800003052\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.12738686800003052  to: 0.1273667335510254\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1273667335510254  to: 0.12734651565551758\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.12734651565551758  to: 0.12732622623443604\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.12732622623443604  to: 0.12730586528778076\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.12730586528778076  to: 0.12728551626205445\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.12728551626205445  to: 0.12726519107818604\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.12726519107818604  to: 0.12724483013153076\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.12724483013153076  to: 0.12722446918487548\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.12722446918487548  to: 0.1272040843963623\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1272040843963623  to: 0.12718366384506224\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12718366384506224  to: 0.12716319561004638\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.12716319561004638  to: 0.12714269161224365\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.12714269161224365  to: 0.1271221399307251\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.1271221399307251  to: 0.12710151672363282\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12710151672363282  to: 0.12708089351654053\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.12708089351654053  to: 0.12706018686294557\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.12706018686294557  to: 0.12703940868377686\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.12703940868377686  to: 0.12701859474182128\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.12701859474182128  to: 0.12699772119522096\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.12699772119522096  to: 0.1269767999649048\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.1269767999649048  to: 0.12695581912994386\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.12695581912994386  to: 0.1269347548484802\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.1269347548484802  to: 0.12691361904144288\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.12691361904144288  to: 0.12689239978790284\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.12689239978790284  to: 0.1268710732460022\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.1268710732460022  to: 0.12684967517852783\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.12684967517852783  to: 0.126828134059906\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.126828134059906  to: 0.12680649757385254\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.12680649757385254  to: 0.12678474187850952\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12678474187850952  to: 0.12676287889480592\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.12676287889480592  to: 0.1267408847808838\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1267408847808838  to: 0.12671878337860107\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.12671878337860107  to: 0.12669656276702881\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12669656276702881  to: 0.12667436599731446\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12667436599731446  to: 0.12665213346481324\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.12665213346481324  to: 0.12662990093231202\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.12662990093231202  to: 0.12660760879516603\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.12660760879516603  to: 0.12658536434173584\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.12658536434173584  to: 0.12656311988830565\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.12656311988830565  to: 0.1265408515930176\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.1265408515930176  to: 0.12651869058609008\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.12651869058609008  to: 0.12649673223495483\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.12649673223495483  to: 0.12647472620010375\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.12647472620010375  to: 0.12645266056060792\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.12645266056060792  to: 0.12643053531646728\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.12643053531646728  to: 0.12640832662582396\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.12640832662582396  to: 0.12638607025146484\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.12638607025146484  to: 0.12636371850967407\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.12636371850967407  to: 0.12634127140045165\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.12634127140045165  to: 0.12631871700286865\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.12631871700286865  to: 0.1262960910797119\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.1262960910797119  to: 0.12627334594726564\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12627334594726564  to: 0.1262504816055298\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.1262504816055298  to: 0.1262274980545044\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.1262274980545044  to: 0.12620433568954467\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.12620433568954467  to: 0.1261807680130005\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.1261807680130005  to: 0.12615687847137452\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.12615687847137452  to: 0.12613270282745362\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.12613270282745362  to: 0.12610855102539062\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12610855102539062  to: 0.12608439922332765\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12608439922332765  to: 0.12606018781661987\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.12606018781661987  to: 0.1260359525680542\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.1260359525680542  to: 0.1260116934776306\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.1260116934776306  to: 0.125987446308136\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.125987446308136  to: 0.1259630560874939\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.1259630560874939  to: 0.12593772411346435\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.12593772411346435  to: 0.1259114623069763\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.1259114623069763  to: 0.12588526010513307\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.12588526010513307  to: 0.12585923671722413\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.12585923671722413  to: 0.1258331060409546\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.1258331060409546  to: 0.1258068561553955\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.1258068561553955  to: 0.12578046321868896\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.12578046321868896  to: 0.12575393915176392\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12575393915176392  to: 0.1257272481918335\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.1257272481918335  to: 0.12570041418075562\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12570041418075562  to: 0.12567341327667236\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12567341327667236  to: 0.12564622163772582\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.12564622163772582  to: 0.12561886310577391\n",
      "Training iteration: 510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12561886310577391  to: 0.12559131383895875\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.12559131383895875  to: 0.12556357383728028\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12556357383728028  to: 0.12553566694259644\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.12553566694259644  to: 0.12550777196884155\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.12550777196884155  to: 0.12547965049743653\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12547965049743653  to: 0.12545137405395507\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.12545137405395507  to: 0.12542299032211304\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12542299032211304  to: 0.12539442777633666\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.12539442777633666  to: 0.12536580562591554\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.12536580562591554  to: 0.1253371238708496\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.1253371238708496  to: 0.12530834674835206\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.12530834674835206  to: 0.12527947425842284\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.12527947425842284  to: 0.12525049448013306\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.12525049448013306  to: 0.1252214193344116\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.1252214193344116  to: 0.12519218921661376\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.12519218921661376  to: 0.12516286373138427\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.12516286373138427  to: 0.12513339519500732\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12513339519500732  to: 0.12510378360748292\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.12510378360748292  to: 0.12507404088974\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.12507404088974  to: 0.12504416704177856\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12504416704177856  to: 0.1250141143798828\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.1250141143798828  to: 0.12498395442962647\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12498395442962647  to: 0.1249536395072937\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.1249536395072937  to: 0.12492319345474243\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.12492319345474243  to: 0.12489259243011475\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.12489259243011475  to: 0.1248618483543396\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1248618483543396  to: 0.12483094930648804\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.12483094930648804  to: 0.12479994297027588\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.12479994297027588  to: 0.12476875782012939\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.12476875782012939  to: 0.1247374415397644\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.1247374415397644  to: 0.12470608949661255\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.12470608949661255  to: 0.12467472553253174\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.12467472553253174  to: 0.12464333772659301\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.12464333772659301  to: 0.12461192607879638\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.12461192607879638  to: 0.1245805025100708\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.1245805025100708  to: 0.12454900741577149\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.12454900741577149  to: 0.12451747655868531\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12451747655868531  to: 0.1244858980178833\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.1244858980178833  to: 0.12445409297943115\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12445409297943115  to: 0.1244208574295044\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.1244208574295044  to: 0.12438752651214599\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.12438752651214599  to: 0.12435410022735596\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12435410022735596  to: 0.12432060241699219\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12432060241699219  to: 0.12428699731826783\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12428699731826783  to: 0.12425330877304078\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.12425330877304078  to: 0.12421951293945313\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12421951293945313  to: 0.12418560981750489\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.12418560981750489  to: 0.124151611328125\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.124151611328125  to: 0.12411754131317139\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12411754131317139  to: 0.12408337593078614\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.12408337593078614  to: 0.12404911518096924\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12404911518096924  to: 0.12401478290557862\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12401478290557862  to: 0.12398035526275634\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12398035526275634  to: 0.12394583225250244\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12394583225250244  to: 0.12391121387481689\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12391121387481689  to: 0.12387650012969971\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.12387650012969971  to: 0.12384169101715088\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.12384169101715088  to: 0.12380678653717041\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12380678653717041  to: 0.1237717866897583\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1237717866897583  to: 0.12373685836791992\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.12373685836791992  to: 0.1237019419670105\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.1237019419670105  to: 0.12366713285446167\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.12366713285446167  to: 0.12363235950469971\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.12363235950469971  to: 0.12359760999679566\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12359760999679566  to: 0.12356288433074951\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.12356288433074951  to: 0.12352837324142456\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12352837324142456  to: 0.12349388599395753\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12349388599395753  to: 0.12345937490463257\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.12345937490463257  to: 0.1234249234199524\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1234249234199524  to: 0.12339048385620117\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.12339048385620117  to: 0.12335608005523682\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12335608005523682  to: 0.12332168817520142\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.12332168817520142  to: 0.1232872724533081\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.1232872724533081  to: 0.12325284481048585\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.12325284481048585  to: 0.1232183814048767\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1232183814048767  to: 0.12318389415740967\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12318389415740967  to: 0.1231493353843689\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.1231493353843689  to: 0.12311471700668335\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.12311471700668335  to: 0.12308001518249512\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12308001518249512  to: 0.1230452299118042\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.1230452299118042  to: 0.12301037311553956\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.12301037311553956  to: 0.12297537326812744\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.12297537326812744  to: 0.1229402780532837\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.1229402780532837  to: 0.12290505170822144\n",
      "Training iteration: 594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12290505170822144  to: 0.12286972999572754\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12286972999572754  to: 0.12283433675765991\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12283433675765991  to: 0.12279882431030273\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12279882431030273  to: 0.12276334762573242\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12276334762573242  to: 0.1227278470993042\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.1227278470993042  to: 0.12269222736358643\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.12269222736358643  to: 0.12265646457672119\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12265646457672119  to: 0.1226205587387085\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.1226205587387085  to: 0.12258453369140625\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12258453369140625  to: 0.12254836559295654\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.12254836559295654  to: 0.12251211404800415\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.12251211404800415  to: 0.1224757194519043\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.1224757194519043  to: 0.12243921756744384\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.12243921756744384  to: 0.12240262031555176\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12240262031555176  to: 0.12236595153808594\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12236595153808594  to: 0.12232917547225952\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.12232917547225952  to: 0.12229235172271728\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12229235172271728  to: 0.12225542068481446\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12225542068481446  to: 0.12221840620040894\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12221840620040894  to: 0.12218133211135865\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12218133211135865  to: 0.12214418649673461\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12214418649673461  to: 0.12210669517517089\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12210669517517089  to: 0.12206891775131226\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12206891775131226  to: 0.12203086614608764\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12203086614608764  to: 0.12199233770370484\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12199233770370484  to: 0.12195339202880859\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12195339202880859  to: 0.12191450595855713\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12191450595855713  to: 0.12187565565109253\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12187565565109253  to: 0.12183687686920167\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12183687686920167  to: 0.12179814577102661\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12179814577102661  to: 0.12175948619842529\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12175948619842529  to: 0.12172085046768188\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12172085046768188  to: 0.1216822624206543\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.1216822624206543  to: 0.12164371013641358\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.12164371013641358  to: 0.12160520553588867\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12160520553588867  to: 0.12156671285629272\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.12156671285629272  to: 0.12152822017669677\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12152822017669677  to: 0.12148940563201904\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12148940563201904  to: 0.12145028114318848\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12145028114318848  to: 0.1214109182357788\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.1214109182357788  to: 0.12137165069580078\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12137165069580078  to: 0.12133252620697021\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12133252620697021  to: 0.1212935209274292\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.1212935209274292  to: 0.12125468254089355\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12125468254089355  to: 0.12121596336364746\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12121596336364746  to: 0.121177339553833\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.121177339553833  to: 0.12113844156265259\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12113844156265259  to: 0.12109925746917724\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12109925746917724  to: 0.1210598111152649\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1210598111152649  to: 0.12102048397064209\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12102048397064209  to: 0.12098124027252197\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12098124027252197  to: 0.12094208002090454\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12094208002090454  to: 0.12090294361114502\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12090294361114502  to: 0.12086381912231445\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12086381912231445  to: 0.12082470655441284\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12082470655441284  to: 0.12078555822372436\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12078555822372436  to: 0.12074592113494872\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12074592113494872  to: 0.12070581912994385\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12070581912994385  to: 0.12066522836685181\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12066522836685181  to: 0.12062463760375977\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12062463760375977  to: 0.12058401107788086\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12058401107788086  to: 0.12054332494735717\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12054332494735717  to: 0.12050259113311768\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12050259113311768  to: 0.12046133279800415\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12046133279800415  to: 0.12042002677917481\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12042002677917481  to: 0.12037802934646606\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12037802934646606  to: 0.12033582925796509\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12033582925796509  to: 0.12029345035552978\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12029345035552978  to: 0.12025035619735717\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12025035619735717  to: 0.12020713090896606\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12020713090896606  to: 0.1201637864112854\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.1201637864112854  to: 0.12011988162994384\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12011988162994384  to: 0.12007547616958618\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12007547616958618  to: 0.1200311541557312\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.1200311541557312  to: 0.11998794078826905\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.11998794078826905  to: 0.1199449896812439\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.1199449896812439  to: 0.1199022650718689\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.1199022650718689  to: 0.11985924243927001\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.11985924243927001  to: 0.11981598138809205\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.11981598138809205  to: 0.11977248191833496\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.11977248191833496  to: 0.11972935199737549\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.11972935199737549  to: 0.11968653202056885\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.11968653202056885  to: 0.11964402198791504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 677\n",
      "Improved validation loss from: 0.11964402198791504  to: 0.1196018934249878\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.1196018934249878  to: 0.11956002712249755\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.11956002712249755  to: 0.1195177435874939\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.1195177435874939  to: 0.11947505474090576\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.11947505474090576  to: 0.11943197250366211\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.11943197250366211  to: 0.11938917636871338\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.11938917636871338  to: 0.11934692859649658\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.11934692859649658  to: 0.11930484771728515\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.11930484771728515  to: 0.11926287412643433\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.11926287412643433  to: 0.11922038793563842\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.11922038793563842  to: 0.11917740106582642\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.11917740106582642  to: 0.11913392543792725\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.11913392543792725  to: 0.11909064054489135\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.11909064054489135  to: 0.11904747486114502\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.11904747486114502  to: 0.11900445222854614\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.11900445222854614  to: 0.11896146535873413\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.11896146535873413  to: 0.1189178466796875\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.1189178466796875  to: 0.11887362003326415\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.11887362003326415  to: 0.11882954835891724\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.11882954835891724  to: 0.11878559589385987\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.11878559589385987  to: 0.11874101161956788\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.11874101161956788  to: 0.11869580745697021\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.11869580745697021  to: 0.11865074634552002\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.11865074634552002  to: 0.11860582828521729\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.11860582828521729  to: 0.118561053276062\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.118561053276062  to: 0.11851564645767212\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.11851564645767212  to: 0.11846959590911865\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.11846959590911865  to: 0.11842368841171265\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.11842368841171265  to: 0.1183779239654541\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1183779239654541  to: 0.11833111047744752\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.11833111047744752  to: 0.11828327178955078\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.11828327178955078  to: 0.11823477745056152\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.11823477745056152  to: 0.11818562746047974\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.11818562746047974  to: 0.11813668012619019\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.11813668012619019  to: 0.11808786392211915\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.11808786392211915  to: 0.11803915500640869\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.11803915500640869  to: 0.11799051761627197\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.11799051761627197  to: 0.11794108152389526\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.11794108152389526  to: 0.11789083480834961\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.11789083480834961  to: 0.11784071922302246\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.11784071922302246  to: 0.11779062747955323\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.11779062747955323  to: 0.11773946285247802\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.11773946285247802  to: 0.1176875352859497\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.1176875352859497  to: 0.11763585805892944\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.11763585805892944  to: 0.117584228515625\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.117584228515625  to: 0.11753271818161011\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.11753271818161011  to: 0.1174803614616394\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.1174803614616394  to: 0.1174271821975708\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.1174271821975708  to: 0.11737325191497802\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.11737325191497802  to: 0.11731961965560914\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.11731961965560914  to: 0.11726629734039307\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.11726629734039307  to: 0.11721327304840087\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.11721327304840087  to: 0.11716057062149048\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.11716057062149048  to: 0.11710717678070068\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.11710717678070068  to: 0.11705443859100342\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.11705443859100342  to: 0.11700092554092408\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.11700092554092408  to: 0.11694782972335815\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.11694782972335815  to: 0.11689502000808716\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.11689502000808716  to: 0.11684248447418213\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.11684248447418213  to: 0.11679034233093262\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.11679034233093262  to: 0.11673734188079835\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.11673734188079835  to: 0.1166835904121399\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.1166835904121399  to: 0.11662909984588624\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.11662909984588624  to: 0.11657502651214599\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.11657502651214599  to: 0.11652076244354248\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.11652076244354248  to: 0.11646686792373658\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.11646686792373658  to: 0.11641194820404052\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.11641194820404052  to: 0.11635618209838867\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.11635618209838867  to: 0.11630091667175294\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.11630091667175294  to: 0.1162459373474121\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.1162459373474121  to: 0.11619117259979247\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.11619117259979247  to: 0.1161353349685669\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.1161353349685669  to: 0.11607848405838013\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.11607848405838013  to: 0.11602073907852173\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.11602073907852173  to: 0.11596342325210571\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.11596342325210571  to: 0.11590652465820313\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.11590652465820313  to: 0.11584999561309814\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.11584999561309814  to: 0.11579393148422241\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.11579393148422241  to: 0.11573688983917237\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.11573688983917237  to: 0.11567887067794799\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.11567887067794799  to: 0.1156199336051941\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1156199336051941  to: 0.11556154489517212\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.11556154489517212  to: 0.11550365686416626\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.11550365686416626  to: 0.11544624567031861\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.11544624567031861  to: 0.11538941860198974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 762\n",
      "Improved validation loss from: 0.11538941860198974  to: 0.11533143520355224\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.11533143520355224  to: 0.11527236700057983\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.11527236700057983  to: 0.11521209478378296\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.11521209478378296  to: 0.11515239477157593\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.11515239477157593  to: 0.11509324312210083\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.11509324312210083  to: 0.11503461599349976\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.11503461599349976  to: 0.11497645378112793\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.11497645378112793  to: 0.11491785049438477\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.11491785049438477  to: 0.11485872268676758\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.11485872268676758  to: 0.11479837894439697\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.11479837894439697  to: 0.11473859548568725\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.11473859548568725  to: 0.11467930078506469\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.11467930078506469  to: 0.11462042331695557\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.11462042331695557  to: 0.1145618200302124\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.1145618200302124  to: 0.11450163125991822\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.11450163125991822  to: 0.1144402265548706\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.1144402265548706  to: 0.11437963247299195\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.11437963247299195  to: 0.11431962251663208\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.11431962251663208  to: 0.11426007747650146\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.11426007747650146  to: 0.11419858932495117\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.11419858932495117  to: 0.11413530111312867\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.11413530111312867  to: 0.11407159566879273\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.11407159566879273  to: 0.11400775909423828\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.11400775909423828  to: 0.11394380331039429\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.11394380331039429  to: 0.1138774037361145\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.1138774037361145  to: 0.11380882263183593\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.11380882263183593  to: 0.1137404203414917\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.1137404203414917  to: 0.11367225646972656\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.11367225646972656  to: 0.11360448598861694\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.11360448598861694  to: 0.1135370373725891\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.1135370373725891  to: 0.11346744298934937\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.11346744298934937  to: 0.11339563131332397\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.11339563131332397  to: 0.11332439184188843\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.11332439184188843  to: 0.11325393915176392\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.11325393915176392  to: 0.11318420171737671\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.11318420171737671  to: 0.11311236619949341\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.11311236619949341  to: 0.11303822994232178\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.11303822994232178  to: 0.11296484470367432\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.11296484470367432  to: 0.11289241313934326\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.11289241313934326  to: 0.11282088756561279\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.11282088756561279  to: 0.11275020837783814\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.11275020837783814  to: 0.11268018484115601\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.11268018484115601  to: 0.11260735988616943\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.11260735988616943  to: 0.11253224611282349\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.11253224611282349  to: 0.11245789527893066\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.11245789527893066  to: 0.11238460540771485\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.11238460540771485  to: 0.11231238842010498\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.11231238842010498  to: 0.11224088668823243\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.11224088668823243  to: 0.11216692924499512\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.11216692924499512  to: 0.11209065914154052\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.11209065914154052  to: 0.11201578378677368\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.11201578378677368  to: 0.11194183826446533\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.11194183826446533  to: 0.11186873912811279\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.11186873912811279  to: 0.11179654598236084\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.11179654598236084  to: 0.11172515153884888\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.11172515153884888  to: 0.11165335178375244\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.11165335178375244  to: 0.1115772008895874\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.1115772008895874  to: 0.11149704456329346\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.11149704456329346  to: 0.11141717433929443\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.11141717433929443  to: 0.11133739948272706\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.11133739948272706  to: 0.11125776767730713\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.11125776767730713  to: 0.11117861270904542\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.11117861270904542  to: 0.11109992265701293\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.11109992265701293  to: 0.11102167367935181\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.11102167367935181  to: 0.11093956232070923\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.11093956232070923  to: 0.11085367202758789\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.11085367202758789  to: 0.11076842546463013\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.11076842546463013  to: 0.11068426370620728\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.11068426370620728  to: 0.11060111522674561\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.11060111522674561  to: 0.11051928997039795\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.11051928997039795  to: 0.11043903827667237\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.11043903827667237  to: 0.11035997867584228\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.11035997867584228  to: 0.11028188467025757\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.11028188467025757  to: 0.11020472049713134\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.11020472049713134  to: 0.11012839078903199\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.11012839078903199  to: 0.11005280017852784\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.11005280017852784  to: 0.10997178554534912\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.10997178554534912  to: 0.10989174842834473\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.10989174842834473  to: 0.10981214046478271\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.10981214046478271  to: 0.10973281860351562\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.10973281860351562  to: 0.10965417623519898\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.10965417623519898  to: 0.1095759391784668\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.1095759391784668  to: 0.10949797630310058\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.10949797630310058  to: 0.10941965579986572\n",
      "Training iteration: 846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10941965579986572  to: 0.10933501720428467\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.10933501720428467  to: 0.10925002098083496\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.10925002098083496  to: 0.10916467905044555\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.10916467905044555  to: 0.10907900333404541\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.10907900333404541  to: 0.10899317264556885\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.10899317264556885  to: 0.1089072585105896\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.1089072585105896  to: 0.10882138013839722\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.10882138013839722  to: 0.1087355375289917\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.1087355375289917  to: 0.10864981412887573\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.10864981412887573  to: 0.10856411457061768\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.10856411457061768  to: 0.10847828388214112\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.10847828388214112  to: 0.10839097499847412\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.10839097499847412  to: 0.10830309391021728\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.10830309391021728  to: 0.10821462869644165\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.10821462869644165  to: 0.10812551975250244\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.10812551975250244  to: 0.10803568363189697\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.10803568363189697  to: 0.10794522762298583\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.10794522762298583  to: 0.10785501003265381\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.10785501003265381  to: 0.10776413679122925\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.10776413679122925  to: 0.1076725959777832\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1076725959777832  to: 0.10758044719696044\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.10758044719696044  to: 0.10748770236968994\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.10748770236968994  to: 0.10739428997039795\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.10739428997039795  to: 0.10730016231536865\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.10730016231536865  to: 0.10720517635345458\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.10720517635345458  to: 0.1071091890335083\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.1071091890335083  to: 0.10701205730438232\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.10701205730438232  to: 0.10691359043121337\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.10691359043121337  to: 0.10681369304656982\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.10681369304656982  to: 0.10671218633651733\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.10671218633651733  to: 0.1066094994544983\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.1066094994544983  to: 0.10650432109832764\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.10650432109832764  to: 0.10639673471450806\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.10639673471450806  to: 0.10628685951232911\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.10628685951232911  to: 0.10617481470108033\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.10617481470108033  to: 0.10606074333190918\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.10606074333190918  to: 0.10594475269317627\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.10594475269317627  to: 0.10582692623138427\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.10582692623138427  to: 0.10570719242095947\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.10570719242095947  to: 0.10558574199676514\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.10558574199676514  to: 0.10546268224716186\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.10546268224716186  to: 0.10533791780471802\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.10533791780471802  to: 0.10521128177642822\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.10521128177642822  to: 0.10508263111114502\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.10508263111114502  to: 0.10495166778564453\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.10495166778564453  to: 0.10481824874877929\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.10481824874877929  to: 0.10468451976776123\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.10468451976776123  to: 0.10454761981964111\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.10454761981964111  to: 0.1044075608253479\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.1044075608253479  to: 0.104264235496521\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.104264235496521  to: 0.10411772727966309\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.10411772727966309  to: 0.10396813154220581\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.10396813154220581  to: 0.10381567478179932\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.10381567478179932  to: 0.1036605954170227\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.1036605954170227  to: 0.10350306034088134\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.10350306034088134  to: 0.10334309339523315\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.10334309339523315  to: 0.10318056344985962\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.10318056344985962  to: 0.10301517248153687\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.10301517248153687  to: 0.10284657478332519\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.10284657478332519  to: 0.10267449617385864\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.10267449617385864  to: 0.10249850749969483\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.10249850749969483  to: 0.10231832265853882\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.10231832265853882  to: 0.10213382244110107\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.10213382244110107  to: 0.10194507837295533\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.10194507837295533  to: 0.10175224542617797\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.10175224542617797  to: 0.10155565738677978\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.10155565738677978  to: 0.10135588645935059\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.10135588645935059  to: 0.10115336179733277\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.10115336179733277  to: 0.1009484887123108\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.1009484887123108  to: 0.10074105262756347\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.10074105262756347  to: 0.10053001642227173\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.10053001642227173  to: 0.10031464099884033\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.10031464099884033  to: 0.10009428262710571\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.10009428262710571  to: 0.09986837506294251\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.09986837506294251  to: 0.09963674545288086\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.09963674545288086  to: 0.09939982295036316\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.09939982295036316  to: 0.09915820956230163\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.09915820956230163  to: 0.09891253709793091\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.09891253709793091  to: 0.09866317510604858\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.09866317510604858  to: 0.09841024279594421\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.09841024279594421  to: 0.09815354347229004\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.09815354347229004  to: 0.09789258241653442\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.09789258241653442  to: 0.09762689471244812\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.09762689471244812  to: 0.09735603332519531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 930\n",
      "Improved validation loss from: 0.09735603332519531  to: 0.09707980155944824\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.09707980155944824  to: 0.09679840207099914\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.09679840207099914  to: 0.09651228189468383\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.09651228189468383  to: 0.0962220311164856\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.0962220311164856  to: 0.09592822790145875\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.09592822790145875  to: 0.09563124775886536\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.09563124775886536  to: 0.09533113241195679\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.09533113241195679  to: 0.0950278401374817\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.0950278401374817  to: 0.09472097158432007\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.09472097158432007  to: 0.09441033601760865\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.09441033601760865  to: 0.09409593343734741\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.09409593343734741  to: 0.09377808570861816\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.09377808570861816  to: 0.09345730543136596\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.09345730543136596  to: 0.09313424229621887\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.09313424229621887  to: 0.09280972480773926\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.09280972480773926  to: 0.09248421788215637\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.09248421788215637  to: 0.0921576201915741\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.0921576201915741  to: 0.09182729721069335\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.09182729721069335  to: 0.09149357676506042\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.09149357676506042  to: 0.09115718007087707\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.09115718007087707  to: 0.09081838726997375\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.09081838726997375  to: 0.09047831296920776\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.09047831296920776  to: 0.09014089703559876\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.09014089703559876  to: 0.08980674743652343\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.08980674743652343  to: 0.08947574496269226\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.08947574496269226  to: 0.08914745450019837\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.08914745450019837  to: 0.08881715536117554\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.08881715536117554  to: 0.08848382830619812\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.08848382830619812  to: 0.08815408945083618\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.08815408945083618  to: 0.08782979846000671\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.08782979846000671  to: 0.0875065803527832\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.0875065803527832  to: 0.08718425631523133\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.08718425631523133  to: 0.08685938119888306\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.08685938119888306  to: 0.08653227090835572\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.08653227090835572  to: 0.08620441555976868\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.08620441555976868  to: 0.08587807416915894\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.08587807416915894  to: 0.08555544018745423\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.08555544018745423  to: 0.08523770570755004\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.08523770570755004  to: 0.0849249541759491\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.0849249541759491  to: 0.08461689949035645\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.08461689949035645  to: 0.08431341052055359\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.08431341052055359  to: 0.08401558995246887\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.08401558995246887  to: 0.08372503519058228\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.08372503519058228  to: 0.08344016075134278\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.08344016075134278  to: 0.08315640687942505\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.08315640687942505  to: 0.08287535905838013\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.08287535905838013  to: 0.08259523510932923\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.08259523510932923  to: 0.08231682777404785\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.08231682777404785  to: 0.0820408046245575\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.0820408046245575  to: 0.08176682591438293\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.08176682591438293  to: 0.08149359822273254\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.08149359822273254  to: 0.08121999502182006\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.08121999502182006  to: 0.08094576001167297\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.08094576001167297  to: 0.08067151308059692\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.08067151308059692  to: 0.08039709329605102\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.08039709329605102  to: 0.08012204170227051\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.08012204170227051  to: 0.07984517812728882\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.07984517812728882  to: 0.07956594228744507\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.07956594228744507  to: 0.07928491234779358\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.07928491234779358  to: 0.07900289297103882\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.07900289297103882  to: 0.07871787548065186\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.07871787548065186  to: 0.0784304141998291\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.0784304141998291  to: 0.07814069390296936\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.07814069390296936  to: 0.07784978747367859\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.07784978747367859  to: 0.07755951881408692\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.07755951881408692  to: 0.07727031111717224\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.07727031111717224  to: 0.07698023915290833\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.07698023915290833  to: 0.07668912410736084\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.07668912410736084  to: 0.07639989852905274\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.07639989852905274  to: 0.07611382603645325\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.07611382603645325  to: 0.07582862973213196\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.07582862973213196  to: 0.07554444670677185\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.07554444670677185  to: 0.07526395916938781\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.07526395916938781  to: 0.07498942613601685\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.07498942613601685  to: 0.07472430467605591\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.07472430467605591  to: 0.07446311712265015\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.07446311712265015  to: 0.07410943508148193\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.07410943508148193  to: 0.07368119955062866\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.07368119955062866  to: 0.07319173812866211\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.07319173812866211  to: 0.07264440059661866\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.07264440059661866  to: 0.07206565141677856\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.07206565141677856  to: 0.07148827314376831\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.07148827314376831  to: 0.0709195613861084\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.0709195613861084  to: 0.07036884427070618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.07036884427070618  to: 0.06985540390014648\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.06985540390014648  to: 0.06937607526779174\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.06937607526779174  to: 0.06892232298851013\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.06892232298851013  to: 0.06850113272666931\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.06850113272666931  to: 0.06811001300811767\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.06811001300811767  to: 0.06773807406425476\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.06773807406425476  to: 0.0673896074295044\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.0673896074295044  to: 0.067070472240448\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.067070472240448  to: 0.0667749285697937\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.0667749285697937  to: 0.0665122389793396\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.0665122389793396  to: 0.06626728773117066\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.06626728773117066  to: 0.06604905128479004\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.06604905128479004  to: 0.06580631732940674\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.06580631732940674  to: 0.06539390087127686\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.06539390087127686  to: 0.06496984958648681\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.06496984958648681  to: 0.0645099937915802\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.0645099937915802  to: 0.06403614282608032\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.06403614282608032  to: 0.06350665092468262\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.06350665092468262  to: 0.06299166083335876\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.06299166083335876  to: 0.062369680404663085\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.062369680404663085  to: 0.06192300915718078\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.06192300915718078  to: 0.061110281944274904\n",
      "Training iteration: 1036\n",
      "Validation loss (no improvement): 0.06115121245384216\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.061110281944274904  to: 0.05991259813308716\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.05991259813308716  to: 0.05953693985939026\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.05953693985939026  to: 0.05933369398117065\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.05933369398117065  to: 0.0582796573638916\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.0582796573638916  to: 0.057922810316085815\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.057922810316085815  to: 0.057800137996673585\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.057800137996673585  to: 0.056828320026397705\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.056828320026397705  to: 0.05642908215522766\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.05642908215522766  to: 0.05632238388061524\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.05632238388061524  to: 0.05537956953048706\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.05537956953048706  to: 0.054947710037231444\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.054947710037231444  to: 0.05483363270759582\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.05483363270759582  to: 0.053965669870376584\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.053965669870376584  to: 0.053644055128097536\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.053644055128097536  to: 0.053504884243011475\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.053504884243011475  to: 0.05281109809875488\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.05281109809875488  to: 0.05265774726867676\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.05265774726867676  to: 0.052294576168060304\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.052294576168060304  to: 0.05179861783981323\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.05179861783981323  to: 0.0517542839050293\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.0517542839050293  to: 0.05105509161949158\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.05105509161949158  to: 0.05077458620071411\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.05077458620071411  to: 0.050409460067749025\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.050409460067749025  to: 0.04991981387138367\n",
      "Training iteration: 1061\n",
      "Validation loss (no improvement): 0.04994791448116302\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.04991981387138367  to: 0.049352416396141054\n",
      "Training iteration: 1063\n",
      "Validation loss (no improvement): 0.04965342581272125\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.049352416396141054  to: 0.0489074558019638\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.0489074558019638  to: 0.04889795184135437\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.04889795184135437  to: 0.04826785624027252\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.04826785624027252  to: 0.04799522757530213\n",
      "Training iteration: 1068\n",
      "Validation loss (no improvement): 0.04808593392372131\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.04799522757530213  to: 0.047708073258399965\n",
      "Training iteration: 1070\n",
      "Validation loss (no improvement): 0.048169559240341185\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.047708073258399965  to: 0.04757246971130371\n",
      "Training iteration: 1072\n",
      "Validation loss (no improvement): 0.047574251890182495\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.04757246971130371  to: 0.04727669656276703\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.04727669656276703  to: 0.04725867807865143\n",
      "Training iteration: 1075\n",
      "Validation loss (no improvement): 0.04768380522727966\n",
      "Training iteration: 1076\n",
      "Validation loss (no improvement): 0.04769291877746582\n",
      "Training iteration: 1077\n",
      "Validation loss (no improvement): 0.04807422161102295\n",
      "Training iteration: 1078\n",
      "Validation loss (no improvement): 0.047905707359313966\n",
      "Training iteration: 1079\n",
      "Validation loss (no improvement): 0.04799643158912659\n",
      "Training iteration: 1080\n",
      "Validation loss (no improvement): 0.047897475957870486\n",
      "Training iteration: 1081\n",
      "Validation loss (no improvement): 0.047705641388893126\n",
      "Training iteration: 1082\n",
      "Validation loss (no improvement): 0.04822467863559723\n",
      "Training iteration: 1083\n",
      "Validation loss (no improvement): 0.04770651757717133\n",
      "Training iteration: 1084\n",
      "Validation loss (no improvement): 0.04788965582847595\n",
      "Training iteration: 1085\n",
      "Validation loss (no improvement): 0.047308439016342164\n",
      "Training iteration: 1086\n",
      "Validation loss (no improvement): 0.04728841781616211\n",
      "Training iteration: 1087\n",
      "Validation loss (no improvement): 0.047697561979293826\n",
      "Training iteration: 1088\n",
      "Validation loss (no improvement): 0.04748639166355133\n",
      "Training iteration: 1089\n",
      "Validation loss (no improvement): 0.04812203049659729\n",
      "Training iteration: 1090\n",
      "Validation loss (no improvement): 0.047273296117782596\n",
      "Training iteration: 1091\n",
      "Validation loss (no improvement): 0.0472933441400528\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.04725867807865143  to: 0.04702159762382507\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.04702159762382507  to: 0.046953043341636656\n",
      "Training iteration: 1094\n",
      "Validation loss (no improvement): 0.04752093255519867\n",
      "Training iteration: 1095\n",
      "Validation loss (no improvement): 0.046975350379943846\n",
      "Training iteration: 1096\n",
      "Validation loss (no improvement): 0.04726042747497559\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.046953043341636656  to: 0.046324700117111206\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.046324700117111206  to: 0.04621818661689758\n",
      "Training iteration: 1099\n",
      "Validation loss (no improvement): 0.046286994218826295\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.04621818661689758  to: 0.04614066183567047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1101\n",
      "Validation loss (no improvement): 0.04697214663028717\n",
      "Training iteration: 1102\n",
      "Validation loss (no improvement): 0.04657178521156311\n",
      "Training iteration: 1103\n",
      "Validation loss (no improvement): 0.04716504216194153\n",
      "Training iteration: 1104\n",
      "Validation loss (no improvement): 0.04675118327140808\n",
      "Training iteration: 1105\n",
      "Validation loss (no improvement): 0.04698503613471985\n",
      "Training iteration: 1106\n",
      "Validation loss (no improvement): 0.0472935289144516\n",
      "Training iteration: 1107\n",
      "Validation loss (no improvement): 0.04742506146430969\n",
      "Training iteration: 1108\n",
      "Validation loss (no improvement): 0.04822854995727539\n",
      "Training iteration: 1109\n",
      "Validation loss (no improvement): 0.048148447275161745\n",
      "Training iteration: 1110\n",
      "Validation loss (no improvement): 0.04886583685874939\n",
      "Training iteration: 1111\n",
      "Validation loss (no improvement): 0.04840091168880463\n",
      "Training iteration: 1112\n",
      "Validation loss (no improvement): 0.048581117391586305\n",
      "Training iteration: 1113\n",
      "Validation loss (no improvement): 0.04825598299503327\n",
      "Training iteration: 1114\n",
      "Validation loss (no improvement): 0.04830112457275391\n",
      "Training iteration: 1115\n",
      "Validation loss (no improvement): 0.048634973168373105\n",
      "Training iteration: 1116\n",
      "Validation loss (no improvement): 0.048657020926475524\n",
      "Training iteration: 1117\n",
      "Validation loss (no improvement): 0.049456873536109926\n",
      "Training iteration: 1118\n",
      "Validation loss (no improvement): 0.049302059412002566\n",
      "Training iteration: 1119\n",
      "Validation loss (no improvement): 0.04924860596656799\n",
      "Training iteration: 1120\n",
      "Validation loss (no improvement): 0.04884101450443268\n",
      "Training iteration: 1121\n",
      "Validation loss (no improvement): 0.04801212251186371\n",
      "Training iteration: 1122\n",
      "Validation loss (no improvement): 0.04740065038204193\n",
      "Training iteration: 1123\n",
      "Validation loss (no improvement): 0.04708693027496338\n",
      "Training iteration: 1124\n",
      "Validation loss (no improvement): 0.047023612260818484\n",
      "Training iteration: 1125\n",
      "Validation loss (no improvement): 0.04714501798152924\n",
      "Training iteration: 1126\n",
      "Validation loss (no improvement): 0.0472243070602417\n",
      "Training iteration: 1127\n",
      "Validation loss (no improvement): 0.04695691168308258\n",
      "Training iteration: 1128\n",
      "Validation loss (no improvement): 0.046571412682533266\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.04614066183567047  to: 0.04579586386680603\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.04579586386680603  to: 0.04546194076538086\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.04546194076538086  to: 0.04532398283481598\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.04532398283481598  to: 0.04526714384555817\n",
      "Training iteration: 1133\n",
      "Validation loss (no improvement): 0.045642775297164914\n",
      "Training iteration: 1134\n",
      "Validation loss (no improvement): 0.04556568264961243\n",
      "Training iteration: 1135\n",
      "Validation loss (no improvement): 0.046396785974502565\n",
      "Training iteration: 1136\n",
      "Validation loss (no improvement): 0.04558389782905579\n",
      "Training iteration: 1137\n",
      "Validation loss (no improvement): 0.045406651496887204\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.04526714384555817  to: 0.04362277090549469\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.04362277090549469  to: 0.04307592511177063\n",
      "Training iteration: 1140\n",
      "Validation loss (no improvement): 0.04361806511878967\n",
      "Training iteration: 1141\n",
      "Validation loss (no improvement): 0.04339139461517334\n",
      "Training iteration: 1142\n",
      "Validation loss (no improvement): 0.044001135230064395\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.04307592511177063  to: 0.042349976301193235\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.042349976301193235  to: 0.04199539721012115\n",
      "Training iteration: 1145\n",
      "Validation loss (no improvement): 0.042090243101119994\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.04199539721012115  to: 0.04197968542575836\n",
      "Training iteration: 1147\n",
      "Validation loss (no improvement): 0.04321812093257904\n",
      "Training iteration: 1148\n",
      "Validation loss (no improvement): 0.04205535054206848\n",
      "Training iteration: 1149\n",
      "Validation loss (no improvement): 0.042687028646469116\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.04197968542575836  to: 0.04132513999938965\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.04132513999938965  to: 0.04128711223602295\n",
      "Training iteration: 1152\n",
      "Validation loss (no improvement): 0.041472554206848145\n",
      "Training iteration: 1153\n",
      "Validation loss (no improvement): 0.0415961354970932\n",
      "Training iteration: 1154\n",
      "Validation loss (no improvement): 0.04310550689697266\n",
      "Training iteration: 1155\n",
      "Validation loss (no improvement): 0.042513704299926756\n",
      "Training iteration: 1156\n",
      "Validation loss (no improvement): 0.04525167047977448\n",
      "Training iteration: 1157\n",
      "Validation loss (no improvement): 0.04300562739372253\n",
      "Training iteration: 1158\n",
      "Validation loss (no improvement): 0.04366262555122376\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.04128711223602295  to: 0.03979852497577667\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.03979852497577667  to: 0.03913246989250183\n",
      "Training iteration: 1161\n",
      "Validation loss (no improvement): 0.040452033281326294\n",
      "Training iteration: 1162\n",
      "Validation loss (no improvement): 0.039613866806030275\n",
      "Training iteration: 1163\n",
      "Validation loss (no improvement): 0.04067777991294861\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.03913246989250183  to: 0.038519614934921266\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.038519614934921266  to: 0.03831799626350403\n",
      "Training iteration: 1166\n",
      "Validation loss (no improvement): 0.03990616500377655\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.03831799626350403  to: 0.03807614743709564\n",
      "Training iteration: 1168\n",
      "Validation loss (no improvement): 0.03809603750705719\n",
      "Training iteration: 1169\n",
      "Validation loss (no improvement): 0.038133269548416136\n",
      "Training iteration: 1170\n",
      "Validation loss (no improvement): 0.038386395573616026\n",
      "Training iteration: 1171\n",
      "Validation loss (no improvement): 0.04033864438533783\n",
      "Training iteration: 1172\n",
      "Validation loss (no improvement): 0.03868672549724579\n",
      "Training iteration: 1173\n",
      "Validation loss (no improvement): 0.03966917395591736\n",
      "Training iteration: 1174\n",
      "Validation loss (no improvement): 0.03937073349952698\n",
      "Training iteration: 1175\n",
      "Validation loss (no improvement): 0.039621958136558534\n",
      "Training iteration: 1176\n",
      "Validation loss (no improvement): 0.041043490171432495\n",
      "Training iteration: 1177\n",
      "Validation loss (no improvement): 0.03972010016441345\n",
      "Training iteration: 1178\n",
      "Validation loss (no improvement): 0.04322969019412994\n",
      "Training iteration: 1179\n",
      "Validation loss (no improvement): 0.04111528992652893\n",
      "Training iteration: 1180\n",
      "Validation loss (no improvement): 0.04600189328193664\n",
      "Training iteration: 1181\n",
      "Validation loss (no improvement): 0.04015775322914124\n",
      "Training iteration: 1182\n",
      "Validation loss (no improvement): 0.040028294920921324\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.03807614743709564  to: 0.03773910403251648\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.03773910403251648  to: 0.037064206600189206\n",
      "Training iteration: 1185\n",
      "Validation loss (no improvement): 0.03993389904499054\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.037064206600189206  to: 0.036452823877334596\n",
      "Training iteration: 1187\n",
      "Validation loss (no improvement): 0.03654553294181824\n",
      "Training iteration: 1188\n",
      "Validation loss (no improvement): 0.03893195986747742\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.036452823877334596  to: 0.03611369132995605\n",
      "Training iteration: 1190\n",
      "Validation loss (no improvement): 0.03685269951820373\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.037476474046707155\n",
      "Training iteration: 1192\n",
      "Validation loss (no improvement): 0.03662392199039459\n",
      "Training iteration: 1193\n",
      "Validation loss (no improvement): 0.03869466483592987\n",
      "Training iteration: 1194\n",
      "Validation loss (no improvement): 0.036638832092285155\n",
      "Training iteration: 1195\n",
      "Validation loss (no improvement): 0.036862355470657346\n",
      "Training iteration: 1196\n",
      "Validation loss (no improvement): 0.03934503197669983\n",
      "Training iteration: 1197\n",
      "Validation loss (no improvement): 0.03809825778007507\n",
      "Training iteration: 1198\n",
      "Validation loss (no improvement): 0.04070072174072266\n",
      "Training iteration: 1199\n",
      "Validation loss (no improvement): 0.038638702034950255\n",
      "Training iteration: 1200\n",
      "Validation loss (no improvement): 0.039966431260108945\n",
      "Training iteration: 1201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.040244069695472715\n",
      "Training iteration: 1202\n",
      "Validation loss (no improvement): 0.04065651297569275\n",
      "Training iteration: 1203\n",
      "Validation loss (no improvement): 0.04308958649635315\n",
      "Training iteration: 1204\n",
      "Validation loss (no improvement): 0.04161334037780762\n",
      "Training iteration: 1205\n",
      "Validation loss (no improvement): 0.04866265654563904\n",
      "Training iteration: 1206\n",
      "Validation loss (no improvement): 0.046073856949806216\n",
      "Training iteration: 1207\n",
      "Validation loss (no improvement): 0.06400675177574158\n",
      "Training iteration: 1208\n",
      "Validation loss (no improvement): 0.042494001984596255\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.03611369132995605  to: 0.034904268383979795\n",
      "Training iteration: 1210\n",
      "Validation loss (no improvement): 0.04413747787475586\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.034904268383979795  to: 0.03266812860965729\n",
      "Training iteration: 1212\n",
      "Validation loss (no improvement): 0.03527866005897522\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.03266812860965729  to: 0.02974662184715271\n",
      "Training iteration: 1214\n",
      "Validation loss (no improvement): 0.03642716109752655\n",
      "Training iteration: 1215\n",
      "Validation loss (no improvement): 0.032196801900863645\n",
      "Training iteration: 1216\n",
      "Validation loss (no improvement): 0.0322040319442749\n",
      "Training iteration: 1217\n",
      "Validation loss (no improvement): 0.030791884660720824\n",
      "Training iteration: 1218\n",
      "Validation loss (no improvement): 0.030213579535484314\n",
      "Training iteration: 1219\n",
      "Validation loss (no improvement): 0.03253253400325775\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.02974662184715271  to: 0.029715505242347718\n",
      "Training iteration: 1221\n",
      "Validation loss (no improvement): 0.0310078501701355\n",
      "Training iteration: 1222\n",
      "Validation loss (no improvement): 0.030116000771522523\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.029715505242347718  to: 0.02954474985599518\n",
      "Training iteration: 1224\n",
      "Validation loss (no improvement): 0.030198970437049867\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.02954474985599518  to: 0.028559932112693788\n",
      "Training iteration: 1226\n",
      "Validation loss (no improvement): 0.030290931463241577\n",
      "Training iteration: 1227\n",
      "Validation loss (no improvement): 0.02962994873523712\n",
      "Training iteration: 1228\n",
      "Validation loss (no improvement): 0.030042997002601622\n",
      "Training iteration: 1229\n",
      "Validation loss (no improvement): 0.03026063144207001\n",
      "Training iteration: 1230\n",
      "Validation loss (no improvement): 0.03048798143863678\n",
      "Training iteration: 1231\n",
      "Validation loss (no improvement): 0.03203122019767761\n",
      "Training iteration: 1232\n",
      "Validation loss (no improvement): 0.0334542840719223\n",
      "Training iteration: 1233\n",
      "Validation loss (no improvement): 0.03409599959850311\n",
      "Training iteration: 1234\n",
      "Validation loss (no improvement): 0.03333587348461151\n",
      "Training iteration: 1235\n",
      "Validation loss (no improvement): 0.034499934315681456\n",
      "Training iteration: 1236\n",
      "Validation loss (no improvement): 0.0372850775718689\n",
      "Training iteration: 1237\n",
      "Validation loss (no improvement): 0.03737970888614654\n",
      "Training iteration: 1238\n",
      "Validation loss (no improvement): 0.0377153605222702\n",
      "Training iteration: 1239\n",
      "Validation loss (no improvement): 0.039648258686065675\n",
      "Training iteration: 1240\n",
      "Validation loss (no improvement): 0.04072257578372955\n",
      "Training iteration: 1241\n",
      "Validation loss (no improvement): 0.04131776690483093\n",
      "Training iteration: 1242\n",
      "Validation loss (no improvement): 0.04384517073631287\n",
      "Training iteration: 1243\n",
      "Validation loss (no improvement): 0.0441422164440155\n",
      "Training iteration: 1244\n",
      "Validation loss (no improvement): 0.04488663673400879\n",
      "Training iteration: 1245\n",
      "Validation loss (no improvement): 0.04825571477413178\n",
      "Training iteration: 1246\n",
      "Validation loss (no improvement): 0.046628648042678834\n",
      "Training iteration: 1247\n",
      "Validation loss (no improvement): 0.04876239895820618\n",
      "Training iteration: 1248\n",
      "Validation loss (no improvement): 0.04809271395206451\n",
      "Training iteration: 1249\n",
      "Validation loss (no improvement): 0.04848601818084717\n",
      "Training iteration: 1250\n",
      "Validation loss (no improvement): 0.04925173223018646\n",
      "Training iteration: 1251\n",
      "Validation loss (no improvement): 0.04761249125003815\n",
      "Training iteration: 1252\n",
      "Validation loss (no improvement): 0.051117938756942746\n",
      "Training iteration: 1253\n",
      "Validation loss (no improvement): 0.04804260730743408\n",
      "Training iteration: 1254\n",
      "Validation loss (no improvement): 0.05449228286743164\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.04732963442802429\n",
      "Training iteration: 1256\n",
      "Validation loss (no improvement): 0.05645779967308044\n",
      "Training iteration: 1257\n",
      "Validation loss (no improvement): 0.044775190949440005\n",
      "Training iteration: 1258\n",
      "Validation loss (no improvement): 0.04318336546421051\n",
      "Training iteration: 1259\n",
      "Validation loss (no improvement): 0.04193675518035889\n",
      "Training iteration: 1260\n",
      "Validation loss (no improvement): 0.03788376748561859\n",
      "Training iteration: 1261\n",
      "Validation loss (no improvement): 0.03731802999973297\n",
      "Training iteration: 1262\n",
      "Validation loss (no improvement): 0.03821015954017639\n",
      "Training iteration: 1263\n",
      "Validation loss (no improvement): 0.03696339726448059\n",
      "Training iteration: 1264\n",
      "Validation loss (no improvement): 0.03392710089683533\n",
      "Training iteration: 1265\n",
      "Validation loss (no improvement): 0.03460804224014282\n",
      "Training iteration: 1266\n",
      "Validation loss (no improvement): 0.03335363268852234\n",
      "Training iteration: 1267\n",
      "Validation loss (no improvement): 0.03443382382392883\n",
      "Training iteration: 1268\n",
      "Validation loss (no improvement): 0.03632849454879761\n",
      "Training iteration: 1269\n",
      "Validation loss (no improvement): 0.03441074788570404\n",
      "Training iteration: 1270\n",
      "Validation loss (no improvement): 0.034598201513290405\n",
      "Training iteration: 1271\n",
      "Validation loss (no improvement): 0.03652970194816589\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.03643031716346741\n",
      "Training iteration: 1273\n",
      "Validation loss (no improvement): 0.03665869235992432\n",
      "Training iteration: 1274\n",
      "Validation loss (no improvement): 0.038169264793395996\n",
      "Training iteration: 1275\n",
      "Validation loss (no improvement): 0.038366001844406125\n",
      "Training iteration: 1276\n",
      "Validation loss (no improvement): 0.039386361837387085\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.04143060147762299\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.04017798900604248\n",
      "Training iteration: 1279\n",
      "Validation loss (no improvement): 0.041348233819007874\n",
      "Training iteration: 1280\n",
      "Validation loss (no improvement): 0.043621912598609924\n",
      "Training iteration: 1281\n",
      "Validation loss (no improvement): 0.04378607273101807\n",
      "Training iteration: 1282\n",
      "Validation loss (no improvement): 0.04671974182128906\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.046139365434646605\n",
      "Training iteration: 1284\n",
      "Validation loss (no improvement): 0.052849984169006346\n",
      "Training iteration: 1285\n",
      "Validation loss (no improvement): 0.05163646936416626\n",
      "Training iteration: 1286\n",
      "Validation loss (no improvement): 0.06251772046089173\n",
      "Training iteration: 1287\n",
      "Validation loss (no improvement): 0.050244152545928955\n",
      "Training iteration: 1288\n",
      "Validation loss (no improvement): 0.040204310417175294\n",
      "Training iteration: 1289\n",
      "Validation loss (no improvement): 0.042338410019874574\n",
      "Training iteration: 1290\n",
      "Validation loss (no improvement): 0.03807397484779358\n",
      "Training iteration: 1291\n",
      "Validation loss (no improvement): 0.03224773705005646\n",
      "Training iteration: 1292\n",
      "Validation loss (no improvement): 0.03727083802223206\n",
      "Training iteration: 1293\n",
      "Validation loss (no improvement): 0.03228972852230072\n",
      "Training iteration: 1294\n",
      "Validation loss (no improvement): 0.034009867906570436\n",
      "Training iteration: 1295\n",
      "Validation loss (no improvement): 0.029722970724105836\n",
      "Training iteration: 1296\n",
      "Validation loss (no improvement): 0.030813631415367127\n",
      "Training iteration: 1297\n",
      "Validation loss (no improvement): 0.029325655102729796\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.029388397932052612\n",
      "Training iteration: 1299\n",
      "Validation loss (no improvement): 0.030165201425552367\n",
      "Training iteration: 1300\n",
      "Validation loss (no improvement): 0.028970354795455934\n",
      "Training iteration: 1301\n",
      "Validation loss (no improvement): 0.02993571162223816\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.028559932112693788  to: 0.027938568592071535\n",
      "Training iteration: 1303\n",
      "Validation loss (no improvement): 0.029396387934684753\n",
      "Training iteration: 1304\n",
      "Validation loss (no improvement): 0.028700557351112366\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.029896289110183716\n",
      "Training iteration: 1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02936539053916931\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.030337366461753845\n",
      "Training iteration: 1308\n",
      "Validation loss (no improvement): 0.030722910165786745\n",
      "Training iteration: 1309\n",
      "Validation loss (no improvement): 0.03260808289051056\n",
      "Training iteration: 1310\n",
      "Validation loss (no improvement): 0.03250614106655121\n",
      "Training iteration: 1311\n",
      "Validation loss (no improvement): 0.03321300148963928\n",
      "Training iteration: 1312\n",
      "Validation loss (no improvement): 0.03355070650577545\n",
      "Training iteration: 1313\n",
      "Validation loss (no improvement): 0.035708260536193845\n",
      "Training iteration: 1314\n",
      "Validation loss (no improvement): 0.03634429275989533\n",
      "Training iteration: 1315\n",
      "Validation loss (no improvement): 0.0380169540643692\n",
      "Training iteration: 1316\n",
      "Validation loss (no improvement): 0.04010899662971497\n",
      "Training iteration: 1317\n",
      "Validation loss (no improvement): 0.03996514678001404\n",
      "Training iteration: 1318\n",
      "Validation loss (no improvement): 0.0414656400680542\n",
      "Training iteration: 1319\n",
      "Validation loss (no improvement): 0.04449943900108337\n",
      "Training iteration: 1320\n",
      "Validation loss (no improvement): 0.0444868266582489\n",
      "Training iteration: 1321\n",
      "Validation loss (no improvement): 0.045626839995384215\n",
      "Training iteration: 1322\n",
      "Validation loss (no improvement): 0.047721576690673825\n",
      "Training iteration: 1323\n",
      "Validation loss (no improvement): 0.04778290688991547\n",
      "Training iteration: 1324\n",
      "Validation loss (no improvement): 0.05101551413536072\n",
      "Training iteration: 1325\n",
      "Validation loss (no improvement): 0.04979623258113861\n",
      "Training iteration: 1326\n",
      "Validation loss (no improvement): 0.05278683304786682\n",
      "Training iteration: 1327\n",
      "Validation loss (no improvement): 0.05096877813339233\n",
      "Training iteration: 1328\n",
      "Validation loss (no improvement): 0.05395573377609253\n",
      "Training iteration: 1329\n",
      "Validation loss (no improvement): 0.05124865770339966\n",
      "Training iteration: 1330\n",
      "Validation loss (no improvement): 0.05845412015914917\n",
      "Training iteration: 1331\n",
      "Validation loss (no improvement): 0.05417564511299133\n",
      "Training iteration: 1332\n",
      "Validation loss (no improvement): 0.066995507478714\n",
      "Training iteration: 1333\n",
      "Validation loss (no improvement): 0.052258861064910886\n",
      "Training iteration: 1334\n",
      "Validation loss (no improvement): 0.04649887681007385\n",
      "Training iteration: 1335\n",
      "Validation loss (no improvement): 0.04270920157432556\n",
      "Training iteration: 1336\n",
      "Validation loss (no improvement): 0.0404166042804718\n",
      "Training iteration: 1337\n",
      "Validation loss (no improvement): 0.034328114986419675\n",
      "Training iteration: 1338\n",
      "Validation loss (no improvement): 0.03902064263820648\n",
      "Training iteration: 1339\n",
      "Validation loss (no improvement): 0.033402228355407716\n",
      "Training iteration: 1340\n",
      "Validation loss (no improvement): 0.03465743660926819\n",
      "Training iteration: 1341\n",
      "Validation loss (no improvement): 0.030657297372817992\n",
      "Training iteration: 1342\n",
      "Validation loss (no improvement): 0.03255845010280609\n",
      "Training iteration: 1343\n",
      "Validation loss (no improvement): 0.029715701937675476\n",
      "Training iteration: 1344\n",
      "Validation loss (no improvement): 0.03185674846172333\n",
      "Training iteration: 1345\n",
      "Validation loss (no improvement): 0.030423462390899658\n",
      "Training iteration: 1346\n",
      "Validation loss (no improvement): 0.03170250654220581\n",
      "Training iteration: 1347\n",
      "Validation loss (no improvement): 0.029843097925186156\n",
      "Training iteration: 1348\n",
      "Validation loss (no improvement): 0.029999804496765137\n",
      "Training iteration: 1349\n",
      "Validation loss (no improvement): 0.029860132932662965\n",
      "Training iteration: 1350\n",
      "Validation loss (no improvement): 0.03076724112033844\n",
      "Training iteration: 1351\n",
      "Validation loss (no improvement): 0.031574538350105284\n",
      "Training iteration: 1352\n",
      "Validation loss (no improvement): 0.03190544247627258\n",
      "Training iteration: 1353\n",
      "Validation loss (no improvement): 0.03260546326637268\n",
      "Training iteration: 1354\n",
      "Validation loss (no improvement): 0.03356603980064392\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.03428228497505188\n",
      "Training iteration: 1356\n",
      "Validation loss (no improvement): 0.03436139225959778\n",
      "Training iteration: 1357\n",
      "Validation loss (no improvement): 0.034892800450325015\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.036384057998657224\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.037077537178993224\n",
      "Training iteration: 1360\n",
      "Validation loss (no improvement): 0.038755562901496884\n",
      "Training iteration: 1361\n",
      "Validation loss (no improvement): 0.04025647640228271\n",
      "Training iteration: 1362\n",
      "Validation loss (no improvement): 0.04146849513053894\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.04171114861965179\n",
      "Training iteration: 1364\n",
      "Validation loss (no improvement): 0.043286317586898805\n",
      "Training iteration: 1365\n",
      "Validation loss (no improvement): 0.04533375799655914\n",
      "Training iteration: 1366\n",
      "Validation loss (no improvement): 0.045753097534179686\n",
      "Training iteration: 1367\n",
      "Validation loss (no improvement): 0.04753717482089996\n",
      "Training iteration: 1368\n",
      "Validation loss (no improvement): 0.04859520792961121\n",
      "Training iteration: 1369\n",
      "Validation loss (no improvement): 0.0495344340801239\n",
      "Training iteration: 1370\n",
      "Validation loss (no improvement): 0.052109891176223756\n",
      "Training iteration: 1371\n",
      "Validation loss (no improvement): 0.051032745838165285\n",
      "Training iteration: 1372\n",
      "Validation loss (no improvement): 0.05329183340072632\n",
      "Training iteration: 1373\n",
      "Validation loss (no improvement): 0.05245527029037476\n",
      "Training iteration: 1374\n",
      "Validation loss (no improvement): 0.05399643182754517\n",
      "Training iteration: 1375\n",
      "Validation loss (no improvement): 0.053501570224761964\n",
      "Training iteration: 1376\n",
      "Validation loss (no improvement): 0.05507144331932068\n",
      "Training iteration: 1377\n",
      "Validation loss (no improvement): 0.05464175343513489\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.05566790103912354\n",
      "Training iteration: 1379\n",
      "Validation loss (no improvement): 0.05392927527427673\n",
      "Training iteration: 1380\n",
      "Validation loss (no improvement): 0.05855449438095093\n",
      "Training iteration: 1381\n",
      "Validation loss (no improvement): 0.05655469298362732\n",
      "Training iteration: 1382\n",
      "Validation loss (no improvement): 0.07749489545822144\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.0783752977848053\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.07503007054328918\n",
      "Training iteration: 1385\n",
      "Validation loss (no improvement): 0.03854417502880096\n",
      "Training iteration: 1386\n",
      "Validation loss (no improvement): 0.050200462341308594\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.027938568592071535  to: 0.026228898763656618\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.03684956431388855\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.031118828058242797\n",
      "Training iteration: 1390\n",
      "Validation loss (no improvement): 0.030493128299713134\n",
      "Training iteration: 1391\n",
      "Validation loss (no improvement): 0.03789306879043579\n",
      "Training iteration: 1392\n",
      "Validation loss (no improvement): 0.02748013734817505\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.026228898763656618  to: 0.02349763661623001\n",
      "Training iteration: 1394\n",
      "Validation loss (no improvement): 0.029237717390060425\n",
      "Training iteration: 1395\n",
      "Validation loss (no improvement): 0.02938517928123474\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.02886176109313965\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.03425679802894592\n",
      "Training iteration: 1398\n",
      "Validation loss (no improvement): 0.03630091547966004\n",
      "Training iteration: 1399\n",
      "Validation loss (no improvement): 0.030360689759254454\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.025101447105407716\n",
      "Training iteration: 1401\n",
      "Validation loss (no improvement): 0.025409331917762755\n",
      "Training iteration: 1402\n",
      "Validation loss (no improvement): 0.026856955885887147\n",
      "Training iteration: 1403\n",
      "Validation loss (no improvement): 0.026774057745933534\n",
      "Training iteration: 1404\n",
      "Validation loss (no improvement): 0.0288431316614151\n",
      "Training iteration: 1405\n",
      "Validation loss (no improvement): 0.033477574586868286\n",
      "Training iteration: 1406\n",
      "Validation loss (no improvement): 0.03409084379673004\n",
      "Training iteration: 1407\n",
      "Validation loss (no improvement): 0.02866702675819397\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.02349763661623001  to: 0.023475411534309387\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.023475411534309387  to: 0.022474999725818633\n",
      "Training iteration: 1410\n",
      "Validation loss (no improvement): 0.022951474785804747\n",
      "Training iteration: 1411\n",
      "Validation loss (no improvement): 0.023628215491771697\n",
      "Training iteration: 1412\n",
      "Validation loss (no improvement): 0.027289119362831116\n",
      "Training iteration: 1413\n",
      "Validation loss (no improvement): 0.03146721422672272\n",
      "Training iteration: 1414\n",
      "Validation loss (no improvement): 0.02938319444656372\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.023409581184387206\n",
      "Training iteration: 1416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.022474999725818633  to: 0.020797333121299742\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.02104179412126541\n",
      "Training iteration: 1418\n",
      "Validation loss (no improvement): 0.02233881950378418\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.027107936143875123\n",
      "Training iteration: 1420\n",
      "Validation loss (no improvement): 0.03103216290473938\n",
      "Training iteration: 1421\n",
      "Validation loss (no improvement): 0.02755293548107147\n",
      "Training iteration: 1422\n",
      "Validation loss (no improvement): 0.022698214650154112\n",
      "Training iteration: 1423\n",
      "Validation loss (no improvement): 0.02187703102827072\n",
      "Training iteration: 1424\n",
      "Validation loss (no improvement): 0.02321907728910446\n",
      "Training iteration: 1425\n",
      "Validation loss (no improvement): 0.027886968851089478\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.032212170958518985\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.02929103672504425\n",
      "Training iteration: 1428\n",
      "Validation loss (no improvement): 0.025073158740997314\n",
      "Training iteration: 1429\n",
      "Validation loss (no improvement): 0.02430327832698822\n",
      "Training iteration: 1430\n",
      "Validation loss (no improvement): 0.026536884903907775\n",
      "Training iteration: 1431\n",
      "Validation loss (no improvement): 0.03163284659385681\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.033103948831558226\n",
      "Training iteration: 1433\n",
      "Validation loss (no improvement): 0.029875856637954713\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.027533885836601258\n",
      "Training iteration: 1435\n",
      "Validation loss (no improvement): 0.02819233536720276\n",
      "Training iteration: 1436\n",
      "Validation loss (no improvement): 0.031876140832901\n",
      "Training iteration: 1437\n",
      "Validation loss (no improvement): 0.03481277525424957\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.03338086307048797\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.030572032928466795\n",
      "Training iteration: 1440\n",
      "Validation loss (no improvement): 0.030387380719184877\n",
      "Training iteration: 1441\n",
      "Validation loss (no improvement): 0.0331078827381134\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.036000579595565796\n",
      "Training iteration: 1443\n",
      "Validation loss (no improvement): 0.03559132218360901\n",
      "Training iteration: 1444\n",
      "Validation loss (no improvement): 0.03347095847129822\n",
      "Training iteration: 1445\n",
      "Validation loss (no improvement): 0.033104196190834045\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.03465721011161804\n",
      "Training iteration: 1447\n",
      "Validation loss (no improvement): 0.036887872219085696\n",
      "Training iteration: 1448\n",
      "Validation loss (no improvement): 0.03676798939704895\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.03485741019248963\n",
      "Training iteration: 1450\n",
      "Validation loss (no improvement): 0.0342001736164093\n",
      "Training iteration: 1451\n",
      "Validation loss (no improvement): 0.03582229018211365\n",
      "Training iteration: 1452\n",
      "Validation loss (no improvement): 0.037458425760269164\n",
      "Training iteration: 1453\n",
      "Validation loss (no improvement): 0.03695080280303955\n",
      "Training iteration: 1454\n",
      "Validation loss (no improvement): 0.035562795400619504\n",
      "Training iteration: 1455\n",
      "Validation loss (no improvement): 0.03518462777137756\n",
      "Training iteration: 1456\n",
      "Validation loss (no improvement): 0.03609398007392883\n",
      "Training iteration: 1457\n",
      "Validation loss (no improvement): 0.03731574714183807\n",
      "Training iteration: 1458\n",
      "Validation loss (no improvement): 0.03691349923610687\n",
      "Training iteration: 1459\n",
      "Validation loss (no improvement): 0.03565927445888519\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.035680252313613894\n",
      "Training iteration: 1461\n",
      "Validation loss (no improvement): 0.03670229315757752\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.037261849641799925\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.03676963448524475\n",
      "Training iteration: 1464\n",
      "Validation loss (no improvement): 0.03600330948829651\n",
      "Training iteration: 1465\n",
      "Validation loss (no improvement): 0.03612134456634521\n",
      "Training iteration: 1466\n",
      "Validation loss (no improvement): 0.0371321976184845\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.03744269907474518\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.036834496259689334\n",
      "Training iteration: 1469\n",
      "Validation loss (no improvement): 0.036540544033050536\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.03696209490299225\n",
      "Training iteration: 1471\n",
      "Validation loss (no improvement): 0.03774089515209198\n",
      "Training iteration: 1472\n",
      "Validation loss (no improvement): 0.03790070414543152\n",
      "Training iteration: 1473\n",
      "Validation loss (no improvement): 0.03758423626422882\n",
      "Training iteration: 1474\n",
      "Validation loss (no improvement): 0.03779641985893249\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.03849492371082306\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.03910098373889923\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.039198070764541626\n",
      "Training iteration: 1478\n",
      "Validation loss (no improvement): 0.03915113806724548\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.03969140350818634\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.04039377272129059\n",
      "Training iteration: 1481\n",
      "Validation loss (no improvement): 0.04076852798461914\n",
      "Training iteration: 1482\n",
      "Validation loss (no improvement): 0.04084542393684387\n",
      "Training iteration: 1483\n",
      "Validation loss (no improvement): 0.04112668931484222\n",
      "Training iteration: 1484\n",
      "Validation loss (no improvement): 0.04184584021568298\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.04235933423042297\n",
      "Training iteration: 1486\n",
      "Validation loss (no improvement): 0.04263642430305481\n",
      "Training iteration: 1487\n",
      "Validation loss (no improvement): 0.04281899034976959\n",
      "Training iteration: 1488\n",
      "Validation loss (no improvement): 0.04351955056190491\n",
      "Training iteration: 1489\n",
      "Validation loss (no improvement): 0.04402787685394287\n",
      "Training iteration: 1490\n",
      "Validation loss (no improvement): 0.04460458755493164\n",
      "Training iteration: 1491\n",
      "Validation loss (no improvement): 0.044597354531288144\n",
      "Training iteration: 1492\n",
      "Validation loss (no improvement): 0.04591276049613953\n",
      "Training iteration: 1493\n",
      "Validation loss (no improvement): 0.04621929228305817\n",
      "Training iteration: 1494\n",
      "Validation loss (no improvement): 0.05066443085670471\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.05493184328079224\n",
      "Training iteration: 1496\n",
      "Validation loss (no improvement): 0.08185213208198547\n",
      "Training iteration: 1497\n",
      "Validation loss (no improvement): 0.08390771746635436\n",
      "Training iteration: 1498\n",
      "Validation loss (no improvement): 0.05204170346260071\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.05458393096923828\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.043350082635879514\n",
      "Training iteration: 1501\n",
      "Validation loss (no improvement): 0.04095789492130279\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.02854011654853821\n",
      "Training iteration: 1503\n",
      "Validation loss (no improvement): 0.04505133628845215\n",
      "Training iteration: 1504\n",
      "Validation loss (no improvement): 0.03352619707584381\n",
      "Training iteration: 1505\n",
      "Validation loss (no improvement): 0.03312230110168457\n",
      "Training iteration: 1506\n",
      "Validation loss (no improvement): 0.041011518239974974\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.03280834555625915\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.02714292109012604\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.03228268325328827\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.034225243330001834\n",
      "Training iteration: 1511\n",
      "Validation loss (no improvement): 0.030726894736289978\n",
      "Training iteration: 1512\n",
      "Validation loss (no improvement): 0.03188373446464539\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.03791389763355255\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.03896116614341736\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.033171352744102475\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.028558367490768434\n"
     ]
    }
   ],
   "source": [
    "mixture_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 9.866224670410157\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 9.866224670410157  to: 7.325105285644531\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 7.325105285644531  to: 5.5609077453613285\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 5.5609077453613285  to: 4.275096893310547\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 4.275096893310547  to: 3.328715515136719\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 3.328715515136719  to: 2.6254886627197265\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 2.6254886627197265  to: 2.1019466400146483\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 2.1019466400146483  to: 1.7078779220581055\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.7078779220581055  to: 1.4054283142089843\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.4054283142089843  to: 1.1725131034851075\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.1725131034851075  to: 0.9902783393859863\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.9902783393859863  to: 0.8456031799316406\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.8456031799316406  to: 0.7299112796783447\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.7299112796783447  to: 0.6369889259338379\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.6369889259338379  to: 0.5619557857513428\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.5619557857513428  to: 0.500833797454834\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.500833797454834  to: 0.4505885124206543\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.4505885124206543  to: 0.40904664993286133\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.40904664993286133  to: 0.37452712059021\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.37452712059021  to: 0.3456780672073364\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.3456780672073364  to: 0.32144343852996826\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.32144343852996826  to: 0.30097184181213377\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.30097184181213377  to: 0.2835961103439331\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.2835961103439331  to: 0.26877949237823484\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.26877949237823484  to: 0.2560814619064331\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.2560814619064331  to: 0.24515671730041505\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.24515671730041505  to: 0.23571884632110596\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.23571884632110596  to: 0.22752938270568848\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.22752938270568848  to: 0.22038803100585938\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.22038803100585938  to: 0.2141430377960205\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.2141430377960205  to: 0.20866119861602783\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.20866119861602783  to: 0.20383148193359374\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.20383148193359374  to: 0.19956115484237671\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.19956115484237671  to: 0.19577213525772094\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.19577213525772094  to: 0.19239864349365235\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.19239864349365235  to: 0.189385724067688\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.189385724067688  to: 0.1866821050643921\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.1866821050643921  to: 0.18424965143203736\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.18424965143203736  to: 0.18205543756484985\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.18205543756484985  to: 0.18007005453109742\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.18007005453109742  to: 0.1782682180404663\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.1782682180404663  to: 0.17662811279296875\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.17662811279296875  to: 0.1751307725906372\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1751307725906372  to: 0.1737598180770874\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.1737598180770874  to: 0.1725010633468628\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.1725010633468628  to: 0.17134206295013427\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.17134206295013427  to: 0.17027202844619752\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.17027202844619752  to: 0.1692790985107422\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.1692790985107422  to: 0.16835591793060303\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.16835591793060303  to: 0.16749658584594726\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.16749658584594726  to: 0.16669464111328125\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.16669464111328125  to: 0.16594431400299073\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.16594431400299073  to: 0.16524059772491456\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.16524059772491456  to: 0.1645790457725525\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.1645790457725525  to: 0.16395580768585205\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.16395580768585205  to: 0.1633671998977661\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.1633671998977661  to: 0.1628101348876953\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1628101348876953  to: 0.1622818350791931\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.1622818350791931  to: 0.16177978515625\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.16177978515625  to: 0.16130170822143555\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.16130170822143555  to: 0.1608456254005432\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.1608456254005432  to: 0.16040971279144287\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.16040971279144287  to: 0.15999233722686768\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.15999233722686768  to: 0.15959203243255615\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.15959203243255615  to: 0.15920736789703369\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.15920736789703369  to: 0.15883718729019164\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.15883718729019164  to: 0.15848041772842408\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.15848041772842408  to: 0.15813612937927246\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.15813612937927246  to: 0.15780346393585204\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.15780346393585204  to: 0.1574816346168518\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.1574816346168518  to: 0.15716993808746338\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.15716993808746338  to: 0.15686771869659424\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.15686771869659424  to: 0.15657436847686768\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.15657436847686768  to: 0.15628936290740966\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.15628936290740966  to: 0.156012225151062\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.156012225151062  to: 0.15574252605438232\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.15574252605438232  to: 0.15547986030578614\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.15547986030578614  to: 0.15522382259368897\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.15522382259368897  to: 0.15497407913208008\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.15497407913208008  to: 0.15473031997680664\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.15473031997680664  to: 0.15449225902557373\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.15449225902557373  to: 0.15425964593887329\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.15425964593887329  to: 0.1540321946144104\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1540321946144104  to: 0.15380966663360596\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.15380966663360596  to: 0.1535918593406677\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.1535918593406677  to: 0.15337857007980346\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.15337857007980346  to: 0.1531696319580078\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1531696319580078  to: 0.15296484231948854\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.15296484231948854  to: 0.15276404619216918\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.15276404619216918  to: 0.1525670886039734\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1525670886039734  to: 0.15237385034561157\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.15237385034561157  to: 0.1521841764450073\n",
      "Training iteration: 92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1521841764450073  to: 0.15199795961380005\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.15199795961380005  to: 0.15181505680084229\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15181505680084229  to: 0.1516353726387024\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.1516353726387024  to: 0.15145705938339232\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15145705938339232  to: 0.15128161907196044\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.15128161907196044  to: 0.15110907554626465\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.15110907554626465  to: 0.15093936920166015\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.15093936920166015  to: 0.15077240467071534\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.15077240467071534  to: 0.1506080746650696\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.1506080746650696  to: 0.15044634342193602\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.15044634342193602  to: 0.15028712749481202\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.15028712749481202  to: 0.15013030767440796\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.15013030767440796  to: 0.14997587203979493\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14997587203979493  to: 0.14982376098632813\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14982376098632813  to: 0.14967186450958253\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14967186450958253  to: 0.14952012300491332\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.14952012300491332  to: 0.14937055110931396\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14937055110931396  to: 0.14922307729721068\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14922307729721068  to: 0.1490777015686035\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.1490777015686035  to: 0.14893434047698975\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14893434047698975  to: 0.14879294633865356\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14879294633865356  to: 0.1486534833908081\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.1486534833908081  to: 0.14851586818695067\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14851586818695067  to: 0.1483801007270813\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.1483801007270813  to: 0.1482461214065552\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.1482461214065552  to: 0.1481138586997986\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1481138586997986  to: 0.14798331260681152\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14798331260681152  to: 0.14785443544387816\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14785443544387816  to: 0.14772716760635377\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14772716760635377  to: 0.1476014733314514\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.1476014733314514  to: 0.14747735261917114\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14747735261917114  to: 0.14735469818115235\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14735469818115235  to: 0.14723351001739501\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14723351001739501  to: 0.14711374044418335\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14711374044418335  to: 0.14699535369873046\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14699535369873046  to: 0.14687834978103637\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14687834978103637  to: 0.146762216091156\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.146762216091156  to: 0.14664616584777831\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14664616584777831  to: 0.14653141498565675\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14653141498565675  to: 0.1464176058769226\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.1464176058769226  to: 0.14630355834960937\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14630355834960937  to: 0.14619040489196777\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14619040489196777  to: 0.1460784673690796\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.1460784673690796  to: 0.14596771001815795\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14596771001815795  to: 0.14585808515548707\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14585808515548707  to: 0.14574960470199586\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14574960470199586  to: 0.14564225673675538\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14564225673675538  to: 0.1455359697341919\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.1455359697341919  to: 0.14543074369430542\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14543074369430542  to: 0.145326566696167\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.145326566696167  to: 0.14522335529327393\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14522335529327393  to: 0.14512109756469727\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14512109756469727  to: 0.145019793510437\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.145019793510437  to: 0.14491943120956421\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14491943120956421  to: 0.1448199510574341\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1448199510574341  to: 0.1447213888168335\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.1447213888168335  to: 0.1446237087249756\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1446237087249756  to: 0.14452688694000243\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14452688694000243  to: 0.14443089962005615\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14443089962005615  to: 0.14433574676513672\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14433574676513672  to: 0.14424140453338624\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14424140453338624  to: 0.14414786100387572\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14414786100387572  to: 0.14405510425567628\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14405510425567628  to: 0.14396311044692994\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14396311044692994  to: 0.14387186765670776\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14387186765670776  to: 0.14378135204315184\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14378135204315184  to: 0.14369157552719117\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14369157552719117  to: 0.14360250234603883\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14360250234603883  to: 0.14351403713226318\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14351403713226318  to: 0.14342617988586426\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14342617988586426  to: 0.143338942527771\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.143338942527771  to: 0.14325228929519654\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14325228929519654  to: 0.1431662321090698\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1431662321090698  to: 0.1430807590484619\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.1430807590484619  to: 0.14299585819244384\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.14299585819244384  to: 0.14291155338287354\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.14291155338287354  to: 0.14282772541046143\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.14282772541046143  to: 0.14274442195892334\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14274442195892334  to: 0.14266159534454345\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14266159534454345  to: 0.1425792932510376\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.1425792932510376  to: 0.14249746799468993\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14249746799468993  to: 0.14241617918014526\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14241617918014526  to: 0.14233545064926148\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14233545064926148  to: 0.1422552227973938\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.1422552227973938  to: 0.14217549562454224\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14217549562454224  to: 0.14209622144699097\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.14209622144699097  to: 0.1420174479484558\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.1420174479484558  to: 0.1419391393661499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 180\n",
      "Improved validation loss from: 0.1419391393661499  to: 0.1418613076210022\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.1418613076210022  to: 0.14178396463394166\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14178396463394166  to: 0.14170706272125244\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14170706272125244  to: 0.14163062572479249\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14163062572479249  to: 0.14155466556549073\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14155466556549073  to: 0.1414791464805603\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.1414791464805603  to: 0.14140405654907226\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.14140405654907226  to: 0.14132940769195557\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.14132940769195557  to: 0.1412551999092102\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.1412551999092102  to: 0.14118138551712037\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.14118138551712037  to: 0.14110801219940186\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.14110801219940186  to: 0.14103504419326782\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.14103504419326782  to: 0.14096248149871826\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.14096248149871826  to: 0.14089031219482423\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.14089031219482423  to: 0.1408185362815857\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.1408185362815857  to: 0.14074716567993165\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.14074716567993165  to: 0.14067614078521729\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14067614078521729  to: 0.1406055212020874\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.1406055212020874  to: 0.14053525924682617\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.14053525924682617  to: 0.1404653787612915\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.1404653787612915  to: 0.1403958320617676\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.1403958320617676  to: 0.14032665491104127\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14032665491104127  to: 0.1402578115463257\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.1402578115463257  to: 0.1401893377304077\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.1401893377304077  to: 0.14012120962142943\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.14012120962142943  to: 0.1400534152984619\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.1400534152984619  to: 0.13998595476150513\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13998595476150513  to: 0.13991880416870117\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13991880416870117  to: 0.13985199928283693\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13985199928283693  to: 0.1397855043411255\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.1397855043411255  to: 0.13971933126449584\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.13971933126449584  to: 0.13965346813201904\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.13965346813201904  to: 0.1395879030227661\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.1395879030227661  to: 0.13952265977859496\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13952265977859496  to: 0.13945769071578978\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.13945769071578978  to: 0.13939304351806642\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13939304351806642  to: 0.13932868242263793\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13932868242263793  to: 0.1392646074295044\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.1392646074295044  to: 0.13920080661773682\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.13920080661773682  to: 0.1391371011734009\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.1391371011734009  to: 0.13907324075698851\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13907324075698851  to: 0.1390096664428711\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.1390096664428711  to: 0.13894637823104858\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13894637823104858  to: 0.138883376121521\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.138883376121521  to: 0.1388206124305725\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.1388206124305725  to: 0.13875813484191896\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13875813484191896  to: 0.13869593143463135\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13869593143463135  to: 0.13863399028778076\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13863399028778076  to: 0.1385722875595093\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.1385722875595093  to: 0.13851085901260377\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13851085901260377  to: 0.13844969272613525\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13844969272613525  to: 0.13838876485824586\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.13838876485824586  to: 0.1383280873298645\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.1383280873298645  to: 0.13826764822006227\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13826764822006227  to: 0.13820748329162597\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13820748329162597  to: 0.13814754486083985\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.13814754486083985  to: 0.13808784484863282\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13808784484863282  to: 0.13802841901779175\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.13802841901779175  to: 0.13796935081481934\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13796935081481934  to: 0.13791053295135497\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.13791053295135497  to: 0.13785194158554076\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13785194158554076  to: 0.13779357671737671\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.13779357671737671  to: 0.13773545026779174\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13773545026779174  to: 0.13767755031585693\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13767755031585693  to: 0.13761988878250123\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13761988878250123  to: 0.1375624418258667\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.1375624418258667  to: 0.13750520944595337\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13750520944595337  to: 0.1374482035636902\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.1374482035636902  to: 0.13739142417907715\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13739142417907715  to: 0.13733484745025634\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13733484745025634  to: 0.13727850914001466\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13727850914001466  to: 0.13722237348556518\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.13722237348556518  to: 0.13716646432876586\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13716646432876586  to: 0.13711075782775878\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13711075782775878  to: 0.1370552659034729\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.1370552659034729  to: 0.13699997663497926\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13699997663497926  to: 0.1369449257850647\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1369449257850647  to: 0.13689005374908447\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13689005374908447  to: 0.13683539628982544\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13683539628982544  to: 0.1367809534072876\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.1367809534072876  to: 0.13672670125961303\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13672670125961303  to: 0.13667266368865966\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.13667266368865966  to: 0.13661884069442748\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13661884069442748  to: 0.1365652084350586\n",
      "Training iteration: 264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1365652084350586  to: 0.13651177883148194\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13651177883148194  to: 0.13645856380462645\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13645856380462645  to: 0.1364055395126343\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.1364055395126343  to: 0.13635270595550536\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.13635270595550536  to: 0.13630011081695556\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13630011081695556  to: 0.13624765872955322\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.13624765872955322  to: 0.13619544506072997\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13619544506072997  to: 0.13614342212677003\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13614342212677003  to: 0.13609158992767334\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13609158992767334  to: 0.13603994846343995\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13603994846343995  to: 0.13598852157592772\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13598852157592772  to: 0.13593728542327882\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13593728542327882  to: 0.13588622808456421\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13588622808456421  to: 0.1358353853225708\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.1358353853225708  to: 0.13578473329544066\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.13578473329544066  to: 0.13573429584503174\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13573429584503174  to: 0.13568401336669922\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13568401336669922  to: 0.13563398122787476\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13563398122787476  to: 0.13558416366577147\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13558416366577147  to: 0.13553458452224731\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13553458452224731  to: 0.13548519611358642\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13548519611358642  to: 0.1354360342025757\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.1354360342025757  to: 0.1353870749473572\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1353870749473572  to: 0.13533833026885986\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13533833026885986  to: 0.13528980016708375\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13528980016708375  to: 0.13524147272109985\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13524147272109985  to: 0.13519333600997924\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13519333600997924  to: 0.1351454257965088\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.1351454257965088  to: 0.13509769439697267\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13509769439697267  to: 0.13505017757415771\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13505017757415771  to: 0.1350028395652771\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.1350028395652771  to: 0.1349557399749756\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.1349557399749756  to: 0.13490880727767945\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13490880727767945  to: 0.1348620891571045\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1348620891571045  to: 0.1348155617713928\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.1348155617713928  to: 0.1347692370414734\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.1347692370414734  to: 0.13472311496734618\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13472311496734618  to: 0.13467719554901122\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13467719554901122  to: 0.13463146686553956\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13463146686553956  to: 0.134585964679718\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.134585964679718  to: 0.13454062938690187\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13454062938690187  to: 0.13449552059173583\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13449552059173583  to: 0.1344506025314331\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.1344506025314331  to: 0.1344057559967041\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.1344057559967041  to: 0.1343611240386963\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.1343611240386963  to: 0.13431669473648072\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13431669473648072  to: 0.13427245616912842\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13427245616912842  to: 0.1342284321784973\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.1342284321784973  to: 0.13418463468551636\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13418463468551636  to: 0.13414102792739868\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13414102792739868  to: 0.1340976357460022\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.1340976357460022  to: 0.13405444622039794\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13405444622039794  to: 0.1340114951133728\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.1340114951133728  to: 0.1339687466621399\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.1339687466621399  to: 0.13392622470855714\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13392622470855714  to: 0.13388391733169555\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13388391733169555  to: 0.13384180068969725\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13384180068969725  to: 0.13379993438720703\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13379993438720703  to: 0.13375827074050903\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13375827074050903  to: 0.1337168335914612\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.1337168335914612  to: 0.13367561101913453\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13367561101913453  to: 0.13363460302352906\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13363460302352906  to: 0.1335938334465027\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.1335938334465027  to: 0.13355329036712646\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13355329036712646  to: 0.1335129737854004\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1335129737854004  to: 0.13347288370132446\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13347288370132446  to: 0.13343303203582763\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13343303203582763  to: 0.13339340686798096\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.13339340686798096  to: 0.1333540201187134\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.1333540201187134  to: 0.1333148717880249\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.1333148717880249  to: 0.13327592611312866\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13327592611312866  to: 0.13323720693588256\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13323720693588256  to: 0.13319861888885498\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13319861888885498  to: 0.1331602454185486\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.1331602454185486  to: 0.13312211036682128\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13312211036682128  to: 0.13308422565460204\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13308422565460204  to: 0.1330465316772461\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.1330465316772461  to: 0.13300909996032714\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13300909996032714  to: 0.13297189474105836\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13297189474105836  to: 0.13293498754501343\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13293498754501343  to: 0.1328985571861267\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.1328985571861267  to: 0.1328623652458191\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.1328623652458191  to: 0.13282639980316163\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13282639980316163  to: 0.13279069662094117\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13279069662094117  to: 0.13275524377822875\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13275524377822875  to: 0.1327200174331665\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.1327200174331665  to: 0.13268502950668334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13268502950668334  to: 0.1326502799987793\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1326502799987793  to: 0.13261579275131224\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13261579275131224  to: 0.13258154392242433\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13258154392242433  to: 0.13254754543304442\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13254754543304442  to: 0.1325137972831726\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1325137972831726  to: 0.13248029947280884\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13248029947280884  to: 0.1324470281600952\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1324470281600952  to: 0.1324140429496765\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.1324140429496765  to: 0.13238128423690795\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.13238128423690795  to: 0.13234879970550537\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13234879970550537  to: 0.13231654167175294\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13231654167175294  to: 0.13228455781936646\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13228455781936646  to: 0.13225282430648805\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13225282430648805  to: 0.13222134113311768\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.13222134113311768  to: 0.13219010829925537\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13219010829925537  to: 0.13215914964675904\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13215914964675904  to: 0.1321284294128418\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.1321284294128418  to: 0.13209798336029052\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.13209798336029052  to: 0.13206779956817627\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13206779956817627  to: 0.13203786611557006\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.13203786611557006  to: 0.13200820684432985\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.13200820684432985  to: 0.13197879791259765\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13197879791259765  to: 0.13194961547851564\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13194961547851564  to: 0.1319206953048706\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.1319206953048706  to: 0.1318920373916626\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.1318920373916626  to: 0.13186365365982056\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.13186365365982056  to: 0.13183552026748657\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13183552026748657  to: 0.1318076491355896\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.1318076491355896  to: 0.1317800760269165\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.1317800760269165  to: 0.1317527174949646\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.1317527174949646  to: 0.13172564506530762\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13172564506530762  to: 0.13169883489608764\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13169883489608764  to: 0.13167228698730468\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13167228698730468  to: 0.13164600133895873\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.13164600133895873  to: 0.1316200852394104\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.1316200852394104  to: 0.13159441947937012\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13159441947937012  to: 0.13156901597976683\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13156901597976683  to: 0.13154388666152955\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.13154388666152955  to: 0.13151900768280028\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.13151900768280028  to: 0.13149439096450805\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.13149439096450805  to: 0.1314700126647949\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.1314700126647949  to: 0.13144592046737671\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.13144592046737671  to: 0.1314220666885376\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.1314220666885376  to: 0.13139846324920654\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13139846324920654  to: 0.13137511014938355\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13137511014938355  to: 0.13135201930999757\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13135201930999757  to: 0.13132917881011963\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.13132917881011963  to: 0.1313065767288208\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1313065767288208  to: 0.13128424882888795\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.13128424882888795  to: 0.13126213550567628\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13126213550567628  to: 0.13124029636383056\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13124029636383056  to: 0.13121869564056396\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.13121869564056396  to: 0.13119734525680543\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.13119734525680543  to: 0.13117620944976807\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13117620944976807  to: 0.13115499019622803\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13115499019622803  to: 0.13113397359848022\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13113397359848022  to: 0.13111319541931152\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13111319541931152  to: 0.13109264373779297\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.13109264373779297  to: 0.13107234239578247\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13107234239578247  to: 0.13105226755142213\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13105226755142213  to: 0.13103240728378296\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.13103240728378296  to: 0.1310127854347229\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.1310127854347229  to: 0.13099337816238404\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.13099337816238404  to: 0.1309741735458374\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1309741735458374  to: 0.13095520734786986\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13095520734786986  to: 0.13093645572662355\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13093645572662355  to: 0.1309179425239563\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.1309179425239563  to: 0.13089959621429442\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.13089959621429442  to: 0.13088150024414064\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13088150024414064  to: 0.13086360692977905\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.13086360692977905  to: 0.13084592819213867\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.13084592819213867  to: 0.13082844018936157\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13082844018936157  to: 0.13081114292144774\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.13081114292144774  to: 0.13079404830932617\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13079404830932617  to: 0.13077714443206787\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13077714443206787  to: 0.13076043128967285\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13076043128967285  to: 0.13074389696121216\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.13074389696121216  to: 0.13072755336761474\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13072755336761474  to: 0.1307114005088806\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1307114005088806  to: 0.1306954026222229\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1306954026222229  to: 0.13067958354949952\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.13067958354949952  to: 0.1306639313697815\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1306639313697815  to: 0.13064844608306886\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.13064844608306886  to: 0.13063313961029052\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13063313961029052  to: 0.13061797618865967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13061797618865967  to: 0.13060296773910524\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13060296773910524  to: 0.13058812618255616\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13058812618255616  to: 0.13057342767715455\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13057342767715455  to: 0.13055888414382935\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.13055888414382935  to: 0.1305444836616516\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1305444836616516  to: 0.13053022623062133\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.13053022623062133  to: 0.13051608800888062\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13051608800888062  to: 0.1305021047592163\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.1305021047592163  to: 0.13048824071884155\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.13048824071884155  to: 0.13047451972961427\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13047451972961427  to: 0.13046090602874755\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13046090602874755  to: 0.1304474115371704\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1304474115371704  to: 0.13043406009674072\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13043406009674072  to: 0.13042080402374268\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13042080402374268  to: 0.13040765523910522\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.13040765523910522  to: 0.13039461374282837\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13039461374282837  to: 0.13038167953491211\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.13038167953491211  to: 0.13036887645721434\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13036887645721434  to: 0.13035614490509034\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.13035614490509034  to: 0.13034350872039796\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13034350872039796  to: 0.13033100366592407\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13033100366592407  to: 0.13031857013702391\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.13031857013702391  to: 0.13030622005462647\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13030622005462647  to: 0.1302939772605896\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1302939772605896  to: 0.13028180599212646\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.13028180599212646  to: 0.13026973009109497\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13026973009109497  to: 0.13025771379470824\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.13025771379470824  to: 0.13024575710296632\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13024575710296632  to: 0.13023387193679808\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13023387193679808  to: 0.13022207021713256\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13022207021713256  to: 0.13021034002304077\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13021034002304077  to: 0.1301986813545227\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.1301986813545227  to: 0.13018709421157837\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.13018709421157837  to: 0.13017557859420775\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13017557859420775  to: 0.13016411066055297\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13016411066055297  to: 0.13015271425247193\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.13015271425247193  to: 0.1301413893699646\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.1301413893699646  to: 0.13013010025024413\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.13013010025024413  to: 0.13011888265609742\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.13011888265609742  to: 0.13010771274566652\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13010771274566652  to: 0.13009659051895142\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.13009659051895142  to: 0.13008551597595214\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.13008551597595214  to: 0.13007447719573975\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13007447719573975  to: 0.13006348609924318\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13006348609924318  to: 0.1300525426864624\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.1300525426864624  to: 0.13004146814346312\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.13004146814346312  to: 0.13003032207489013\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.13003032207489013  to: 0.130019211769104\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.130019211769104  to: 0.13000812530517578\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13000812530517578  to: 0.12999706268310546\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12999706268310546  to: 0.1299860119819641\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.1299860119819641  to: 0.12997499704360962\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.12997499704360962  to: 0.12996398210525512\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.12996398210525512  to: 0.1299529790878296\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.1299529790878296  to: 0.12994201183319093\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12994201183319093  to: 0.12993104457855226\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12993104457855226  to: 0.1299201011657715\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.1299201011657715  to: 0.12990915775299072\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12990915775299072  to: 0.12989821434020996\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.12989821434020996  to: 0.12988728284835815\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.12988728284835815  to: 0.1298763632774353\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.1298763632774353  to: 0.1298654556274414\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.1298654556274414  to: 0.1298545479774475\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.1298545479774475  to: 0.1298436403274536\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.1298436403274536  to: 0.12983275651931764\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.12983275651931764  to: 0.12982184886932374\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.12982184886932374  to: 0.12981096506118775\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.12981096506118775  to: 0.1298000693321228\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.1298000693321228  to: 0.12978918552398683\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12978918552398683  to: 0.12977827787399293\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12977827787399293  to: 0.12976738214492797\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12976738214492797  to: 0.12975645065307617\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12975645065307617  to: 0.12974555492401124\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.12974555492401124  to: 0.12973461151123047\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12973461151123047  to: 0.12972365617752074\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.12972365617752074  to: 0.12971270084381104\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12971270084381104  to: 0.12970173358917236\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.12970173358917236  to: 0.1296907424926758\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1296907424926758  to: 0.1296797513961792\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.1296797513961792  to: 0.1296687126159668\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.1296687126159668  to: 0.12965770959854125\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12965770959854125  to: 0.1296466827392578\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.1296466827392578  to: 0.12963564395904542\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.12963564395904542  to: 0.12962459325790404\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.12962459325790404  to: 0.1296135425567627\n",
      "Training iteration: 521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1296135425567627  to: 0.12960247993469237\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.12960247993469237  to: 0.12959139347076415\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.12959139347076415  to: 0.12958028316497802\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.12958028316497802  to: 0.1295691728591919\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1295691728591919  to: 0.12955801486968993\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.12955801486968993  to: 0.12954682111740112\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12954682111740112  to: 0.12953561544418335\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.12953561544418335  to: 0.12952436208724977\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.12952436208724977  to: 0.12951309680938722\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12951309680938722  to: 0.1295017957687378\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.1295017957687378  to: 0.12949047088623047\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12949047088623047  to: 0.1294790983200073\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.1294790983200073  to: 0.1294676899909973\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.1294676899909973  to: 0.1294562578201294\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.1294562578201294  to: 0.12944480180740356\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.12944480180740356  to: 0.12943328619003297\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.12943328619003297  to: 0.12942174673080445\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.12942174673080445  to: 0.12941015958786012\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.12941015958786012  to: 0.12939854860305786\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.12939854860305786  to: 0.1293870449066162\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.1293870449066162  to: 0.1293755888938904\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.1293755888938904  to: 0.12936416864395142\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.12936416864395142  to: 0.12935271263122558\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.12935271263122558  to: 0.1293412446975708\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.1293412446975708  to: 0.12932974100112915\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.12932974100112915  to: 0.12931820154190063\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12931820154190063  to: 0.12930662631988527\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.12930662631988527  to: 0.12929502725601197\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12929502725601197  to: 0.1292833685874939\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.1292833685874939  to: 0.1292717218399048\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.1292717218399048  to: 0.12926002740859985\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12926002740859985  to: 0.129248309135437\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.129248309135437  to: 0.12923656702041625\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12923656702041625  to: 0.1292247772216797\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.1292247772216797  to: 0.12921298742294313\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12921298742294313  to: 0.12920116186141967\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.12920116186141967  to: 0.12918936014175414\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.12918936014175414  to: 0.1291775107383728\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.1291775107383728  to: 0.12916566133499147\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.12916566133499147  to: 0.1291537880897522\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.1291537880897522  to: 0.12914186716079712\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12914186716079712  to: 0.12912992238998414\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12912992238998414  to: 0.12911795377731322\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12911795377731322  to: 0.12910593748092652\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12910593748092652  to: 0.1290938973426819\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.1290938973426819  to: 0.1290818214416504\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.1290818214416504  to: 0.12906951904296876\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12906951904296876  to: 0.12905712127685548\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.12905712127685548  to: 0.12904468774795533\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.12904468774795533  to: 0.12903220653533937\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.12903220653533937  to: 0.1290196657180786\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.1290196657180786  to: 0.129007089138031\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.129007089138031  to: 0.12899444103240967\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12899444103240967  to: 0.12898175716400145\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.12898175716400145  to: 0.12896902561187745\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12896902561187745  to: 0.12895622253417968\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12895622253417968  to: 0.1289433717727661\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.1289433717727661  to: 0.1289304494857788\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1289304494857788  to: 0.12891749143600464\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.12891749143600464  to: 0.1289044737815857\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.1289044737815857  to: 0.12889169454574584\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.12889169454574584  to: 0.12887892723083497\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12887892723083497  to: 0.1288661241531372\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.1288661241531372  to: 0.12885323762893677\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.12885323762893677  to: 0.12884029150009155\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12884029150009155  to: 0.12882730960845948\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.12882730960845948  to: 0.12881425619125367\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.12881425619125367  to: 0.12880113124847412\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12880113124847412  to: 0.12878797054290772\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.12878797054290772  to: 0.1287747621536255\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.1287747621536255  to: 0.12876149415969848\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.12876149415969848  to: 0.12874819040298463\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12874819040298463  to: 0.12873485088348388\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.12873485088348388  to: 0.12872146368026732\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12872146368026732  to: 0.12870802879333496\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12870802879333496  to: 0.12869455814361572\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12869455814361572  to: 0.12868103981018067\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12868103981018067  to: 0.12866748571395875\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.12866748571395875  to: 0.1286539077758789\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.1286539077758789  to: 0.12864028215408324\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12864028215408324  to: 0.12862660884857177\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12862660884857177  to: 0.1286128878593445\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.1286128878593445  to: 0.12859913110733032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 604\n",
      "Improved validation loss from: 0.12859913110733032  to: 0.1285853147506714\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.1285853147506714  to: 0.12857143878936766\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12857143878936766  to: 0.12855756282806396\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.12855756282806396  to: 0.1285436272621155\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.1285436272621155  to: 0.1285296320915222\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.1285296320915222  to: 0.128515625\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.128515625  to: 0.1285015821456909\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.1285015821456909  to: 0.128487491607666\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.128487491607666  to: 0.12847334146499634\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12847334146499634  to: 0.12845878601074218\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12845878601074218  to: 0.1284439444541931\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.1284439444541931  to: 0.1284290909767151\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1284290909767151  to: 0.12841417789459228\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12841417789459228  to: 0.12839921712875366\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12839921712875366  to: 0.12838422060012816\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12838422060012816  to: 0.12836917638778686\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12836917638778686  to: 0.12835409641265869\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12835409641265869  to: 0.12833898067474364\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12833898067474364  to: 0.1283238172531128\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.1283238172531128  to: 0.12830862998962403\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12830862998962403  to: 0.12829339504241943\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12829339504241943  to: 0.12827812433242797\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12827812433242797  to: 0.12826281785964966\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12826281785964966  to: 0.12824747562408448\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.12824747562408448  to: 0.12823210954666137\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12823210954666137  to: 0.1282167077064514\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1282167077064514  to: 0.12820127010345458\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12820127010345458  to: 0.12818582057952882\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12818582057952882  to: 0.12817029953002929\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12817029953002929  to: 0.128154718875885\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.128154718875885  to: 0.12813879251480104\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12813879251480104  to: 0.12812283039093017\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12812283039093017  to: 0.12810684442520143\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12810684442520143  to: 0.12809079885482788\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12809079885482788  to: 0.1280747175216675\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.1280747175216675  to: 0.12805861234664917\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12805861234664917  to: 0.128042471408844\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.128042471408844  to: 0.128026282787323\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.128026282787323  to: 0.12801005840301513\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12801005840301513  to: 0.12799379825592042\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12799379825592042  to: 0.1279774785041809\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.1279774785041809  to: 0.12796114683151244\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12796114683151244  to: 0.12794474363327027\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12794474363327027  to: 0.12792831659317017\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12792831659317017  to: 0.1279118299484253\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.1279118299484253  to: 0.12789530754089357\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12789530754089357  to: 0.127878737449646\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.127878737449646  to: 0.12786214351654052\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12786214351654052  to: 0.12784546613693237\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12784546613693237  to: 0.1278287649154663\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1278287649154663  to: 0.12781200408935547\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12781200408935547  to: 0.12779519557952881\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12779519557952881  to: 0.12777836322784425\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12777836322784425  to: 0.12776147127151488\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12776147127151488  to: 0.12774454355239867\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12774454355239867  to: 0.12772756814956665\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12772756814956665  to: 0.12771055698394776\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12771055698394776  to: 0.12769346237182616\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12769346237182616  to: 0.12767635583877562\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12767635583877562  to: 0.12765920162200928\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12765920162200928  to: 0.1276419997215271\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.1276419997215271  to: 0.127624773979187\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.127624773979187  to: 0.12760746479034424\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12760746479034424  to: 0.1275901675224304\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.1275901675224304  to: 0.1275728464126587\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1275728464126587  to: 0.1275554895401001\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.1275554895401001  to: 0.1275381088256836\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.1275381088256836  to: 0.12752069234848024\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12752069234848024  to: 0.12750324010848998\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.12750324010848998  to: 0.12748572826385499\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.12748572826385499  to: 0.12746816873550415\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12746816873550415  to: 0.1274505853652954\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.1274505853652954  to: 0.12743290662765502\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12743290662765502  to: 0.12741520404815673\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12741520404815673  to: 0.12739742994308473\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.12739742994308473  to: 0.12737960815429689\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.12737960815429689  to: 0.12736170291900634\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.12736170291900634  to: 0.12734373807907104\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12734373807907104  to: 0.127325701713562\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.127325701713562  to: 0.12730759382247925\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12730759382247925  to: 0.1272897481918335\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.1272897481918335  to: 0.12727210521697999\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12727210521697999  to: 0.12725436687469482\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12725436687469482  to: 0.12723654508590698\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12723654508590698  to: 0.1272186517715454\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.1272186517715454  to: 0.12720067501068116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12720067501068116  to: 0.12718263864517212\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12718263864517212  to: 0.12716450691223144\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12716450691223144  to: 0.127146315574646\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.127146315574646  to: 0.1271280527114868\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.1271280527114868  to: 0.12710976600646973\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12710976600646973  to: 0.12709146738052368\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.12709146738052368  to: 0.12707315683364867\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12707315683364867  to: 0.12705483436584472\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12705483436584472  to: 0.12703651189804077\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12703651189804077  to: 0.12701818943023682\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12701818943023682  to: 0.1269998550415039\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.1269998550415039  to: 0.126981520652771\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.126981520652771  to: 0.12696319818496704\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12696319818496704  to: 0.1269448518753052\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.1269448518753052  to: 0.12692651748657227\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12692651748657227  to: 0.1269081711769104\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1269081711769104  to: 0.12688984870910644\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12688984870910644  to: 0.12687151432037352\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12687151432037352  to: 0.12685316801071167\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12685316801071167  to: 0.12683483362197875\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12683483362197875  to: 0.1268164873123169\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.1268164873123169  to: 0.12679812908172608\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.12679812908172608  to: 0.1267797827720642\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.1267797827720642  to: 0.12676142454147338\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12676142454147338  to: 0.12674305438995362\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12674305438995362  to: 0.12672473192214967\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12672473192214967  to: 0.12670643329620362\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12670643329620362  to: 0.12668811082839965\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12668811082839965  to: 0.12666977643966676\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.12666977643966676  to: 0.12665141820907594\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12665141820907594  to: 0.12663307189941406\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.12663307189941406  to: 0.1266147017478943\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.1266147017478943  to: 0.1265963077545166\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.1265963077545166  to: 0.126577889919281\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.126577889919281  to: 0.1265594482421875\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.1265594482421875  to: 0.12654101848602295\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12654101848602295  to: 0.1265225887298584\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.1265225887298584  to: 0.1265041708946228\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1265041708946228  to: 0.12648576498031616\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12648576498031616  to: 0.12646740674972534\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12646740674972534  to: 0.12644903659820556\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12644903659820556  to: 0.12643060684204102\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12643060684204102  to: 0.12641215324401855\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12641215324401855  to: 0.12639365196228028\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12639365196228028  to: 0.12637511491775513\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12637511491775513  to: 0.12635654211044312\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12635654211044312  to: 0.12633793354034423\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12633793354034423  to: 0.1263192653656006\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.1263192653656006  to: 0.126300585269928\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.126300585269928  to: 0.12628185749053955\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12628185749053955  to: 0.12626307010650634\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12626307010650634  to: 0.12624424695968628\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12624424695968628  to: 0.12622538805007935\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12622538805007935  to: 0.12620646953582765\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12620646953582765  to: 0.126187527179718\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.126187527179718  to: 0.12616850137710572\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12616850137710572  to: 0.12614940404891967\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12614940404891967  to: 0.12613027095794677\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12613027095794677  to: 0.12611105442047119\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12611105442047119  to: 0.1260917901992798\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.1260917901992798  to: 0.12607247829437257\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12607247829437257  to: 0.12605310678482057\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12605310678482057  to: 0.12603368759155273\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12603368759155273  to: 0.12601422071456908\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12601422071456908  to: 0.1259947419166565\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.1259947419166565  to: 0.12597519159317017\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12597519159317017  to: 0.12595560550689697\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12595560550689697  to: 0.12593597173690796\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12593597173690796  to: 0.12591631412506105\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12591631412506105  to: 0.1258965849876404\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.1258965849876404  to: 0.1258768320083618\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.1258768320083618  to: 0.12585705518722534\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12585705518722534  to: 0.125837242603302\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.125837242603302  to: 0.12581738233566284\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12581738233566284  to: 0.12579749822616576\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12579749822616576  to: 0.12577756643295288\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12577756643295288  to: 0.12575756311416625\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12575756311416625  to: 0.12573753595352172\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12573753595352172  to: 0.12571747303009034\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12571747303009034  to: 0.12569737434387207\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12569737434387207  to: 0.12567723989486695\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12567723989486695  to: 0.1256570816040039\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.1256570816040039  to: 0.12563688755035402\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12563688755035402  to: 0.12561668157577516\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12561668157577516  to: 0.12559645175933837\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12559645175933837  to: 0.12557618618011473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12557618618011473  to: 0.12555588483810426\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12555588483810426  to: 0.12553555965423585\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12553555965423585  to: 0.12551522254943848\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12551522254943848  to: 0.1254948616027832\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1254948616027832  to: 0.1254744529724121\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.1254744529724121  to: 0.12545404434204102\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12545404434204102  to: 0.12543359994888306\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12543359994888306  to: 0.1254131555557251\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.1254131555557251  to: 0.12539272308349608\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12539272308349608  to: 0.12537230253219606\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12537230253219606  to: 0.125351881980896\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.125351881980896  to: 0.12533144950866698\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12533144950866698  to: 0.125311017036438\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.125311017036438  to: 0.125290584564209\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.125290584564209  to: 0.12527014017105104\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12527014017105104  to: 0.12524967193603515\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12524967193603515  to: 0.12522920370101928\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12522920370101928  to: 0.12520872354507445\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12520872354507445  to: 0.12518823146820068\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12518823146820068  to: 0.12516769170761108\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12516769170761108  to: 0.12514714002609253\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12514714002609253  to: 0.12512657642364503\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12512657642364503  to: 0.12510602474212645\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12510602474212645  to: 0.12508552074432372\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12508552074432372  to: 0.12506500482559205\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.12506500482559205  to: 0.12504444122314454\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12504444122314454  to: 0.12502386569976806\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12502386569976806  to: 0.12500327825546265\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12500327825546265  to: 0.12498264312744141\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12498264312744141  to: 0.12496200799942017\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12496200799942017  to: 0.12494084835052491\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12494084835052491  to: 0.1249193787574768\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.1249193787574768  to: 0.12489787340164185\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.12489787340164185  to: 0.12487632036209106\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.12487632036209106  to: 0.12485473155975342\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12485473155975342  to: 0.12483313083648681\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12483313083648681  to: 0.12481145858764649\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12481145858764649  to: 0.12478978633880615\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12478978633880615  to: 0.12476810216903686\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12476810216903686  to: 0.12474641799926758\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.12474641799926758  to: 0.12472470998764038\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.12472470998764038  to: 0.12470300197601318\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12470300197601318  to: 0.12468127012252808\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12468127012252808  to: 0.12465951442718506\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12465951442718506  to: 0.12463774681091308\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12463774681091308  to: 0.12461593151092529\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.12461593151092529  to: 0.12459409236907959\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12459409236907959  to: 0.12457221746444702\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12457221746444702  to: 0.12455030679702758\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12455030679702758  to: 0.12452837228775024\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12452837228775024  to: 0.12450640201568604\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12450640201568604  to: 0.12448440790176392\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12448440790176392  to: 0.12446236610412598\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12446236610412598  to: 0.12444026470184326\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12444026470184326  to: 0.12441812753677368\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.12441812753677368  to: 0.12439591884613037\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12439591884613037  to: 0.12437366247177124\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12437366247177124  to: 0.12435137033462525\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.12435137033462525  to: 0.12432903051376343\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12432903051376343  to: 0.1243066668510437\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.1243066668510437  to: 0.12428429126739501\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12428429126739501  to: 0.12426185607910156\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12426185607910156  to: 0.12423940896987914\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12423940896987914  to: 0.12421696186065674\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12421696186065674  to: 0.12419440746307372\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12419440746307372  to: 0.12417176961898804\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12417176961898804  to: 0.12414906024932862\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12414906024932862  to: 0.12412632703781128\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12412632703781128  to: 0.12410355806350708\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12410355806350708  to: 0.1240807294845581\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.1240807294845581  to: 0.12405788898468018\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.12405788898468018  to: 0.12403502464294433\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12403502464294433  to: 0.12401214838027955\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12401214838027955  to: 0.1239892601966858\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.1239892601966858  to: 0.12396633625030518\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12396633625030518  to: 0.12394349575042725\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.12394349575042725  to: 0.1239207148551941\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.1239207148551941  to: 0.12389799356460571\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.12389799356460571  to: 0.1238753080368042\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.1238753080368042  to: 0.1238526701927185\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.1238526701927185  to: 0.12383009195327759\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12383009195327759  to: 0.12380754947662354\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12380754947662354  to: 0.12378500699996949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 859\n",
      "Improved validation loss from: 0.12378500699996949  to: 0.12376250028610229\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.12376250028610229  to: 0.12374002933502197\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12374002933502197  to: 0.12371755838394165\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12371755838394165  to: 0.12369509935379028\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.12369509935379028  to: 0.12367264032363892\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12367264032363892  to: 0.1236501932144165\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.1236501932144165  to: 0.12362772226333618\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.12362772226333618  to: 0.12360525131225586\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12360525131225586  to: 0.12358278036117554\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.12358278036117554  to: 0.12356054782867432\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12356054782867432  to: 0.12353851795196533\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.12353851795196533  to: 0.12351645231246948\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12351645231246948  to: 0.12349437475204468\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.12349437475204468  to: 0.12347229719161987\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12347229719161987  to: 0.1234501838684082\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.1234501838684082  to: 0.12342805862426758\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12342805862426758  to: 0.12340590953826905\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12340590953826905  to: 0.12338374853134156\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.12338374853134156  to: 0.12336156368255616\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12336156368255616  to: 0.1233393669128418\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.1233393669128418  to: 0.12331714630126953\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.12331714630126953  to: 0.12329494953155518\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.12329494953155518  to: 0.12327274084091186\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.12327274084091186  to: 0.1232505202293396\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.1232505202293396  to: 0.12322828769683838\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12322828769683838  to: 0.12320601940155029\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12320601940155029  to: 0.1231837511062622\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.1231837511062622  to: 0.12316147089004517\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12316147089004517  to: 0.12313916683197021\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12313916683197021  to: 0.12311685085296631\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12311685085296631  to: 0.12309449911117554\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.12309449911117554  to: 0.12307212352752686\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12307212352752686  to: 0.12304971218109131\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.12304971218109131  to: 0.1230272889137268\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.1230272889137268  to: 0.12300482988357545\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.12300482988357545  to: 0.1229823112487793\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.1229823112487793  to: 0.1229597806930542\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.1229597806930542  to: 0.12293719053268433\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12293719053268433  to: 0.12291457653045654\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.12291457653045654  to: 0.12289184331893921\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.12289184331893921  to: 0.12286901473999023\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12286901473999023  to: 0.12284610271453858\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.12284610271453858  to: 0.12282307147979736\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.12282307147979736  to: 0.12279999256134033\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.12279999256134033  to: 0.12277681827545166\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12277681827545166  to: 0.12275357246398926\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12275357246398926  to: 0.12273024320602417\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12273024320602417  to: 0.12270691394805908\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.12270691394805908  to: 0.12268350124359131\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.12268350124359131  to: 0.12266005277633667\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12266005277633667  to: 0.1226365327835083\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.1226365327835083  to: 0.12261296510696411\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12261296510696411  to: 0.12258936166763305\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12258936166763305  to: 0.12256568670272827\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.12256568670272827  to: 0.12254199981689454\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12254199981689454  to: 0.12251828908920288\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.12251828908920288  to: 0.12249453067779541\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.12249453067779541  to: 0.12247073650360107\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.12247073650360107  to: 0.12244691848754882\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.12244691848754882  to: 0.12242310047149658\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.12242310047149658  to: 0.12239925861358643\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12239925861358643  to: 0.12237541675567627\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12237541675567627  to: 0.12235152721405029\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.12235152721405029  to: 0.12232767343521118\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.12232767343521118  to: 0.12230377197265625\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12230377197265625  to: 0.12227989435195923\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.12227989435195923  to: 0.12225601673126221\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12225601673126221  to: 0.12223210334777831\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12223210334777831  to: 0.12220820188522338\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12220820188522338  to: 0.12218430042266845\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.12218430042266845  to: 0.12216042280197144\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.12216042280197144  to: 0.12213653326034546\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.12213653326034546  to: 0.1221126675605774\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.1221126675605774  to: 0.12208878993988037\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12208878993988037  to: 0.12206491231918334\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12206491231918334  to: 0.12204104661941528\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12204104661941528  to: 0.12201719284057617\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12201719284057617  to: 0.12199333906173707\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.12199333906173707  to: 0.12196950912475586\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12196950912475586  to: 0.12194569110870361\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.12194569110870361  to: 0.1219218611717224\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.1219218611717224  to: 0.121898353099823\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.121898353099823  to: 0.12187477350234985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12187477350234985  to: 0.12185112237930298\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12185112237930298  to: 0.12182738780975341\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12182738780975341  to: 0.121803617477417\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.121803617477417  to: 0.1217797875404358\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.1217797875404358  to: 0.12175590991973877\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12175590991973877  to: 0.12173202037811279\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12173202037811279  to: 0.121708083152771\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.121708083152771  to: 0.12168412208557129\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.12168412208557129  to: 0.12166016101837158\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.12166016101837158  to: 0.12163617610931396\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.12163617610931396  to: 0.12161219120025635\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.12161219120025635  to: 0.12158819437026977\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12158819437026977  to: 0.12156417369842529\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12156417369842529  to: 0.12154017686843872\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.12154017686843872  to: 0.12151614427566529\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.12151614427566529  to: 0.12149208784103394\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12149208784103394  to: 0.12146801948547363\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12146801948547363  to: 0.12144432067871094\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12144432067871094  to: 0.1214209794998169\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.1214209794998169  to: 0.12139761447906494\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.12139761447906494  to: 0.12137428522109986\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.12137428522109986  to: 0.12135093212127686\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.12135093212127686  to: 0.12132755517959595\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12132755517959595  to: 0.12130416631698608\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12130416631698608  to: 0.12128077745437622\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12128077745437622  to: 0.1212573528289795\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.1212573528289795  to: 0.12123395204544067\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12123395204544067  to: 0.12121052742004394\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12121052742004394  to: 0.12118709087371826\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12118709087371826  to: 0.12116367816925049\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.12116367816925049  to: 0.12114022970199585\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12114022970199585  to: 0.12111679315567017\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12111679315567017  to: 0.12109335660934448\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.12109335660934448  to: 0.12106993198394775\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12106993198394775  to: 0.12104648351669312\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12104648351669312  to: 0.12102304697036743\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12102304697036743  to: 0.12099961042404175\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12099961042404175  to: 0.1209761619567871\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.1209761619567871  to: 0.12095272541046143\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12095272541046143  to: 0.12092926502227783\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12092926502227783  to: 0.12090580463409424\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12090580463409424  to: 0.12088236808776856\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12088236808776856  to: 0.12085891962051391\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12085891962051391  to: 0.12083543539047241\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.12083543539047241  to: 0.12081197500228882\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12081197500228882  to: 0.12078850269317627\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12078850269317627  to: 0.12076499462127685\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12076499462127685  to: 0.1207414984703064\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.1207414984703064  to: 0.12071797847747803\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12071797847747803  to: 0.12069443464279175\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.12069443464279175  to: 0.12067086696624756\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12067086696624756  to: 0.1206472635269165\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.1206472635269165  to: 0.12062362432479859\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12062362432479859  to: 0.12059998512268066\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.12059998512268066  to: 0.12057629823684693\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12057629823684693  to: 0.12055257558822632\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12055257558822632  to: 0.1205288290977478\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.1205288290977478  to: 0.12050507068634034\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.12050507068634034  to: 0.12048127651214599\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.12048127651214599  to: 0.12045745849609375\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.12045745849609375  to: 0.1204336166381836\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.1204336166381836  to: 0.12040976285934449\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12040976285934449  to: 0.12038587331771851\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12038587331771851  to: 0.12036199569702148\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.12036199569702148  to: 0.12033817768096924\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12033817768096924  to: 0.12031457424163819\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12031457424163819  to: 0.12029091119766236\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12029091119766236  to: 0.12026722431182861\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12026722431182861  to: 0.12024346590042115\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.12024346590042115  to: 0.1202196717262268\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.1202196717262268  to: 0.12019580602645874\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.12019580602645874  to: 0.12017186880111694\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.12017186880111694  to: 0.12014789581298828\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12014789581298828  to: 0.12012383937835694\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12012383937835694  to: 0.12009971141815186\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.12009971141815186  to: 0.12007551193237305\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.12007551193237305  to: 0.12005122900009155\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.12005122900009155  to: 0.12002687454223633\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12002687454223633  to: 0.12000247240066528\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.12000247240066528  to: 0.11997798681259156\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.11997798681259156  to: 0.11995344161987305\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.11995344161987305  to: 0.11992886066436767\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.11992886066436767  to: 0.11990418434143066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.11990418434143066  to: 0.11987946033477784\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.11987946033477784  to: 0.11985466480255128\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.11985466480255128  to: 0.11982982158660889\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.11982982158660889  to: 0.11980491876602173\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.11980491876602173  to: 0.11977994441986084\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.11977994441986084  to: 0.11975492238998413\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.11975492238998413  to: 0.11972985267639161\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.11972985267639161  to: 0.11970471143722534\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.11970471143722534  to: 0.11967949867248535\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.11967949867248535  to: 0.1196542501449585\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.1196542501449585  to: 0.1196288824081421\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1196288824081421  to: 0.11960347890853881\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.11960347890853881  to: 0.11957801580429077\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.11957801580429077  to: 0.11955244541168213\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.11955244541168213  to: 0.11952680349349976\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.11952680349349976  to: 0.11950107812881469\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.11950107812881469  to: 0.11947528123855591\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.11947528123855591  to: 0.11944944858551025\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.11944944858551025  to: 0.11942360401153565\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.11942360401153565  to: 0.11939777135848999\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.11939777135848999  to: 0.11937189102172852\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.11937189102172852  to: 0.11934598684310913\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.11934598684310913  to: 0.11932008266448975\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.11932008266448975  to: 0.1192941427230835\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.1192941427230835  to: 0.11926816701889038\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.11926816701889038  to: 0.11924222707748414\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.11924222707748414  to: 0.1192167043685913\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.1192167043685913  to: 0.11919114589691163\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.11919114589691163  to: 0.11916552782058716\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.11916552782058716  to: 0.11913983821868897\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.11913983821868897  to: 0.119114089012146\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.119114089012146  to: 0.11908824443817138\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.11908824443817138  to: 0.11906235218048096\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.11906235218048096  to: 0.11903636455535889\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.11903636455535889  to: 0.11901028156280517\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.11901028156280517  to: 0.11898411512374878\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.11898411512374878  to: 0.1189578652381897\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.1189578652381897  to: 0.11893150806427003\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.11893150806427003  to: 0.11890506744384766\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.11890506744384766  to: 0.1188785195350647\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.1188785195350647  to: 0.1188518762588501\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.1188518762588501  to: 0.11882507801055908\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.11882507801055908  to: 0.11879818439483643\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.11879818439483643  to: 0.1187711477279663\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.1187711477279663  to: 0.11874401569366455\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.11874401569366455  to: 0.11871678829193115\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.11871678829193115  to: 0.1186894416809082\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.1186894416809082  to: 0.11866197586059571\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.11866197586059571  to: 0.11863440275192261\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.11863440275192261  to: 0.11860673427581787\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.11860673427581787  to: 0.11857895851135254\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.11857895851135254  to: 0.11855108737945556\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.11855108737945556  to: 0.11852309703826905\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.11852309703826905  to: 0.11849502325057984\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.11849502325057984  to: 0.11846683025360108\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.11846683025360108  to: 0.11843854188919067\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.11843854188919067  to: 0.11841014623641968\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.11841014623641968  to: 0.11838163137435913\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.11838163137435913  to: 0.11835305690765381\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.11835305690765381  to: 0.11832435131072998\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.11832435131072998  to: 0.11829555034637451\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.11829555034637451  to: 0.11826670169830322\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.11826670169830322  to: 0.11823776960372925\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.11823776960372925  to: 0.11820876598358154\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.11820876598358154  to: 0.11817967891693115\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.11817967891693115  to: 0.11815050840377808\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.11815050840377808  to: 0.11812126636505127\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.11812126636505127  to: 0.11809194087982178\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.11809194087982178  to: 0.11806256771087646\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.11806256771087646  to: 0.11803312301635742\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.11803312301635742  to: 0.11800358295440674\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.11800358295440674  to: 0.11797399520874023\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.11797399520874023  to: 0.11794431209564209\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.11794431209564209  to: 0.11791456937789917\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.11791456937789917  to: 0.11788475513458252\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.11788475513458252  to: 0.11785483360290527\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.11785483360290527  to: 0.1178248643875122\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.1178248643875122  to: 0.1177947998046875\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.1177947998046875  to: 0.11776465177536011\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.11776465177536011  to: 0.11773442029953003\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.11773442029953003  to: 0.11770416498184204\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.11770416498184204  to: 0.11767385005950928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.11767385005950928  to: 0.11764352321624756\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.11764352321624756  to: 0.11761314868927002\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.11761314868927002  to: 0.11758278608322144\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.11758278608322144  to: 0.11755242347717285\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.11755242347717285  to: 0.11752208471298217\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.11752208471298217  to: 0.1174917459487915\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.1174917459487915  to: 0.11746143102645874\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.11746143102645874  to: 0.11743110418319702\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.11743110418319702  to: 0.11740076541900635\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.11740076541900635  to: 0.11737041473388672\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.11737041473388672  to: 0.11734002828598022\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.11734002828598022  to: 0.1173095941543579\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.1173095941543579  to: 0.11727908849716187\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.11727908849716187  to: 0.1172485113143921\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.1172485113143921  to: 0.11721785068511963\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.11721785068511963  to: 0.11718652248382569\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.11718652248382569  to: 0.11715490818023681\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.11715490818023681  to: 0.11712276935577393\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.11712276935577393  to: 0.11709007024765014\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.11709007024765014  to: 0.11705679893493652\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.11705679893493652  to: 0.11702302694320679\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.11702302694320679  to: 0.11698880195617675\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.11698880195617675  to: 0.11695419549942017\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.11695419549942017  to: 0.11691925525665284\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.11691925525665284  to: 0.11688399314880371\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.11688399314880371  to: 0.11684849262237548\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.11684849262237548  to: 0.11681281328201294\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.11681281328201294  to: 0.11677693128585816\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.11677693128585816  to: 0.11674091815948487\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.11674091815948487  to: 0.11670480966567993\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.11670480966567993  to: 0.11666861772537232\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.11666861772537232  to: 0.11663233041763306\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.11663233041763306  to: 0.1165959119796753\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.1165959119796753  to: 0.11655935049057006\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.11655935049057006  to: 0.11652206182479859\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.11652206182479859  to: 0.11648415327072144\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.11648415327072144  to: 0.11644575595855713\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.11644575595855713  to: 0.11640698909759521\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.11640698909759521  to: 0.11636795997619628\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.11636795997619628  to: 0.11632874011993408\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.11632874011993408  to: 0.11628942489624024\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.11628942489624024  to: 0.11625006198883056\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.11625006198883056  to: 0.1162107229232788\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.1162107229232788  to: 0.11617145538330079\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.11617145538330079  to: 0.11613228321075439\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.11613228321075439  to: 0.11609326601028443\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.11609326601028443  to: 0.11605437994003295\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.11605437994003295  to: 0.11601561307907104\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.11601561307907104  to: 0.11597702503204346\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.11597702503204346  to: 0.11593856811523437\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.11593856811523437  to: 0.11590025424957276\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.11590025424957276  to: 0.11586223840713501\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.11586223840713501  to: 0.11582467555999756\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.11582467555999756  to: 0.11578716039657592\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.11578716039657592  to: 0.11574971675872803\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.11574971675872803  to: 0.1157123327255249\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.1157123327255249  to: 0.11567504405975342\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.11567504405975342  to: 0.11563788652420044\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.11563788652420044  to: 0.11560078859329223\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.11560078859329223  to: 0.11556373834609986\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.11556373834609986  to: 0.11552671194076539\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.11552671194076539  to: 0.1154896855354309\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.1154896855354309  to: 0.11545263528823853\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.11545263528823853  to: 0.11541594266891479\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.11541594266891479  to: 0.11537926197052002\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.11537926197052002  to: 0.11534254550933838\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.11534254550933838  to: 0.11530575752258301\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.11530575752258301  to: 0.1152688980102539\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.1152688980102539  to: 0.11523120403289795\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.11523120403289795  to: 0.11519287824630738\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.11519287824630738  to: 0.11515424251556397\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.11515424251556397  to: 0.11511503458023072\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.11511503458023072  to: 0.11507532596588135\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.11507532596588135  to: 0.11503441333770752\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.11503441333770752  to: 0.11499240398406982\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.11499240398406982  to: 0.1149495005607605\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.1149495005607605  to: 0.11490585803985595\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.11490585803985595  to: 0.11486160755157471\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.11486160755157471  to: 0.11481688022613526\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.11481688022613526  to: 0.11477181911468506\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.11477181911468506  to: 0.11472651958465577\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.11472651958465577  to: 0.11468102931976318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.11468102931976318  to: 0.11463546752929688\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.11463546752929688  to: 0.11458988189697265\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.11458988189697265  to: 0.11454432010650635\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.11454432010650635  to: 0.1144987940788269\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.1144987940788269  to: 0.11445338726043701\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.11445338726043701  to: 0.11440809965133666\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.11440809965133666  to: 0.11436311006546021\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.11436311006546021  to: 0.11431818008422852\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.11431818008422852  to: 0.11427334547042847\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.11427334547042847  to: 0.11422855854034424\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.11422855854034424  to: 0.11418383121490479\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.11418383121490479  to: 0.11413915157318115\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.11413915157318115  to: 0.11409448385238648\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.11409448385238648  to: 0.11404985189437866\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.11404985189437866  to: 0.11400524377822877\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.11400524377822877  to: 0.11396063566207885\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.11396063566207885  to: 0.11391605138778686\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.11391605138778686  to: 0.11387147903442382\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.11387147903442382  to: 0.11382694244384765\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.11382694244384765  to: 0.11378239393234253\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.11378239393234253  to: 0.11373683214187622\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.11373683214187622  to: 0.11369004249572753\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.11369004249572753  to: 0.11364220380783081\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.11364220380783081  to: 0.11359350681304932\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.11359350681304932  to: 0.11354309320449829\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.11354309320449829  to: 0.11349122524261475\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.11349122524261475  to: 0.11343820095062256\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.11343820095062256  to: 0.11338460445404053\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.11338460445404053  to: 0.11333057880401612\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.11333057880401612  to: 0.11327645778656006\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.11327645778656006  to: 0.11322246789932251\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.11322246789932251  to: 0.11316859722137451\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.11316859722137451  to: 0.11311494112014771\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.11311494112014771  to: 0.11306155920028686\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.11306155920028686  to: 0.11300851106643676\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.11300851106643676  to: 0.11295578479766846\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.11295578479766846  to: 0.11290308237075805\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.11290308237075805  to: 0.1128503441810608\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.1128503441810608  to: 0.11279760599136353\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.11279760599136353  to: 0.11274485588073731\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.11274485588073731  to: 0.11269248723983764\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.11269248723983764  to: 0.1126404047012329\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.1126404047012329  to: 0.11258852481842041\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.11258852481842041  to: 0.11253700256347657\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.11253700256347657  to: 0.11248600482940674\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.11248600482940674  to: 0.11243473291397095\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.11243473291397095  to: 0.11238323450088501\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.11238323450088501  to: 0.11233150959014893\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.11233150959014893  to: 0.11227962970733643\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.11227962970733643  to: 0.11222808361053467\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.11222808361053467  to: 0.11217682361602783\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.11217682361602783  to: 0.11212584972381592\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.11212584972381592  to: 0.11207512617111207\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.11207512617111207  to: 0.11202428340911866\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.11202428340911866  to: 0.11197335720062256\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.11197335720062256  to: 0.11192238330841064\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.11192238330841064  to: 0.11187137365341186\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.11187137365341186  to: 0.11182081699371338\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.11182081699371338  to: 0.11177098751068115\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.11177098751068115  to: 0.11172149181365967\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.11172149181365967  to: 0.11167227029800415\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.11167227029800415  to: 0.1116228461265564\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.1116228461265564  to: 0.11157317161560058\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.11157317161560058  to: 0.11152318716049195\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.11152318716049195  to: 0.11147297620773315\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.11147297620773315  to: 0.1114223837852478\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.1114223837852478  to: 0.11137126684188843\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.11137126684188843  to: 0.11131967306137085\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.11131967306137085  to: 0.11126759052276611\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.11126759052276611  to: 0.11121466159820556\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.11121466159820556  to: 0.11116091012954712\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.11116091012954712  to: 0.11110643148422242\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.11110643148422242  to: 0.11105127334594726\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.11105127334594726  to: 0.11099600791931152\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.11099600791931152  to: 0.11094063520431519\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.11094063520431519  to: 0.11088519096374512\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.11088519096374512  to: 0.1108291745185852\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.1108291745185852  to: 0.11077260971069336\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.11077260971069336  to: 0.11071557998657226\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.11071557998657226  to: 0.11065807342529296\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.11065807342529296  to: 0.1106001615524292\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.1106001615524292  to: 0.1105419635772705\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.1105419635772705  to: 0.11048398017883301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.11048398017883301  to: 0.11042629480361939\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.11042629480361939  to: 0.11036884784698486\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.11036884784698486  to: 0.11031110286712646\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.11031110286712646  to: 0.11025302410125733\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.11025302410125733  to: 0.1101946473121643\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.1101946473121643  to: 0.11013585329055786\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.11013585329055786  to: 0.11007684469223022\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.11007684469223022  to: 0.11001756191253662\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.11001756191253662  to: 0.10995798110961914\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.10995798110961914  to: 0.10989866256713868\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.10989866256713868  to: 0.1098395824432373\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.1098395824432373  to: 0.10978010892868043\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.10978010892868043  to: 0.10972028970718384\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.10972028970718384  to: 0.10966010093688965\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.10966010093688965  to: 0.10959957838058472\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.10959957838058472  to: 0.1095387101173401\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.1095387101173401  to: 0.1094776749610901\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.1094776749610901  to: 0.1094164252281189\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.1094164252281189  to: 0.10935556888580322\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.10935556888580322  to: 0.10929428339004517\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.10929428339004517  to: 0.10923253297805786\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.10923253297805786  to: 0.10917093753814697\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.10917093753814697  to: 0.1091088056564331\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.1091088056564331  to: 0.10904611349105835\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.10904611349105835  to: 0.10898206233978272\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.10898206233978272  to: 0.10891683101654052\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.10891683101654052  to: 0.10885057449340821\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.10885057449340821  to: 0.10878348350524902\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.10878348350524902  to: 0.10871572494506836\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.10871572494506836  to: 0.1086474061012268\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.1086474061012268  to: 0.10857865810394288\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.10857865810394288  to: 0.10850951671600342\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.10850951671600342  to: 0.10844011306762695\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.10844011306762695  to: 0.10837042331695557\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.10837042331695557  to: 0.10830045938491821\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.10830045938491821  to: 0.1082308530807495\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.1082308530807495  to: 0.10816076993942261\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.10816076993942261  to: 0.10809023380279541\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.10809023380279541  to: 0.10801914930343628\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.10801914930343628  to: 0.10794756412506104\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.10794756412506104  to: 0.10787544250488282\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.10787544250488282  to: 0.10780284404754639\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.10780284404754639  to: 0.10772980451583862\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.10772980451583862  to: 0.10765640735626221\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.10765640735626221  to: 0.10758270025253296\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.10758270025253296  to: 0.10750875473022461\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.10750875473022461  to: 0.10743458271026611\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.10743458271026611  to: 0.1073602557182312\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.1073602557182312  to: 0.1072847843170166\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.1072847843170166  to: 0.10720834732055665\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.10720834732055665  to: 0.10713106393814087\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.10713106393814087  to: 0.10705310106277466\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.10705310106277466  to: 0.1069745421409607\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.1069745421409607  to: 0.10689315795898438\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.10689315795898438  to: 0.10680935382843018\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.10680935382843018  to: 0.10672361850738525\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.10672361850738525  to: 0.10663630962371826\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.10663630962371826  to: 0.10654782056808472\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.10654782056808472  to: 0.10645843744277954\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.10645843744277954  to: 0.1063683271408081\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.1063683271408081  to: 0.1062777042388916\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.1062777042388916  to: 0.10618667602539063\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.10618667602539063  to: 0.10609532594680786\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.10609532594680786  to: 0.10600147247314454\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.10600147247314454  to: 0.10590562820434571\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.10590562820434571  to: 0.10580835342407227\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.10580835342407227  to: 0.10571013689041138\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.10571013689041138  to: 0.10561150312423706\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.10561150312423706  to: 0.10551283359527588\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.10551283359527588  to: 0.10541452169418335\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.10541452169418335  to: 0.10531675815582275\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.10531675815582275  to: 0.10521970987319947\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.10521970987319947  to: 0.10512340068817139\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.10512340068817139  to: 0.10502790212631226\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.10502790212631226  to: 0.10493316650390624\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.10493316650390624  to: 0.10483916997909545\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.10483916997909545  to: 0.10474582910537719\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.10474582910537719  to: 0.10465319156646728\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.10465319156646728  to: 0.10456129312515258\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.10456129312515258  to: 0.10447016954421998\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.10447016954421998  to: 0.10437991619110107\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.10437991619110107  to: 0.10429062843322753\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.10429062843322753  to: 0.10420231819152832\n",
      "Training iteration: 1355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10420231819152832  to: 0.10411491394042968\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.10411491394042968  to: 0.10402835607528686\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.10402835607528686  to: 0.10394246578216552\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.10394246578216552  to: 0.10385713577270508\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.10385713577270508  to: 0.10377217531204223\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.10377217531204223  to: 0.10368732213974\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.10368732213974  to: 0.10360243320465087\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.10360243320465087  to: 0.10351719856262206\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.10351719856262206  to: 0.10343146324157715\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.10343146324157715  to: 0.10334426164627075\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.10334426164627075  to: 0.1032560110092163\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.1032560110092163  to: 0.10316671133041382\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.10316671133041382  to: 0.1030765414237976\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.1030765414237976  to: 0.10298569202423095\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.10298569202423095  to: 0.102894127368927\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.102894127368927  to: 0.10280176401138305\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.10280176401138305  to: 0.10270836353302001\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.10270836353302001  to: 0.10261375904083252\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.10261375904083252  to: 0.10251764059066773\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.10251764059066773  to: 0.10242005586624145\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.10242005586624145  to: 0.10232117176055908\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.10232117176055908  to: 0.10222104787826539\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.10222104787826539  to: 0.10212029218673706\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.10212029218673706  to: 0.10201941728591919\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.10201941728591919  to: 0.10191857814788818\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.10191857814788818  to: 0.10181777477264405\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.10181777477264405  to: 0.10171672105789184\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.10171672105789184  to: 0.10161505937576294\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.10161505937576294  to: 0.10151242017745972\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.10151242017745972  to: 0.10140857696533204\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.10140857696533204  to: 0.10130342245101928\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.10130342245101928  to: 0.10119720697402954\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.10119720697402954  to: 0.10108774900436401\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.10108774900436401  to: 0.10097672939300537\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.10097672939300537  to: 0.10086555480957031\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.10086555480957031  to: 0.10075496435165406\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.10075496435165406  to: 0.1006439208984375\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.1006439208984375  to: 0.10053300857543945\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.10053300857543945  to: 0.10042210817337036\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.10042210817337036  to: 0.10031073093414307\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.10031073093414307  to: 0.10019822120666504\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.10019822120666504  to: 0.10008451938629151\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.10008451938629151  to: 0.09996997714042663\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.09996997714042663  to: 0.09985567331314087\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.09985567331314087  to: 0.09974279403686523\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.09974279403686523  to: 0.09963270425796508\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.09963270425796508  to: 0.0995262622833252\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.0995262622833252  to: 0.09942353367805482\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.09942353367805482  to: 0.09932408332824708\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.09932408332824708  to: 0.0992268443107605\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.0992268443107605  to: 0.09913057088851929\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.09913057088851929  to: 0.09903457760810852\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.09903457760810852  to: 0.09893883466720581\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.09893883466720581  to: 0.09884370565414428\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.09884370565414428  to: 0.09874968528747559\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.09874968528747559  to: 0.0986572265625\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.0986572265625  to: 0.09856644868850709\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.09856644868850709  to: 0.09847682118415832\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.09847682118415832  to: 0.09838587641716004\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.09838587641716004  to: 0.09829322695732116\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.09829322695732116  to: 0.09819844365119934\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.09819844365119934  to: 0.09810136556625366\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.09810136556625366  to: 0.0980023205280304\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.0980023205280304  to: 0.09790166616439819\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.09790166616439819  to: 0.09779985547065735\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.09779985547065735  to: 0.09769706726074219\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.09769706726074219  to: 0.09759324193000793\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.09759324193000793  to: 0.09748802185058594\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.09748802185058594  to: 0.09738103151321412\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.09738103151321412  to: 0.09727259874343872\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.09727259874343872  to: 0.0971630871295929\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.0971630871295929  to: 0.0970528244972229\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.0970528244972229  to: 0.09694207906723022\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.09694207906723022  to: 0.0968309223651886\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.0968309223651886  to: 0.09672046899795532\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.09672046899795532  to: 0.09661067128181458\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.09661067128181458  to: 0.09650133848190308\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.09650133848190308  to: 0.09639244079589844\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.09639244079589844  to: 0.09628408551216125\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.09628408551216125  to: 0.09617626070976257\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.09617626070976257  to: 0.09606887698173523\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.09606887698173523  to: 0.0959617018699646\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.0959617018699646  to: 0.09585455060005188\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.09585455060005188  to: 0.09574716687202453\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.09574716687202453  to: 0.09563778638839722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.09563778638839722  to: 0.09552625417709351\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.09552625417709351  to: 0.09541279077529907\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.09541279077529907  to: 0.09529984593391419\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.09529984593391419  to: 0.09518780708312988\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.09518780708312988  to: 0.09507675170898437\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.09507675170898437  to: 0.09496453404426575\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.09496453404426575  to: 0.09485238194465637\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.09485238194465637  to: 0.09474138021469117\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.09474138021469117  to: 0.09463326334953308\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.09463326334953308  to: 0.0945292592048645\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.0945292592048645  to: 0.09442952275276184\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.09442952275276184  to: 0.09433522224426269\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.09433522224426269  to: 0.09424357414245606\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.09424357414245606  to: 0.09415231943130493\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.09415231943130493  to: 0.09405971765518188\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.09405971765518188  to: 0.0939676284790039\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.0939676284790039  to: 0.09387750625610351\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.09387750625610351  to: 0.09379057884216309\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.09379057884216309  to: 0.09370667338371277\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.09370667338371277  to: 0.09362140893936158\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.09362140893936158  to: 0.09352844953536987\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.09352844953536987  to: 0.09342896342277526\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.09342896342277526  to: 0.09332681894302368\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.09332681894302368  to: 0.09322293996810913\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.09322293996810913  to: 0.09311728477478028\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.09311728477478028  to: 0.09300925135612488\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.09300925135612488  to: 0.09289872050285339\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.09289872050285339  to: 0.09278672337532043\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.09278672337532043  to: 0.0926716148853302\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.0926716148853302  to: 0.09255515933036804\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.09255515933036804  to: 0.09243795275688171\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.09243795275688171  to: 0.09232196807861329\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.09232196807861329  to: 0.09220632314682006\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.09220632314682006  to: 0.09209051132202148\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.09209051132202148  to: 0.09197607040405273\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.09197607040405273  to: 0.09186350107192993\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.09186350107192993  to: 0.09175153970718383\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.09175153970718383  to: 0.09163557887077331\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.09163557887077331  to: 0.0915158748626709\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.0915158748626709  to: 0.09139328002929688\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.09139328002929688  to: 0.09127206802368164\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.09127206802368164  to: 0.09115131497383118\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.09115131497383118  to: 0.09102925062179565\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.09102925062179565  to: 0.09090545773506165\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.09090545773506165  to: 0.09078062772750854\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.09078062772750854  to: 0.09065536260604859\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.09065536260604859  to: 0.09052882194519044\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.09052882194519044  to: 0.09040002822875977\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.09040002822875977  to: 0.09026466608047486\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.09026466608047486  to: 0.09012397527694702\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.09012397527694702  to: 0.0899789035320282\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.0899789035320282  to: 0.0898330807685852\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.0898330807685852  to: 0.08968567848205566\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.08968567848205566  to: 0.08953670263290406\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.08953670263290406  to: 0.08938735127449035\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.08938735127449035  to: 0.0892381489276886\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.0892381489276886  to: 0.08908823728561402\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.08908823728561402  to: 0.08893694877624511\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.08893694877624511  to: 0.08878499865531922\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.08878499865531922  to: 0.08863309621810914\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.08863309621810914  to: 0.08848042488098144\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.08848042488098144  to: 0.08832700848579407\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.08832700848579407  to: 0.0881744384765625\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.0881744384765625  to: 0.08802434802055359\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.08802434802055359  to: 0.08788172602653503\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.08788172602653503  to: 0.08774617910385132\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.08774617910385132  to: 0.08761274218559265\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.08761274218559265  to: 0.08748058080673218\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.08748058080673218  to: 0.08734275698661804\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.08734275698661804  to: 0.08720199465751648\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.08720199465751648  to: 0.08705697059631348\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.08705697059631348  to: 0.08690682649612427\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.08690682649612427  to: 0.08675819635391235\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.08675819635391235  to: 0.08661220669746399\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.08661220669746399  to: 0.08646356463432311\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.08646356463432311  to: 0.08631345629692078\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.08631345629692078  to: 0.08616754412651062\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.08616754412651062  to: 0.08602360486984253\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.08602360486984253  to: 0.08587688207626343\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.08587688207626343  to: 0.0857251763343811\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.0857251763343811  to: 0.08557806015014649\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.08557806015014649  to: 0.08542956113815307\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.08542956113815307  to: 0.08527328372001648\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.08527328372001648  to: 0.08512131571769714\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.08512131571769714  to: 0.08496918678283691\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.08496918678283691  to: 0.08481439352035522\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.08481439352035522  to: 0.08466185331344604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.08466185331344604  to: 0.0845095157623291\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.0845095157623291  to: 0.08435386419296265\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.08435386419296265  to: 0.08420316576957702\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.08420316576957702  to: 0.08406036496162414\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.08406036496162414  to: 0.08392728567123413\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.08392728567123413  to: 0.08381902575492858\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.08381902575492858  to: 0.08371609449386597\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.08371609449386597  to: 0.08362134099006653\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.08362134099006653  to: 0.08353384733200073\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.08353384733200073  to: 0.08343990445137024\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.08343990445137024  to: 0.08334795236587525\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.08334795236587525  to: 0.08325392007827759\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.08325392007827759  to: 0.08314210176467896\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.08314210176467896  to: 0.08302865028381348\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.08302865028381348  to: 0.08288750648498536\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.08288750648498536  to: 0.08273747563362122\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.08273747563362122  to: 0.0825815200805664\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.0825815200805664  to: 0.08241138458251954\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.08241138458251954  to: 0.08224822878837586\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.08224822878837586  to: 0.08207155466079712\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.08207155466079712  to: 0.08191211819648743\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.08191211819648743  to: 0.081728994846344\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.081728994846344  to: 0.08157842755317687\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.08157842755317687  to: 0.08139551877975464\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.08139551877975464  to: 0.08128257989883422\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.08128257989883422  to: 0.08106295466423034\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.08106295466423034  to: 0.08105831146240235\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.08105831146240235  to: 0.08063507080078125\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.08103405833244323\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.08063507080078125  to: 0.08007115125656128\n",
      "Training iteration: 1557\n",
      "Validation loss (no improvement): 0.08068614006042481\n",
      "Training iteration: 1558\n",
      "Validation loss (no improvement): 0.08024047613143921\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.08007115125656128  to: 0.07960814237594604\n",
      "Training iteration: 1560\n",
      "Validation loss (no improvement): 0.08002599477767944\n",
      "Training iteration: 1561\n",
      "Validation loss (no improvement): 0.0797336220741272\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.07960814237594604  to: 0.07903605699539185\n",
      "Training iteration: 1563\n",
      "Validation loss (no improvement): 0.07926336526870728\n",
      "Training iteration: 1564\n",
      "Validation loss (no improvement): 0.07920588850975037\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.07903605699539185  to: 0.07851560115814209\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.078599613904953\n",
      "Training iteration: 1567\n",
      "Validation loss (no improvement): 0.07872658967971802\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.07851560115814209  to: 0.07812234163284301\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.07812234163284301  to: 0.07807772159576416\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.07826247811317444\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.07807772159576416  to: 0.0777039349079132\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.0777039349079132  to: 0.07755818367004394\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.07771459817886353\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.07755818367004394  to: 0.07719563841819763\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.07719563841819763  to: 0.07703412771224975\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.07716310024261475\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.07703412771224975  to: 0.07668477892875672\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.07668477892875672  to: 0.07662307024002075\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.07669233679771423\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.07662307024002075  to: 0.07622758746147155\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.0762685477733612\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.07622758746147155  to: 0.07611870169639587\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.07611870169639587  to: 0.0757172703742981\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.07582976818084716\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.0757172703742981  to: 0.07540531158447265\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.07540531158447265  to: 0.07527982592582702\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.07527982592582702  to: 0.07521417140960693\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.07521417140960693  to: 0.07481933236122132\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.07494357824325562\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.07481933236122132  to: 0.07445659637451171\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.07456008791923523\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.07445659637451171  to: 0.07415726780891418\n",
      "Training iteration: 1593\n",
      "Validation loss (no improvement): 0.07420268654823303\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.07415726780891418  to: 0.0738893449306488\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.07392607927322388\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.0738893449306488  to: 0.07357843518257141\n",
      "Training iteration: 1597\n",
      "Validation loss (no improvement): 0.07374362945556641\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.07357843518257141  to: 0.0731102168560028\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.07383915781974792\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.0731102168560028  to: 0.07246901392936707\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.07381088137626649\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.07246901392936707  to: 0.07222596406936646\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.07222596406936646  to: 0.0720319390296936\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.07290205955505372\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.0720319390296936  to: 0.07142235040664673\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.07152860760688781\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.07194595336914063\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.07142235040664673  to: 0.07072147130966186\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.07096902132034302\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.0711055040359497\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.07072147130966186  to: 0.07014327645301818\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.0705130934715271\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.07034252285957336\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.07014327645301818  to: 0.06967169046401978\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.0702170968055725\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.06975938081741333\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.06967169046401978  to: 0.06947414875030518\n",
      "Training iteration: 1618\n",
      "Validation loss (no improvement): 0.07012603878974914\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.06957798004150391\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.06990604996681213\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.0702275276184082\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.0696528434753418\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.07022237181663513\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.06947414875030518  to: 0.06943686604499817\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.06943686604499817  to: 0.06930004954338073\n",
      "Training iteration: 1626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.06930004954338073  to: 0.06916473507881164\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.06916473507881164  to: 0.06834580302238465\n",
      "Training iteration: 1628\n",
      "Validation loss (no improvement): 0.06866205930709839\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.06834580302238465  to: 0.06764155626296997\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.06797871589660645\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.06764155626296997  to: 0.06709759831428527\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.06736173033714295\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.06709759831428527  to: 0.0667033076286316\n",
      "Training iteration: 1634\n",
      "Validation loss (no improvement): 0.06706399321556092\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.0667033076286316  to: 0.06649900078773499\n",
      "Training iteration: 1636\n",
      "Validation loss (no improvement): 0.0672453761100769\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.06649900078773499  to: 0.06629555821418762\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.06805582046508789\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.06629555821418762  to: 0.06576983332633972\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.06855450868606568\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.06576983332633972  to: 0.06501513719558716\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.06513370871543885\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.0661420226097107\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.06501513719558716  to: 0.06351336240768432\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.06445298194885254\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.06395169496536254\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.06351336240768432  to: 0.06229297518730163\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.06356731653213502\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.06229297518730163  to: 0.062187212705612185\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.062187212705612185  to: 0.06130966544151306\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.06274067163467408\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.06130966544151306  to: 0.06093271374702454\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.06093271374702454  to: 0.06088265180587769\n",
      "Training iteration: 1654\n",
      "Validation loss (no improvement): 0.06166149377822876\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.06088265180587769  to: 0.059881079196929934\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.06061810851097107\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.059881079196929934  to: 0.059820395708084104\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.059820395708084104  to: 0.059138756990432736\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.0601487934589386\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.059138756990432736  to: 0.05857959985733032\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.05973080992698669\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.05857959985733032  to: 0.05830033421516419\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.05858423113822937\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.05830033421516419  to: 0.05806667804718017\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.05806667804718017  to: 0.05747252702713013\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.057942676544189456\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.05747252702713013  to: 0.05635277032852173\n",
      "Training iteration: 1668\n",
      "Validation loss (no improvement): 0.05834605693817139\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.05635277032852173  to: 0.05501052737236023\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.06004434823989868\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.05501052737236023  to: 0.05305892825126648\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.05362294912338257\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.05494896173477173\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.05305892825126648  to: 0.05069291591644287\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.051504874229431154\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.051915043592453004\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.05069291591644287  to: 0.048474574089050294\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.04883131086826324\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.050071752071380614\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.048474574089050294  to: 0.04723609089851379\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.047458347678184507\n",
      "Training iteration: 1682\n",
      "Validation loss (no improvement): 0.04913475513458252\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.04723609089851379  to: 0.046375769376754764\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.046375769376754764  to: 0.04631047248840332\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.04797305166721344\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.04631047248840332  to: 0.04560219645500183\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.045788058638572694\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.04721340537071228\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.04560219645500183  to: 0.044822126626968384\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.04549321532249451\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.04551277160644531\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.044822126626968384  to: 0.043976879119873045\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.046004295349121094\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.043976879119873045  to: 0.04371845126152039\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.044411516189575194\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.043848901987075806\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.04371845126152039  to: 0.04295859932899475\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.044374093413352966\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.04295859932899475  to: 0.04202782511711121\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.04612414836883545\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.04202782511711121  to: 0.041754788160324095\n",
      "Training iteration: 1702\n",
      "Validation loss (no improvement): 0.048298388719558716\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.041754788160324095  to: 0.03885306715965271\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.03885306715965271  to: 0.038185220956802365\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.04319678246974945\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.038185220956802365  to: 0.03671301007270813\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.03671301007270813  to: 0.03622608482837677\n",
      "Training iteration: 1708\n",
      "Validation loss (no improvement): 0.039627328515052795\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.03622608482837677  to: 0.03564839065074921\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.03564839065074921  to: 0.0352083295583725\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.03659304678440094\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.0365713357925415\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.0352083295583725  to: 0.034677475690841675\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.034726384282112124\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.03658339977264404\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.034677475690841675  to: 0.03425248563289642\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.03425248563289642  to: 0.03381034731864929\n",
      "Training iteration: 1718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.035037532448768616\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.034474903345108034\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.03381034731864929  to: 0.033376994729042056\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.03382737040519714\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.03410658240318298\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.033376994729042056  to: 0.03272248208522797\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.03307962119579315\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.03359637856483459\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.03272248208522797  to: 0.03238411843776703\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.03337109088897705\n",
      "Training iteration: 1728\n",
      "Validation loss (no improvement): 0.032531163096427916\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.03238411843776703  to: 0.031908005475997925\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.03345631659030914\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.031908005475997925  to: 0.03167874217033386\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.033484351634979245\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.03167874217033386  to: 0.031359356641769406\n",
      "Training iteration: 1734\n",
      "Validation loss (no improvement): 0.03301261365413666\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.031359356641769406  to: 0.030923444032669067\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.03308743238449097\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.030923444032669067  to: 0.030399414896965026\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.03531373143196106\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.03064400851726532\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.037573319673538205\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.030399414896965026  to: 0.028595361113548278\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.028595361113548278  to: 0.028424611687660216\n",
      "Training iteration: 1743\n",
      "Validation loss (no improvement): 0.03583088517189026\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.028424611687660216  to: 0.027985304594039917\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.027985304594039917  to: 0.027106708288192748\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.030782976746559144\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.027106708288192748  to: 0.026867049932479858\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.026867049932479858  to: 0.025795572996139528\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.02801557183265686\n",
      "Training iteration: 1750\n",
      "Validation loss (no improvement): 0.028188616037368774\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.025795572996139528  to: 0.025722438097000123\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.026671284437179567\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.028931286931037904\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.025722438097000123  to: 0.02566770613193512\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.02566770613193512  to: 0.025470936298370363\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.028000280261039734\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.025586429238319396\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.025470936298370363  to: 0.024797311425209044\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.02692929804325104\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.024797311425209044  to: 0.024630291759967803\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.024630291759967803  to: 0.024139776825904846\n",
      "Training iteration: 1762\n",
      "Validation loss (no improvement): 0.026710033416748047\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.024139776825904846  to: 0.023812849819660187\n",
      "Training iteration: 1764\n",
      "Validation loss (no improvement): 0.025268632173538207\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.02493010014295578\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.023812849819660187  to: 0.02328663319349289\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.026521843671798707\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.02328663319349289  to: 0.022487464547157287\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.03126373887062073\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.023191413283348082\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.04071435332298279\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.022487464547157287  to: 0.02127929627895355\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.021324172616004944\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.032186192274093625\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.02127929627895355  to: 0.02094026505947113\n",
      "Training iteration: 1776\n",
      "Validation loss (no improvement): 0.021216049790382385\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.024938568472862244\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.026531249284744263\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.022066378593444826\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.022513222694396973\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.02362765520811081\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.026948627829551697\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.024932336807250977\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.023688893020153045\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.023788662254810335\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.02529250383377075\n",
      "Training iteration: 1787\n",
      "Validation loss (no improvement): 0.026309728622436523\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.02395918667316437\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.022995670139789582\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.022851195931434632\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.024036233127117158\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.023461250960826872\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.021710805594921112\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.021224911510944366\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.021802346408367156\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.021993395686149598\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.02094026505947113  to: 0.020100304484367372\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.020100304484367372  to: 0.019566191732883452\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.020606806874275206\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.02046632468700409\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.019566191732883452  to: 0.018882249295711518\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.019155482947826385\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.020290549099445342\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.018882249295711518  to: 0.01847541332244873\n",
      "Training iteration: 1805\n",
      "Validation loss (no improvement): 0.01879523992538452\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.020680613815784454\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.018874135613441468\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.020294913649559022\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.02058854103088379\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.019289539754390718\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.02267848700284958\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.019078148901462554\n",
      "Training iteration: 1813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.025571492314338685\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.018847624957561492\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.034210997819900515\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.020725443959236145\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.030647745728492735\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.01847541332244873  to: 0.01824124753475189\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.01824124753475189  to: 0.01635085642337799\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.020665378868579866\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.023224537074565888\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.01644839495420456\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.016708533465862273\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.019640275835990907\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.022255127131938935\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.018722791969776154\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.01847051680088043\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.019124403595924377\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.02190864384174347\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.021562115848064424\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.019231812655925752\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.018819794058799744\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.019115015864372253\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.020520496368408202\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.01947605311870575\n",
      "Training iteration: 1836\n",
      "Validation loss (no improvement): 0.017889606952667236\n",
      "Training iteration: 1837\n",
      "Validation loss (no improvement): 0.017513848841190338\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.01802487373352051\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.0186087965965271\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.016880400478839874\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.01635085642337799  to: 0.015840975940227507\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.015884406864643097\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.016833457350730895\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.015840975940227507  to: 0.015616466104984284\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.015616466104984284  to: 0.014571775496006013\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.014884595572948457\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.01571417897939682\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.014571775496006013  to: 0.014151248335838317\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.014151248335838317  to: 0.013611359894275666\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.014873723685741424\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.014405378699302673\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.013611359894275666  to: 0.013486987352371216\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.014893223345279694\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.014406581223011018\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.013486987352371216  to: 0.013268858194351196\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.015083310008049012\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.013583731651306153\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.014014388620853423\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.014970110356807708\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.013268858194351196  to: 0.01304536908864975\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.01607040911912918\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.01304536908864975  to: 0.012275371700525284\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.01874421536922455\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.012275371700525284  to: 0.011551403999328613\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.028154593706130982\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.01408371925354004\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.029255163669586182\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.011551403999328613  to: 0.009842665493488311\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.009842665493488311  to: 0.009617098420858384\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.0166957288980484\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.017139777541160583\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.010651731491088867\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.011106915771961212\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.01439790427684784\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.01738213747739792\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.013738641142845153\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.01355176866054535\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.014028404653072358\n",
      "Training iteration: 1879\n",
      "Validation loss (no improvement): 0.016337350010871887\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.01757764220237732\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.015414278209209441\n",
      "Training iteration: 1882\n",
      "Validation loss (no improvement): 0.014666542410850525\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.014571651816368103\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.015250256657600403\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.01618334949016571\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.014903606474399566\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.013903319835662842\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.013661965727806091\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.013830050826072693\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.014425721764564515\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.013282331824302673\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.012113623321056366\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.01165868490934372\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.011951003223657608\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.012055711448192596\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.010765312612056733\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.010166908800601959\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.010274939239025116\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.010676513612270355\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.009617098420858384  to: 0.009426060318946838\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.009426060318946838  to: 0.008691124618053436\n",
      "Training iteration: 1902\n",
      "Validation loss (no improvement): 0.00912514328956604\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.009438839554786683\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.008691124618053436  to: 0.0082189179956913\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.00822722539305687\n",
      "Training iteration: 1906\n",
      "Validation loss (no improvement): 0.009249977022409438\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.0082189179956913  to: 0.008006107062101364\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.008006107062101364  to: 0.0076586328446865085\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.009047712385654449\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.007960446178913116\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.00812850147485733\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.00948706716299057\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.007802777737379074\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.009414563328027726\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.008687552809715272\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.008959811925888062\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.009931297600269317\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.008238673955202103\n",
      "Training iteration: 1919\n",
      "Validation loss (no improvement): 0.011261818557977676\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.0076586328446865085  to: 0.007116710394620895\n",
      "Training iteration: 1921\n",
      "Validation loss (no improvement): 0.0164670467376709\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.007420700788497925\n",
      "Training iteration: 1923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03883098065853119\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.015001365542411804\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.013877122104167939\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.0218440979719162\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.007116710394620895  to: 0.006374673545360565\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.008113742619752885\n",
      "Training iteration: 1929\n",
      "Validation loss (no improvement): 0.012440229952335357\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.01991164982318878\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.012709894776344299\n",
      "Training iteration: 1932\n",
      "Validation loss (no improvement): 0.011928664147853851\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.013575875759124756\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.014072069525718689\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.017876407504081725\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.020358307659626006\n",
      "Training iteration: 1937\n",
      "Validation loss (no improvement): 0.018810662627220153\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.017771057784557343\n",
      "Training iteration: 1939\n",
      "Validation loss (no improvement): 0.018719194829463957\n",
      "Training iteration: 1940\n",
      "Validation loss (no improvement): 0.019165053963661194\n",
      "Training iteration: 1941\n",
      "Validation loss (no improvement): 0.01904556304216385\n",
      "Training iteration: 1942\n",
      "Validation loss (no improvement): 0.019907622039318083\n",
      "Training iteration: 1943\n",
      "Validation loss (no improvement): 0.02093552052974701\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.020541286468505858\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.01939358711242676\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.018901751935482027\n",
      "Training iteration: 1947\n",
      "Validation loss (no improvement): 0.018814729154109956\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.018392305076122283\n",
      "Training iteration: 1949\n",
      "Validation loss (no improvement): 0.01805783212184906\n",
      "Training iteration: 1950\n",
      "Validation loss (no improvement): 0.018174147605895995\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.017963503301143647\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.017023760080337524\n",
      "Training iteration: 1953\n",
      "Validation loss (no improvement): 0.016132056713104248\n",
      "Training iteration: 1954\n",
      "Validation loss (no improvement): 0.01562248021364212\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.014985206723213195\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.014334638416767121\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.014043119549751282\n",
      "Training iteration: 1958\n",
      "Validation loss (no improvement): 0.01353420615196228\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.012490630149841309\n",
      "Training iteration: 1960\n",
      "Validation loss (no improvement): 0.011573681980371476\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.010926277935504913\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.01036854162812233\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.010135426372289657\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.009745294600725174\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.008804802596569062\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.007954287528991699\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.007345259934663772\n",
      "Training iteration: 1968\n",
      "Validation loss (no improvement): 0.007044117152690888\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.006849338114261627\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.006374673545360565  to: 0.00601128451526165\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.00601128451526165  to: 0.005116066336631775\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.005116066336631775  to: 0.004696400836110115\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.004788660258054733\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.004696400836110115  to: 0.004612737894058227\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.004612737894058227  to: 0.0036742068827152253\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.0036742068827152253  to: 0.0030618006363511085\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.0031786978244781492\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.003286801278591156\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.0030618006363511085  to: 0.0024703968316316604\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.0024703968316316604  to: 0.0020588360726833343\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.002592880465090275\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.0026213547214865686\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.0020588360726833343  to: 0.0017394864931702613\n",
      "Training iteration: 1984\n",
      "Validation loss (no improvement): 0.0018467012792825698\n",
      "Training iteration: 1985\n",
      "Validation loss (no improvement): 0.0024996785447001456\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.0018184348940849305\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.0017394864931702613  to: 0.0016940994188189506\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.0025561902672052383\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.001969909109175205\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.0016940994188189506  to: 0.0016706999391317367\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.0024564674124121664\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.0017614107578992843\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.0017730994150042534\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.0023565890267491342\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.0016706999391317367  to: 0.0014210222288966179\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.0019121486693620682\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.0017037490382790566\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.0014210222288966179  to: 0.0012709936127066612\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.0019287103787064552\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.0012709936127066612  to: 0.0009630006738007068\n",
      "Training iteration: 2001\n",
      "Validation loss (no improvement): 0.0015892090275883674\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.0009630006738007068  to: 0.0008584439754486084\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.001140540838241577\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.0008830225095152854\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.0008584439754486084  to: 0.0007039998658001423\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.0007605480961501599\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.0007039998658001423  to: 0.00019207119476050138\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.0007050767540931702\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.00019207119476050138  to: -0.000372602348215878\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.000981895811855793\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: -0.000372602348215878  to: -0.0013855218887329102\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.002663607522845268\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: -0.0013855218887329102  to: -0.0025814762338995934\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.011959178745746613\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.003284651041030884\n",
      "Training iteration: 2016\n",
      "Validation loss (no improvement): 0.03558704555034638\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): -0.0018338065594434739\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: -0.0025814762338995934  to: -0.005232694000005722\n",
      "Training iteration: 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.016051843762397766\n",
      "Training iteration: 2020\n",
      "Validation loss (no improvement): 0.0008544081822037697\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.0005053545348346234\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): -0.0004452554974704981\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.0075137302279472355\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.010982568562030792\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.00459604561328888\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.004775211960077286\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.005991248041391372\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.007279008626937866\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.01132548600435257\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.012557703256607055\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.01047964096069336\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.009920042008161545\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.010566816478967667\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.010490845143795013\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.011154403537511825\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.012715601921081543\n",
      "Training iteration: 2037\n",
      "Validation loss (no improvement): 0.012751547992229462\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.011217150837182999\n",
      "Training iteration: 2039\n",
      "Validation loss (no improvement): 0.010271736234426499\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.010116109997034073\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.00978270173072815\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.009711512178182603\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.010081608593463898\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.009720461815595627\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.00855187401175499\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.0077073536813259125\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.0072151079773902895\n",
      "Training iteration: 2048\n",
      "Validation loss (no improvement): 0.006740090250968933\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.006707216799259186\n",
      "Training iteration: 2050\n",
      "Validation loss (no improvement): 0.006682725995779038\n",
      "Training iteration: 2051\n",
      "Validation loss (no improvement): 0.005843058973550796\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.004832971841096878\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.00422729030251503\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.0037702109664678575\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.003636987879872322\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.0033859588205814362\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.0023862358182668688\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.0013607179746031762\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.000726247439160943\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.0005271583795547485\n",
      "Training iteration: 2061\n",
      "Validation loss (no improvement): 0.0005088961217552424\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): -0.00024166293442249299\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): -0.0011970203369855881\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): -0.001652265526354313\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): -0.0015213711187243462\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): -0.0015486998483538628\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): -0.002403658255934715\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): -0.003045783005654812\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): -0.0029574329033493995\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): -0.0026570329442620277\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): -0.0032515030354261397\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): -0.0038451574742794036\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): -0.003632504865527153\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): -0.003311338275671005\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): -0.003912439197301864\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): -0.004134076088666916\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): -0.0034827645868062973\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): -0.0034213699400424957\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): -0.003896895796060562\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): -0.0034616798162460326\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): -0.0030833184719085693\n",
      "Training iteration: 2082\n",
      "Validation loss (no improvement): -0.0036009132862091066\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): -0.0032400164753198625\n",
      "Training iteration: 2084\n",
      "Validation loss (no improvement): -0.0026986077427864075\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): -0.0031465012580156326\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): -0.002709968015551567\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): -0.0023087244480848312\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): -0.0026512166485190393\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): -0.0019032860174775124\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): -0.0018467484042048454\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): -0.0018708571791648866\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): -0.001177856046706438\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): -0.0015882520005106927\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): -0.0009530383162200451\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): -0.0010189592838287353\n",
      "Training iteration: 2096\n",
      "Validation loss (no improvement): -0.0009071843698620797\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): -0.000538725545629859\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): -0.0008784031495451927\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): -0.0001296257949434221\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): -0.000774396350607276\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.00031882480252534153\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): -0.0010398447513580323\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.0006540763191878795\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): -0.0017994830384850502\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.0023429917171597483\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): -0.0026076680049300196\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.009753607213497162\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.001197170652449131\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.029502132534980775\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.000817779265344143\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): -0.0032801683992147446\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.01092979907989502\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): -0.005052508786320686\n",
      "Training iteration: 2114\n",
      "Validation loss (no improvement): -0.004403167590498924\n",
      "Training iteration: 2115\n",
      "Validation loss (no improvement): 0.0016009822487831117\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.004119403287768364\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): -0.0035461146384477616\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): -0.0021973604336380957\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): -0.0018771108239889144\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.003446958586573601\n",
      "Training iteration: 2121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0036712724715471266\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.0005087214522063732\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.001198458857834339\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.0014013566076755523\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.0028792086988687514\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.005206798389554024\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.00409286767244339\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.0026842903345823286\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.002994544431567192\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.002931109629571438\n",
      "Training iteration: 2131\n",
      "Validation loss (no improvement): 0.0033855341374874116\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.004189098626375199\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.0034884069114923476\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.002723327837884426\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.00266391821205616\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.0022716090083122254\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.0023464515805244448\n",
      "Training iteration: 2138\n",
      "Validation loss (no improvement): 0.002550002001225948\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.001677650585770607\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.0009215069934725761\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.0005862732417881489\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.0001744854496791959\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.00018529966473579407\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): -0.0001852095127105713\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): -0.0010019523091614247\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): -0.001442878507077694\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): -0.001804007962346077\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): -0.0018065202981233596\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): -0.0020668542012572287\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): -0.002846868894994259\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): -0.003315858542919159\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): -0.0035610925406217575\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): -0.0034942768514156343\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): -0.0039441011846065525\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): -0.00460333377122879\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): -0.004865102842450142\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): -0.004701751470565796\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): -0.0048115067183971405\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: -0.005232694000005722  to: -0.005469929054379463\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: -0.005469929054379463  to: -0.005750157311558724\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): -0.005514109879732132\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): -0.005574464797973633\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: -0.005750157311558724  to: -0.0061743855476379395\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: -0.0061743855476379395  to: -0.006215549260377884\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): -0.005729924514889717\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): -0.005982840806245804\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: -0.006215549260377884  to: -0.0064294286072254184\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): -0.0061028383672237395\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): -0.005885925143957138\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): -0.006426752358675003\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): -0.006248198822140694\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): -0.005720746517181396\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): -0.006189316511154175\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): -0.006156155467033386\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): -0.005596597865223885\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): -0.006020859628915787\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): -0.005876076966524124\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): -0.005293475463986397\n",
      "Training iteration: 2179\n",
      "Validation loss (no improvement): -0.005731289833784103\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): -0.0053926117718219755\n",
      "Training iteration: 2181\n",
      "Validation loss (no improvement): -0.005082292109727859\n",
      "Training iteration: 2182\n",
      "Validation loss (no improvement): -0.005474289134144783\n",
      "Training iteration: 2183\n",
      "Validation loss (no improvement): -0.0048472266644239426\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): -0.005008733272552491\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): -0.00501948706805706\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): -0.004546523094177246\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): -0.005006985738873482\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): -0.004441911727190018\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): -0.004723687842488289\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): -0.004570174962282181\n",
      "Training iteration: 2191\n",
      "Validation loss (no improvement): -0.004407158493995667\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): -0.00466233566403389\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): -0.0041369423270225525\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): -0.004656897112727165\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): -0.003969865292310715\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): -0.004685458540916443\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): -0.003801039606332779\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): -0.004752687737345695\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): -0.0034147176891565325\n",
      "Training iteration: 2200\n",
      "Validation loss (no improvement): -0.005009878799319268\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): -0.00253925621509552\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): -0.005637869983911514\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.00012717380886897444\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): -0.00605335421860218\n",
      "Training iteration: 2205\n",
      "Validation loss (no improvement): 0.009569627046585084\n",
      "Training iteration: 2206\n",
      "Validation loss (no improvement): -0.0014799440279603005\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.025192198157310487\n",
      "Training iteration: 2208\n",
      "Validation loss (no improvement): -0.005991232395172119\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: -0.0064294286072254184  to: -0.007990604639053345\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.008432550728321076\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: -0.007990604639053345  to: -0.008277037739753723\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): -0.0075834766030311584\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): -0.0008839553222060204\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.0013555636629462241\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): -0.006385194510221482\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): -0.005588309094309807\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): -0.0038477133959531782\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.001505851186811924\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): -0.001230333186686039\n",
      "Training iteration: 2220\n",
      "Validation loss (no improvement): -0.0030128825455904007\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): -0.0023464033380150794\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): -0.0012050926685333251\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.001710517331957817\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.000695501547306776\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): -0.001047044713050127\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): -0.0007969355210661888\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): -0.0007907879538834095\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.00039227008819580077\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.0008292457088828087\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): -0.0004055386874824762\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): -0.0007355257868766785\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): -0.0008682671003043651\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): -0.0005738899111747741\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): -9.617179748602211e-05\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): -0.001087296288460493\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): -0.0018907766789197922\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): -0.0021679844707250596\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): -0.002213459275662899\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): -0.0018239259719848634\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): -0.0024223852902650832\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): -0.0031566254794597624\n",
      "Training iteration: 2242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.003459993749856949\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): -0.0034247443079948427\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): -0.003296081721782684\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): -0.004095948487520218\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): -0.004723756760358811\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): -0.004898141697049141\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): -0.004573193937540054\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): -0.004804059863090515\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): -0.00553879626095295\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): -0.0057757187634706495\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): -0.005470528081059456\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): -0.005532970279455185\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): -0.006286109983921051\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): -0.006496171653270722\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): -0.006026027724146843\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): -0.00606755018234253\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): -0.006679902225732804\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): -0.006581089645624161\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): -0.0060512196272611615\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): -0.006523870676755905\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): -0.0069064699113368985\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): -0.0063555754721164705\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): -0.006257262080907822\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): -0.006777000427246094\n",
      "Training iteration: 2266\n",
      "Validation loss (no improvement): -0.006346990913152694\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): -0.005959381908178329\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): -0.006541889905929565\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): -0.006218916177749634\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): -0.005715049058198929\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): -0.006203752756118774\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): -0.005762193724513054\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): -0.005359538644552231\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): -0.005837569385766983\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): -0.005232549458742142\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): -0.005156712606549263\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): -0.005391839891672134\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): -0.004662974923849106\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): -0.005038986727595329\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): -0.004768232256174088\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): -0.004443291574716568\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): -0.004810630157589912\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): -0.004133833944797516\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): -0.0045599061995744705\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): -0.004132185131311417\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): -0.0041355274617671965\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): -0.004166219383478165\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): -0.0037682946771383286\n",
      "Training iteration: 2289\n",
      "Validation loss (no improvement): -0.004182147607207298\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): -0.0035042606294155123\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): -0.004131671413779259\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): -0.0031912975013256074\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): -0.004117421433329582\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): -0.0027784425765275955\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): -0.0043283142149448395\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): -0.0019152130931615829\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): -0.004858386516571045\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.00048378556966781615\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): -0.005533631518483162\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.008512720465660095\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): -0.002413123846054077\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.027333661913871765\n",
      "Training iteration: 2303\n",
      "Validation loss (no improvement): -0.0015753250569105147\n",
      "Training iteration: 2304\n",
      "Validation loss (no improvement): -0.001481387484818697\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.005070178583264351\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): -0.007242623716592789\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): -0.007916684448719024\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): 0.004982708767056465\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): -0.0025789204984903334\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): -0.006365495920181275\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): -0.005916227772831917\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): -0.0004176488611847162\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.0017971906810998917\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): -0.002927473187446594\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): -0.002682563103735447\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): -0.002156778052449226\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): 0.0008415764197707177\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.0027113500982522964\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.00011443197727203369\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): -0.0007169491145759821\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): -0.0005039848387241363\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): -5.092978244647384e-05\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.0015255552716553211\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.0011438190937042235\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): -3.9730669232085344e-05\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): -9.80696058832109e-05\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): -0.00017861246597021817\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.000451467651873827\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.0007621133234351873\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): -0.0004537832923233509\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): -0.0012210307642817498\n",
      "Training iteration: 2332\n",
      "Validation loss (no improvement): -0.001504804939031601\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): -0.0014779919758439064\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): -0.0010292941704392433\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): -0.0015996968373656272\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): -0.0023876484483480453\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): -0.0027129331603646277\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): -0.0027581388130784035\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): -0.002533676102757454\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): -0.0031875405460596085\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): -0.003976205736398697\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): -0.0042697668075561525\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): -0.004066199064254761\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): -0.003903428465127945\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): -0.004616445302963257\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): -0.005134117603302002\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): -0.005124968290328979\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): -0.004811402410268784\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): -0.005308997631072998\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): -0.00593922957777977\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): -0.00589054599404335\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): -0.0053903419524431225\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): -0.005717235803604126\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): -0.0062665179371833805\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): -0.006076579540967941\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): -0.005661780759692192\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): -0.006207946687936783\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): -0.006537928432226181\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): -0.0059843618422746655\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): -0.005838809162378311\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): -0.006376601755619049\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): -0.006088227033615112\n",
      "Training iteration: 2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.00557226650416851\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): -0.006080559641122818\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): -0.006023908779025078\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): -0.005316890403628349\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): -0.005624379962682724\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): -0.00562964491546154\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): -0.00492587611079216\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): -0.005246804282069206\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): -0.0052081149071455\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): -0.004534346982836723\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): -0.004880479723215103\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): -0.0046071458607912065\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): -0.0041367806494236\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): -0.004546351358294487\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): -0.004010358452796936\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): -0.0039700020104646684\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): -0.004118989780545234\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): -0.0035112719982862473\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): -0.003876601159572601\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): -0.0034745905548334123\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): -0.003401702269911766\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): -0.0035444535315036774\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): -0.0030495617538690567\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): -0.0034355919808149336\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): -0.0028684332966804503\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): -0.0031317859888076783\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): -0.0027985548600554465\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): -0.002826834097504616\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): -0.0027770008891820908\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): -0.002533956244587898\n",
      "Training iteration: 2393\n",
      "Validation loss (no improvement): -0.0026885509490966798\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): -0.002220003493130207\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): -0.0026291454210877418\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): -0.0018876265734434129\n",
      "Training iteration: 2397\n",
      "Validation loss (no improvement): -0.0026728296652436256\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): -0.0013140295632183552\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): -0.0029275501146912575\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): -4.864111542701721e-05\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): -0.0036632083356380463\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.0037372447550296783\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): -0.003985067456960678\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.01816360205411911\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.004360273480415344\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.03854677081108093\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): -0.0065295003354549404\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: -0.008277037739753723  to: -0.008569557219743729\n",
      "Training iteration: 2409\n",
      "Validation loss (no improvement): 0.014386892318725586\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): -0.003914706781506539\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): -0.005023925378918648\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): -0.0052770238369703295\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.006741271913051605\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.0038279835134744644\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): -0.003136467933654785\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): -0.0019287656992673873\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): -0.0011969747953116893\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.0039021957665681837\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.006206797435879707\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.0028345491737127304\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.0018231580033898354\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.0024886315688490866\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.002839260920882225\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.004864717647433281\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.006107707694172859\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.004569686204195023\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.003363049775362015\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.0034988824278116225\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.00341213122010231\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.0038259543478488924\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.0048607498407363895\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.004582018405199051\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.0033244211226701736\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.002715756930410862\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.0023893430829048158\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.002109801210463047\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.0023688990622758865\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.0022665876895189285\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.0013650056906044483\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.0006954902317374944\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.0003583559300750494\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.00012497976422309875\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.0002257102634757757\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): -4.4601858826354145e-05\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): -0.000900370441377163\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): -0.0015293863601982594\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): -0.001900290884077549\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): -0.0019816720858216287\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): -0.0019216710701584817\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): -0.0024449413642287253\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): -0.00314614400267601\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): -0.003535713627934456\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): -0.0036087196320295336\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): -0.0035314328968524935\n",
      "Training iteration: 2455\n",
      "Validation loss (no improvement): -0.003983132168650627\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): -0.004623604565858841\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): -0.00490478090941906\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): -0.004773584380745888\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): -0.004713230580091476\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): -0.005256965011358261\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): -0.005718130990862847\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): -0.005679609626531601\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): -0.005407150462269783\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): -0.005712733417749405\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): -0.006223857402801514\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): -0.0062077570706605915\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): -0.005850566178560257\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): -0.006055586412549019\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): -0.006477890163660049\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): -0.006292631477117538\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): -0.005865685269236564\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): -0.006115858629345894\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): -0.006350304931402206\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): -0.00592135414481163\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): -0.0056682489812374115\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): -0.006001008674502373\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): -0.005837293714284897\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): -0.005315925553441047\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): -0.00547557957470417\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): -0.005547100305557251\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): -0.005012804269790649\n",
      "Training iteration: 2482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.004963615536689758\n",
      "Training iteration: 2483\n",
      "Validation loss (no improvement): -0.005129287391901017\n",
      "Training iteration: 2484\n",
      "Validation loss (no improvement): -0.00464794859290123\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): -0.004486803710460663\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): -0.004677171632647514\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): -0.004255220293998718\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): -0.004103277996182442\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): -0.004279934614896774\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): -0.0038592182099819183\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): -0.003760325163602829\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): -0.0038707315921783446\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): -0.0034592468291521072\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): -0.0035051513463258744\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): -0.00351361520588398\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): -0.0031299613416194915\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): -0.0032438091933727265\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): -0.0030419111251831056\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): -0.002790907584130764\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): -0.0029155749827623366\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): -0.00258316732943058\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): -0.0025526231154799463\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): -0.0024891819804906843\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): -0.0021814445033669473\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): -0.00228036530315876\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): -0.0020079389214515686\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): -0.001958346739411354\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): -0.001917198672890663\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): -0.001693500764667988\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): -0.0018088718876242637\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): -0.001557951606810093\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): -0.0016056226566433906\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): -0.0014518323354423047\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): -0.001362930703908205\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): -0.0013636112213134765\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): -0.0011732247658073902\n",
      "Training iteration: 2517\n",
      "Validation loss (no improvement): -0.0012557429261505603\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): -0.0009845681488513947\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): -0.0010940350592136383\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): -0.0007929926738142967\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): -0.0009517441503703594\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): -0.0005971124861389399\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): -0.0008204598911106586\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): -0.0003000038908794522\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): -0.0007251341827213764\n",
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.00015607357490807772\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): -0.0008356462232768536\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.0010338112711906432\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): -0.0013887939043343068\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.003574512526392937\n",
      "Training iteration: 2531\n",
      "Validation loss (no improvement): -0.002332105301320553\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.014105015993118286\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.003645182028412819\n",
      "Training iteration: 2534\n",
      "Validation loss (no improvement): 0.05335544347763062\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.013733167946338654\n",
      "Training iteration: 2536\n",
      "Validation loss (no improvement): 0.002967677637934685\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.022540935873985292\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): -0.005878983065485954\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): -0.002851678989827633\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.0022572891786694525\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.018154655396938325\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.004972332715988159\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): -0.0002490511629730463\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.002326681092381477\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.002531575784087181\n",
      "Training iteration: 2546\n",
      "Validation loss (no improvement): 0.008442042768001557\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.013792461156845093\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.011372828483581543\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.00785115510225296\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.008281935006380081\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.009441961348056794\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.009555773437023162\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.010863101482391358\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.013392822444438934\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.014307007193565369\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.012847107648849488\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.011281432956457138\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.011088430881500244\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.011339670419692994\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.011104140430688858\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.011001056432724\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.011589524894952774\n",
      "Training iteration: 2563\n",
      "Validation loss (no improvement): 0.01204637885093689\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.011464748531579971\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.010302622616291047\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.009545373171567917\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.009229413419961929\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.008796338737010957\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.008299307525157928\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.008173210918903351\n",
      "Training iteration: 2571\n",
      "Validation loss (no improvement): 0.008203662931919098\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.007774475961923599\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.006906969845294953\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.006141399219632149\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.005612558126449585\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.005086345225572586\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.004614212363958359\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.004335242509841919\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.003993606194853783\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.0033634811639785766\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.002675028145313263\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.002133641205728054\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.001652204617857933\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.001251080073416233\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): 0.000983100850135088\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.0006040838547050953\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): -1.1444836854934692e-05\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): -0.0006153449416160583\n",
      "Training iteration: 2589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.00106896935030818\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): -0.0014024173840880394\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): -0.001599937118589878\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): -0.0018474245443940163\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): -0.0023555872961878777\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): -0.002946973405778408\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): -0.003400793671607971\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): -0.003671662136912346\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): -0.0037897627800703047\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): -0.004004233703017235\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): -0.004447328299283982\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): -0.004891010373830796\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): -0.005159366875886917\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): -0.005252972990274429\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): -0.00539228692650795\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): -0.005774557590484619\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): -0.006181907653808594\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): -0.006357679516077042\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): -0.006309659034013748\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): -0.006336673349142075\n",
      "Training iteration: 2609\n",
      "Validation loss (no improvement): -0.00663662999868393\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): -0.006930936872959137\n",
      "Training iteration: 2611\n",
      "Validation loss (no improvement): -0.006961420178413391\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): -0.006835983693599701\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): -0.006906047463417053\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): -0.007145149260759353\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): -0.007191980630159378\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): -0.00699542760848999\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): -0.006914349645376206\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): -0.007092859596014023\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): -0.007160063087940216\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): -0.0069523915648460385\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): -0.006802114099264145\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): -0.006910504400730133\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): -0.0069270037114620205\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): -0.006693004071712494\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): -0.006558537483215332\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): -0.006651864945888519\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): -0.0065935090184211734\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): -0.006332038342952729\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): -0.006248750537633896\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): -0.006309688091278076\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): -0.006165485829114914\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): -0.0059615403413772585\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): -0.005986797064542771\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): -0.0059766583144664764\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): -0.005760208517313003\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): -0.005658550187945366\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): -0.005695860460400581\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): -0.0055592071264982225\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): -0.005389147251844406\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): -0.0054040074348449705\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): -0.005338436365127564\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): -0.0051471501588821415\n",
      "Training iteration: 2643\n",
      "Validation loss (no improvement): -0.005111617594957351\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): -0.005098460987210274\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): -0.004934276640415192\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): -0.004857273772358894\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): -0.004852158948779106\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): -0.004704026505351067\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): -0.004593639448285103\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): -0.004584603756666183\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): -0.00446338951587677\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): -0.004346216097474098\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): -0.004330393671989441\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): -0.004218827188014984\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): -0.004092640429735184\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): -0.004055772349238396\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): -0.003934384137392044\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): -0.0037998251616954805\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): -0.0037453919649124146\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): -0.0036155618727207184\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): -0.003483995422720909\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): -0.0034169744700193404\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): -0.0032740723341703414\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): -0.003150412440299988\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): -0.0030738269910216332\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): -0.0029284343123435973\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): -0.00282170120626688\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): -0.0027297938242554663\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): -0.002579626441001892\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): -0.0024819515645504\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): -0.0023678354918956757\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): -0.002225911431014538\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): -0.002133898623287678\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): -0.0019970770925283434\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): -0.001869039051234722\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): -0.001762850396335125\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): -0.0016118451952934266\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): -0.0014981361106038093\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): -0.0013608746230602264\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): -0.0012109583243727685\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): -0.0010912487283349036\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): -0.0009289865382015705\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): -0.0008022460155189037\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): -0.0006539225578308105\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): -0.00051558674313128\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): -0.00041302843019366267\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): -0.00028176784981042146\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): -0.00017783448565751315\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): -2.943679573945701e-05\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 9.65055834967643e-05\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.000228358362801373\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.00034620806109160186\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.0004544965922832489\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.0005867819301784039\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.0007001514546573162\n",
      "Training iteration: 2696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0008480439893901349\n",
      "Training iteration: 2697\n",
      "Validation loss (no improvement): 0.0009459191933274269\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.0011006219312548638\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.0011837196536362172\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.001384707074612379\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.0014313841238617897\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.0017105910927057266\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.0016221342608332633\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.002122180536389351\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.0016665033996105193\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.002893189713358879\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.0012699978426098824\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.005150661990046501\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.00016391396056860685\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.016662809252738952\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.009314751625061036\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.08480933904647828\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.04783453941345215\n",
      "Training iteration: 2714\n",
      "Validation loss (no improvement): 0.00866461917757988\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.050296437740325925\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): -0.004486489295959473\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.008570761978626251\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): -0.0009646175429224968\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.0213225319981575\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.03566901087760925\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.01947033852338791\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): 0.009068964421749115\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): 0.012771916389465333\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.01567632257938385\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.015140339732170105\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.01815860569477081\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.025394749641418458\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): 0.030828222632408142\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.031115564703941345\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.02760983109474182\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.02446066588163376\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.02399353086948395\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.02531484067440033\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.026368385553359984\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): 0.02624046504497528\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.025668463110923766\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.02574504017829895\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.026710385084152223\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.02821788489818573\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.029140168428421022\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.02873637080192566\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.027305904030799865\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.025899824500083924\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.025112754106521605\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.024894829094409942\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.02478483021259308\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.02442571669816971\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.023864448070526123\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.023405759036540984\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.022850184142589568\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.022603456676006318\n",
      "Training iteration: 2752\n",
      "Validation loss (no improvement): 0.02246808260679245\n",
      "Training iteration: 2753\n",
      "Validation loss (no improvement): 0.02222312241792679\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): 0.021417269110679628\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.020477613806724547\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.019765833020210268\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.01925215423107147\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.01875111609697342\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.018163318932056426\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.01757787764072418\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.017135746777057648\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.016827861964702606\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.016467706859111787\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.015871183574199678\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.0150450199842453\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.014177623391151428\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.013448505103588105\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.012874636054039\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.01235673651099205\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): 0.011840416491031647\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.011371127516031265\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.010984363406896592\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.010600392520427705\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.010081632435321808\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.009384638071060181\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.008612437546253205\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.007901232689619064\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.0072928622364997866\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.006749249994754791\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.006255424022674561\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.005837572738528252\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): 0.005474255606532097\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.00506063811480999\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.0045049645006656645\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.0038315348327159883\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.003149477392435074\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.002539178356528282\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.002012718841433525\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.0015677317976951599\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.0012057770974934102\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.0008839080110192299\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.0005076846573501825\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 1.5558004088234155e-05\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): -0.0005574283190071583\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): -0.0011241194792091847\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): -0.0016226643696427346\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): -0.002032344043254852\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): -0.002358103357255459\n",
      "Training iteration: 2799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.002639073133468628\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): -0.0029461462050676345\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): -0.003332754597067833\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): -0.0037889320403337477\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): -0.004256778955459594\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): -0.004678523913025856\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): -0.005027886480093002\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): -0.005316738039255142\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): -0.005580342561006546\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): -0.0058538965880870816\n"
     ]
    }
   ],
   "source": [
    "swa_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.model_inference()\n",
    "bayesian_model.model_inference()\n",
    "dropout_model.model_inference()\n",
    "mixture_model.model_inference()\n",
    "swa_model.model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline\n",
    "baseline = np.load('Baseline_validation.npy')\n",
    "\n",
    "# Load dropout\n",
    "dropout = np.load('Dropout_validation.npy')\n",
    "dropout_mean = np.mean(dropout,axis=0)\n",
    "dropout_std = np.std(dropout,axis=0)\n",
    "\n",
    "# Load dropout\n",
    "bayesian = np.load('Bayesian_validation.npy')\n",
    "bayesian_mean = np.mean(bayesian,axis=0)\n",
    "bayesian_std = np.std(bayesian,axis=0)\n",
    "\n",
    "# Load mixture\n",
    "mixture_mean = np.load('mixture_mean_validation.npy')\n",
    "mixture_logvar = np.load('mixture_var_validation.npy')\n",
    "mixture_std = np.sqrt(np.exp(mixture_logvar))\n",
    "\n",
    "# Load swa\n",
    "swa_mean = np.load('SWA_ensembles_mean_multiple.npy')\n",
    "swa_std = np.load('SWA_ensembles_std_multiple.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 6.282499313354492\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 6.282499313354492  to: 4.423407363891601\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 4.423407363891601  to: 3.16173095703125\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 3.16173095703125  to: 2.309260368347168\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 2.309260368347168  to: 1.7208538055419922\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 1.7208538055419922  to: 1.3067669868469238\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 1.3067669868469238  to: 1.0138667106628418\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 1.0138667106628418  to: 0.8037264823913575\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 0.8037264823913575  to: 0.650498342514038\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 0.650498342514038  to: 0.5372502326965332\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 0.5372502326965332  to: 0.45256571769714354\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.45256571769714354  to: 0.38864479064941404\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.38864479064941404  to: 0.3396934509277344\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.3396934509277344  to: 0.3018021821975708\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.3018021821975708  to: 0.2722496032714844\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.2722496032714844  to: 0.2490095615386963\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.2490095615386963  to: 0.23060061931610107\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.23060061931610107  to: 0.21591582298278808\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.21591582298278808  to: 0.2041329860687256\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.2041329860687256  to: 0.1946190118789673\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.1946190118789673  to: 0.18689136505126952\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.18689136505126952  to: 0.18058093786239623\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.18058093786239623  to: 0.1753994345664978\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.1753994345664978  to: 0.17111839056015016\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.17111839056015016  to: 0.16756742000579833\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.16756742000579833  to: 0.1646108627319336\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.1646108627319336  to: 0.1621392011642456\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.1621392011642456  to: 0.16006391048431395\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.16006391048431395  to: 0.15831525325775148\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.15831525325775148  to: 0.15683629512786865\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.15683629512786865  to: 0.1555820345878601\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.1555820345878601  to: 0.15451364517211913\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.15451364517211913  to: 0.15360013246536255\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.15360013246536255  to: 0.15281665325164795\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.15281665325164795  to: 0.15214214324951172\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.15214214324951172  to: 0.15155885219573975\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.15155885219573975  to: 0.15105245113372803\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.15105245113372803  to: 0.15061120986938475\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.15061120986938475  to: 0.15022518634796142\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.15022518634796142  to: 0.1498858094215393\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.1498858094215393  to: 0.14958603382110597\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.14958603382110597  to: 0.14932043552398683\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.14932043552398683  to: 0.1490841507911682\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1490841507911682  to: 0.14887287616729736\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.14887287616729736  to: 0.14868274927139283\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.14868274927139283  to: 0.14851077795028686\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.14851077795028686  to: 0.14835472106933595\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.14835472106933595  to: 0.14821231365203857\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.14821231365203857  to: 0.1480816125869751\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.1480816125869751  to: 0.14796102046966553\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.14796102046966553  to: 0.14784911870956421\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.14784911870956421  to: 0.14774471521377563\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.14774471521377563  to: 0.14764676094055176\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.14764676094055176  to: 0.14755451679229736\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.14755451679229736  to: 0.14746729135513306\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.14746729135513306  to: 0.1473844289779663\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.1473844289779663  to: 0.14730532169342042\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.14730532169342042  to: 0.14722944498062135\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.14722944498062135  to: 0.14715646505355834\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.14715646505355834  to: 0.1470860004425049\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.1470860004425049  to: 0.1470176935195923\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.1470176935195923  to: 0.1469512701034546\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.1469512701034546  to: 0.14688645601272582\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.14688645601272582  to: 0.14682304859161377\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.14682304859161377  to: 0.14676084518432617\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.14676084518432617  to: 0.14669970273971558\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.14669970273971558  to: 0.14663946628570557\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.14663946628570557  to: 0.1465800404548645\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.1465800404548645  to: 0.14652128219604493\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.14652128219604493  to: 0.14646310806274415\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.14646310806274415  to: 0.14640533924102783\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14640533924102783  to: 0.14634691476821898\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.14634691476821898  to: 0.14628884792327881\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.14628884792327881  to: 0.14623115062713624\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.14623115062713624  to: 0.1461738109588623\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.1461738109588623  to: 0.14611680507659913\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.14611680507659913  to: 0.1460600733757019\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.1460600733757019  to: 0.14600359201431273\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.14600359201431273  to: 0.14594733715057373\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.14594733715057373  to: 0.14589128494262696\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.14589128494262696  to: 0.14583542346954345\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14583542346954345  to: 0.14577969312667846\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.14577969312667846  to: 0.14572373628616334\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.14572373628616334  to: 0.1456673264503479\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.1456673264503479  to: 0.145611035823822\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.145611035823822  to: 0.14555482864379882\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.14555482864379882  to: 0.1454986810684204\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1454986810684204  to: 0.14544256925582885\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14544256925582885  to: 0.14538651704788208\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.14538651704788208  to: 0.14533050060272218\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.14533050060272218  to: 0.14527451992034912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 91\n",
      "Improved validation loss from: 0.14527451992034912  to: 0.1452185869216919\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1452185869216919  to: 0.14516265392303468\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14516265392303468  to: 0.14510676860809327\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.14510676860809327  to: 0.14505089521408082\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14505089521408082  to: 0.14499505758285522\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.14499505758285522  to: 0.14493927955627442\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14493927955627442  to: 0.14488351345062256\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.14488351345062256  to: 0.14482775926589966\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14482775926589966  to: 0.144771945476532\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.144771945476532  to: 0.14471617937088013\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14471617937088013  to: 0.14466042518615724\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.14466042518615724  to: 0.14460470676422119\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14460470676422119  to: 0.14454901218414307\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.14454901218414307  to: 0.14449336528778076\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14449336528778076  to: 0.14443771839141845\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14443771839141845  to: 0.14438211917877197\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14438211917877197  to: 0.1443265199661255\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1443265199661255  to: 0.14427095651626587\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14427095651626587  to: 0.14421539306640624\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14421539306640624  to: 0.14415985345840454\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14415985345840454  to: 0.14410433769226075\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14410433769226075  to: 0.14404884576797486\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14404884576797486  to: 0.14399335384368897\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14399335384368897  to: 0.14393789768218995\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14393789768218995  to: 0.14388242959976197\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14388242959976197  to: 0.14382703304290773\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14382703304290773  to: 0.1437716841697693\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1437716841697693  to: 0.14371635913848876\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14371635913848876  to: 0.14366101026535033\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14366101026535033  to: 0.14360567331314086\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14360567331314086  to: 0.1435500979423523\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.1435500979423523  to: 0.1434943914413452\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.1434943914413452  to: 0.14343866109848022\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14343866109848022  to: 0.14338295459747313\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14338295459747313  to: 0.1433272361755371\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.1433272361755371  to: 0.14327151775360109\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14327151775360109  to: 0.14321575164794922\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14321575164794922  to: 0.14315993785858155\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.14315993785858155  to: 0.1431041479110718\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.1431041479110718  to: 0.14304835796356202\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14304835796356202  to: 0.14299254417419432\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14299254417419432  to: 0.14293676614761353\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14293676614761353  to: 0.14288097620010376\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14288097620010376  to: 0.142825186252594\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.142825186252594  to: 0.14276939630508423\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14276939630508423  to: 0.14271361827850343\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14271361827850343  to: 0.1426578402519226\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1426578402519226  to: 0.1426020860671997\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.1426020860671997  to: 0.1425463080406189\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.1425463080406189  to: 0.14249054193496705\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14249054193496705  to: 0.1424347996711731\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1424347996711731  to: 0.14237903356552123\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14237903356552123  to: 0.14232327938079833\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14232327938079833  to: 0.14226757287979125\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14226757287979125  to: 0.14221184253692626\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14221184253692626  to: 0.14215610027313233\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14215610027313233  to: 0.14210039377212524\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14210039377212524  to: 0.14204469919204712\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.14204469919204712  to: 0.1419889211654663\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.1419889211654663  to: 0.1419331431388855\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.1419331431388855  to: 0.14187737703323364\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14187737703323364  to: 0.14182161092758178\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14182161092758178  to: 0.14176585674285888\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14176585674285888  to: 0.14171011447906495\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14171011447906495  to: 0.14165438413619996\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14165438413619996  to: 0.1415986657142639\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.1415986657142639  to: 0.14154293537139892\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14154293537139892  to: 0.1414872407913208\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.1414872407913208  to: 0.14143154621124268\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14143154621124268  to: 0.14137585163116456\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14137585163116456  to: 0.14132016897201538\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14132016897201538  to: 0.14126449823379517\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14126449823379517  to: 0.14120883941650392\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14120883941650392  to: 0.14115320444107055\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14115320444107055  to: 0.14109755754470826\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14109755754470826  to: 0.1410419225692749\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.1410419225692749  to: 0.14098634719848632\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.14098634719848632  to: 0.1409307599067688\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1409307599067688  to: 0.14087518453598022\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14087518453598022  to: 0.14081957340240478\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14081957340240478  to: 0.14076392650604247\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14076392650604247  to: 0.14070823192596435\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14070823192596435  to: 0.14065250158309936\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14065250158309936  to: 0.14059674739837646\n",
      "Training iteration: 175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14059674739837646  to: 0.14054094552993773\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14054094552993773  to: 0.14048504829406738\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14048504829406738  to: 0.1404291272163391\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1404291272163391  to: 0.14037318229675294\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14037318229675294  to: 0.14031747579574586\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14031747579574586  to: 0.14026181697845458\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14026181697845458  to: 0.14020615816116333\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14020615816116333  to: 0.14015047550201415\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14015047550201415  to: 0.14009475708007812\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14009475708007812  to: 0.1400390386581421\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.1400390386581421  to: 0.1399833083152771\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.1399833083152771  to: 0.13992757797241212\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.13992757797241212  to: 0.13987185955047607\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13987185955047607  to: 0.13981612920761108\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13981612920761108  to: 0.1397603988647461\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.1397603988647461  to: 0.1397046685218811\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.1397046685218811  to: 0.1396489381790161\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.1396489381790161  to: 0.13959320783615112\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13959320783615112  to: 0.13953747749328613\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13953747749328613  to: 0.13948159217834472\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13948159217834472  to: 0.13942571878433227\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13942571878433227  to: 0.13936984539031982\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13936984539031982  to: 0.13931400775909425\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.13931400775909425  to: 0.1392581582069397\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1392581582069397  to: 0.1392023205757141\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.1392023205757141  to: 0.13914649486541747\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13914649486541747  to: 0.1390907049179077\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.1390907049179077  to: 0.13903491497039794\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13903491497039794  to: 0.13897912502288817\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.13897912502288817  to: 0.13892334699630737\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13892334699630737  to: 0.13886759281158448\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13886759281158448  to: 0.13881183862686158\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13881183862686158  to: 0.13875608444213866\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13875608444213866  to: 0.13870035409927367\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13870035409927367  to: 0.13864462375640868\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.13864462375640868  to: 0.13858890533447266\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.13858890533447266  to: 0.13853318691253663\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.13853318691253663  to: 0.13847749233245848\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.13847749233245848  to: 0.13842179775238037\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13842179775238037  to: 0.1383661150932312\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.1383661150932312  to: 0.138310444355011\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.138310444355011  to: 0.13825477361679078\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13825477361679078  to: 0.1381991147994995\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.1381991147994995  to: 0.1381434679031372\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1381434679031372  to: 0.1380878210067749\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.1380878210067749  to: 0.13803220987319947\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13803220987319947  to: 0.13797658681869507\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13797658681869507  to: 0.13792097568511963\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13792097568511963  to: 0.13786537647247316\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.13786537647247316  to: 0.13780978918075562\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13780978918075562  to: 0.13775420188903809\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13775420188903809  to: 0.13769863843917846\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13769863843917846  to: 0.13764307498931885\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13764307498931885  to: 0.13758739233016967\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13758739233016967  to: 0.1375317692756653\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.1375317692756653  to: 0.13747622966766357\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13747622966766357  to: 0.137420654296875\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.137420654296875  to: 0.13736498355865479\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.13736498355865479  to: 0.13730919361114502\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13730919361114502  to: 0.13725334405899048\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13725334405899048  to: 0.1371974229812622\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.1371974229812622  to: 0.13714144229888917\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13714144229888917  to: 0.13708540201187133\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.13708540201187133  to: 0.13702929019927979\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13702929019927979  to: 0.13697315454483033\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.13697315454483033  to: 0.13691695928573608\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13691695928573608  to: 0.13686075210571289\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.13686075210571289  to: 0.13680450916290282\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13680450916290282  to: 0.13674824237823485\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13674824237823485  to: 0.13669192790985107\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13669192790985107  to: 0.13663564920425414\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.13663564920425414  to: 0.1365793228149414\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.1365793228149414  to: 0.13652300834655762\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.13652300834655762  to: 0.1364666700363159\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.1364666700363159  to: 0.13641035556793213\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13641035556793213  to: 0.13635404109954835\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13635404109954835  to: 0.13629772663116455\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.13629772663116455  to: 0.1362414240837097\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.1362414240837097  to: 0.13618513345718383\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13618513345718383  to: 0.13612887859344483\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13612887859344483  to: 0.13607261180877686\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13607261180877686  to: 0.1360163927078247\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1360163927078247  to: 0.13596017360687257\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13596017360687257  to: 0.13590400218963622\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13590400218963622  to: 0.1358478546142578\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.1358478546142578  to: 0.13579173088073732\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13579173088073732  to: 0.13573564291000367\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.13573564291000367  to: 0.13567959070205687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13567959070205687  to: 0.13562357425689697\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13562357425689697  to: 0.13556759357452391\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13556759357452391  to: 0.13551164865493776\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13551164865493776  to: 0.13545575141906738\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13545575141906738  to: 0.1353998899459839\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.1353998899459839  to: 0.13534409999847413\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13534409999847413  to: 0.1352883458137512\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.1352883458137512  to: 0.13523263931274415\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13523263931274415  to: 0.13517698049545288\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13517698049545288  to: 0.13512136936187744\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13512136936187744  to: 0.13506582975387574\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13506582975387574  to: 0.13501033782958985\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13501033782958985  to: 0.13495492935180664\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13495492935180664  to: 0.1348995566368103\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.1348995566368103  to: 0.1348442554473877\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.1348442554473877  to: 0.13478902578353882\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.13478902578353882  to: 0.13473384380340575\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13473384380340575  to: 0.13467874526977539\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13467874526977539  to: 0.13462371826171876\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13462371826171876  to: 0.13456875085830688\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13456875085830688  to: 0.13451385498046875\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13451385498046875  to: 0.1344591736793518\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1344591736793518  to: 0.13440464735031127\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13440464735031127  to: 0.13435029983520508\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13435029983520508  to: 0.13429613113403321\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13429613113403321  to: 0.13424211740493774\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13424211740493774  to: 0.13418824672698976\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13418824672698976  to: 0.13413454294204713\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13413454294204713  to: 0.13408095836639405\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13408095836639405  to: 0.13402751684188843\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13402751684188843  to: 0.13397419452667236\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13397419452667236  to: 0.13392102718353271\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13392102718353271  to: 0.13386797904968262\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13386797904968262  to: 0.1338150382041931\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.1338150382041931  to: 0.13376224040985107\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13376224040985107  to: 0.13370954990386963\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13370954990386963  to: 0.13365700244903564\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13365700244903564  to: 0.1336045503616333\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.1336045503616333  to: 0.1335522174835205\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.1335522174835205  to: 0.13350001573562623\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13350001573562623  to: 0.13344793319702147\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13344793319702147  to: 0.13339598178863527\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13339598178863527  to: 0.1333438515663147\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.1333438515663147  to: 0.13329156637191772\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13329156637191772  to: 0.13323915004730225\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13323915004730225  to: 0.13318661451339722\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13318661451339722  to: 0.13313398361206055\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13313398361206055  to: 0.13308130502700805\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13308130502700805  to: 0.1330285668373108\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.1330285668373108  to: 0.13297579288482667\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13297579288482667  to: 0.13292300701141357\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13292300701141357  to: 0.13287023305892945\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13287023305892945  to: 0.13281748294830323\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13281748294830323  to: 0.1327647566795349\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.1327647566795349  to: 0.13271210193634034\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13271210193634034  to: 0.13265954256057738\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13265954256057738  to: 0.1326070547103882\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1326070547103882  to: 0.13255465030670166\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13255465030670166  to: 0.132502543926239\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.132502543926239  to: 0.13245060443878173\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13245060443878173  to: 0.132398784160614\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.132398784160614  to: 0.13234711885452272\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13234711885452272  to: 0.13229596614837646\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13229596614837646  to: 0.1322453498840332\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.1322453498840332  to: 0.13219515085220337\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13219515085220337  to: 0.13214536905288696\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13214536905288696  to: 0.1320960283279419\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.1320960283279419  to: 0.13204712867736818\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13204712867736818  to: 0.131998610496521\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.131998610496521  to: 0.13195046186447143\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13195046186447143  to: 0.1319027066230774\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.1319027066230774  to: 0.13185532093048097\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13185532093048097  to: 0.13180825710296631\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13180825710296631  to: 0.13176157474517822\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13176157474517822  to: 0.13171522617340087\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13171522617340087  to: 0.13166921138763427\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13166921138763427  to: 0.13162351846694947\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13162351846694947  to: 0.13157813549041747\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13157813549041747  to: 0.13153307437896727\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13153307437896727  to: 0.13148832321166992\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13148832321166992  to: 0.13144385814666748\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13144385814666748  to: 0.13139971494674682\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13139971494674682  to: 0.1313558578491211\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.1313558578491211  to: 0.13131229877471923\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13131229877471923  to: 0.13126901388168336\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13126901388168336  to: 0.1312260389328003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 349\n",
      "Improved validation loss from: 0.1312260389328003  to: 0.13118331432342528\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13118331432342528  to: 0.13114086389541627\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13114086389541627  to: 0.1310986876487732\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1310986876487732  to: 0.13105679750442506\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13105679750442506  to: 0.13101515769958497\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13101515769958497  to: 0.13097381591796875\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13097381591796875  to: 0.1309327483177185\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1309327483177185  to: 0.13089196681976317\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13089196681976317  to: 0.13085153102874755\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13085153102874755  to: 0.13081135749816894\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13081135749816894  to: 0.13077147006988527\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.13077147006988527  to: 0.1307318925857544\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1307318925857544  to: 0.13069254159927368\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13069254159927368  to: 0.13065345287323\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13065345287323  to: 0.13061460256576538\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13061460256576538  to: 0.13057600259780883\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.13057600259780883  to: 0.13053762912750244\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13053762912750244  to: 0.13049949407577516\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13049949407577516  to: 0.13046162128448485\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13046162128448485  to: 0.13042398691177368\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.13042398691177368  to: 0.1303865432739258\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.1303865432739258  to: 0.13034932613372802\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.13034932613372802  to: 0.13031233549118043\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.13031233549118043  to: 0.13027555942535402\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13027555942535402  to: 0.1302388548851013\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.1302388548851013  to: 0.13020238876342774\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.13020238876342774  to: 0.1301661729812622\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.1301661729812622  to: 0.1301302433013916\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1301302433013916  to: 0.13009458780288696\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13009458780288696  to: 0.13005920648574829\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13005920648574829  to: 0.13002408742904664\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13002408742904664  to: 0.12998921871185304\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.12998921871185304  to: 0.12995460033416747\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.12995460033416747  to: 0.12992026805877685\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.12992026805877685  to: 0.12988617420196533\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.12988617420196533  to: 0.1298523187637329\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.1298523187637329  to: 0.12981870174407958\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.12981870174407958  to: 0.1297853708267212\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.1297853708267212  to: 0.1297522783279419\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.1297522783279419  to: 0.1297194242477417\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1297194242477417  to: 0.1296868085861206\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1296868085861206  to: 0.12965444326400757\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.12965444326400757  to: 0.12962231636047364\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.12962231636047364  to: 0.12959039211273193\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12959039211273193  to: 0.12955873012542723\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12955873012542723  to: 0.12952725887298583\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.12952725887298583  to: 0.12949602603912352\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.12949602603912352  to: 0.12946500778198242\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.12946500778198242  to: 0.1294342279434204\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1294342279434204  to: 0.12940362691879273\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.12940362691879273  to: 0.1293732762336731\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.1293732762336731  to: 0.12934331893920897\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12934331893920897  to: 0.12931357622146605\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.12931357622146605  to: 0.1292840361595154\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.1292840361595154  to: 0.12925468683242797\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.12925468683242797  to: 0.1292255163192749\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.1292255163192749  to: 0.12919653654098512\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.12919653654098512  to: 0.1291677713394165\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.1291677713394165  to: 0.12913916110992432\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.12913916110992432  to: 0.12911075353622437\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.12911075353622437  to: 0.1290825128555298\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.1290825128555298  to: 0.12905439138412475\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.12905439138412475  to: 0.1290264129638672\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1290264129638672  to: 0.12899855375289918\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.12899855375289918  to: 0.12897083759307862\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.12897083759307862  to: 0.1289432406425476\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1289432406425476  to: 0.12891570329666138\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.12891570329666138  to: 0.12888824939727783\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.12888824939727783  to: 0.12886093854904174\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.12886093854904174  to: 0.12883374691009522\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.12883374691009522  to: 0.128806734085083\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.128806734085083  to: 0.12877999544143676\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.12877999544143676  to: 0.1287533760070801\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1287533760070801  to: 0.12872688770294188\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.12872688770294188  to: 0.12870050668716432\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.12870050668716432  to: 0.12867422103881837\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.12867422103881837  to: 0.12864794731140136\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.12864794731140136  to: 0.12862176895141603\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.12862176895141603  to: 0.12859573364257812\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.12859573364257812  to: 0.12856976985931395\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.12856976985931395  to: 0.1285439372062683\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1285439372062683  to: 0.1285181760787964\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1285181760787964  to: 0.12849252223968505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 432\n",
      "Improved validation loss from: 0.12849252223968505  to: 0.12846696376800537\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.12846696376800537  to: 0.1284415006637573\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.1284415006637573  to: 0.12841613292694093\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.12841613292694093  to: 0.1283908247947693\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.1283908247947693  to: 0.12836560010910034\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.12836560010910034  to: 0.12834035158157348\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.12834035158157348  to: 0.12831511497497558\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.12831511497497558  to: 0.12828994989395143\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.12828994989395143  to: 0.12826484441757202\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.12826484441757202  to: 0.12823978662490845\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12823978662490845  to: 0.1282147765159607\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.1282147765159607  to: 0.12818981409072877\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.12818981409072877  to: 0.12816524505615234\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.12816524505615234  to: 0.12814080715179443\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12814080715179443  to: 0.12811644077301027\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.12811644077301027  to: 0.1280921220779419\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1280921220779419  to: 0.12806780338287355\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.12806780338287355  to: 0.1280435085296631\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.1280435085296631  to: 0.12801923751831054\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.12801923751831054  to: 0.12799499034881592\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.12799499034881592  to: 0.1279707670211792\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.1279707670211792  to: 0.12794651985168456\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.12794651985168456  to: 0.12792227268218995\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.12792227268218995  to: 0.12789801359176636\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.12789801359176636  to: 0.12787376642227172\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.12787376642227172  to: 0.1278494954109192\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.1278494954109192  to: 0.12782527208328248\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.12782527208328248  to: 0.1278010368347168\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1278010368347168  to: 0.12777680158615112\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12777680158615112  to: 0.12775254249572754\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.12775254249572754  to: 0.12772828340530396\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.12772828340530396  to: 0.1277039885520935\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.1277039885520935  to: 0.12767966985702514\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12767966985702514  to: 0.12765541076660156\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12765541076660156  to: 0.12763146162033082\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.12763146162033082  to: 0.1276074767112732\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.1276074767112732  to: 0.1275834321975708\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.1275834321975708  to: 0.1275593161582947\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.1275593161582947  to: 0.12753517627716066\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.12753517627716066  to: 0.12751098871231079\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.12751098871231079  to: 0.12748677730560304\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.12748677730560304  to: 0.1274625301361084\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1274625301361084  to: 0.1274382710456848\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.1274382710456848  to: 0.1274140000343323\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.1274140000343323  to: 0.127389657497406\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.127389657497406  to: 0.1273653268814087\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1273653268814087  to: 0.1273409605026245\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.1273409605026245  to: 0.12731653451919556\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.12731653451919556  to: 0.12729208469390868\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.12729208469390868  to: 0.127267324924469\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.127267324924469  to: 0.12724225521087645\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.12724225521087645  to: 0.12721691131591797\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12721691131591797  to: 0.12719128131866456\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.12719128131866456  to: 0.12716542482376098\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12716542482376098  to: 0.12713963985443116\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.12713963985443116  to: 0.1271139144897461\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.1271139144897461  to: 0.1270882487297058\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.1270882487297058  to: 0.1270626664161682\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.1270626664161682  to: 0.12703710794448853\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12703710794448853  to: 0.12701157331466675\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12701157331466675  to: 0.1269860863685608\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.1269860863685608  to: 0.12696059942245483\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12696059942245483  to: 0.12693469524383544\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.12693469524383544  to: 0.126908278465271\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.126908278465271  to: 0.12688162326812744\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.12688162326812744  to: 0.1268546462059021\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.1268546462059021  to: 0.12682778835296632\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.12682778835296632  to: 0.1268010377883911\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.1268010377883911  to: 0.1267743706703186\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1267743706703186  to: 0.1267477750778198\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.1267477750778198  to: 0.12672123908996583\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.12672123908996583  to: 0.12669475078582765\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.12669475078582765  to: 0.1266679048538208\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.1266679048538208  to: 0.12664068937301637\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12664068937301637  to: 0.1266131281852722\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.1266131281852722  to: 0.12658522129058838\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12658522129058838  to: 0.1265574336051941\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.1265574336051941  to: 0.12652978897094727\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12652978897094727  to: 0.12650225162506104\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.12650225162506104  to: 0.12647483348846436\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12647483348846436  to: 0.12644675970077515\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.12644675970077515  to: 0.1264182448387146\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1264182448387146  to: 0.12638938426971436\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12638938426971436  to: 0.12636020183563232\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.12636020183563232  to: 0.12633121013641357\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12633121013641357  to: 0.12630239725112916\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.12630239725112916  to: 0.1262737274169922\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.1262737274169922  to: 0.12624467611312867\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.12624467611312867  to: 0.12621526718139647\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.12621526718139647  to: 0.1261855721473694\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.1261855721473694  to: 0.12615619897842406\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.12615619897842406  to: 0.126127028465271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 524\n",
      "Improved validation loss from: 0.126127028465271  to: 0.12609747648239136\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.12609747648239136  to: 0.12606756687164306\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.12606756687164306  to: 0.126037859916687\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.126037859916687  to: 0.1260077714920044\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.1260077714920044  to: 0.12597733736038208\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.12597733736038208  to: 0.12594717741012573\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12594717741012573  to: 0.12591665983200073\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.12591665983200073  to: 0.12588576078414918\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12588576078414918  to: 0.1258551836013794\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.1258551836013794  to: 0.12582484483718873\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.12582484483718873  to: 0.12579407691955566\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.12579407691955566  to: 0.1257629156112671\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1257629156112671  to: 0.125731360912323\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.125731360912323  to: 0.12569949626922608\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.12569949626922608  to: 0.1256677031517029\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.1256677031517029  to: 0.12563568353652954\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.12563568353652954  to: 0.12560408115386962\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.12560408115386962  to: 0.12557289600372315\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.12557289600372315  to: 0.1255420684814453\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.1255420684814453  to: 0.12551079988479613\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.12551079988479613  to: 0.1254791498184204\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.1254791498184204  to: 0.1254471182823181\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.1254471182823181  to: 0.12541474103927613\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12541474103927613  to: 0.12538203001022338\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.12538203001022338  to: 0.12534902095794678\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12534902095794678  to: 0.12531569004058837\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.12531569004058837  to: 0.12528209686279296\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.12528209686279296  to: 0.12524824142456054\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12524824142456054  to: 0.12521413564682007\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12521413564682007  to: 0.12517980337142945\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12517980337142945  to: 0.12514616250991822\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.12514616250991822  to: 0.1251131296157837\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.1251131296157837  to: 0.12507975101470947\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.12507975101470947  to: 0.1250460147857666\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.1250460147857666  to: 0.12501194477081298\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12501194477081298  to: 0.12497756481170655\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.12497756481170655  to: 0.12494291067123413\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12494291067123413  to: 0.12490794658660889\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12490794658660889  to: 0.12487270832061767\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12487270832061767  to: 0.12483723163604736\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12483723163604736  to: 0.12480146884918213\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12480146884918213  to: 0.12476549148559571\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.12476549148559571  to: 0.12472928762435913\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.12472928762435913  to: 0.12469285726547241\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12469285726547241  to: 0.12465627193450927\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.12465627193450927  to: 0.1246194839477539\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.1246194839477539  to: 0.12458252906799316\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.12458252906799316  to: 0.12454540729522705\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.12454540729522705  to: 0.12450811862945557\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.12450811862945557  to: 0.12447069883346558\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12447069883346558  to: 0.12443313598632813\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.12443313598632813  to: 0.12439543008804321\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12439543008804321  to: 0.12435759305953979\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12435759305953979  to: 0.12431962490081787\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.12431962490081787  to: 0.1242814302444458\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1242814302444458  to: 0.12424304485321044\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.12424304485321044  to: 0.12420444488525391\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12420444488525391  to: 0.12416565418243408\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.12416565418243408  to: 0.12412667274475098\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12412667274475098  to: 0.12408754825592042\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.12408754825592042  to: 0.12404825687408447\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.12404825687408447  to: 0.12400884628295898\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12400884628295898  to: 0.1239693284034729\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.1239693284034729  to: 0.12392971515655518\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.12392971515655518  to: 0.12388999462127685\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12388999462127685  to: 0.12385017871856689\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.12385017871856689  to: 0.1238102674484253\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.1238102674484253  to: 0.12377027273178101\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.12377027273178101  to: 0.12373020648956298\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12373020648956298  to: 0.12369003295898437\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.12369003295898437  to: 0.12364983558654785\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12364983558654785  to: 0.12360953092575074\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12360953092575074  to: 0.12356922626495362\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12356922626495362  to: 0.12352882623672486\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12352882623672486  to: 0.12348833084106445\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.12348833084106445  to: 0.12344980239868164\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.12344980239868164  to: 0.12341302633285522\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12341302633285522  to: 0.12337779998779297\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12337779998779297  to: 0.12334387302398682\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12334387302398682  to: 0.1233111023902893\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.1233111023902893  to: 0.12327923774719238\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.12327923774719238  to: 0.12324812412261962\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12324812412261962  to: 0.1232154130935669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 607\n",
      "Improved validation loss from: 0.1232154130935669  to: 0.12318117618560791\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12318117618560791  to: 0.12314544916152954\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12314544916152954  to: 0.1231081485748291\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.1231081485748291  to: 0.12306935787200927\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12306935787200927  to: 0.12302919626235961\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12302919626235961  to: 0.12298769950866699\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12298769950866699  to: 0.12294502258300781\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12294502258300781  to: 0.12290126085281372\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12290126085281372  to: 0.12285646200180053\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12285646200180053  to: 0.12281100749969483\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12281100749969483  to: 0.12276493310928345\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12276493310928345  to: 0.12271831035614014\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12271831035614014  to: 0.12267119884490967\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12267119884490967  to: 0.12262132167816162\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12262132167816162  to: 0.12256896495819092\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12256896495819092  to: 0.12251435518264771\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12251435518264771  to: 0.12245773077011109\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12245773077011109  to: 0.12239936590194703\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12239936590194703  to: 0.12233947515487671\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12233947515487671  to: 0.12227830886840821\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12227830886840821  to: 0.1222155213356018\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1222155213356018  to: 0.12215185165405273\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12215185165405273  to: 0.1220900535583496\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1220900535583496  to: 0.1220324993133545\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.1220324993133545  to: 0.12197884321212768\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12197884321212768  to: 0.12192882299423217\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12192882299423217  to: 0.12187931537628174\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12187931537628174  to: 0.12183022499084473\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12183022499084473  to: 0.12178148031234741\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12178148031234741  to: 0.12173304557800294\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12173304557800294  to: 0.12168480157852173\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12168480157852173  to: 0.12163665294647216\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12163665294647216  to: 0.12158856391906739\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12158856391906739  to: 0.12154043912887573\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12154043912887573  to: 0.12149531841278076\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12149531841278076  to: 0.1214529037475586\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1214529037475586  to: 0.12140963077545167\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12140963077545167  to: 0.12136290073394776\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12136290073394776  to: 0.12131296396255493\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12131296396255493  to: 0.12126004695892334\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12126004695892334  to: 0.1212043046951294\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.1212043046951294  to: 0.12114547491073609\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12114547491073609  to: 0.12108442783355713\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12108442783355713  to: 0.12102137804031372\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12102137804031372  to: 0.12095659971237183\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12095659971237183  to: 0.12089035511016846\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12089035511016846  to: 0.12082288265228272\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12082288265228272  to: 0.12075437307357788\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12075437307357788  to: 0.12068512439727783\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12068512439727783  to: 0.12061530351638794\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12061530351638794  to: 0.12054781913757324\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12054781913757324  to: 0.12048256397247314\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12048256397247314  to: 0.12041938304901123\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12041938304901123  to: 0.1203581690788269\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.1203581690788269  to: 0.12029874324798584\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12029874324798584  to: 0.12024091482162476\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12024091482162476  to: 0.12018358707427979\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12018358707427979  to: 0.12012741565704346\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12012741565704346  to: 0.12007232904434204\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12007232904434204  to: 0.12001811265945435\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12001811265945435  to: 0.11996461153030395\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.11996461153030395  to: 0.11991163492202758\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.11991163492202758  to: 0.11985902786254883\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.11985902786254883  to: 0.11980661153793334\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.11980661153793334  to: 0.11975421905517578\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.11975421905517578  to: 0.1197016954421997\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1197016954421997  to: 0.11964890956878663\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.11964890956878663  to: 0.11959573030471801\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.11959573030471801  to: 0.11954202651977539\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.11954202651977539  to: 0.11948753595352173\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.11948753595352173  to: 0.11943217515945434\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.11943217515945434  to: 0.11937587261199951\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.11937587261199951  to: 0.11931858062744141\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.11931858062744141  to: 0.11926028728485108\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.11926028728485108  to: 0.11920092105865479\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.11920092105865479  to: 0.11914050579071045\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.11914050579071045  to: 0.11907904148101807\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.11907904148101807  to: 0.11901651620864868\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.11901651620864868  to: 0.11895294189453125\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.11895294189453125  to: 0.1188883662223816\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.1188883662223816  to: 0.11882287263870239\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.11882287263870239  to: 0.1187558650970459\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.1187558650970459  to: 0.11868784427642823\n",
      "Training iteration: 690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11868784427642823  to: 0.11861903667449951\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.11861903667449951  to: 0.1185495138168335\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.1185495138168335  to: 0.11847877502441406\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.11847877502441406  to: 0.11840716600418091\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.11840716600418091  to: 0.11832559108734131\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.11832559108734131  to: 0.11823508739471436\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.11823508739471436  to: 0.11813676357269287\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.11813676357269287  to: 0.11803171634674073\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.11803171634674073  to: 0.11792111396789551\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.11792111396789551  to: 0.11780955791473388\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.11780955791473388  to: 0.11769769191741944\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.11769769191741944  to: 0.11758626699447632\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.11758626699447632  to: 0.11748218536376953\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.11748218536376953  to: 0.1173851728439331\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.1173851728439331  to: 0.1172949194908142\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.1172949194908142  to: 0.11721104383468628\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.11721104383468628  to: 0.11713309288024902\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.11713309288024902  to: 0.11705021858215332\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.11705021858215332  to: 0.1169629454612732\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.1169629454612732  to: 0.11687184572219848\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.11687184572219848  to: 0.11677747964859009\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.11677747964859009  to: 0.11668046712875366\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.11668046712875366  to: 0.11658133268356323\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.11658133268356323  to: 0.11648067235946655\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.11648067235946655  to: 0.1163789987564087\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.1163789987564087  to: 0.11627681255340576\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.11627681255340576  to: 0.11617462635040283\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.11617462635040283  to: 0.11607279777526855\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.11607279777526855  to: 0.11597535610198975\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.11597535610198975  to: 0.11588220596313477\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.11588220596313477  to: 0.11578943729400634\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.11578943729400634  to: 0.11569721698760986\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.11569721698760986  to: 0.11560567617416381\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.11560567617416381  to: 0.11551492214202881\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.11551492214202881  to: 0.1154250144958496\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.1154250144958496  to: 0.115336012840271\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.115336012840271  to: 0.11525156497955322\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.11525156497955322  to: 0.11517126560211181\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.11517126560211181  to: 0.11509460210800171\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.11509460210800171  to: 0.1150210976600647\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.1150210976600647  to: 0.11495020389556884\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.11495020389556884  to: 0.11488132476806641\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.11488132476806641  to: 0.11481322050094604\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.11481322050094604  to: 0.11474586725234985\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.11474586725234985  to: 0.11467472314834595\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.11467472314834595  to: 0.11459968090057374\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.11459968090057374  to: 0.1145207166671753\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.1145207166671753  to: 0.11443792581558228\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.11443792581558228  to: 0.11435143947601319\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.11435143947601319  to: 0.11426146030426025\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.11426146030426025  to: 0.11416828632354736\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.11416828632354736  to: 0.11407222747802734\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.11407222747802734  to: 0.11397364139556884\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.11397364139556884  to: 0.11387290954589843\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.11387290954589843  to: 0.11377042531967163\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.11377042531967163  to: 0.1136665940284729\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.1136665940284729  to: 0.11356182098388672\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.11356182098388672  to: 0.11345646381378174\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.11345646381378174  to: 0.11335086822509766\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.11335086822509766  to: 0.11324532032012939\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.11324532032012939  to: 0.11314013004302978\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.11314013004302978  to: 0.11303548812866211\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.11303548812866211  to: 0.11293160915374756\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.11293160915374756  to: 0.11282862424850464\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.11282862424850464  to: 0.11272659301757812\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.11272659301757812  to: 0.1126255989074707\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.1126255989074707  to: 0.11252562999725342\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.11252562999725342  to: 0.1124266266822815\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1124266266822815  to: 0.11232844591140748\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.11232844591140748  to: 0.11223094463348389\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.11223094463348389  to: 0.11213444471359253\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.11213444471359253  to: 0.11203868389129638\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.11203868389129638  to: 0.11194337606430053\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.11194337606430053  to: 0.1118482232093811\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.1118482232093811  to: 0.11175289154052734\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.11175289154052734  to: 0.11165710687637329\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.11165710687637329  to: 0.11156055927276612\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.11156055927276612  to: 0.11146297454833984\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.11146297454833984  to: 0.11136394739151001\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.11136394739151001  to: 0.11126327514648438\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.11126327514648438  to: 0.11116077899932861\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.11116077899932861  to: 0.1110562801361084\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.1110562801361084  to: 0.11094968318939209\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.11094968318939209  to: 0.11084092855453491\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.11084092855453491  to: 0.11072995662689208\n",
      "Training iteration: 775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11072995662689208  to: 0.11061681509017944\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.11061681509017944  to: 0.11050151586532593\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.11050151586532593  to: 0.11038416624069214\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.11038416624069214  to: 0.11026499271392823\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.11026499271392823  to: 0.11014410257339477\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.11014410257339477  to: 0.11002165079116821\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.11002165079116821  to: 0.1098979353904724\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1098979353904724  to: 0.10977312326431274\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.10977312326431274  to: 0.10964721441268921\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.10964721441268921  to: 0.1095203161239624\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.1095203161239624  to: 0.10939255952835084\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.10939255952835084  to: 0.10926401615142822\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.10926401615142822  to: 0.1091347336769104\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.1091347336769104  to: 0.10900462865829467\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.10900462865829467  to: 0.10887374877929687\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.10887374877929687  to: 0.10872716903686523\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.10872716903686523  to: 0.10856701135635376\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.10856701135635376  to: 0.10839577913284301\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.10839577913284301  to: 0.10821635723114013\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.10821635723114013  to: 0.10803148746490479\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.10803148746490479  to: 0.10784378051757812\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.10784378051757812  to: 0.10767033100128173\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.10767033100128173  to: 0.1075108528137207\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.1075108528137207  to: 0.10736463069915772\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.10736463069915772  to: 0.10723042488098145\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.10723042488098145  to: 0.10710657835006714\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.10710657835006714  to: 0.10697629451751708\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.10697629451751708  to: 0.10683937072753906\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.10683937072753906  to: 0.1066960334777832\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.1066960334777832  to: 0.10654652118682861\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.10654652118682861  to: 0.10639134645462037\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.10639134645462037  to: 0.10623109340667725\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.10623109340667725  to: 0.10606658458709717\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.10606658458709717  to: 0.10589838027954102\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.10589838027954102  to: 0.10572760105133057\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.10572760105133057  to: 0.10555492639541626\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.10555492639541626  to: 0.10538136959075928\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.10538136959075928  to: 0.1052077054977417\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.1052077054977417  to: 0.1050345778465271\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.1050345778465271  to: 0.1048622488975525\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.1048622488975525  to: 0.10469133853912353\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.10469133853912353  to: 0.10452203750610352\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.10452203750610352  to: 0.1043541669845581\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.1043541669845581  to: 0.10418787002563476\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.10418787002563476  to: 0.10402292013168335\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.10402292013168335  to: 0.10385894775390625\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.10385894775390625  to: 0.1036952257156372\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.1036952257156372  to: 0.10353152751922608\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.10353152751922608  to: 0.1033672571182251\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.1033672571182251  to: 0.10320144891738892\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.10320144891738892  to: 0.10303382873535157\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.10303382873535157  to: 0.1028635859489441\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.1028635859489441  to: 0.10269029140472412\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.10269029140472412  to: 0.10251333713531494\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.10251333713531494  to: 0.10233283042907715\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.10233283042907715  to: 0.10214875936508179\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.10214875936508179  to: 0.10196119546890259\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.10196119546890259  to: 0.10177004337310791\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.10177004337310791  to: 0.10157591104507446\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.10157591104507446  to: 0.1013791799545288\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.1013791799545288  to: 0.10118024349212647\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.10118024349212647  to: 0.1009791612625122\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.1009791612625122  to: 0.10077663660049438\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.10077663660049438  to: 0.10057294368743896\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.10057294368743896  to: 0.10036824941635132\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.10036824941635132  to: 0.10016227960586548\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.10016227960586548  to: 0.09995533227920532\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.09995533227920532  to: 0.09974753260612487\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.09974753260612487  to: 0.09953923225402832\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.09953923225402832  to: 0.0992763876914978\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.0992763876914978  to: 0.09896852374076844\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.09896852374076844  to: 0.09863003492355346\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.09863003492355346  to: 0.09827756881713867\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.09827756881713867  to: 0.09792613983154297\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.09792613983154297  to: 0.0975882351398468\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.0975882351398468  to: 0.09727376699447632\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.09727376699447632  to: 0.09698973894119263\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.09698973894119263  to: 0.0967395007610321\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.0967395007610321  to: 0.09652413129806518\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.09652413129806518  to: 0.09634122848510743\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.09634122848510743  to: 0.09618566632270813\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.09618566632270813  to: 0.09605028033256531\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.09605028033256531  to: 0.09592483639717102\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.09592483639717102  to: 0.09579869508743286\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.09579869508743286  to: 0.09566141366958618\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.09566141366958618  to: 0.09550317525863647\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.09550317525863647  to: 0.09515973329544067\n",
      "Training iteration: 862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.09515973329544067  to: 0.09467212557792663\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.09467212557792663  to: 0.0942461609840393\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.0942461609840393  to: 0.09389337301254272\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.09389337301254272  to: 0.09361686706542968\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.09361686706542968  to: 0.09340051412582398\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.09340051412582398  to: 0.09324022531509399\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.09324022531509399  to: 0.0931285560131073\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.0931285560131073  to: 0.09287111163139343\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.09287111163139343  to: 0.09248441457748413\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.09248441457748413  to: 0.09202098846435547\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.09202098846435547  to: 0.09169038534164428\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.09169038534164428  to: 0.0914817988872528\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.0914817988872528  to: 0.09138486981391906\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.09138486981391906  to: 0.09120014905929566\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.09120014905929566  to: 0.0909235119819641\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.0909235119819641  to: 0.09058027267456055\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.09058027267456055  to: 0.09021221399307251\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.09021221399307251  to: 0.0899986445903778\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.0899986445903778  to: 0.08991852998733521\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.08991852998733521  to: 0.08977895975112915\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.08977895975112915  to: 0.08956488370895385\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.08956488370895385  to: 0.08928747177124023\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.08928747177124023  to: 0.08897778391838074\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.08897778391838074  to: 0.08884216547012329\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.08884216547012329  to: 0.08868894577026368\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.08868894577026368  to: 0.08849127888679505\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.08849127888679505  to: 0.08825885653495788\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.08825885653495788  to: 0.08803108930587769\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.08803108930587769  to: 0.08782189488410949\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.08782189488410949  to: 0.08782140016555787\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.08782140016555787  to: 0.08780345916748047\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.08780345916748047  to: 0.08769969940185547\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.08769969940185547  to: 0.0874997317790985\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.0874997317790985  to: 0.08723633885383605\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.08723633885383605  to: 0.08698123097419738\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.08698123097419738  to: 0.08676607012748719\n",
      "Training iteration: 898\n",
      "Validation loss (no improvement): 0.0867669403553009\n",
      "Training iteration: 899\n",
      "Validation loss (no improvement): 0.08677644729614258\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.08676607012748719  to: 0.08675447702407837\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.08675447702407837  to: 0.0866478443145752\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.0866478443145752  to: 0.0864651083946228\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.0864651083946228  to: 0.08626737594604492\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.08626737594604492  to: 0.08608848452568055\n",
      "Training iteration: 905\n",
      "Validation loss (no improvement): 0.08613469004631043\n",
      "Training iteration: 906\n",
      "Validation loss (no improvement): 0.08616951704025269\n",
      "Training iteration: 907\n",
      "Validation loss (no improvement): 0.0861250102519989\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.08608848452568055  to: 0.08599282503128051\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.08599282503128051  to: 0.08581035733222961\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.08581035733222961  to: 0.08564645051956177\n",
      "Training iteration: 911\n",
      "Validation loss (no improvement): 0.08571375608444214\n",
      "Training iteration: 912\n",
      "Validation loss (no improvement): 0.08576663136482239\n",
      "Training iteration: 913\n",
      "Validation loss (no improvement): 0.08574141263961792\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.08564645051956177  to: 0.08563216328620911\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.08563216328620911  to: 0.08547648191452026\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.08547648191452026  to: 0.08534008860588074\n",
      "Training iteration: 917\n",
      "Validation loss (no improvement): 0.08542519807815552\n",
      "Training iteration: 918\n",
      "Validation loss (no improvement): 0.08547541499137878\n",
      "Training iteration: 919\n",
      "Validation loss (no improvement): 0.08545045852661133\n",
      "Training iteration: 920\n",
      "Validation loss (no improvement): 0.08534873127937317\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.08534008860588074  to: 0.08520573377609253\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.08520573377609253  to: 0.08508090972900391\n",
      "Training iteration: 923\n",
      "Validation loss (no improvement): 0.08517335653305054\n",
      "Training iteration: 924\n",
      "Validation loss (no improvement): 0.08523408770561218\n",
      "Training iteration: 925\n",
      "Validation loss (no improvement): 0.08521815538406372\n",
      "Training iteration: 926\n",
      "Validation loss (no improvement): 0.0851227581501007\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.08508090972900391  to: 0.08498240709304809\n",
      "Training iteration: 928\n",
      "Validation loss (no improvement): 0.08503879308700561\n",
      "Training iteration: 929\n",
      "Validation loss (no improvement): 0.08503789901733398\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.08498240709304809  to: 0.08496595621109009\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.08496595621109009  to: 0.08484588861465454\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.08484588861465454  to: 0.08471921682357789\n",
      "Training iteration: 933\n",
      "Validation loss (no improvement): 0.0848117709159851\n",
      "Training iteration: 934\n",
      "Validation loss (no improvement): 0.08486510515213012\n",
      "Training iteration: 935\n",
      "Validation loss (no improvement): 0.08483744859695434\n",
      "Training iteration: 936\n",
      "Validation loss (no improvement): 0.0847329318523407\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.08471921682357789  to: 0.08459441065788269\n",
      "Training iteration: 938\n",
      "Validation loss (no improvement): 0.08464556932449341\n",
      "Training iteration: 939\n",
      "Validation loss (no improvement): 0.08464740514755249\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.08459441065788269  to: 0.0845833659172058\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.0845833659172058  to: 0.08447084426879883\n",
      "Training iteration: 942\n",
      "Validation loss (no improvement): 0.08454772233963012\n",
      "Training iteration: 943\n",
      "Validation loss (no improvement): 0.08455066680908203\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.08447084426879883  to: 0.08445848226547241\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.08445848226547241  to: 0.08430635333061218\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.08430635333061218  to: 0.08415502309799194\n",
      "Training iteration: 947\n",
      "Validation loss (no improvement): 0.08422266840934753\n",
      "Training iteration: 948\n",
      "Validation loss (no improvement): 0.08450204133987427\n",
      "Training iteration: 949\n",
      "Validation loss (no improvement): 0.08457235097885132\n",
      "Training iteration: 950\n",
      "Validation loss (no improvement): 0.08436740040779114\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.08415502309799194  to: 0.08406917452812195\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.08406917452812195  to: 0.08381271362304688\n",
      "Training iteration: 953\n",
      "Validation loss (no improvement): 0.08383848071098328\n",
      "Training iteration: 954\n",
      "Validation loss (no improvement): 0.08389226794242859\n",
      "Training iteration: 955\n",
      "Validation loss (no improvement): 0.08394385576248169\n",
      "Training iteration: 956\n",
      "Validation loss (no improvement): 0.08396695256233215\n",
      "Training iteration: 957\n",
      "Validation loss (no improvement): 0.08414114713668823\n",
      "Training iteration: 958\n",
      "Validation loss (no improvement): 0.08409148454666138\n",
      "Training iteration: 959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0838370680809021\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.08381271362304688  to: 0.08375433087348938\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.08375433087348938  to: 0.08363502621650695\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.08363502621650695  to: 0.08350359201431275\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.08350359201431275  to: 0.08339210748672485\n",
      "Training iteration: 964\n",
      "Validation loss (no improvement): 0.08352894783020019\n",
      "Training iteration: 965\n",
      "Validation loss (no improvement): 0.08362153768539429\n",
      "Training iteration: 966\n",
      "Validation loss (no improvement): 0.08359910845756531\n",
      "Training iteration: 967\n",
      "Validation loss (no improvement): 0.08346694707870483\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.08339210748672485  to: 0.08328374624252319\n",
      "Training iteration: 969\n",
      "Validation loss (no improvement): 0.08329325914382935\n",
      "Training iteration: 970\n",
      "Validation loss (no improvement): 0.08346719741821289\n",
      "Training iteration: 971\n",
      "Validation loss (no improvement): 0.08341822624206544\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.08328374624252319  to: 0.08315023183822631\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.08315023183822631  to: 0.0828550934791565\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.0828550934791565  to: 0.08264079093933105\n",
      "Training iteration: 975\n",
      "Validation loss (no improvement): 0.08271690607070922\n",
      "Training iteration: 976\n",
      "Validation loss (no improvement): 0.08307460546493531\n",
      "Training iteration: 977\n",
      "Validation loss (no improvement): 0.08333891034126281\n",
      "Training iteration: 978\n",
      "Validation loss (no improvement): 0.08323307037353515\n",
      "Training iteration: 979\n",
      "Validation loss (no improvement): 0.08282853364944458\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.08264079093933105  to: 0.0824437141418457\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.0824437141418457  to: 0.0823598861694336\n",
      "Training iteration: 982\n",
      "Validation loss (no improvement): 0.08253222703933716\n",
      "Training iteration: 983\n",
      "Validation loss (no improvement): 0.08268074989318848\n",
      "Training iteration: 984\n",
      "Validation loss (no improvement): 0.08269926905632019\n",
      "Training iteration: 985\n",
      "Validation loss (no improvement): 0.08256804347038268\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.0823598861694336  to: 0.08234399557113647\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.08234399557113647  to: 0.08230829238891602\n",
      "Training iteration: 988\n",
      "Validation loss (no improvement): 0.08244882822036743\n",
      "Training iteration: 989\n",
      "Validation loss (no improvement): 0.08235596418380738\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.08230829238891602  to: 0.08212684392929077\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.08212684392929077  to: 0.08185493350028991\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.08185493350028991  to: 0.0816281020641327\n",
      "Training iteration: 993\n",
      "Validation loss (no improvement): 0.08168147802352906\n",
      "Training iteration: 994\n",
      "Validation loss (no improvement): 0.08200945854187011\n",
      "Training iteration: 995\n",
      "Validation loss (no improvement): 0.0822176456451416\n",
      "Training iteration: 996\n",
      "Validation loss (no improvement): 0.08202790021896363\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.0816281020641327  to: 0.08158891797065734\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.08158891797065734  to: 0.08120840191841125\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.08120840191841125  to: 0.08115291595458984\n",
      "Training iteration: 1000\n",
      "Validation loss (no improvement): 0.08137577772140503\n",
      "Training iteration: 1001\n",
      "Validation loss (no improvement): 0.08159093856811524\n",
      "Training iteration: 1002\n",
      "Validation loss (no improvement): 0.08165129423141479\n",
      "Training iteration: 1003\n",
      "Validation loss (no improvement): 0.08138059377670288\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.08115291595458984  to: 0.08104649782180787\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.08104649782180787  to: 0.08097044229507447\n",
      "Training iteration: 1006\n",
      "Validation loss (no improvement): 0.08113383054733277\n",
      "Training iteration: 1007\n",
      "Validation loss (no improvement): 0.08119975328445435\n",
      "Training iteration: 1008\n",
      "Validation loss (no improvement): 0.08098708391189575\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.08097044229507447  to: 0.08070603609085084\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.08070603609085084  to: 0.08070173263549804\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.08070173263549804  to: 0.08066239356994628\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.08066239356994628  to: 0.08058450818061828\n",
      "Training iteration: 1013\n",
      "Validation loss (no improvement): 0.08074588775634765\n",
      "Training iteration: 1014\n",
      "Validation loss (no improvement): 0.08075206875801086\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.08058450818061828  to: 0.08046973943710327\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.08046973943710327  to: 0.08041908144950867\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.08041908144950867  to: 0.08029014468193055\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.08029014468193055  to: 0.08011189699172974\n",
      "Training iteration: 1019\n",
      "Validation loss (no improvement): 0.08020145297050477\n",
      "Training iteration: 1020\n",
      "Validation loss (no improvement): 0.08017926216125489\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.08011189699172974  to: 0.0800258457660675\n",
      "Training iteration: 1022\n",
      "Validation loss (no improvement): 0.08008168339729309\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.0800258457660675  to: 0.07997726202011109\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.07997726202011109  to: 0.07974411249160766\n",
      "Training iteration: 1025\n",
      "Validation loss (no improvement): 0.07975924611091614\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.07974411249160766  to: 0.0796679675579071\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.0796679675579071  to: 0.07949110269546508\n",
      "Training iteration: 1028\n",
      "Validation loss (no improvement): 0.07957533597946168\n",
      "Training iteration: 1029\n",
      "Validation loss (no improvement): 0.07953888177871704\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.07949110269546508  to: 0.0793741762638092\n",
      "Training iteration: 1031\n",
      "Validation loss (no improvement): 0.07944098711013795\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.0793741762638092  to: 0.07924127578735352\n",
      "Training iteration: 1033\n",
      "Validation loss (no improvement): 0.07929229736328125\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.07924127578735352  to: 0.07920388579368591\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.07920388579368591  to: 0.07899693846702575\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.07899693846702575  to: 0.07875524163246155\n",
      "Training iteration: 1037\n",
      "Validation loss (no improvement): 0.07884309887886047\n",
      "Training iteration: 1038\n",
      "Validation loss (no improvement): 0.07920913100242614\n",
      "Training iteration: 1039\n",
      "Validation loss (no improvement): 0.07917740941047668\n",
      "Training iteration: 1040\n",
      "Validation loss (no improvement): 0.07875685691833496\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.07875524163246155  to: 0.07831035852432251\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.07831035852432251  to: 0.07827933430671692\n",
      "Training iteration: 1043\n",
      "Validation loss (no improvement): 0.07859984636306763\n",
      "Training iteration: 1044\n",
      "Validation loss (no improvement): 0.07879403829574586\n",
      "Training iteration: 1045\n",
      "Validation loss (no improvement): 0.07858428955078126\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.07827933430671692  to: 0.07821481823921203\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.07821481823921203  to: 0.07813870310783386\n",
      "Training iteration: 1048\n",
      "Validation loss (no improvement): 0.0783225417137146\n",
      "Training iteration: 1049\n",
      "Validation loss (no improvement): 0.07829899191856385\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.07813870310783386  to: 0.07791401147842407\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.07791401147842407  to: 0.0774952232837677\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.0774952232837677  to: 0.07744616866111756\n",
      "Training iteration: 1053\n",
      "Validation loss (no improvement): 0.07772074937820435\n",
      "Training iteration: 1054\n",
      "Validation loss (no improvement): 0.07783983945846558\n",
      "Training iteration: 1055\n",
      "Validation loss (no improvement): 0.07754791975021362\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.07744616866111756  to: 0.07712792754173278\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.07712792754173278  to: 0.0770205318927765\n",
      "Training iteration: 1058\n",
      "Validation loss (no improvement): 0.07719361782073975\n",
      "Training iteration: 1059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.07719163298606872\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.0770205318927765  to: 0.07682658433914184\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.07682658433914184  to: 0.07640985250473023\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.07640985250473023  to: 0.07634590864181519\n",
      "Training iteration: 1063\n",
      "Validation loss (no improvement): 0.07659299373626709\n",
      "Training iteration: 1064\n",
      "Validation loss (no improvement): 0.07666475176811219\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.07634590864181519  to: 0.07631039023399352\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.07631039023399352  to: 0.07584438920021057\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.07584438920021057  to: 0.07570958733558655\n",
      "Training iteration: 1068\n",
      "Validation loss (no improvement): 0.07586954236030578\n",
      "Training iteration: 1069\n",
      "Validation loss (no improvement): 0.07586458921432496\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.07570958733558655  to: 0.0754876971244812\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.0754876971244812  to: 0.0750518262386322\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.0750518262386322  to: 0.07496135234832764\n",
      "Training iteration: 1073\n",
      "Validation loss (no improvement): 0.07517256140708924\n",
      "Training iteration: 1074\n",
      "Validation loss (no improvement): 0.07521208524703979\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.07496135234832764  to: 0.074701988697052\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.074701988697052  to: 0.07420623898506165\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.07420623898506165  to: 0.07410460710525513\n",
      "Training iteration: 1078\n",
      "Validation loss (no improvement): 0.07435409426689148\n",
      "Training iteration: 1079\n",
      "Validation loss (no improvement): 0.0745012640953064\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.07410460710525513  to: 0.0739867091178894\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.0739867091178894  to: 0.0734963595867157\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.0734963595867157  to: 0.07340078353881836\n",
      "Training iteration: 1083\n",
      "Validation loss (no improvement): 0.07365639805793762\n",
      "Training iteration: 1084\n",
      "Validation loss (no improvement): 0.07381346821784973\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.07340078353881836  to: 0.07331933975219726\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.07331933975219726  to: 0.07283698916435241\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.07283698916435241  to: 0.07273095846176147\n",
      "Training iteration: 1088\n",
      "Validation loss (no improvement): 0.07295856475830079\n",
      "Training iteration: 1089\n",
      "Validation loss (no improvement): 0.0730985939502716\n",
      "Training iteration: 1090\n",
      "Validation loss (no improvement): 0.07275474667549134\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.07273095846176147  to: 0.07234357595443726\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.07234357595443726  to: 0.07223744988441468\n",
      "Training iteration: 1093\n",
      "Validation loss (no improvement): 0.07239683270454407\n",
      "Training iteration: 1094\n",
      "Validation loss (no improvement): 0.07227221727371216\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.07223744988441468  to: 0.07189176678657531\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.07189176678657531  to: 0.07179921269416809\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.07179921269416809  to: 0.07166033983230591\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.07166033983230591  to: 0.07148597240447999\n",
      "Training iteration: 1099\n",
      "Validation loss (no improvement): 0.07156714797019958\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.07148597240447999  to: 0.07130438089370728\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.07130438089370728  to: 0.07127188444137574\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.07127188444137574  to: 0.07102133631706238\n",
      "Training iteration: 1103\n",
      "Validation loss (no improvement): 0.07102185487747192\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.07102133631706238  to: 0.07081547975540162\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.07081547975540162  to: 0.07057780027389526\n",
      "Training iteration: 1106\n",
      "Validation loss (no improvement): 0.07061437368392945\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.07057780027389526  to: 0.07045630216598511\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.07045630216598511  to: 0.07040532231330872\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.07040532231330872  to: 0.07015553712844849\n",
      "Training iteration: 1110\n",
      "Validation loss (no improvement): 0.07016436457633972\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.07015553712844849  to: 0.06997145414352417\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.06997145414352417  to: 0.06973829865455627\n",
      "Training iteration: 1113\n",
      "Validation loss (no improvement): 0.06977835893630982\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.06973829865455627  to: 0.06961511373519898\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.06961511373519898  to: 0.06954743266105652\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.06954743266105652  to: 0.06928443908691406\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.06928443908691406  to: 0.06928273439407348\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.06928273439407348  to: 0.06910482048988342\n",
      "Training iteration: 1119\n",
      "Validation loss (no improvement): 0.0691779911518097\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.06910482048988342  to: 0.06902400851249695\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.06902400851249695  to: 0.0687096357345581\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.0687096357345581  to: 0.06865859627723694\n",
      "Training iteration: 1123\n",
      "Validation loss (no improvement): 0.06886186599731445\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.06865859627723694  to: 0.06861592531204223\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.06861592531204223  to: 0.06822391152381897\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.06822391152381897  to: 0.06812074184417724\n",
      "Training iteration: 1127\n",
      "Validation loss (no improvement): 0.0682719349861145\n",
      "Training iteration: 1128\n",
      "Validation loss (no improvement): 0.06822368502616882\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.06812074184417724  to: 0.0678294062614441\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.0678294062614441  to: 0.06763360500335694\n",
      "Training iteration: 1131\n",
      "Validation loss (no improvement): 0.06768305897712708\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.06763360500335694  to: 0.06759621500968933\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.06759621500968933  to: 0.06735612154006958\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.06735612154006958  to: 0.06704409122467041\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.06704409122467041  to: 0.06701993346214294\n",
      "Training iteration: 1136\n",
      "Validation loss (no improvement): 0.06719950437545777\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.06701993346214294  to: 0.06693525314331054\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.06693525314331054  to: 0.06652033925056458\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.06652033925056458  to: 0.0664239764213562\n",
      "Training iteration: 1140\n",
      "Validation loss (no improvement): 0.06663201451301574\n",
      "Training iteration: 1141\n",
      "Validation loss (no improvement): 0.06652072668075562\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.0664239764213562  to: 0.06608694791793823\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.06608694791793823  to: 0.06599818468093872\n",
      "Training iteration: 1144\n",
      "Validation loss (no improvement): 0.06624348163604736\n",
      "Training iteration: 1145\n",
      "Validation loss (no improvement): 0.06611166000366211\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.06599818468093872  to: 0.06586297750473022\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.06586297750473022  to: 0.06557159423828125\n",
      "Training iteration: 1148\n",
      "Validation loss (no improvement): 0.0656066119670868\n",
      "Training iteration: 1149\n",
      "Validation loss (no improvement): 0.06588667035102844\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.06557159423828125  to: 0.06526591181755066\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.06526591181755066  to: 0.06501856446266174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1152\n",
      "Validation loss (no improvement): 0.06509467363357543\n",
      "Training iteration: 1153\n",
      "Validation loss (no improvement): 0.06509482860565186\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.06501856446266174  to: 0.0648235559463501\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.0648235559463501  to: 0.06442720890045166\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.06442720890045166  to: 0.06434227824211121\n",
      "Training iteration: 1157\n",
      "Validation loss (no improvement): 0.06450932621955871\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.06434227824211121  to: 0.06418426036834717\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.06418426036834717  to: 0.06389148831367493\n",
      "Training iteration: 1160\n",
      "Validation loss (no improvement): 0.06389360427856446\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.06389148831367493  to: 0.06385807991027832\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.06385807991027832  to: 0.06360138654708862\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.06360138654708862  to: 0.06331244707107545\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.06331244707107545  to: 0.0632935643196106\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.0632935643196106  to: 0.06324664354324341\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.06324664354324341  to: 0.06311494708061219\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.06311494708061219  to: 0.06282945275306702\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.06282945275306702  to: 0.06258659362792969\n",
      "Training iteration: 1169\n",
      "Validation loss (no improvement): 0.06271918416023255\n",
      "Training iteration: 1170\n",
      "Validation loss (no improvement): 0.06270244717597961\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.06258659362792969  to: 0.06224663853645325\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.06224663853645325  to: 0.06209843754768372\n",
      "Training iteration: 1173\n",
      "Validation loss (no improvement): 0.062187415361404416\n",
      "Training iteration: 1174\n",
      "Validation loss (no improvement): 0.062395143508911136\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.06209843754768372  to: 0.06195540428161621\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.06195540428161621  to: 0.061656725406646726\n",
      "Training iteration: 1177\n",
      "Validation loss (no improvement): 0.061659210920333864\n",
      "Training iteration: 1178\n",
      "Validation loss (no improvement): 0.062007796764373777\n",
      "Training iteration: 1179\n",
      "Validation loss (no improvement): 0.06172269582748413\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.061656725406646726  to: 0.06132264137268066\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.06132264137268066  to: 0.061288362741470336\n",
      "Training iteration: 1182\n",
      "Validation loss (no improvement): 0.06149628162384033\n",
      "Training iteration: 1183\n",
      "Validation loss (no improvement): 0.06138325333595276\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.061288362741470336  to: 0.0610524594783783\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.0610524594783783  to: 0.0609872579574585\n",
      "Training iteration: 1186\n",
      "Validation loss (no improvement): 0.061116749048233034\n",
      "Training iteration: 1187\n",
      "Validation loss (no improvement): 0.061108213663101194\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.0609872579574585  to: 0.06076791286468506\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.06076791286468506  to: 0.060659343004226686\n",
      "Training iteration: 1190\n",
      "Validation loss (no improvement): 0.060797262191772464\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.06072447896003723\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.060659343004226686  to: 0.06039391160011291\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.06039391160011291  to: 0.0603293240070343\n",
      "Training iteration: 1194\n",
      "Validation loss (no improvement): 0.06047024726867676\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.0603293240070343  to: 0.06027475595474243\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.06027475595474243  to: 0.05986606478691101\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.05986606478691101  to: 0.05971143245697021\n",
      "Training iteration: 1198\n",
      "Validation loss (no improvement): 0.05980125665664673\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.05971143245697021  to: 0.05970044136047363\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.05970044136047363  to: 0.05923030376434326\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.05923030376434326  to: 0.0590844988822937\n",
      "Training iteration: 1202\n",
      "Validation loss (no improvement): 0.05917457342147827\n",
      "Training iteration: 1203\n",
      "Validation loss (no improvement): 0.05919739603996277\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.0590844988822937  to: 0.05889009237289429\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.05889009237289429  to: 0.05874665975570679\n",
      "Training iteration: 1206\n",
      "Validation loss (no improvement): 0.058761441707611085\n",
      "Training iteration: 1207\n",
      "Validation loss (no improvement): 0.05887082815170288\n",
      "Training iteration: 1208\n",
      "Validation loss (no improvement): 0.05874737501144409\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.05874665975570679  to: 0.05848466753959656\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.05848466753959656  to: 0.05817948579788208\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.05817948579788208  to: 0.05810834169387817\n",
      "Training iteration: 1212\n",
      "Validation loss (no improvement): 0.05820597410202026\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.05810834169387817  to: 0.05781968235969544\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.05781968235969544  to: 0.057649314403533936\n",
      "Training iteration: 1215\n",
      "Validation loss (no improvement): 0.05767441391944885\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.057649314403533936  to: 0.057585179805755615\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.057585179805755615  to: 0.05738452672958374\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.05738452672958374  to: 0.057113480567932126\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.057113480567932126  to: 0.056952488422393796\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.056952488422393796  to: 0.056889545917510984\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.056889545917510984  to: 0.05687304139137268\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.05687304139137268  to: 0.05670294761657715\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.05670294761657715  to: 0.056330817937850955\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.056330817937850955  to: 0.056112664937973025\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.056112664937973025  to: 0.05581554174423218\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.05581554174423218  to: 0.055576658248901366\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.055576658248901366  to: 0.05553988218307495\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.05553988218307495  to: 0.0554448127746582\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.0554448127746582  to: 0.055369728803634645\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.055369728803634645  to: 0.05529716610908508\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.05529716610908508  to: 0.05494760274887085\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.05494760274887085  to: 0.05464226007461548\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.05464226007461548  to: 0.05456127524375916\n",
      "Training iteration: 1234\n",
      "Validation loss (no improvement): 0.054611760377883914\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.05456127524375916  to: 0.05454583168029785\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.05454583168029785  to: 0.054301989078521726\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.054301989078521726  to: 0.05387776494026184\n",
      "Training iteration: 1238\n",
      "Validation loss (no improvement): 0.053923189640045166\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.05387776494026184  to: 0.05367481112480164\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.05367481112480164  to: 0.05352736711502075\n",
      "Training iteration: 1241\n",
      "Validation loss (no improvement): 0.053586375713348386\n",
      "Training iteration: 1242\n",
      "Validation loss (no improvement): 0.0535941481590271\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.05352736711502075  to: 0.05334830284118652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.05334830284118652  to: 0.05308700799942016\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.05308700799942016  to: 0.0529960036277771\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.0529960036277771  to: 0.05291401743888855\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.05291401743888855  to: 0.05269510746002197\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.05269510746002197  to: 0.05266997218132019\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.05266997218132019  to: 0.05260254740715027\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.05260254740715027  to: 0.05256849527359009\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.05256849527359009  to: 0.05247541069984436\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.05247541069984436  to: 0.05232102274894714\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.05232102274894714  to: 0.05214433073997497\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.05214433073997497  to: 0.052126365900039676\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.0521795928478241\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.052126365900039676  to: 0.05209627151489258\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.05209627151489258  to: 0.0518979012966156\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.0518979012966156  to: 0.05180656909942627\n",
      "Training iteration: 1259\n",
      "Validation loss (no improvement): 0.051893317699432374\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.05180656909942627  to: 0.05177398920059204\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.05177398920059204  to: 0.051649314165115354\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.051649314165115354  to: 0.05156722068786621\n",
      "Training iteration: 1263\n",
      "Validation loss (no improvement): 0.051745283603668216\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.05156722068786621  to: 0.05155078768730163\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.05155078768730163  to: 0.051374185085296634\n",
      "Training iteration: 1266\n",
      "Validation loss (no improvement): 0.05138024091720581\n",
      "Training iteration: 1267\n",
      "Validation loss (no improvement): 0.05150192975997925\n",
      "Training iteration: 1268\n",
      "Validation loss (no improvement): 0.051580381393432614\n",
      "Training iteration: 1269\n",
      "Validation loss (no improvement): 0.05149877071380615\n",
      "Training iteration: 1270\n",
      "Validation loss (no improvement): 0.05149505138397217\n",
      "Training iteration: 1271\n",
      "Validation loss (no improvement): 0.051530444622039796\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.051701945066452024\n",
      "Training iteration: 1273\n",
      "Validation loss (no improvement): 0.05190605521202087\n",
      "Training iteration: 1274\n",
      "Validation loss (no improvement): 0.05184860229492187\n",
      "Training iteration: 1275\n",
      "Validation loss (no improvement): 0.051733368635177614\n",
      "Training iteration: 1276\n",
      "Validation loss (no improvement): 0.051698076725006106\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.051656323671340945\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.05169845819473266\n",
      "Training iteration: 1279\n",
      "Validation loss (no improvement): 0.05162569284439087\n",
      "Training iteration: 1280\n",
      "Validation loss (no improvement): 0.05161718130111694\n",
      "Training iteration: 1281\n",
      "Validation loss (no improvement): 0.051613879203796384\n",
      "Training iteration: 1282\n",
      "Validation loss (no improvement): 0.05148864984512329\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.05142801403999329\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.051374185085296634  to: 0.05131814479827881\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.05131814479827881  to: 0.05109344124794006\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.05109344124794006  to: 0.05090212821960449\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.05090212821960449  to: 0.050865447521209715\n",
      "Training iteration: 1288\n",
      "Validation loss (no improvement): 0.05090983510017395\n",
      "Training iteration: 1289\n",
      "Validation loss (no improvement): 0.05096675753593445\n",
      "Training iteration: 1290\n",
      "Validation loss (no improvement): 0.05087292790412903\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.050865447521209715  to: 0.05072086453437805\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.05072086453437805  to: 0.05054481625556946\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.05054481625556946  to: 0.05033079385757446\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.05033079385757446  to: 0.05021860003471375\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.05021860003471375  to: 0.05009254217147827\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.05009254217147827  to: 0.04992615282535553\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.04992615282535553  to: 0.0499168872833252\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.04998041093349457\n",
      "Training iteration: 1299\n",
      "Validation loss (no improvement): 0.049983492493629454\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.0499168872833252  to: 0.04989672303199768\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.04989672303199768  to: 0.04956872463226318\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.04956872463226318  to: 0.04932464063167572\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.04932464063167572  to: 0.04920122027397156\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.04920122027397156  to: 0.049189668893814084\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.049275445938110354\n",
      "Training iteration: 1306\n",
      "Validation loss (no improvement): 0.049382343888282776\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.04933915138244629\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.049189668893814084  to: 0.049134629964828494\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.049134629964828494  to: 0.048863381147384644\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.048863381147384644  to: 0.048696160316467285\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.048696160316467285  to: 0.04860162138938904\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.04860162138938904  to: 0.04849291741847992\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.04849291741847992  to: 0.04846586287021637\n",
      "Training iteration: 1314\n",
      "Validation loss (no improvement): 0.04858991503715515\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.04846586287021637  to: 0.0484574556350708\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.0484574556350708  to: 0.048220714926719664\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.048220714926719664  to: 0.048057809472084045\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.048057809472084045  to: 0.047984915971755984\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.047984915971755984  to: 0.04783455729484558\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.04783455729484558  to: 0.047629141807556154\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.047629141807556154  to: 0.04759184420108795\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.04759184420108795  to: 0.047479987144470215\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.047479987144470215  to: 0.047358670830726625\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.047358670830726625  to: 0.04723795056343079\n",
      "Training iteration: 1325\n",
      "Validation loss (no improvement): 0.04733101725578308\n",
      "Training iteration: 1326\n",
      "Validation loss (no improvement): 0.04728783965110779\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.04723795056343079  to: 0.047049275040626524\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.047049275040626524  to: 0.04693695604801178\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.04693695604801178  to: 0.04685955047607422\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.04685955047607422  to: 0.046621900796890256\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.046621900796890256  to: 0.04649507999420166\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.04649507999420166  to: 0.04649246633052826\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.04649246633052826  to: 0.0464294821023941\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.0464294821023941  to: 0.04625173509120941\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.04625173509120941  to: 0.04623110890388489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1336\n",
      "Validation loss (no improvement): 0.04631306529045105\n",
      "Training iteration: 1337\n",
      "Validation loss (no improvement): 0.046279612183570865\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.04623110890388489  to: 0.04611019194126129\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.04611019194126129  to: 0.0460521399974823\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.0460521399974823  to: 0.046021145582199094\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.046021145582199094  to: 0.045823806524276735\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.045823806524276735  to: 0.04577099680900574\n",
      "Training iteration: 1343\n",
      "Validation loss (no improvement): 0.045902404189109805\n",
      "Training iteration: 1344\n",
      "Validation loss (no improvement): 0.04598233103752136\n",
      "Training iteration: 1345\n",
      "Validation loss (no improvement): 0.04584197998046875\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.04577099680900574  to: 0.045654711127281186\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.045654711127281186  to: 0.04552890360355377\n",
      "Training iteration: 1348\n",
      "Validation loss (no improvement): 0.04557926058769226\n",
      "Training iteration: 1349\n",
      "Validation loss (no improvement): 0.04574847221374512\n",
      "Training iteration: 1350\n",
      "Validation loss (no improvement): 0.04590265154838562\n",
      "Training iteration: 1351\n",
      "Validation loss (no improvement): 0.04578725695610046\n",
      "Training iteration: 1352\n",
      "Validation loss (no improvement): 0.04560334682464599\n",
      "Training iteration: 1353\n",
      "Validation loss (no improvement): 0.04553682208061218\n",
      "Training iteration: 1354\n",
      "Validation loss (no improvement): 0.045651334524154666\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.04577078223228455\n",
      "Training iteration: 1356\n",
      "Validation loss (no improvement): 0.045807117223739625\n",
      "Training iteration: 1357\n",
      "Validation loss (no improvement): 0.04578292965888977\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.04577056467533112\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.045851579308509825\n",
      "Training iteration: 1360\n",
      "Validation loss (no improvement): 0.045701584219932555\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.04552890360355377  to: 0.04542571902275085\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.04542571902275085  to: 0.04538107812404633\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.04547989964485168\n",
      "Training iteration: 1364\n",
      "Validation loss (no improvement): 0.04560913443565369\n",
      "Training iteration: 1365\n",
      "Validation loss (no improvement): 0.04563709795475006\n",
      "Training iteration: 1366\n",
      "Validation loss (no improvement): 0.045542606711387636\n",
      "Training iteration: 1367\n",
      "Validation loss (no improvement): 0.045528721809387204\n",
      "Training iteration: 1368\n",
      "Validation loss (no improvement): 0.0455719530582428\n",
      "Training iteration: 1369\n",
      "Validation loss (no improvement): 0.04558202624320984\n",
      "Training iteration: 1370\n",
      "Validation loss (no improvement): 0.04545451104640961\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.04538107812404633  to: 0.04537917971611023\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.04537917971611023  to: 0.04531624913215637\n",
      "Training iteration: 1373\n",
      "Validation loss (no improvement): 0.04536927342414856\n",
      "Training iteration: 1374\n",
      "Validation loss (no improvement): 0.04561449587345123\n",
      "Training iteration: 1375\n",
      "Validation loss (no improvement): 0.04560122489929199\n",
      "Training iteration: 1376\n",
      "Validation loss (no improvement): 0.04546453356742859\n",
      "Training iteration: 1377\n",
      "Validation loss (no improvement): 0.04538385272026062\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.045399841666221616\n",
      "Training iteration: 1379\n",
      "Validation loss (no improvement): 0.04543498456478119\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.04531624913215637  to: 0.045295557379722594\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.045295557379722594  to: 0.04512768387794495\n",
      "Training iteration: 1382\n",
      "Validation loss (no improvement): 0.045155423879623416\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.045279321074485776\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.045324334502220155\n",
      "Training iteration: 1385\n",
      "Validation loss (no improvement): 0.04526486396789551\n",
      "Training iteration: 1386\n",
      "Validation loss (no improvement): 0.04515309333801269\n",
      "Training iteration: 1387\n",
      "Validation loss (no improvement): 0.045140084624290464\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.0452280193567276\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.04542431831359863\n",
      "Training iteration: 1390\n",
      "Validation loss (no improvement): 0.04532559812068939\n",
      "Training iteration: 1391\n",
      "Validation loss (no improvement): 0.04515652656555176\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.04512768387794495  to: 0.044992494583129886\n",
      "Training iteration: 1393\n",
      "Validation loss (no improvement): 0.0450366735458374\n",
      "Training iteration: 1394\n",
      "Validation loss (no improvement): 0.045114573836326596\n",
      "Training iteration: 1395\n",
      "Validation loss (no improvement): 0.04501868188381195\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.0451366513967514\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.045331114530563356\n",
      "Training iteration: 1398\n",
      "Validation loss (no improvement): 0.04532512128353119\n",
      "Training iteration: 1399\n",
      "Validation loss (no improvement): 0.045110052824020384\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.044992494583129886  to: 0.04497767984867096\n",
      "Training iteration: 1401\n",
      "Validation loss (no improvement): 0.044981497526168826\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.04497767984867096  to: 0.04490137696266174\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.04490137696266174  to: 0.04474751949310303\n",
      "Training iteration: 1404\n",
      "Validation loss (no improvement): 0.044793042540550235\n",
      "Training iteration: 1405\n",
      "Validation loss (no improvement): 0.045026722550392154\n",
      "Training iteration: 1406\n",
      "Validation loss (no improvement): 0.045241570472717284\n",
      "Training iteration: 1407\n",
      "Validation loss (no improvement): 0.045249351859092714\n",
      "Training iteration: 1408\n",
      "Validation loss (no improvement): 0.045098820328712465\n",
      "Training iteration: 1409\n",
      "Validation loss (no improvement): 0.044920405745506285\n",
      "Training iteration: 1410\n",
      "Validation loss (no improvement): 0.044881540536880496\n",
      "Training iteration: 1411\n",
      "Validation loss (no improvement): 0.04500056207180023\n",
      "Training iteration: 1412\n",
      "Validation loss (no improvement): 0.045109730958938596\n",
      "Training iteration: 1413\n",
      "Validation loss (no improvement): 0.04511326253414154\n",
      "Training iteration: 1414\n",
      "Validation loss (no improvement): 0.04522925019264221\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.04515050053596496\n",
      "Training iteration: 1416\n",
      "Validation loss (no improvement): 0.04503642022609711\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.044950437545776364\n",
      "Training iteration: 1418\n",
      "Validation loss (no improvement): 0.044837886095047\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.0447873204946518\n",
      "Training iteration: 1420\n",
      "Validation loss (no improvement): 0.04476495385169983\n",
      "Training iteration: 1421\n",
      "Validation loss (no improvement): 0.044923096895217896\n",
      "Training iteration: 1422\n",
      "Validation loss (no improvement): 0.044993418455123904\n",
      "Training iteration: 1423\n",
      "Validation loss (no improvement): 0.04490557610988617\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.04474751949310303  to: 0.04473366737365723\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.04473366737365723  to: 0.04461576342582703\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.044632631540298465\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.04474677443504334\n",
      "Training iteration: 1428\n",
      "Validation loss (no improvement): 0.04491114616394043\n",
      "Training iteration: 1429\n",
      "Validation loss (no improvement): 0.04483081698417664\n",
      "Training iteration: 1430\n",
      "Validation loss (no improvement): 0.04479564130306244\n",
      "Training iteration: 1431\n",
      "Validation loss (no improvement): 0.04470173716545105\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.04472452998161316\n",
      "Training iteration: 1433\n",
      "Validation loss (no improvement): 0.04477384686470032\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.04467483162879944\n",
      "Training iteration: 1435\n",
      "Validation loss (no improvement): 0.04476861953735352\n",
      "Training iteration: 1436\n",
      "Validation loss (no improvement): 0.04490913450717926\n",
      "Training iteration: 1437\n",
      "Validation loss (no improvement): 0.045016083121299746\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.04490202069282532\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.04465159773826599\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.04461576342582703  to: 0.04442327618598938\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.04442327618598938  to: 0.04436112344264984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.044478589296340944\n",
      "Training iteration: 1443\n",
      "Validation loss (no improvement): 0.044602543115615845\n",
      "Training iteration: 1444\n",
      "Validation loss (no improvement): 0.044612151384353635\n",
      "Training iteration: 1445\n",
      "Validation loss (no improvement): 0.0446207582950592\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.044651460647583005\n",
      "Training iteration: 1447\n",
      "Validation loss (no improvement): 0.044552621245384214\n",
      "Training iteration: 1448\n",
      "Validation loss (no improvement): 0.044590145349502563\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.04456731677055359\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.04436112344264984  to: 0.044358295202255246\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.044358295202255246  to: 0.0443138986825943\n",
      "Training iteration: 1452\n",
      "Validation loss (no improvement): 0.04439456462860107\n",
      "Training iteration: 1453\n",
      "Validation loss (no improvement): 0.04449649453163147\n",
      "Training iteration: 1454\n",
      "Validation loss (no improvement): 0.044598379731178285\n",
      "Training iteration: 1455\n",
      "Validation loss (no improvement): 0.04455004334449768\n",
      "Training iteration: 1456\n",
      "Validation loss (no improvement): 0.04457168579101563\n",
      "Training iteration: 1457\n",
      "Validation loss (no improvement): 0.044604760408401486\n",
      "Training iteration: 1458\n",
      "Validation loss (no improvement): 0.04463880956172943\n",
      "Training iteration: 1459\n",
      "Validation loss (no improvement): 0.0445708692073822\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.044408011436462405\n",
      "Training iteration: 1461\n",
      "Validation loss (no improvement): 0.04436460435390473\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.04440550804138184\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.04440947473049164\n",
      "Training iteration: 1464\n",
      "Validation loss (no improvement): 0.044391116499900816\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.0443138986825943  to: 0.04429834485054016\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.04429834485054016  to: 0.044251450896263124\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.04434801638126373\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.04427076280117035\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.044251450896263124  to: 0.044059190154075625\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.044059190154075625  to: 0.044010797142982484\n",
      "Training iteration: 1471\n",
      "Validation loss (no improvement): 0.044094786047935486\n",
      "Training iteration: 1472\n",
      "Validation loss (no improvement): 0.044187498092651364\n",
      "Training iteration: 1473\n",
      "Validation loss (no improvement): 0.044247770309448244\n",
      "Training iteration: 1474\n",
      "Validation loss (no improvement): 0.04421653747558594\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.04412913918495178\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.044124183058738706\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.044205349683761594\n",
      "Training iteration: 1478\n",
      "Validation loss (no improvement): 0.044169992208480835\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.04418303072452545\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.04407081604003906\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.044010797142982484  to: 0.043983626365661624\n",
      "Training iteration: 1482\n",
      "Validation loss (no improvement): 0.04402872622013092\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.043983626365661624  to: 0.04395439624786377\n",
      "Training iteration: 1484\n",
      "Validation loss (no improvement): 0.043961769342422484\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.04396008551120758\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.04395439624786377  to: 0.04395217895507812\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.04395217895507812  to: 0.043906277418136595\n",
      "Training iteration: 1488\n",
      "Validation loss (no improvement): 0.043926280736923215\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.043906277418136595  to: 0.043861931562423705\n",
      "Training iteration: 1490\n",
      "Validation loss (no improvement): 0.043878611922264096\n",
      "Training iteration: 1491\n",
      "Validation loss (no improvement): 0.043917837738990786\n",
      "Training iteration: 1492\n",
      "Validation loss (no improvement): 0.04389720857143402\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.043861931562423705  to: 0.04379170536994934\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.04379170536994934  to: 0.04378700852394104\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.04380287230014801\n",
      "Training iteration: 1496\n",
      "Validation loss (no improvement): 0.043798035383224486\n",
      "Training iteration: 1497\n",
      "Validation loss (no improvement): 0.04386441707611084\n",
      "Training iteration: 1498\n",
      "Validation loss (no improvement): 0.04390315115451813\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.04378700852394104  to: 0.043765288591384885\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.043765288591384885  to: 0.04368981420993805\n",
      "Training iteration: 1501\n",
      "Validation loss (no improvement): 0.043690663576126096\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.04380901753902435\n",
      "Training iteration: 1503\n",
      "Validation loss (no improvement): 0.04387743473052978\n",
      "Training iteration: 1504\n",
      "Validation loss (no improvement): 0.04371159076690674\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.04368981420993805  to: 0.04368816912174225\n",
      "Training iteration: 1506\n",
      "Validation loss (no improvement): 0.04377734065055847\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.04388163983821869\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.04391173720359802\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.043859702348709104\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.043764108419418336\n",
      "Training iteration: 1511\n",
      "Validation loss (no improvement): 0.04377298951148987\n",
      "Training iteration: 1512\n",
      "Validation loss (no improvement): 0.04385519027709961\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.0438097894191742\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.04386887550354004\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.043912744522094725\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.043827754259109494\n",
      "Training iteration: 1517\n",
      "Validation loss (no improvement): 0.04379239976406098\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.043697959184646605\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.04368816912174225  to: 0.04368233680725098\n",
      "Training iteration: 1520\n",
      "Validation loss (no improvement): 0.04371255934238434\n",
      "Training iteration: 1521\n",
      "Validation loss (no improvement): 0.04379414618015289\n",
      "Training iteration: 1522\n",
      "Validation loss (no improvement): 0.04376667141914368\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.04368233680725098  to: 0.04366934299468994\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.04366934299468994  to: 0.04357802867889404\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.04357802867889404  to: 0.04349936842918396\n",
      "Training iteration: 1526\n",
      "Validation loss (no improvement): 0.04356911778450012\n",
      "Training iteration: 1527\n",
      "Validation loss (no improvement): 0.043790823221206664\n",
      "Training iteration: 1528\n",
      "Validation loss (no improvement): 0.04384462833404541\n",
      "Training iteration: 1529\n",
      "Validation loss (no improvement): 0.04380687177181244\n",
      "Training iteration: 1530\n",
      "Validation loss (no improvement): 0.04368743896484375\n",
      "Training iteration: 1531\n",
      "Validation loss (no improvement): 0.043690276145935056\n",
      "Training iteration: 1532\n",
      "Validation loss (no improvement): 0.043681201338768\n",
      "Training iteration: 1533\n",
      "Validation loss (no improvement): 0.04351320266723633\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.04349936842918396  to: 0.04335435926914215\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.04348031878471374\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.043729791045188905\n",
      "Training iteration: 1537\n",
      "Validation loss (no improvement): 0.043880000710487366\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.04381313920021057\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.04362536370754242\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.04348219931125641\n",
      "Training iteration: 1541\n",
      "Validation loss (no improvement): 0.04353447556495667\n",
      "Training iteration: 1542\n",
      "Validation loss (no improvement): 0.043556660413742065\n",
      "Training iteration: 1543\n",
      "Validation loss (no improvement): 0.043538409471511844\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.0435302734375\n",
      "Training iteration: 1545\n",
      "Validation loss (no improvement): 0.04350292682647705\n",
      "Training iteration: 1546\n",
      "Validation loss (no improvement): 0.04355707168579102\n",
      "Training iteration: 1547\n",
      "Validation loss (no improvement): 0.04360485076904297\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.04354731440544128\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.043494635820388795\n",
      "Training iteration: 1550\n",
      "Validation loss (no improvement): 0.04345869123935699\n",
      "Training iteration: 1551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.043467774987220764\n",
      "Training iteration: 1552\n",
      "Validation loss (no improvement): 0.04335910677909851\n",
      "Training iteration: 1553\n",
      "Validation loss (no improvement): 0.043359813094139096\n",
      "Training iteration: 1554\n",
      "Validation loss (no improvement): 0.04346824586391449\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.04345965385437012\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.04335435926914215  to: 0.04332717061042786\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.04332717061042786  to: 0.043213766813278195\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.043213766813278195  to: 0.0431595504283905\n",
      "Training iteration: 1559\n",
      "Validation loss (no improvement): 0.04316072463989258\n",
      "Training iteration: 1560\n",
      "Validation loss (no improvement): 0.04330448508262634\n",
      "Training iteration: 1561\n",
      "Validation loss (no improvement): 0.04345409274101257\n",
      "Training iteration: 1562\n",
      "Validation loss (no improvement): 0.04345125257968903\n",
      "Training iteration: 1563\n",
      "Validation loss (no improvement): 0.04334052205085755\n",
      "Training iteration: 1564\n",
      "Validation loss (no improvement): 0.04322695732116699\n",
      "Training iteration: 1565\n",
      "Validation loss (no improvement): 0.04323652386665344\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.0431781530380249\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.0431595504283905  to: 0.043066161870956424\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.043066161870956424  to: 0.04305872917175293\n",
      "Training iteration: 1569\n",
      "Validation loss (no improvement): 0.04310999810695648\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.04312788844108582\n",
      "Training iteration: 1571\n",
      "Validation loss (no improvement): 0.043079248070716857\n",
      "Training iteration: 1572\n",
      "Validation loss (no improvement): 0.04306996762752533\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.043134135007858274\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.04326610565185547\n",
      "Training iteration: 1575\n",
      "Validation loss (no improvement): 0.04322598874568939\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.04310331344604492\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.04305872917175293  to: 0.04300035536289215\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.04300035536289215  to: 0.042951589822769164\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.04307659566402435\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.04336590766906738\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.043590706586837766\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.04358912408351898\n",
      "Training iteration: 1583\n",
      "Validation loss (no improvement): 0.04349165856838226\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.04323816895484924\n",
      "Training iteration: 1585\n",
      "Validation loss (no improvement): 0.04314824640750885\n",
      "Training iteration: 1586\n",
      "Validation loss (no improvement): 0.0433844655752182\n",
      "Training iteration: 1587\n",
      "Validation loss (no improvement): 0.04314491748809814\n",
      "Training iteration: 1588\n",
      "Validation loss (no improvement): 0.04318306446075439\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.04329231381416321\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.043251451849937436\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.04327763020992279\n",
      "Training iteration: 1592\n",
      "Validation loss (no improvement): 0.04327864646911621\n",
      "Training iteration: 1593\n",
      "Validation loss (no improvement): 0.043192076683044436\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.043129491806030276\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.04317172169685364\n",
      "Training iteration: 1596\n",
      "Validation loss (no improvement): 0.04330664277076721\n",
      "Training iteration: 1597\n",
      "Validation loss (no improvement): 0.04332226812839508\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.043194884061813356\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.04311836361885071\n",
      "Training iteration: 1600\n",
      "Validation loss (no improvement): 0.043023204803466795\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.04299693107604981\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.043039712309837344\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.043125033378601074\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.04304375648498535\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.042951589822769164  to: 0.042920246720314026\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.042920246720314026  to: 0.042846545577049255\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.042846545577049255  to: 0.04279425144195557\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.04283512234687805\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.043032145500183104\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.04311867654323578\n",
      "Training iteration: 1611\n",
      "Validation loss (no improvement): 0.04304472804069519\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.0428656280040741\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.04279623031616211\n",
      "Training iteration: 1614\n",
      "Validation loss (no improvement): 0.042802318930625916\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.04286720156669617\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.04279425144195557  to: 0.04269862174987793\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.04275959134101868\n",
      "Training iteration: 1618\n",
      "Validation loss (no improvement): 0.04286181926727295\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.04286659359931946\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.04272269606590271\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.04269862174987793  to: 0.04265207350254059\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.04265902042388916\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.042770928144454955\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.04286307692527771\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.04295620024204254\n",
      "Training iteration: 1626\n",
      "Validation loss (no improvement): 0.04301029741764069\n",
      "Training iteration: 1627\n",
      "Validation loss (no improvement): 0.04294990599155426\n",
      "Training iteration: 1628\n",
      "Validation loss (no improvement): 0.04281181693077087\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.0427218347787857\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.042709237337112425\n",
      "Training iteration: 1631\n",
      "Validation loss (no improvement): 0.04275175929069519\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.042826014757156375\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.0427354633808136\n",
      "Training iteration: 1634\n",
      "Validation loss (no improvement): 0.0427138477563858\n",
      "Training iteration: 1635\n",
      "Validation loss (no improvement): 0.04270482957363129\n",
      "Training iteration: 1636\n",
      "Validation loss (no improvement): 0.04278683662414551\n",
      "Training iteration: 1637\n",
      "Validation loss (no improvement): 0.04274356961250305\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.04271804690361023\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.04279730916023254\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.04287452697753906\n",
      "Training iteration: 1641\n",
      "Validation loss (no improvement): 0.042858543992042544\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.04266463816165924\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.04265207350254059  to: 0.04250691831111908\n",
      "Training iteration: 1644\n",
      "Validation loss (no improvement): 0.04253371357917786\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.04266354441642761\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.042726239562034606\n",
      "Training iteration: 1647\n",
      "Validation loss (no improvement): 0.0427510678768158\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.0427280992269516\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.04262884259223938\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.042635050415992734\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.04265078902244568\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.04256206452846527\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.04250691831111908  to: 0.0424758106470108\n",
      "Training iteration: 1654\n",
      "Validation loss (no improvement): 0.04255213737487793\n",
      "Training iteration: 1655\n",
      "Validation loss (no improvement): 0.04257861077785492\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.04247645437717438\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.0424758106470108  to: 0.04239109456539154\n",
      "Training iteration: 1658\n",
      "Validation loss (no improvement): 0.0424321711063385\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.04255889058113098\n",
      "Training iteration: 1660\n",
      "Validation loss (no improvement): 0.04253838062286377\n",
      "Training iteration: 1661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.042485445737838745\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.04241939187049866\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.04244118332862854\n",
      "Training iteration: 1664\n",
      "Validation loss (no improvement): 0.04241760671138763\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.04239109456539154  to: 0.04237693846225739\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.04239078462123871\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.04237693846225739  to: 0.0423293262720108\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.0423293262720108  to: 0.04231448769569397\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.04231448769569397  to: 0.04229812622070313\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.04229812622070313  to: 0.042252248525619505\n",
      "Training iteration: 1671\n",
      "Validation loss (no improvement): 0.04230049550533295\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.042252248525619505  to: 0.042227286100387576\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.042227286100387576  to: 0.042142629623413086\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.042181020975112914\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.04220629334449768\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.04216871857643127\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.042142629623413086  to: 0.04211672246456146\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.042169198393821716\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.04220278263092041\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.042147836089134215\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.042139443755149844\n",
      "Training iteration: 1682\n",
      "Validation loss (no improvement): 0.04228244423866272\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.04249833524227142\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.042466840147972106\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.042274728417396545\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.042309927940368655\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.042256927490234374\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.04223684370517731\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.042453151941299436\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.04226457476615906\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.04231096804141998\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.04243689477443695\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.04249153137207031\n",
      "Training iteration: 1694\n",
      "Validation loss (no improvement): 0.042379873991012576\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.04230235517024994\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.042237120866775515\n",
      "Training iteration: 1697\n",
      "Validation loss (no improvement): 0.04228557050228119\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.04247268736362457\n",
      "Training iteration: 1699\n",
      "Validation loss (no improvement): 0.04270800650119781\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.04269002079963684\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.04240188002586365\n",
      "Training iteration: 1702\n",
      "Validation loss (no improvement): 0.04224444031715393\n",
      "Training iteration: 1703\n",
      "Validation loss (no improvement): 0.0422710657119751\n",
      "Training iteration: 1704\n",
      "Validation loss (no improvement): 0.042480605840682986\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.04247421622276306\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.042321881651878356\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.04230335652828217\n",
      "Training iteration: 1708\n",
      "Validation loss (no improvement): 0.04224795699119568\n",
      "Training iteration: 1709\n",
      "Validation loss (no improvement): 0.042393407225608824\n",
      "Training iteration: 1710\n",
      "Validation loss (no improvement): 0.04249202609062195\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.04230253696441651\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.042153143882751466\n",
      "Training iteration: 1713\n",
      "Validation loss (no improvement): 0.04222298562526703\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.042278987169265744\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.04214375913143158\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.04211672246456146  to: 0.042045801877975464\n",
      "Training iteration: 1717\n",
      "Validation loss (no improvement): 0.042067280411720274\n",
      "Training iteration: 1718\n",
      "Validation loss (no improvement): 0.04216157495975494\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.04241794645786286\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.04260958731174469\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.04255174994468689\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.04231367707252502\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.042065364122390744\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.042045801877975464  to: 0.0419520914554596\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.0419520914554596  to: 0.04194369912147522\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.04194369912147522  to: 0.041935008764266965\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.04209143221378327\n",
      "Training iteration: 1728\n",
      "Validation loss (no improvement): 0.04207229018211365\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.041935008764266965  to: 0.04187966287136078\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.04187966287136078  to: 0.04176421165466308\n",
      "Training iteration: 1731\n",
      "Validation loss (no improvement): 0.041929006576538086\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.042097702622413635\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.042093902826309204\n",
      "Training iteration: 1734\n",
      "Validation loss (no improvement): 0.04207147657871246\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.042078590393066405\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.04195796549320221\n",
      "Training iteration: 1737\n",
      "Validation loss (no improvement): 0.04193840622901916\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.041894730925559995\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.04180966019630432\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.04193660616874695\n",
      "Training iteration: 1741\n",
      "Validation loss (no improvement): 0.04206018447875977\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.04201157689094544\n",
      "Training iteration: 1743\n",
      "Validation loss (no improvement): 0.041850775480270386\n",
      "Training iteration: 1744\n",
      "Validation loss (no improvement): 0.04183741509914398\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.04201238751411438\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.0419971376657486\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.041909390687942506\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.04198703169822693\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.04206059575080871\n",
      "Training iteration: 1750\n",
      "Validation loss (no improvement): 0.04209051728248596\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.04203484058380127\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.04197755753993988\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.041942563652992246\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.04179419875144959\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.04176421165466308  to: 0.04170741140842438\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.04182887971401215\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.041981163620948794\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.04179717898368836\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.041822990775108336\n",
      "Training iteration: 1760\n",
      "Validation loss (no improvement): 0.04195029735565185\n",
      "Training iteration: 1761\n",
      "Validation loss (no improvement): 0.04215395450592041\n",
      "Training iteration: 1762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.04214365482330322\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.04190787374973297\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.04170741140842438  to: 0.04169825613498688\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.04169825613498688  to: 0.041638034582138064\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.04170563220977783\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.04164527952671051\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.041638034582138064  to: 0.041524830460548404\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.04153772890567779\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.04167984426021576\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.04187022745609283\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.041789588332176206\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.04166554510593414\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.04156912863254547\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.04153554439544678\n",
      "Training iteration: 1776\n",
      "Validation loss (no improvement): 0.041621160507202146\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.0416079193353653\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.0415686309337616\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.041643986105918886\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.04166538119316101\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.04157271981239319\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.041524830460548404  to: 0.04142301082611084\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.041479524970054624\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.04156068861484528\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.04144846796989441\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.041450795531272885\n",
      "Training iteration: 1787\n",
      "Validation loss (no improvement): 0.04158182740211487\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.04171133041381836\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.041784143447875975\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.041691190004348753\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.04158059060573578\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.04149275720119476\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.04142301082611084  to: 0.04128176271915436\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.04129739701747894\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.04147056043148041\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.04142617285251617\n",
      "Training iteration: 1797\n",
      "Validation loss (no improvement): 0.04149390161037445\n",
      "Training iteration: 1798\n",
      "Validation loss (no improvement): 0.04159438014030457\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.04167512357234955\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.041708764433860776\n",
      "Training iteration: 1801\n",
      "Validation loss (no improvement): 0.041660147905349734\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.041557532548904416\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.04143374562263489\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.04130794405937195\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.04128176271915436  to: 0.0412583202123642\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.0412583202123642  to: 0.04112306535243988\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.041163516044616696\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.04135324358940125\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.041300544142723085\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.04129189550876618\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.041242972016334534\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.04119771122932434\n",
      "Training iteration: 1813\n",
      "Validation loss (no improvement): 0.04115907549858093\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.04113067090511322\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.04121760725975036\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.04125568866729736\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.04114990830421448\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.04112306535243988  to: 0.041098952293395996\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.04111076295375824\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.041098952293395996  to: 0.04098917841911316\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.041022729873657224\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.04117225706577301\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.04100553989410401\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.04102385640144348\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.041086965799331666\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.04123444557189941\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.041336679458618165\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.04119988977909088\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.04112659096717834\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.04118959903717041\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.04112745225429535\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.04115478098392487\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.04098917841911316  to: 0.04091240763664246\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.04091240763664246  to: 0.04085647165775299\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.04095878601074219\n",
      "Training iteration: 1836\n",
      "Validation loss (no improvement): 0.041261449456214905\n",
      "Training iteration: 1837\n",
      "Validation loss (no improvement): 0.04129241406917572\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.04122745990753174\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.04115787148475647\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.041115695238113405\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.04106913506984711\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.0409700870513916\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.04096695482730865\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.04111347198486328\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.04103480875492096\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.04093081057071686\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.04085647165775299  to: 0.04083400368690491\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.04087842404842377\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.041105526685714724\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.040907925367355345\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.04083400368690491  to: 0.04079567492008209\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.04079567492008209  to: 0.0407352864742279\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.04087595939636231\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.04103372693061828\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.04109164774417877\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.04104219377040863\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.040925273299217226\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.040874630212783813\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.04075552523136139\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.0407352864742279  to: 0.04071508347988129\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.040867391228675845\n",
      "Training iteration: 1862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.040986213088035586\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.040890473127365115\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.04075047969818115\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.04081820547580719\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.041183823347091676\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.04113214612007141\n",
      "Training iteration: 1868\n",
      "Validation loss (no improvement): 0.0409928023815155\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.04102141261100769\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.04092887043952942\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.04093853533267975\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.04079278111457825\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.04071508347988129  to: 0.04062905311584473\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.040721893310546875\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.04100160002708435\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.0411836564540863\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.04101377427577972\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.04075919985771179\n",
      "Training iteration: 1879\n",
      "Validation loss (no improvement): 0.040725478529930116\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.04088211953639984\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.04076662063598633\n",
      "Training iteration: 1882\n",
      "Validation loss (no improvement): 0.04063206613063812\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.0408133327960968\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.04094537794589996\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.04081718921661377\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.04062905311584473  to: 0.040618062019348145\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.040618062019348145  to: 0.040588784217834475\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.04073923230171204\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.04061659276485443\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.0406154066324234\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.04069341719150543\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.04064224362373352\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.040588784217834475  to: 0.04051083922386169\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.04051083922386169  to: 0.04046184420585632\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.04046184420585632  to: 0.040453338623046876\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.040497463941574094\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.040559664368629456\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.04062175750732422\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.040605860948562625\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.040547427535057065\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.040488401055336\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.040453338623046876  to: 0.0404416561126709\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.04047908186912537\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.040529146790504456\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.04051252007484436\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.0404416561126709  to: 0.040322956442832944\n",
      "Training iteration: 1907\n",
      "Validation loss (no improvement): 0.04032712876796722\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.04048808217048645\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.040698391199111936\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.04057144522666931\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.04045743048191071\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.04037600159645081\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.04052404761314392\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.04074432849884033\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.04066407084465027\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.040627670288085935\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.04069560170173645\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.04055663049221039\n",
      "Training iteration: 1919\n",
      "Validation loss (no improvement): 0.04040193557739258\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.040322956442832944  to: 0.04031818807125091\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.04031818807125091  to: 0.040280896425247195\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.04040636122226715\n",
      "Training iteration: 1923\n",
      "Validation loss (no improvement): 0.0406035840511322\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.04068722724914551\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.04047229886054993\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.04034302234649658\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.040280896425247195  to: 0.04023180603981018\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.040534192323684694\n",
      "Training iteration: 1929\n",
      "Validation loss (no improvement): 0.04058743417263031\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.04038731455802917\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.040578097105026245\n",
      "Training iteration: 1932\n",
      "Validation loss (no improvement): 0.04084766805171967\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.040697401762008666\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.04040204882621765\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.04039216935634613\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.04035733342170715\n",
      "Training iteration: 1937\n",
      "Validation loss (no improvement): 0.04027157425880432\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.0403714120388031\n",
      "Training iteration: 1939\n",
      "Validation loss (no improvement): 0.04042222499847412\n",
      "Training iteration: 1940\n",
      "Validation loss (no improvement): 0.04034293293952942\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.04023180603981018  to: 0.040155476331710814\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.040155476331710814  to: 0.040126490592956546\n",
      "Training iteration: 1943\n",
      "Validation loss (no improvement): 0.04025723934173584\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.040209132432937625\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.040205922722816465\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.0402126133441925\n",
      "Training iteration: 1947\n",
      "Validation loss (no improvement): 0.040150561928749086\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.040126490592956546  to: 0.040122637152671815\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.040122637152671815  to: 0.03989850878715515\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.03989850878715515  to: 0.03987969160079956\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.03996472954750061\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.03993507027626038\n",
      "Training iteration: 1953\n",
      "Validation loss (no improvement): 0.0399149239063263\n",
      "Training iteration: 1954\n",
      "Validation loss (no improvement): 0.04001685976982117\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.04007160067558289\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.040010714530944826\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.03990606665611267\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.03987969160079956  to: 0.0398405522108078\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.039898842573165894\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.0398405522108078  to: 0.03982498049736023\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.0398699015378952\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.040098723769187924\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.040085607767105104\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.040015965700149536\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.03994447886943817\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.04009598791599274\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.04012502729892731\n",
      "Training iteration: 1968\n",
      "Validation loss (no improvement): 0.040001922845840455\n",
      "Training iteration: 1969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.040062570571899415\n",
      "Training iteration: 1970\n",
      "Validation loss (no improvement): 0.04002806544303894\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.04008927345275879\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.03982498049736023  to: 0.03977498412132263\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.03977498412132263  to: 0.03968248665332794\n",
      "Training iteration: 1974\n",
      "Validation loss (no improvement): 0.039759212732315065\n",
      "Training iteration: 1975\n",
      "Validation loss (no improvement): 0.039999350905418396\n",
      "Training iteration: 1976\n",
      "Validation loss (no improvement): 0.04019598960876465\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.04004404544830322\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.03993361890316009\n",
      "Training iteration: 1979\n",
      "Validation loss (no improvement): 0.039969834685325625\n",
      "Training iteration: 1980\n",
      "Validation loss (no improvement): 0.04004632830619812\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.039948534965515134\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.04008960723876953\n",
      "Training iteration: 1983\n",
      "Validation loss (no improvement): 0.04035233855247498\n",
      "Training iteration: 1984\n",
      "Validation loss (no improvement): 0.04043814539909363\n",
      "Training iteration: 1985\n",
      "Validation loss (no improvement): 0.04015495777130127\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.03989178836345673\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.039916253089904784\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.04008859097957611\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.039964359998703\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.03993414342403412\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.040141114592552186\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.04014166295528412\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.03992651402950287\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.039816954731941225\n",
      "Training iteration: 1995\n",
      "Validation loss (no improvement): 0.03981027603149414\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.03982612788677216\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.03980916142463684\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.03996800780296326\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.04008851945400238\n",
      "Training iteration: 2000\n",
      "Validation loss (no improvement): 0.04007621705532074\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.03968248665332794  to: 0.03965008854866028\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.03965008854866028  to: 0.039455166459083556\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.03954148292541504\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.03969292938709259\n",
      "Training iteration: 2005\n",
      "Validation loss (no improvement): 0.03983686864376068\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.040152111649513246\n",
      "Training iteration: 2007\n",
      "Validation loss (no improvement): 0.04014101922512055\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.04004411101341247\n",
      "Training iteration: 2009\n",
      "Validation loss (no improvement): 0.03997011780738831\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.03973805010318756\n",
      "Training iteration: 2011\n",
      "Validation loss (no improvement): 0.03958922624588013\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.03964776694774628\n",
      "Training iteration: 2013\n",
      "Validation loss (no improvement): 0.039748063683509825\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.03982861340045929\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.03967580795288086\n",
      "Training iteration: 2016\n",
      "Validation loss (no improvement): 0.03953281342983246\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.039713111519813535\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.039939981698989865\n",
      "Training iteration: 2019\n",
      "Validation loss (no improvement): 0.039964991807937625\n",
      "Training iteration: 2020\n",
      "Validation loss (no improvement): 0.03993818461894989\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.03980976939201355\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.03969108164310455\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.039652660489082336\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.039455166459083556  to: 0.03941325843334198\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.03941325843334198  to: 0.03927975296974182\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.0393661230802536\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.03935025632381439\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.03932372033596039\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.03946624398231506\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.03952976763248443\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.03945576548576355\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.03945510983467102\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.03947243392467499\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.03951405882835388\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.03941191136837006\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.03927975296974182  to: 0.03926823735237121\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.03926823735237121  to: 0.039176341891288755\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.039293569326400754\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.039176341891288755  to: 0.03908027708530426\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.03913421034812927\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.039383035898208615\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.03952022194862366\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.03933997750282288\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.039282628893852235\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.039429846405982974\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.039616727828979494\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.03960077464580536\n",
      "Training iteration: 2048\n",
      "Validation loss (no improvement): 0.03960002064704895\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.039567381143569946\n",
      "Training iteration: 2050\n",
      "Validation loss (no improvement): 0.03948425054550171\n",
      "Training iteration: 2051\n",
      "Validation loss (no improvement): 0.039381939172744754\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.03939369320869446\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.03930484652519226\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.03925475180149078\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.03922571241855621\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.03938218951225281\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.03951378464698792\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.03919139504432678\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.03908027708530426  to: 0.03904047310352325\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.039207625389099124\n",
      "Training iteration: 2061\n",
      "Validation loss (no improvement): 0.039483457803726196\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): 0.03948606252670288\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): 0.03949476778507233\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.03940154910087586\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.0392379105091095\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.03928229212760925\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.03912940919399262\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.03904047310352325  to: 0.0390300452709198\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.039178508520126346\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.039278656244277954\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.039321678876876834\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.03928690552711487\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): 0.03952862322330475\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): 0.039505892992019655\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.039324778318405154\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.039282745122909545\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): 0.03928463160991669\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.03932337164878845\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): 0.03918463885784149\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): 0.039043459296226504\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.039030256867408755\n",
      "Training iteration: 2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03904048502445221\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.0390300452709198  to: 0.0389409065246582\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.0389409065246582  to: 0.038901299238204956\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.03911547064781189\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): 0.03931615650653839\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): 0.039245986938476564\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): 0.039404386281967164\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): 0.03950816988945007\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.03949854969978332\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): 0.03928039371967316\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): 0.03911322355270386\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.039146366715431216\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.03924489915370941\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.039146503806114195\n",
      "Training iteration: 2096\n",
      "Validation loss (no improvement): 0.039231449365615845\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): 0.03935620486736298\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.0395108163356781\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.039321741461753844\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.03909823298454285\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.039033979177474976\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.039066153764724734\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.03911965787410736\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.03897224068641662\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.038901299238204956  to: 0.038886943459510805\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.03917044103145599\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.03965713679790497\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.03931536078453064\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.038886943459510805  to: 0.038780012726783754\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.038780012726783754  to: 0.038745331764221194\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.03880676925182343\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.03887197971343994\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.03879945278167725\n",
      "Training iteration: 2114\n",
      "Validation loss (no improvement): 0.03891238570213318\n",
      "Training iteration: 2115\n",
      "Validation loss (no improvement): 0.03911675810813904\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.03910585343837738\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.03908988535404205\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): 0.03910879194736481\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.03913752436637878\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.03918295204639435\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.038869640231132506\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.03875648379325867\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.03885383605957031\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.03883261680603027\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.038846740126609804\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.038826900720596316\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.038856875896453855\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.03901213109493255\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.03902212679386139\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.03881675601005554\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.038745331764221194  to: 0.03874284327030182\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.038749194145202635\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.03874284327030182  to: 0.03860948979854584\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.03864227831363678\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.03871132433414459\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.03888794481754303\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.03860948979854584  to: 0.038536357879638675\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.038536357879638675  to: 0.0384099006652832\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.03861630260944367\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.038990873098373416\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.03889848291873932\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.03892392218112946\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.038876083493232724\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.03925924301147461\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.0393405556678772\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.03902159929275513\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.03898693323135376\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.039011219143867494\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.03904185891151428\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): 0.03883192837238312\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): 0.038606292009353636\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.03850861191749573\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.03877031207084656\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): 0.03898088037967682\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.03856375515460968\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): 0.03861572146415711\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.038856664299964906\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.039203956723213196\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): 0.038873687386512756\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.03869345784187317\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.03868170976638794\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.03890438973903656\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.03894547820091247\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.03879052102565765\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): 0.03875692784786224\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): 0.0388331264257431\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.03897826671600342\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.038723069429397586\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.03868412673473358\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.038739013671875\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.03895971179008484\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.03918604254722595\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): 0.03923345506191254\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): 0.03908578753471374\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.03886076807975769\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.038652199506759646\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.03848362565040588\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.038436365127563474\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.0384099006652832  to: 0.03835188746452332\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): 0.03840904235839844\n",
      "Training iteration: 2181\n",
      "Validation loss (no improvement): 0.038818654417991635\n",
      "Training iteration: 2182\n",
      "Validation loss (no improvement): 0.038843923807144166\n",
      "Training iteration: 2183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03859610557556152\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.03852519989013672\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): 0.038572964072227475\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): 0.03866798877716064\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): 0.03855262398719787\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): 0.03866468369960785\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.03874694108963013\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): 0.03862129747867584\n",
      "Training iteration: 2191\n",
      "Validation loss (no improvement): 0.03868553042411804\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): 0.03870716691017151\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.03859255313873291\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.0384723037481308\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.038561505079269406\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.038570350408554076\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.03868713974952698\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): 0.03865509033203125\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): 0.03843222856521607\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.03835188746452332  to: 0.03833807110786438\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): 0.03845541775226593\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): 0.03854727745056152\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.03855349123477936\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): 0.03851096332073212\n",
      "Training iteration: 2205\n",
      "Validation loss (no improvement): 0.038537648320198056\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.03833807110786438  to: 0.038286879658699036\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.038286879658699036  to: 0.038187462091445926\n",
      "Training iteration: 2208\n",
      "Validation loss (no improvement): 0.03837524056434631\n",
      "Training iteration: 2209\n",
      "Validation loss (no improvement): 0.03855111598968506\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.03862200379371643\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.03848116993904114\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.03832014501094818\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.03840230703353882\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.0385022908449173\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.03829277753829956\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.03825024664402008\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): 0.03825719356536865\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.038269102573394775\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.038207927346229555\n",
      "Training iteration: 2220\n",
      "Validation loss (no improvement): 0.038225552439689635\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): 0.038491946458816526\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.038642865419387815\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.03856622576713562\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.03836371600627899\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): 0.03821613192558289\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.038187462091445926  to: 0.038160857558250424\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.038178443908691406\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.038160857558250424  to: 0.0378661572933197\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.037895360589027406\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.03821069598197937\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.03806102871894836\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.037964668869972226\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.03803337514400482\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.03804885447025299\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): 0.03811380565166474\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): 0.03828733563423157\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.03835178315639496\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.038064485788345336\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.03790154755115509\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.03796042799949646\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.038144621253013614\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.0379662811756134\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.03798587322235107\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): 0.03817287087440491\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.0382070392370224\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.037978291511535645\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): 0.037888675928115845\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): 0.03789801895618439\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.03812139630317688\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.0378661572933197  to: 0.03781842589378357\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.03781842589378357  to: 0.037735334038734435\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.03789602816104889\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): 0.03830403685569763\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.03849749565124512\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.0382209450006485\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.0380627453327179\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.038595476746559144\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.03881693780422211\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.038199836015701295\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.03821659684181213\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.038214048743247984\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.03844580054283142\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.03804710209369659\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.03788542151451111\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): 0.03794325590133667\n",
      "Training iteration: 2266\n",
      "Validation loss (no improvement): 0.038168877363204956\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): 0.03829772174358368\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.03822624087333679\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): 0.03814098238945007\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.038337358832359315\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.03830469250679016\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): 0.03795472085475922\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): 0.037812060117721556\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): 0.037955823540687564\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): 0.037935873866081236\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.037735334038734435  to: 0.03773015141487122\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.03773015141487122  to: 0.03763070106506348\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.03763070106506348  to: 0.03761869072914124\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.03787713646888733\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.03766559958457947\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.037627759575843814\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.0377399355173111\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): 0.03790983557701111\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): 0.03788752257823944\n",
      "Training iteration: 2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03771601319313049\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.03761869072914124  to: 0.03755634129047394\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.03755634129047394  to: 0.037488967180252075\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.03760228157043457\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.037488967180252075  to: 0.03743185102939606\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): 0.03753243088722229\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.037908044457435605\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.03780719935894013\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): 0.03767066597938538\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): 0.03770424425601959\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): 0.03791435658931732\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): 0.037904936075210574\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.03787356019020081\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.037863293290138246\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): 0.03781742155551911\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.03746526837348938\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.03743185102939606  to: 0.03737167119979858\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.03751588463783264\n",
      "Training iteration: 2303\n",
      "Validation loss (no improvement): 0.03754352927207947\n",
      "Training iteration: 2304\n",
      "Validation loss (no improvement): 0.03758801817893982\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.03756960928440094\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): 0.037694099545478824\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): 0.0377200186252594\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): 0.03764219284057617\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): 0.03748260140419006\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.03737167119979858  to: 0.03733380436897278\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): 0.03739810883998871\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.03751151561737061\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.03740746974945068\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.0376375138759613\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.03797505795955658\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.03755277097225189\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): 0.037579292058944704\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.03806295394897461\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.03807846903800964\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): 0.037858182191848756\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.037877225875854494\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): 0.03788788914680481\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.03769268393516541\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.03762798011302948\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): 0.037548485398292544\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): 0.03757665455341339\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.03784217536449432\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.037901613116264346\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.03759685158729553\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): 0.03773507475852966\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): 0.03799445033073425\n",
      "Training iteration: 2332\n",
      "Validation loss (no improvement): 0.03759057819843292\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 0.037425097823143\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): 0.037640196084976194\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): 0.03760003447532654\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): 0.037607821822166446\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): 0.03735539019107818\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.037358146905899045\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): 0.037632149457931516\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): 0.03770173490047455\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): 0.03759835362434387\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.03773358464241028\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.03795166611671448\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): 0.037383294105529784\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.037353810667991635\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.03794801831245422\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.037939295172691345\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.037589386105537415\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): 0.03752924799919129\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.03739548623561859\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.03808590471744537\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.037358996272087094\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.03733380436897278  to: 0.03703280091285706\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.03719650208950043\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.03779456615447998\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.03767072558403015\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.03753020167350769\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.03751346468925476\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.038220956921577454\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): 0.03798876702785492\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.03763610422611237\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.03778336644172668\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.03777290880680084\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.03772118091583252\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.03730260729789734\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.037182649970054625\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.03717745840549469\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.037380582094192503\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): 0.03730073571205139\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.037263882160186765\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.037354621291160586\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.03783767819404602\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.03767661452293396\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.037275117635726926\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.037205669283866885\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.03721729516983032\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.03707432746887207\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.037034842371940616\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.03714178800582886\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): 0.03725777268409729\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.03736642003059387\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.037330719828605655\n",
      "Training iteration: 2383\n",
      "Improved validation loss from: 0.03703280091285706  to: 0.037028568983078006\n",
      "Training iteration: 2384\n",
      "Improved validation loss from: 0.037028568983078006  to: 0.036987999081611635\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.037215018272399904\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.03729900121688843\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.036987999081611635  to: 0.03690608441829681\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.03690608441829681  to: 0.03680538535118103\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.03699285387992859\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.03701496720314026\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): 0.03710700869560242\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): 0.03730628788471222\n",
      "Training iteration: 2393\n",
      "Validation loss (no improvement): 0.03716399669647217\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): 0.0370280921459198\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): 0.03692335188388825\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): 0.03687455654144287\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.03680538535118103  to: 0.03679131865501404\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): 0.0368673026561737\n",
      "Training iteration: 2399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03701123893260956\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.03709761202335358\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.037072718143463135\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.03699430227279663\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): 0.036995401978492735\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.037035229802131656\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.03693276047706604\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.03679131865501404  to: 0.036714988946914676\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.036714988946914676  to: 0.036704257130622864\n",
      "Training iteration: 2408\n",
      "Validation loss (no improvement): 0.036893796920776364\n",
      "Training iteration: 2409\n",
      "Validation loss (no improvement): 0.03683577179908752\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): 0.036855575442314145\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.03697511553764343\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.03676159977912903\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.036704257130622864  to: 0.03652494549751282\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.03661107122898102\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.03654772639274597\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.036614009737968446\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.03677419722080231\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.036718863248825076\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.03666306436061859\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.0365366667509079\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.03684577941894531\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.036567068099975585\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.03652494549751282  to: 0.03646872937679291\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.03703436553478241\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.037132281064987185\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.036992183327674864\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.036831825971603394\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.03710253238677978\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.03699854910373688\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: 0.03646872937679291  to: 0.036402159929275514\n",
      "Training iteration: 2431\n",
      "Improved validation loss from: 0.036402159929275514  to: 0.03632606863975525\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.03669679760932922\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.03653883934020996\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.03657463192939758\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.036765292286872864\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.03676437139511109\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.03673189878463745\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.03671037256717682\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.03675418198108673\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.03650708198547363\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.03632606863975525  to: 0.03624575138092041\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.036263388395309445\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.03624575138092041  to: 0.03613909780979156\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.036395800113677976\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.03638434112071991\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.03613909780979156  to: 0.03613195419311523\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.03617205619812012\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.03651133477687836\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.03613195419311523  to: 0.035998567938804626\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.03617951273918152\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.03733290731906891\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): 0.0372318297624588\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.03644133508205414\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): 0.03636156916618347\n",
      "Training iteration: 2455\n",
      "Validation loss (no improvement): 0.0364993155002594\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): 0.03696665167808533\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.0364017903804779\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.03640108704566956\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.03690171241760254\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.0372771680355072\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): 0.036482995748519896\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.03632303476333618\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.03660441040992737\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.03701797127723694\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.03651339709758759\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.036506292223930356\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.036727112531661985\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.036744576692581174\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): 0.03658718466758728\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.03635944724082947\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.036372947692871097\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.03657588660717011\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.03641465008258819\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): 0.03610807359218597\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.03608806729316712\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.03632561564445495\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.03626647293567657\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.03621304631233215\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.03614349365234375\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.03618772327899933\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.03620003163814545\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.035998567938804626  to: 0.035909521579742434\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.035909521579742434  to: 0.035752058029174805\n",
      "Training iteration: 2484\n",
      "Validation loss (no improvement): 0.03575509488582611\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.036060935258865355\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.035977494716644284\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.035820388793945314\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): 0.035940113663673404\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.03594351708889008\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): 0.03582485616207123\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.03612937033176422\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.03586494624614715\n",
      "Training iteration: 2493\n",
      "Improved validation loss from: 0.035752058029174805  to: 0.035573247075080874\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.035673150420188905\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): 0.036405247449874875\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.03595978319644928\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.035573247075080874  to: 0.03542269468307495\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): 0.03547516465187073\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.03591775000095367\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.03590934574604034\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.03576809763908386\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.035885858535766604\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.036292916536331175\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.03631531298160553\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.03585473299026489\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.03591485023498535\n",
      "Training iteration: 2507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03625029027462005\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.03570096790790558\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.03568733930587768\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.036136379837989806\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.03644792139530182\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.0358408272266388\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.035565045475959775\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.035612308979034425\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.03575778901576996\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.03542269468307495  to: 0.03517584502696991\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.03517584502696991  to: 0.03517491817474365\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.03582857251167297\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.03598061501979828\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.03527452349662781\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.03517491817474365  to: 0.03515985608100891\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.03657199442386627\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.03693501353263855\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.03553524315357208\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.03561444878578186\n",
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.03591881096363068\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): 0.03713953495025635\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.03587434887886047\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): 0.035661786794662476\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.03606509566307068\n",
      "Training iteration: 2531\n",
      "Validation loss (no improvement): 0.03660699725151062\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.035707446932792666\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.0355664074420929\n",
      "Training iteration: 2534\n",
      "Validation loss (no improvement): 0.03636719286441803\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.03678824901580811\n",
      "Training iteration: 2536\n",
      "Validation loss (no improvement): 0.0358087956905365\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.03559185564517975\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): 0.03566916286945343\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.036225587129592896\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.03542793095111847\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.03522107601165771\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.03569684028625488\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): 0.0362823486328125\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.035737639665603636\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.035645657777786256\n",
      "Training iteration: 2546\n",
      "Validation loss (no improvement): 0.03580988049507141\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.03568558096885681\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.03526200950145721\n",
      "Training iteration: 2549\n",
      "Improved validation loss from: 0.03515985608100891  to: 0.035129374265670775\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.03544556498527527\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.035225576162338255\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: 0.035129374265670775  to: 0.03472573459148407\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.03472961783409119\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.03482827246189117\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: 0.03472573459148407  to: 0.03464469611644745\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.0350435197353363\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.03501648604869843\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.03466599881649017\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.03474005162715912\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.0349596917629242\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.03481476902961731\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: 0.03464469611644745  to: 0.03449682891368866\n",
      "Training iteration: 2563\n",
      "Validation loss (no improvement): 0.03461209237575531\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.034774914383888245\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.034793508052825925\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.03464411199092865\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.035072407126426695\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.034925156831741334\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.03497980237007141\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.035102549195289615\n",
      "Training iteration: 2571\n",
      "Validation loss (no improvement): 0.035170912742614746\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.034752371907234195\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.03484448492527008\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.0352886825799942\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.034590548276901244\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.034512028098106384\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.03504605889320374\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.03499719798564911\n",
      "Training iteration: 2579\n",
      "Improved validation loss from: 0.03449682891368866  to: 0.03441185057163239\n",
      "Training iteration: 2580\n",
      "Improved validation loss from: 0.03441185057163239  to: 0.03437856733798981\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.03489800095558167\n",
      "Training iteration: 2582\n",
      "Improved validation loss from: 0.03437856733798981  to: 0.03432268500328064\n",
      "Training iteration: 2583\n",
      "Improved validation loss from: 0.03432268500328064  to: 0.03423207700252533\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.03477292656898499\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): 0.03474880456924438\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.03436764776706695\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.03434814512729645\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.035394692420959474\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.03510300517082214\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.03428327143192291\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.0344186007976532\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.03507305681705475\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.034708532691001895\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): 0.03461972773075104\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): 0.035158935189247134\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.03543784022331238\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.0345135748386383\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.034339460730552676\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.0347229540348053\n",
      "Training iteration: 2600\n",
      "Improved validation loss from: 0.03423207700252533  to: 0.03419641852378845\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): 0.034201774001121524\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): 0.034928178787231444\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.03475337028503418\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): 0.03420957624912262\n",
      "Training iteration: 2605\n",
      "Improved validation loss from: 0.03419641852378845  to: 0.03415353894233704\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): 0.03464429378509522\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.034462863206863405\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.0342642605304718\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.03415353894233704  to: 0.034103882312774655\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): 0.03421238362789154\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.034103882312774655  to: 0.033788859844207764\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): 0.03402805626392365\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.03454374670982361\n",
      "Training iteration: 2614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.034027138352394105\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.03395298719406128\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.034204131364822386\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.034208554029464724\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.03381025791168213\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.03379817008972168\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.0341279923915863\n",
      "Training iteration: 2621\n",
      "Improved validation loss from: 0.033788859844207764  to: 0.033721691370010375\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.03382663428783417\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.03373413383960724\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.03390839099884033\n",
      "Training iteration: 2625\n",
      "Improved validation loss from: 0.033721691370010375  to: 0.03361258506774902\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.03387047946453094\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.03415147364139557\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.0338076114654541\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.03390294015407562\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.03465310633182526\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.03395528793334961\n",
      "Training iteration: 2632\n",
      "Improved validation loss from: 0.03361258506774902  to: 0.03344662189483642\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.03365923166275024\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.03428695499897003\n",
      "Training iteration: 2635\n",
      "Improved validation loss from: 0.03344662189483642  to: 0.033254006505012514\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.033348995447158816\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.03500276207923889\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.034057697653770445\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.033629050850868224\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.03371992111206055\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.034747809171676636\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.034224790334701535\n",
      "Training iteration: 2643\n",
      "Validation loss (no improvement): 0.033743107318878175\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.0339055210351944\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.03428940773010254\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.033321487903594973\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): 0.03334900736808777\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.03430277109146118\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.0341401070356369\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.033446550369262695\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.033617281913757326\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.034347894787788394\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.034150701761245725\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.03378586173057556\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.03382393419742584\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.03411640524864197\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.03357890248298645\n",
      "Training iteration: 2658\n",
      "Improved validation loss from: 0.033254006505012514  to: 0.032915204763412476\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.03317162990570068\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.03510388135910034\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.0336672455072403\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.03319480419158936\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.033647197484970096\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.035003119707107545\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.03399824798107147\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.03368443250656128\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.034041449427604675\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.034537845849990846\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.033835941553115846\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.032952070236206055\n",
      "Training iteration: 2671\n",
      "Improved validation loss from: 0.032915204763412476  to: 0.032863196730613706\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): 0.03409091830253601\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): 0.03399246335029602\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.032953059673309325\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.03290648460388183\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.03350436091423035\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.03337575793266297\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.03295768201351166\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.033361440896987914\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.03343595862388611\n",
      "Training iteration: 2681\n",
      "Improved validation loss from: 0.032863196730613706  to: 0.03283476233482361\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.032904377579689024\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.033904293179512025\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): 0.033165961503982544\n",
      "Training iteration: 2685\n",
      "Improved validation loss from: 0.03283476233482361  to: 0.032535210251808167\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.03325663506984711\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.034006553888320926\n",
      "Training iteration: 2688\n",
      "Improved validation loss from: 0.032535210251808167  to: 0.03248542845249176\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.03251772820949554\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.03366760313510895\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.033647948503494264\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.03281102478504181\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.033000460267066954\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.03351512551307678\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.03304707407951355\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.032655858993530275\n",
      "Training iteration: 2697\n",
      "Validation loss (no improvement): 0.03278457522392273\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.03301441669464111\n",
      "Training iteration: 2699\n",
      "Improved validation loss from: 0.03248542845249176  to: 0.03220915198326111\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.03238809108734131\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.033523136377334596\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.032913804054260254\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.032408338785171506\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.03275589346885681\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.033223995566368104\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.03235042989253998\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.032324370741844174\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.0332971453666687\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.03299589157104492\n",
      "Training iteration: 2710\n",
      "Improved validation loss from: 0.03220915198326111  to: 0.03218263983726501\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.03243231177330017\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.033344107866287234\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.03230121731758118\n",
      "Training iteration: 2714\n",
      "Validation loss (no improvement): 0.03245140910148621\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.03358587622642517\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.033081501722335815\n",
      "Training iteration: 2717\n",
      "Improved validation loss from: 0.03218263983726501  to: 0.031810274720191954\n",
      "Training iteration: 2718\n",
      "Improved validation loss from: 0.031810274720191954  to: 0.03173701167106628\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.03307396471500397\n",
      "Training iteration: 2720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03364112973213196\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.031817442178726195\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): 0.0318985641002655\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): 0.03300600945949554\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.03225078582763672\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.032001766562461856\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.03294044137001038\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.03323406279087067\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): 0.03209368884563446\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.03192299008369446\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.032681864500045774\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.03248099684715271\n",
      "Training iteration: 2732\n",
      "Improved validation loss from: 0.03173701167106628  to: 0.03161152005195618\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.031647789478302005\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.03258906304836273\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.03161152005195618  to: 0.03148369193077087\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.03164526522159576\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.032566016912460326\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.032270416617393494\n",
      "Training iteration: 2739\n",
      "Improved validation loss from: 0.03148369193077087  to: 0.03109161853790283\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.03132721483707428\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.03390182554721832\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.03273041844367981\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.031307125091552736\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.03174248635768891\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.033127954602241515\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.03227864503860474\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.03165922164916992\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.03199257254600525\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.03260056376457214\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.031467705965042114\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.03130518198013306\n",
      "Training iteration: 2752\n",
      "Validation loss (no improvement): 0.03269434571266174\n",
      "Training iteration: 2753\n",
      "Validation loss (no improvement): 0.03266708552837372\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): 0.03132439851760864\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.031228193640708925\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.03218122124671936\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.03217911720275879\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.03145959079265594\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.03136598169803619\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.031469729542732236\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.031278961896896364\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.03134581446647644\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.03185620605945587\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.031196799874305726\n",
      "Training iteration: 2765\n",
      "Improved validation loss from: 0.03109161853790283  to: 0.030786961317062378\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.031374949216842654\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.03159824013710022\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.031099820137023927\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.03134045600891113\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): 0.031414994597435\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.03145297765731812\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.03100050985813141\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.03087020814418793\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.03129221200942993\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.03093504309654236\n",
      "Training iteration: 2776\n",
      "Improved validation loss from: 0.030786961317062378  to: 0.030169302225112916\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.03047264218330383\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.030791154503822325\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.03055645525455475\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.030813318490982056\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.03111807405948639\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): 0.03051866888999939\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.030257648229598998\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.030722007155418396\n",
      "Training iteration: 2785\n",
      "Improved validation loss from: 0.030169302225112916  to: 0.030094283819198608\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.030422231554985045\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.03094833791255951\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.03020797371864319\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.030523961782455443\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.030750104784965517\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.030308985710144044\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.0302497535943985\n",
      "Training iteration: 2793\n",
      "Improved validation loss from: 0.030094283819198608  to: 0.029754909873008727\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.030340930819511412\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.03013082444667816\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.029847103357315063\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): 0.03039107620716095\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): 0.03040749728679657\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.03024396300315857\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.03023148775100708\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.03008067011833191\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.03054175674915314\n",
      "Training iteration: 2803\n",
      "Improved validation loss from: 0.029754909873008727  to: 0.029629993438720702\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.030052390694618226\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.031230843067169188\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.030026426911354064\n",
      "Training iteration: 2807\n",
      "Improved validation loss from: 0.029629993438720702  to: 0.029222393035888673\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): 0.029530218243598937\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.030686569213867188\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.02923378348350525\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.029892510175704955\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): 0.03145481944084168\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.03024825155735016\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): 0.02957811951637268\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): 0.029782614111900328\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.03152533769607544\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.030580848455429077\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.029718700051307678\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.030600029230117797\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.0306499183177948\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.0294630229473114\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): 0.03002367615699768\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.031647485494613645\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.030219095945358276\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.029364633560180663\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.029449379444122313\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.030195236206054688\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.029658880829811097\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.029222393035888673  to: 0.028856700658798216\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.029478353261947633\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): 0.029326018691062928\n",
      "Training iteration: 2832\n",
      "Improved validation loss from: 0.028856700658798216  to: 0.02821579873561859\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.028757268190383913\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.03130112290382385\n",
      "Training iteration: 2835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.029055818915367126\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.028824281692504884\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): 0.030744829773902894\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): 0.030266505479812623\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.029241314530372618\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.02946150302886963\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.0308770626783371\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.02970260977745056\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.02831417918205261\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.028517380356788635\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.0299063503742218\n",
      "Training iteration: 2846\n",
      "Improved validation loss from: 0.02821579873561859  to: 0.02786868214607239\n",
      "Training iteration: 2847\n",
      "Improved validation loss from: 0.02786868214607239  to: 0.02780454158782959\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.029947099089622498\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.030966436862945555\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.028306040167808532\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.02839455008506775\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.030682212114334105\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.031681740283966066\n",
      "Training iteration: 2854\n",
      "Validation loss (no improvement): 0.02922637164592743\n",
      "Training iteration: 2855\n",
      "Validation loss (no improvement): 0.028707218170166016\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): 0.03004063367843628\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.03140917420387268\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): 0.028305277228355408\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.02800458073616028\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.0294866681098938\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): 0.030999985337257386\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): 0.028363388776779175\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.0283297061920166\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.03003801703453064\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.031044259667396545\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): 0.028429993987083436\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.028091317415237425\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.029473546147346496\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.03060763478279114\n",
      "Training iteration: 2870\n",
      "Improved validation loss from: 0.02780454158782959  to: 0.027773183584213258\n",
      "Training iteration: 2871\n",
      "Improved validation loss from: 0.027773183584213258  to: 0.0274669349193573\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.029472243785858155\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.029961413145065306\n",
      "Training iteration: 2874\n",
      "Improved validation loss from: 0.0274669349193573  to: 0.027274131774902344\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.02729552686214447\n",
      "Training iteration: 2876\n",
      "Validation loss (no improvement): 0.02985720634460449\n",
      "Training iteration: 2877\n",
      "Validation loss (no improvement): 0.03025258183479309\n",
      "Training iteration: 2878\n",
      "Validation loss (no improvement): 0.02761679291725159\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.027770805358886718\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.030103904008865357\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.031538328528404234\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.028178781270980835\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.02745463252067566\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.028929299116134642\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): 0.030930495262145995\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.027865153551101685\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.02729079723358154\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): 0.02893182635307312\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.02942209839820862\n",
      "Training iteration: 2890\n",
      "Improved validation loss from: 0.027274131774902344  to: 0.02722390592098236\n",
      "Training iteration: 2891\n",
      "Improved validation loss from: 0.02722390592098236  to: 0.0270937442779541\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.02869083285331726\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.03022574484348297\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.027687075734138488\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.027564316987991333\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.029631230235099792\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.030074819922447205\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.02788251042366028\n",
      "Training iteration: 2899\n",
      "Improved validation loss from: 0.0270937442779541  to: 0.026996782422065733\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.027273079752922057\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.028642106056213378\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): 0.028044429421424866\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.026996782422065733  to: 0.026888388395309448\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.027418416738510133\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.02825469374656677\n",
      "Training iteration: 2906\n",
      "Validation loss (no improvement): 0.02895868718624115\n",
      "Training iteration: 2907\n",
      "Validation loss (no improvement): 0.027246254682540893\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.026888388395309448  to: 0.02651827931404114\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): 0.028499364852905273\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.029123800992965698\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.027015963196754457\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): 0.027260726690292357\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): 0.029481607675552367\n",
      "Training iteration: 2914\n",
      "Validation loss (no improvement): 0.030173611640930176\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): 0.02734106481075287\n",
      "Training iteration: 2916\n",
      "Improved validation loss from: 0.02651827931404114  to: 0.02624896764755249\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.02763528227806091\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.029902270436286925\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.028223329782485963\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): 0.026620927453041076\n",
      "Training iteration: 2921\n",
      "Validation loss (no improvement): 0.027772289514541627\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.029992824792861937\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.027530723810195924\n",
      "Training iteration: 2924\n",
      "Improved validation loss from: 0.02624896764755249  to: 0.025950533151626588\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.026116842031478883\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.02790002226829529\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.026584935188293458\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.026374751329422\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.027839678525924682\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.028150504827499388\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.02690635621547699\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.025956571102142334\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.02640621066093445\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.027463486790657042\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): 0.02675677537918091\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): 0.026136705279350282\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.026549112796783448\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.028633129596710206\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.02715676724910736\n",
      "Training iteration: 2940\n",
      "Improved validation loss from: 0.025950533151626588  to: 0.025295579433441163\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.025758114457130433\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.02632034420967102\n",
      "Training iteration: 2943\n",
      "Improved validation loss from: 0.025295579433441163  to: 0.02520431876182556\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.02536314129829407\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.026478463411331178\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.02767893373966217\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.0268498957157135\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.025344076752662658\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.025595921277999877\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.026643043756484984\n",
      "Training iteration: 2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02520431876182556  to: 0.02490774691104889\n",
      "Training iteration: 2952\n",
      "Improved validation loss from: 0.02490774691104889  to: 0.024245424568653105\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.02583930194377899\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): 0.02685588002204895\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.025308358669281005\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.024687567353248598\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.025198692083358766\n",
      "Training iteration: 2958\n",
      "Validation loss (no improvement): 0.025690585374832153\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.0263854444026947\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): 0.025485047698020936\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.02649158239364624\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.027606719732284547\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.026056450605392457\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): 0.025448495149612428\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.026288625597953797\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.02594679594039917\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.02657022774219513\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.027652722597122193\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.0263711154460907\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.02573334574699402\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.02621624171733856\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.026735636591911315\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.024459123611450195\n",
      "Training iteration: 2974\n",
      "Improved validation loss from: 0.024245424568653105  to: 0.023630011081695556\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.02547914981842041\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.025869232416152955\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.024319911003112794\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.02579991817474365\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.027648019790649413\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.02446366548538208\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.02380698174238205\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.02546485960483551\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.027344995737075807\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.02520589828491211\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.02603062093257904\n",
      "Training iteration: 2986\n",
      "Validation loss (no improvement): 0.029764774441719054\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.027312463521957396\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.025023090839385986\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.02486667186021805\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.027170562744140626\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.026754218339920043\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.025432983040809633\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.0258989155292511\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.02588745653629303\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.024858704209327696\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.025049424171447753\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.02684570848941803\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.025546437501907347\n",
      "Training iteration: 2999\n",
      "Improved validation loss from: 0.023630011081695556  to: 0.02361627370119095\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.02371949702501297\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.025124627351760864\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.025331979990005492\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.02389470338821411\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.0237962007522583\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.02603297829627991\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.024247626960277557\n",
      "Training iteration: 3007\n",
      "Improved validation loss from: 0.02361627370119095  to: 0.023609133064746858\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.024537701904773713\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.02595469355583191\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.024476294219493867\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): 0.02390410006046295\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): 0.024049150943756103\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.024656586349010468\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): 0.0245218425989151\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): 0.02445218563079834\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.023856015503406526\n",
      "Training iteration: 3017\n",
      "Improved validation loss from: 0.023609133064746858  to: 0.022891397774219512\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.02346944510936737\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.023560449481010437\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.023863942921161653\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): 0.02345455139875412\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): 0.024971151351928712\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.023844924569129945\n",
      "Training iteration: 3024\n",
      "Improved validation loss from: 0.022891397774219512  to: 0.022819292545318604\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.02381560504436493\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.024857151508331298\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.024141547083854676\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.02333654910326004\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.02418448030948639\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.025357481837272645\n",
      "Training iteration: 3031\n",
      "Improved validation loss from: 0.022819292545318604  to: 0.022793833911418915\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.023512403666973113\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.024995359778404235\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.02417033016681671\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.022872355580329896\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.023470275104045868\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.025991764664649964\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.02443377673625946\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.023310735821723938\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.023920683562755583\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.025901436805725098\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.02384202927350998\n",
      "Training iteration: 3043\n",
      "Improved validation loss from: 0.022793833911418915  to: 0.022689482569694518\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.022834911942481995\n",
      "Training iteration: 3045\n",
      "Validation loss (no improvement): 0.0236776739358902\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.024629060924053193\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.02467683106660843\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.024597558379173278\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.02519952356815338\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.02380910813808441\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.022860372066497804\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.02370824068784714\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.026353222131729127\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.02372157573699951\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.022964611649513245\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.024881389737129212\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.02562362253665924\n",
      "Training iteration: 3058\n",
      "Improved validation loss from: 0.022689482569694518  to: 0.02248721867799759\n",
      "Training iteration: 3059\n",
      "Improved validation loss from: 0.02248721867799759  to: 0.021599717438220978\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.024609294533729554\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.02410239279270172\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.02267213761806488\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.023132435977458954\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.025091725587844848\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.023184147477149964\n",
      "Training iteration: 3066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.023163679242134094\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): 0.02472902536392212\n",
      "Training iteration: 3068\n",
      "Validation loss (no improvement): 0.02376871556043625\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.02278227061033249\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.023910515010356903\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.024901778995990755\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.022042933106422424\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.022022226452827455\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.024567048251628875\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.02535582482814789\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.022327132523059845\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.02254321575164795\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.025525107979774475\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.02612918019294739\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.022006472945213316\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.022099801898002626\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.024653296172618865\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.028148600459098817\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.022522826492786408\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.02232675105333328\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.02455558329820633\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.027313870191574097\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.025042060017585754\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.02372095137834549\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.02470039576292038\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.02697093188762665\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.02584804594516754\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.02387893944978714\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.02285439521074295\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.024237418174743654\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.026926642656326293\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.02458338737487793\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.02233649492263794\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.02233833372592926\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.023327918350696565\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.025509351491928102\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.023450613021850586\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.021762481331825255\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.022552895545959472\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.024703040719032288\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.023939959704875946\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.022807097434997557\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.02405836135149002\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.025338637828826904\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.025249040126800536\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.024954052269458772\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.022618675231933595\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.02197764813899994\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.022749614715576173\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.02460448443889618\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.02325412482023239\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.022961851954460145\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.023234005272388458\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.022971487045288085\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.02274270951747894\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.022385887801647186\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.0229925200343132\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.02372768372297287\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.023713855445384978\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.023063302040100098\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.023807744681835174\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.02282429039478302\n",
      "Training iteration: 3128\n",
      "Improved validation loss from: 0.021599717438220978  to: 0.021433286368846893\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.021892401576042175\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.02287393808364868\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.0245374396443367\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.024723222851753233\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.024452796578407286\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.0241595983505249\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.024298150837421418\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.023794174194335938\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.023469002544879915\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.023615333437919616\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.024810397624969484\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.02366688698530197\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.023390960693359376\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.02279038429260254\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.022711896896362306\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.021983781456947328\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.02157466858625412\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.02335505485534668\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.02315397560596466\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.022357162833213807\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.02283949851989746\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.024656669795513154\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.022900764644145966\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.022963280975818633\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.024670977890491486\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.023754362761974335\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.022314123809337616\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.023360621929168702\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.023046818375587464\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.02222815006971359\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.022567591071128844\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.02332739382982254\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.02280435562133789\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.022231054306030274\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.023346380889415742\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.02243521511554718\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.02265312671661377\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.02527705430984497\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.022628863155841828\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.022511968016624452\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.024159622192382813\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.02317587435245514\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.0221865177154541\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.023177103698253633\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.024505233764648436\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.023270812630653382\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.02269933670759201\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.023999734222888945\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.023117165267467498\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.02200969010591507\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): 0.02264762222766876\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.02286076098680496\n",
      "Training iteration: 3181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.022036853432655334\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.0236176460981369\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.023154416680336\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.022630417346954347\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.02384043037891388\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.02181226760149002\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.022258965671062468\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.022803954780101776\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.022029855847358705\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.021931210160255434\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.022717876732349394\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.023032471537590027\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.02204062193632126\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.022556042671203612\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.022075536847114562\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.02308637648820877\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.023061421513557435\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.022160768508911133\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.023984992504119874\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.022109346091747285\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.021999749541282653\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.02167600840330124\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.021743083000183107\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.022323231399059295\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.023323576152324676\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.022198209166526796\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.02235958129167557\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.022813332080841065\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.022814106941223145\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.02219775915145874\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.0228561207652092\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.02292925864458084\n",
      "Training iteration: 3213\n",
      "Improved validation loss from: 0.021433286368846893  to: 0.021405045688152314\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.023223766684532167\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.02229183167219162\n",
      "Training iteration: 3216\n",
      "Improved validation loss from: 0.021405045688152314  to: 0.021249926090240477\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.022534868121147154\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.02176188975572586\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.02154650688171387\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.022209660708904268\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.0232222318649292\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.022655753791332243\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.02210245132446289\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.022521428763866425\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.02401532679796219\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.022246453166007995\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.02229006290435791\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.02540438175201416\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): 0.02402852028608322\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.021769225597381592\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.022095827758312224\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.024239745736122132\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.022220578789711\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.021725392341613768\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.02290825843811035\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.02473311722278595\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.022449612617492676\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.022163960337638854\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.023052188754081725\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.022465130686759947\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.021890708804130556\n",
      "Training iteration: 3242\n",
      "Improved validation loss from: 0.021249926090240477  to: 0.021028916537761688\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.022590485215187073\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.02535509467124939\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.02250225841999054\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.02119995355606079\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.022689099609851836\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.02343497723340988\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.021663686633110045\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.021831735968589783\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.023011770844459534\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.022648921608924864\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.02253369092941284\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.022214236855506896\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.023411445319652557\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.022864320874214174\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.0224771648645401\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.024586081504821777\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.02464642971754074\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.022169962525367737\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.02163712531328201\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.022829127311706544\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.022630071640014647\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.02254105806350708\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.02286088466644287\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.022961243987083435\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.021684947609901428\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.02248496264219284\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.025346550345420837\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.023909559845924376\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.022691912949085236\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.023535652458667754\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.022816395759582518\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.02289593666791916\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.023381051421165467\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.021641293168067934\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.021475639939308167\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.022667625546455385\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.022911882400512694\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.02174971401691437\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.021129000186920165\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): 0.021773605048656462\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.023088058829307555\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.02269222289323807\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.022799630463123322\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.02387425899505615\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.02399893254041672\n",
      "Training iteration: 3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.021410822868347168\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.022542376816272736\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.025339895486831666\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.023000161349773406\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.021918173134326934\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.022551675140857697\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.023991115391254425\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.02364614009857178\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.02232916057109833\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.022450163960456848\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.02298686057329178\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.023111876845359803\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.022050563991069794\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.023464226722717287\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.023296673595905305\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.02219582349061966\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.02161305844783783\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.023315973579883575\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.024069364368915557\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.022021098434925078\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.022531287372112276\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.023821869492530824\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.022707220911979676\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.02199336588382721\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.022892217338085174\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.0243173286318779\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.022364544868469238\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.02277538478374481\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.023602095246315003\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.021356475353240967\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.021592064201831816\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.023522567749023438\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.023496589064598082\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.02170195132493973\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.021867017447948455\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.023662808537483215\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.022695699334144594\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.02335217297077179\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.024236223101615904\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.022963719069957735\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.02258450537919998\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.022325921058654784\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.0231743723154068\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.022897472977638243\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.021740496158599854\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.02252764403820038\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.023152665793895723\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.021579428017139433\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.023725609481334686\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.02342524528503418\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.021412932872772218\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.021883685886859894\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.02386356294155121\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.02330854833126068\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.022019127011299135\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_1 = toy_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_1.train_model()\n",
    "ensemble_model_1.model_inference()\n",
    "\n",
    "ensemble_mean_1 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_1 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_1 = np.sqrt(np.exp(ensemble_logvar_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 10.909194183349609\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 10.909194183349609  to: 7.125478363037109\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 7.125478363037109  to: 4.795714569091797\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 4.795714569091797  to: 3.327770233154297\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 3.327770233154297  to: 2.3798297882080077\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 2.3798297882080077  to: 1.7574073791503906\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 1.7574073791503906  to: 1.3429850578308105\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 1.3429850578308105  to: 1.0514018058776855\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.0514018058776855  to: 0.8420825004577637\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 0.8420825004577637  to: 0.6896063804626464\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 0.6896063804626464  to: 0.5762453079223633\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.5762453079223633  to: 0.4903977394104004\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.4903977394104004  to: 0.4243775844573975\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.4243775844573975  to: 0.3730465412139893\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.3730465412139893  to: 0.332740330696106\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.332740330696106  to: 0.30081377029418943\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.30081377029418943  to: 0.2753103017807007\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.2753103017807007  to: 0.2547886610031128\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.2547886610031128  to: 0.2381383180618286\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.2381383180618286  to: 0.2245323896408081\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.2245323896408081  to: 0.21334760189056395\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.21334760189056395  to: 0.20410022735595704\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.20410022735595704  to: 0.19640734195709228\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.19640734195709228  to: 0.18998258113861083\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.18998258113861083  to: 0.18457857370376587\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.18457857370376587  to: 0.1800096869468689\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.1800096869468689  to: 0.17612626552581787\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.17612626552581787  to: 0.1728123903274536\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.1728123903274536  to: 0.16996705532073975\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.16996705532073975  to: 0.16751571893692016\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.16751571893692016  to: 0.1653955101966858\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.1653955101966858  to: 0.16355339288711548\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.16355339288711548  to: 0.16194803714752198\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.16194803714752198  to: 0.16054191589355468\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.16054191589355468  to: 0.159304416179657\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.159304416179657  to: 0.15821202993392944\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.15821202993392944  to: 0.15724465847015381\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.15724465847015381  to: 0.15638458728790283\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.15638458728790283  to: 0.1556159734725952\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.1556159734725952  to: 0.15492703914642333\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.15492703914642333  to: 0.1543074369430542\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.1543074369430542  to: 0.15374813079833985\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.15374813079833985  to: 0.15324130058288574\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.15324130058288574  to: 0.15278031826019287\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.15278031826019287  to: 0.1523594856262207\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.1523594856262207  to: 0.15197334289550782\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.15197334289550782  to: 0.15161737203598022\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.15161737203598022  to: 0.15128870010375978\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.15128870010375978  to: 0.15098415613174437\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.15098415613174437  to: 0.15070092678070068\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.15070092678070068  to: 0.1504366397857666\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.1504366397857666  to: 0.15018922090530396\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.15018922090530396  to: 0.1499568223953247\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.1499568223953247  to: 0.14973804950714112\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.14973804950714112  to: 0.1495314359664917\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.1495314359664917  to: 0.14933570623397827\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.14933570623397827  to: 0.1491499662399292\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1491499662399292  to: 0.14897319078445434\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.14897319078445434  to: 0.14880456924438476\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.14880456924438476  to: 0.14864330291748046\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.14864330291748046  to: 0.14848867654800416\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.14848867654800416  to: 0.14834007024765014\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.14834007024765014  to: 0.14819700717926027\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.14819700717926027  to: 0.14805901050567627\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.14805901050567627  to: 0.14792548418045043\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.14792548418045043  to: 0.1477961301803589\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.1477961301803589  to: 0.1476703405380249\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.1476703405380249  to: 0.14754801988601685\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.14754801988601685  to: 0.14742896556854249\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.14742896556854249  to: 0.14731281995773315\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.14731281995773315  to: 0.14719932079315184\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14719932079315184  to: 0.14708855152130126\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.14708855152130126  to: 0.1469799757003784\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.1469799757003784  to: 0.1468733549118042\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.1468733549118042  to: 0.14676891565322875\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.14676891565322875  to: 0.1466665029525757\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.1466665029525757  to: 0.14656610488891603\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.14656610488891603  to: 0.14646756649017334\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.14646756649017334  to: 0.1463707208633423\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.1463707208633423  to: 0.14627542495727539\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.14627542495727539  to: 0.14618160724639892\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14618160724639892  to: 0.14608913660049438\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.14608913660049438  to: 0.14599788188934326\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.14599788188934326  to: 0.14590780735015868\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.14590780735015868  to: 0.14581878185272218\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.14581878185272218  to: 0.14573075771331787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 86\n",
      "Improved validation loss from: 0.14573075771331787  to: 0.14564365148544312\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.14564365148544312  to: 0.14555757045745848\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14555757045745848  to: 0.1454723834991455\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.1454723834991455  to: 0.1453881621360779\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1453881621360779  to: 0.1453048348426819\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.1453048348426819  to: 0.1452223539352417\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1452223539352417  to: 0.14514070749282837\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14514070749282837  to: 0.14505983591079713\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.14505983591079713  to: 0.14497970342636107\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14497970342636107  to: 0.14490025043487548\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.14490025043487548  to: 0.14482153654098512\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14482153654098512  to: 0.14474347829818726\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.14474347829818726  to: 0.14466602802276612\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14466602802276612  to: 0.14458913803100587\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14458913803100587  to: 0.14451277256011963\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14451277256011963  to: 0.14443689584732056\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.14443689584732056  to: 0.14436147212982178\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14436147212982178  to: 0.14428647756576538\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.14428647756576538  to: 0.14421188831329346\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14421188831329346  to: 0.14413766860961913\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14413766860961913  to: 0.14406378269195558\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14406378269195558  to: 0.14399025440216065\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.14399025440216065  to: 0.1439170241355896\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.1439170241355896  to: 0.1438441038131714\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.1438441038131714  to: 0.14377152919769287\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14377152919769287  to: 0.1436992883682251\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.1436992883682251  to: 0.14362738132476807\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14362738132476807  to: 0.1435555100440979\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.1435555100440979  to: 0.1434839367866516\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.1434839367866516  to: 0.14341261386871337\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14341261386871337  to: 0.1433415651321411\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.1433415651321411  to: 0.14327075481414794\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14327075481414794  to: 0.14320018291473388\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14320018291473388  to: 0.1431298017501831\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.1431298017501831  to: 0.1430596113204956\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.1430596113204956  to: 0.14298959970474243\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14298959970474243  to: 0.14291974306106567\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14291974306106567  to: 0.14284999370574952\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14284999370574952  to: 0.14278037548065187\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14278037548065187  to: 0.14271085262298583\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14271085262298583  to: 0.14264144897460937\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14264144897460937  to: 0.14257211685180665\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14257211685180665  to: 0.14250284433364868\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.14250284433364868  to: 0.14243365526199342\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14243365526199342  to: 0.14236451387405397\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14236451387405397  to: 0.14229540824890136\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14229540824890136  to: 0.14222633838653564\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14222633838653564  to: 0.14215731620788574\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14215731620788574  to: 0.1420888662338257\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.1420888662338257  to: 0.14202048778533935\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14202048778533935  to: 0.14195210933685304\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14195210933685304  to: 0.1418837308883667\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1418837308883667  to: 0.14181535243988036\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14181535243988036  to: 0.14174693822860718\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14174693822860718  to: 0.141678524017334\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.141678524017334  to: 0.14161007404327391\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14161007404327391  to: 0.14154160022735596\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14154160022735596  to: 0.14147310256958007\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14147310256958007  to: 0.14140450954437256\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14140450954437256  to: 0.1413358688354492\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.1413358688354492  to: 0.14126718044281006\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14126718044281006  to: 0.14119843244552613\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14119843244552613  to: 0.14112964868545533\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.14112964868545533  to: 0.14106080532073975\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14106080532073975  to: 0.14099185466766356\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14099185466766356  to: 0.14092276096343995\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14092276096343995  to: 0.14085354804992675\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14085354804992675  to: 0.14078422784805297\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14078422784805297  to: 0.14071478843688964\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14071478843688964  to: 0.14064521789550782\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14064521789550782  to: 0.14057552814483643\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14057552814483643  to: 0.14050571918487548\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14050571918487548  to: 0.140435791015625\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.140435791015625  to: 0.14036574363708496\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14036574363708496  to: 0.14029557704925538\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14029557704925538  to: 0.14022527933120726\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14022527933120726  to: 0.14015481472015381\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14015481472015381  to: 0.14008419513702391\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14008419513702391  to: 0.14001346826553346\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14001346826553346  to: 0.13994262218475342\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.13994262218475342  to: 0.1398717164993286\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.1398717164993286  to: 0.13980069160461425\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.13980069160461425  to: 0.1397295832633972\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1397295832633972  to: 0.13965855836868285\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.13965855836868285  to: 0.13958746194839478\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.13958746194839478  to: 0.13951624631881715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 172\n",
      "Improved validation loss from: 0.13951624631881715  to: 0.13944494724273682\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.13944494724273682  to: 0.13937352895736693\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.13937352895736693  to: 0.13930200338363646\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.13930200338363646  to: 0.1392303466796875\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.1392303466796875  to: 0.13915859460830687\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.13915859460830687  to: 0.13908671140670775\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.13908671140670775  to: 0.13901469707489014\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.13901469707489014  to: 0.13894259929656982\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.13894259929656982  to: 0.1388706922531128\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.1388706922531128  to: 0.13879880905151368\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.13879880905151368  to: 0.13872674703598023\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.13872674703598023  to: 0.13865450620651246\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.13865450620651246  to: 0.1385820984840393\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.1385820984840393  to: 0.1385095238685608\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.1385095238685608  to: 0.13843697309494019\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.13843697309494019  to: 0.13836443424224854\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13836443424224854  to: 0.13829174041748046\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13829174041748046  to: 0.13821887969970703\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.13821887969970703  to: 0.13814586400985718\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13814586400985718  to: 0.13807258605957032\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13807258605957032  to: 0.13799915313720704\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13799915313720704  to: 0.1379256010055542\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.1379256010055542  to: 0.13785192966461182\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13785192966461182  to: 0.1377781391143799\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.1377781391143799  to: 0.1377042055130005\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.1377042055130005  to: 0.13763015270233153\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.13763015270233153  to: 0.137555992603302\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.137555992603302  to: 0.13748170137405397\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13748170137405397  to: 0.13740732669830322\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13740732669830322  to: 0.13733279705047607\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13733279705047607  to: 0.13725818395614625\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13725818395614625  to: 0.1371834397315979\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.1371834397315979  to: 0.13710860013961793\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13710860013961793  to: 0.13703361749649048\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13703361749649048  to: 0.13695852756500243\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13695852756500243  to: 0.13688331842422485\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13688331842422485  to: 0.13680799007415773\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13680799007415773  to: 0.1367325782775879\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.1367325782775879  to: 0.13665707111358644\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.13665707111358644  to: 0.1365814685821533\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.1365814685821533  to: 0.13650575876235962\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.13650575876235962  to: 0.13642995357513427\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13642995357513427  to: 0.1363540530204773\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.1363540530204773  to: 0.13627808094024657\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13627808094024657  to: 0.13620202541351317\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13620202541351317  to: 0.13612604141235352\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.13612604141235352  to: 0.1360499620437622\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1360499620437622  to: 0.13597382307052613\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.13597382307052613  to: 0.13589760065078735\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13589760065078735  to: 0.13582131862640381\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13582131862640381  to: 0.13574496507644654\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13574496507644654  to: 0.13566856384277343\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.13566856384277343  to: 0.13559210300445557\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13559210300445557  to: 0.13551558256149293\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13551558256149293  to: 0.13543903827667236\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13543903827667236  to: 0.13536275625228883\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13536275625228883  to: 0.13528648614883423\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13528648614883423  to: 0.13521020412445067\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13521020412445067  to: 0.13513391017913817\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13513391017913817  to: 0.1350576400756836\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.1350576400756836  to: 0.1349813461303711\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.1349813461303711  to: 0.1349050760269165\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.1349050760269165  to: 0.13482880592346191\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13482880592346191  to: 0.13475255966186522\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.13475255966186522  to: 0.1346763014793396\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.1346763014793396  to: 0.13460009098052977\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.13460009098052977  to: 0.13452391624450682\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13452391624450682  to: 0.13444775342941284\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.13444775342941284  to: 0.13437163829803467\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13437163829803467  to: 0.13429555892944336\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.13429555892944336  to: 0.13421952724456787\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13421952724456787  to: 0.13414354324340821\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13414354324340821  to: 0.13406763076782227\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13406763076782227  to: 0.13399178981781007\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.13399178981781007  to: 0.13391599655151368\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13391599655151368  to: 0.1338403344154358\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.1338403344154358  to: 0.13376471996307374\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13376471996307374  to: 0.1336892366409302\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.1336892366409302  to: 0.13361384868621826\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13361384868621826  to: 0.13353856801986694\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.13353856801986694  to: 0.13346325159072875\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13346325159072875  to: 0.13338791131973265\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13338791131973265  to: 0.13331255912780762\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13331255912780762  to: 0.13323721885681153\n",
      "Training iteration: 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13323721885681153  to: 0.13316190242767334\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.13316190242767334  to: 0.1330866575241089\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.1330866575241089  to: 0.13301148414611816\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13301148414611816  to: 0.13293638229370117\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13293638229370117  to: 0.1328614115715027\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.1328614115715027  to: 0.1327865481376648\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1327865481376648  to: 0.13271185159683227\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13271185159683227  to: 0.13263728618621826\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13263728618621826  to: 0.13256292343139647\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13256292343139647  to: 0.13248875141143798\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13248875141143798  to: 0.13241480588912963\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13241480588912963  to: 0.132341206073761\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.132341206073761  to: 0.13226792812347413\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13226792812347413  to: 0.1321949243545532\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.1321949243545532  to: 0.13212220668792723\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13212220668792723  to: 0.13204978704452514\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13204978704452514  to: 0.13197767734527588\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13197767734527588  to: 0.1319059133529663\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.1319059133529663  to: 0.13183445930480958\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13183445930480958  to: 0.13176338672637938\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13176338672637938  to: 0.13169269561767577\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13169269561767577  to: 0.13162238597869874\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13162238597869874  to: 0.1315524935722351\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1315524935722351  to: 0.1314830183982849\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.1314830183982849  to: 0.1314139723777771\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.1314139723777771  to: 0.1313454270362854\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.1313454270362854  to: 0.13127738237380981\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13127738237380981  to: 0.13121020793914795\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13121020793914795  to: 0.1311438798904419\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1311438798904419  to: 0.13107837438583375\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13107837438583375  to: 0.13101370334625245\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13101370334625245  to: 0.13094985485076904\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13094985485076904  to: 0.13088680505752565\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13088680505752565  to: 0.1308245301246643\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.1308245301246643  to: 0.130763041973114\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.130763041973114  to: 0.1307023286819458\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.1307023286819458  to: 0.1306424021720886\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.1306424021720886  to: 0.13058325052261352\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13058325052261352  to: 0.1305248498916626\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.1305248498916626  to: 0.13046724796295167\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13046724796295167  to: 0.13041040897369385\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13041040897369385  to: 0.13035399913787843\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13035399913787843  to: 0.13029806613922118\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13029806613922118  to: 0.13024262189865113\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13024262189865113  to: 0.13018767833709716\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13018767833709716  to: 0.1301332473754883\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.1301332473754883  to: 0.13007938861846924\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13007938861846924  to: 0.13002607822418213\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13002607822418213  to: 0.1299733877182007\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.1299733877182007  to: 0.12992138862609864\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.12992138862609864  to: 0.12987003326416016\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.12987003326416016  to: 0.12981939315795898\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.12981939315795898  to: 0.1297694206237793\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.1297694206237793  to: 0.12972015142440796\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.12972015142440796  to: 0.12967159748077392\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.12967159748077392  to: 0.12962377071380615\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.12962377071380615  to: 0.1295766592025757\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.1295766592025757  to: 0.12953029870986937\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.12953029870986937  to: 0.1294846773147583\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.1294846773147583  to: 0.1294397830963135\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.1294397830963135  to: 0.12939561605453492\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.12939561605453492  to: 0.12935222387313844\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.12935222387313844  to: 0.1293095827102661\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.1293095827102661  to: 0.12926766872406006\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.12926766872406006  to: 0.12922651767730714\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.12922651767730714  to: 0.1291861891746521\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1291861891746521  to: 0.12914667129516602\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.12914667129516602  to: 0.12910797595977783\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.12910797595977783  to: 0.12907006740570068\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.12907006740570068  to: 0.12903295755386351\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.12903295755386351  to: 0.1289966344833374\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.1289966344833374  to: 0.12896109819412233\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.12896109819412233  to: 0.1289263129234314\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1289263129234314  to: 0.1288922905921936\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.1288922905921936  to: 0.12885901927947999\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.12885901927947999  to: 0.12882654666900634\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.12882654666900634  to: 0.12879480123519899\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.12879480123519899  to: 0.12876378297805785\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.12876378297805785  to: 0.12873404026031493\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.12873404026031493  to: 0.12870553731918336\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.12870553731918336  to: 0.12867820262908936\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.12867820262908936  to: 0.12865197658538818\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.12865197658538818  to: 0.1286267876625061\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.1286267876625061  to: 0.1286026120185852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 340\n",
      "Improved validation loss from: 0.1286026120185852  to: 0.12857935428619385\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.12857935428619385  to: 0.12855700254440308\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.12855700254440308  to: 0.1285354971885681\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.1285354971885681  to: 0.12851479053497314\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.12851479053497314  to: 0.12849483489990235\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.12849483489990235  to: 0.1284755825996399\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.1284755825996399  to: 0.12845700979232788\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.12845700979232788  to: 0.1284390687942505\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.1284390687942505  to: 0.12842171192169188\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.12842171192169188  to: 0.12840481996536254\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.12840481996536254  to: 0.1283883571624756\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.1283883571624756  to: 0.12837235927581786\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.12837235927581786  to: 0.12835681438446045\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.12835681438446045  to: 0.12834163904190063\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.12834163904190063  to: 0.12832677364349365\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.12832677364349365  to: 0.12831223011016846\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.12831223011016846  to: 0.12829793691635133\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.12829793691635133  to: 0.1282839298248291\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1282839298248291  to: 0.12827014923095703\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.12827014923095703  to: 0.12825658321380615\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.12825658321380615  to: 0.12824323177337646\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.12824323177337646  to: 0.12823002338409423\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.12823002338409423  to: 0.12821705341339112\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.12821705341339112  to: 0.1282043695449829\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.1282043695449829  to: 0.12819188833236694\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.12819188833236694  to: 0.12817963361740112\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.12817963361740112  to: 0.12816756963729858\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.12816756963729858  to: 0.12815555334091186\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.12815555334091186  to: 0.128143572807312\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.128143572807312  to: 0.12813160419464112\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.12813160419464112  to: 0.1281196355819702\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1281196355819702  to: 0.12810767889022828\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.12810767889022828  to: 0.12809569835662843\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.12809569835662843  to: 0.1280837059020996\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.1280837059020996  to: 0.12807166576385498\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.12807166576385498  to: 0.12805957794189454\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.12805957794189454  to: 0.12804746627807617\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.12804746627807617  to: 0.12803524732589722\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.12803524732589722  to: 0.12802292108535768\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.12802292108535768  to: 0.12801066637039185\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.12801066637039185  to: 0.1279984474182129\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.1279984474182129  to: 0.12798624038696288\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.12798624038696288  to: 0.1279740571975708\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.1279740571975708  to: 0.12796186208724974\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.12796186208724974  to: 0.12794965505599976\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.12794965505599976  to: 0.12793742418289183\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.12793742418289183  to: 0.12792513370513917\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.12792513370513917  to: 0.1279128074645996\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.1279128074645996  to: 0.12790043354034425\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.12790043354034425  to: 0.1278879761695862\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1278879761695862  to: 0.12787545919418336\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.12787545919418336  to: 0.12786284685134888\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.12786284685134888  to: 0.12785012722015382\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12785012722015382  to: 0.12783725261688234\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12783725261688234  to: 0.1278243064880371\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.1278243064880371  to: 0.1278112292289734\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.1278112292289734  to: 0.12779805660247803\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.12779805660247803  to: 0.12778476476669312\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.12778476476669312  to: 0.1277713656425476\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1277713656425476  to: 0.12775785923004152\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.12775785923004152  to: 0.12774419784545898\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12774419784545898  to: 0.12773052453994752\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.12773052453994752  to: 0.12771672010421753\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.12771672010421753  to: 0.12770278453826905\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.12770278453826905  to: 0.12768871784210206\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.12768871784210206  to: 0.12767454385757446\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.12767454385757446  to: 0.12766021490097046\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.12766021490097046  to: 0.12764577865600585\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.12764577865600585  to: 0.12763117551803588\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.12763117551803588  to: 0.12761642932891845\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.12761642932891845  to: 0.12760154008865357\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.12760154008865357  to: 0.1275865316390991\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1275865316390991  to: 0.12757140398025513\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.12757140398025513  to: 0.12755613327026366\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.12755613327026366  to: 0.1275407075881958\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1275407075881958  to: 0.12752506732940674\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.12752506732940674  to: 0.12750931978225707\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.12750931978225707  to: 0.12749344110488892\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.12749344110488892  to: 0.12747743129730224\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.12747743129730224  to: 0.12746132612228395\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.12746132612228395  to: 0.1274450898170471\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.1274450898170471  to: 0.1274287462234497\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1274287462234497  to: 0.12741230726242064\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.12741230726242064  to: 0.12739576101303102\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.12739576101303102  to: 0.1273790955543518\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.1273790955543518  to: 0.12736232280731202\n",
      "Training iteration: 426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12736232280731202  to: 0.12734544277191162\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.12734544277191162  to: 0.1273284673690796\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1273284673690796  to: 0.1273113012313843\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1273113012313843  to: 0.12729402780532836\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.12729402780532836  to: 0.12727663516998292\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.12727663516998292  to: 0.12725913524627686\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.12725913524627686  to: 0.12724155187606812\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.12724155187606812  to: 0.12722384929656982\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.12722384929656982  to: 0.12720606327056885\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.12720606327056885  to: 0.12718818187713624\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.12718818187713624  to: 0.127170193195343\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.127170193195343  to: 0.12715210914611816\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.12715210914611816  to: 0.12713392972946166\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.12713392972946166  to: 0.1271155595779419\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.1271155595779419  to: 0.1270970582962036\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1270970582962036  to: 0.12707846164703368\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12707846164703368  to: 0.12705976963043214\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.12705976963043214  to: 0.12704098224639893\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.12704098224639893  to: 0.12702211141586303\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.12702211141586303  to: 0.12700315713882446\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12700315713882446  to: 0.1269840955734253\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.1269840955734253  to: 0.1269649624824524\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1269649624824524  to: 0.12694573402404785\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.12694573402404785  to: 0.1269264578819275\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.1269264578819275  to: 0.12690714597702027\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.12690714597702027  to: 0.12688782215118408\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.12688782215118408  to: 0.12686851024627685\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.12686851024627685  to: 0.12684919834136962\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.12684919834136962  to: 0.1268298864364624\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.1268298864364624  to: 0.12681058645248414\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.12681058645248414  to: 0.126791250705719\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.126791250705719  to: 0.12677191495895385\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.12677191495895385  to: 0.12675257921218872\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.12675257921218872  to: 0.1267334222793579\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1267334222793579  to: 0.12671438455581666\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12671438455581666  to: 0.1266953468322754\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.1266953468322754  to: 0.1266762614250183\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1266762614250183  to: 0.12665716409683228\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.12665716409683228  to: 0.12663798332214354\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12663798332214354  to: 0.12661879062652587\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12661879062652587  to: 0.12659953832626342\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.12659953832626342  to: 0.1265802264213562\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.1265802264213562  to: 0.12656086683273315\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.12656086683273315  to: 0.12654145956039428\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.12654145956039428  to: 0.1265220046043396\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.1265220046043396  to: 0.12650247812271118\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.12650247812271118  to: 0.12648290395736694\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.12648290395736694  to: 0.12646328210830687\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.12646328210830687  to: 0.12644360065460206\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.12644360065460206  to: 0.1264238476753235\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.1264238476753235  to: 0.12640403509140014\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.12640403509140014  to: 0.12638416290283203\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.12638416290283203  to: 0.12636423110961914\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.12636423110961914  to: 0.12634425163269042\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.12634425163269042  to: 0.12632420063018798\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.12632420063018798  to: 0.1263040781021118\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.1263040781021118  to: 0.12628369331359862\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.12628369331359862  to: 0.12626324892044066\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12626324892044066  to: 0.12624257802963257\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.12624257802963257  to: 0.12622171640396118\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12622171640396118  to: 0.12620065212249756\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.12620065212249756  to: 0.12617939710617065\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.12617939710617065  to: 0.1261579990386963\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.1261579990386963  to: 0.12613643407821656\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.12613643407821656  to: 0.12611472606658936\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12611472606658936  to: 0.12609288692474366\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12609288692474366  to: 0.12607090473175048\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.12607090473175048  to: 0.12604881525039674\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12604881525039674  to: 0.12602643966674804\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.12602643966674804  to: 0.12600395679473878\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.12600395679473878  to: 0.1259813904762268\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.1259813904762268  to: 0.12595874071121216\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.12595874071121216  to: 0.12593599557876586\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.12593599557876586  to: 0.12591317892074586\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.12591317892074586  to: 0.1258902907371521\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1258902907371521  to: 0.12586734294891358\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.12586734294891358  to: 0.12584433555603028\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.12584433555603028  to: 0.1258212685585022\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.1258212685585022  to: 0.12579811811447145\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12579811811447145  to: 0.12577493190765382\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12577493190765382  to: 0.12575172185897826\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12575172185897826  to: 0.12572844028472902\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12572844028472902  to: 0.12570513486862184\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.12570513486862184  to: 0.12568180561065673\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12568180561065673  to: 0.12565845251083374\n",
      "Training iteration: 511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12565845251083374  to: 0.12563507556915282\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12563507556915282  to: 0.12561168670654296\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.12561168670654296  to: 0.1255882978439331\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1255882978439331  to: 0.12556490898132325\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12556490898132325  to: 0.12554147243499755\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.12554147243499755  to: 0.12551803588867189\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12551803588867189  to: 0.12549453973770142\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.12549453973770142  to: 0.12547101974487304\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.12547101974487304  to: 0.1254472851753235\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.1254472851753235  to: 0.12542335987091063\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.12542335987091063  to: 0.12539923191070557\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.12539923191070557  to: 0.12537487745285034\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.12537487745285034  to: 0.12535033226013184\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.12535033226013184  to: 0.12532613277435303\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.12532613277435303  to: 0.12530190944671632\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.12530190944671632  to: 0.12527754306793212\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12527754306793212  to: 0.125253164768219\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.125253164768219  to: 0.12522876262664795\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.12522876262664795  to: 0.12520434856414794\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12520434856414794  to: 0.12517993450164794\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.12517993450164794  to: 0.12515556812286377\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12515556812286377  to: 0.12513117790222167\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.12513117790222167  to: 0.1251068115234375\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.1251068115234375  to: 0.1250826120376587\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.1250826120376587  to: 0.1250584602355957\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1250584602355957  to: 0.12503435611724853\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.12503435611724853  to: 0.1250102400779724\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.1250102400779724  to: 0.12498612403869629\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.12498612403869629  to: 0.12496203184127808\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.12496203184127808  to: 0.124937903881073\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.124937903881073  to: 0.12491379976272583\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.12491379976272583  to: 0.1248896598815918\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.1248896598815918  to: 0.12486549615859985\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.12486549615859985  to: 0.12484130859375\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.12484130859375  to: 0.1248171091079712\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.1248171091079712  to: 0.12479286193847657\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12479286193847657  to: 0.12476857900619506\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.12476857900619506  to: 0.12474424839019775\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12474424839019775  to: 0.12471987009048462\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.12471987009048462  to: 0.12469545602798462\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.12469545602798462  to: 0.12467098236083984\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12467098236083984  to: 0.12464644908905029\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12464644908905029  to: 0.12462185621261597\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12462185621261597  to: 0.12459719181060791\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.12459719181060791  to: 0.12457246780395508\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12457246780395508  to: 0.12454795837402344\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.12454795837402344  to: 0.12452363967895508\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.12452363967895508  to: 0.12449948787689209\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12449948787689209  to: 0.12447516918182373\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.12447516918182373  to: 0.12445069551467895\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12445069551467895  to: 0.12442605495452881\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12442605495452881  to: 0.1244012713432312\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.1244012713432312  to: 0.12437651157379151\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12437651157379151  to: 0.12435178756713867\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12435178756713867  to: 0.12432705163955689\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.12432705163955689  to: 0.1243023157119751\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.1243023157119751  to: 0.12427756786346436\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12427756786346436  to: 0.1242527961730957\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1242527961730957  to: 0.12422797679901124\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.12422797679901124  to: 0.12420313358306885\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.12420313358306885  to: 0.12417824268341064\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.12417824268341064  to: 0.12415326833724975\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.12415326833724975  to: 0.12412824630737304\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12412824630737304  to: 0.1241031527519226\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.1241031527519226  to: 0.12407797574996948\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12407797574996948  to: 0.12405271530151367\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12405271530151367  to: 0.12402737140655518\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.12402737140655518  to: 0.12400194406509399\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.12400194406509399  to: 0.12397642135620117\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.12397642135620117  to: 0.12395079135894775\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12395079135894775  to: 0.12392505407333373\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.12392505407333373  to: 0.12389924526214599\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12389924526214599  to: 0.12387330532073974\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.12387330532073974  to: 0.12384724617004395\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.12384724617004395  to: 0.1238210916519165\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.1238210916519165  to: 0.12379478216171265\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.12379478216171265  to: 0.1237683892250061\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.1237683892250061  to: 0.12374193668365478\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12374193668365478  to: 0.12371537685394288\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.12371537685394288  to: 0.12368870973587036\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.12368870973587036  to: 0.1236619234085083\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.1236619234085083  to: 0.12363500595092773\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12363500595092773  to: 0.12360799312591553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 594\n",
      "Improved validation loss from: 0.12360799312591553  to: 0.12358086109161377\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12358086109161377  to: 0.12355358600616455\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12355358600616455  to: 0.12352621555328369\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12352621555328369  to: 0.12349894046783447\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12349894046783447  to: 0.12347179651260376\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.12347179651260376  to: 0.12344471216201783\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.12344471216201783  to: 0.12341772317886353\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12341772317886353  to: 0.12339074611663818\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12339074611663818  to: 0.12336416244506836\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12336416244506836  to: 0.12333790063858033\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.12333790063858033  to: 0.12331196069717407\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.12331196069717407  to: 0.12328627109527587\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12328627109527587  to: 0.123260760307312\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.123260760307312  to: 0.12323544025421143\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12323544025421143  to: 0.1232102394104004\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.1232102394104004  to: 0.12318512201309204\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.12318512201309204  to: 0.12316004037857056\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12316004037857056  to: 0.12313495874404908\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12313495874404908  to: 0.12310984134674072\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12310984134674072  to: 0.12308464050292969\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12308464050292969  to: 0.12305938005447388\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12305938005447388  to: 0.12303398847579956\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12303398847579956  to: 0.12300866842269897\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12300866842269897  to: 0.12298338413238526\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12298338413238526  to: 0.12295808792114257\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12295808792114257  to: 0.1229327917098999\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.1229327917098999  to: 0.12290740013122559\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12290740013122559  to: 0.12288191318511962\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12288191318511962  to: 0.12285630702972412\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12285630702972412  to: 0.12283055782318116\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12283055782318116  to: 0.12280464172363281\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12280464172363281  to: 0.12277854681015014\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12277854681015014  to: 0.12275224924087524\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12275224924087524  to: 0.1227257490158081\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1227257490158081  to: 0.12269902229309082\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12269902229309082  to: 0.1226720929145813\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1226720929145813  to: 0.12264492511749267\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12264492511749267  to: 0.12261755466461181\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12261755466461181  to: 0.12259000539779663\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12259000539779663  to: 0.1225622296333313\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.1225622296333313  to: 0.12253420352935791\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12253420352935791  to: 0.12250596284866333\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12250596284866333  to: 0.1224774956703186\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.1224774956703186  to: 0.12244877815246583\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12244877815246583  to: 0.1224198579788208\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.1224198579788208  to: 0.12239071130752563\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12239071130752563  to: 0.12236135005950928\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12236135005950928  to: 0.12233178615570069\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12233178615570069  to: 0.12230199575424194\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12230199575424194  to: 0.12227202653884887\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12227202653884887  to: 0.12224184274673462\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12224184274673462  to: 0.12221148014068603\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12221148014068603  to: 0.12218090295791625\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12218090295791625  to: 0.12215015888214112\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12215015888214112  to: 0.12211923599243164\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12211923599243164  to: 0.12208815813064575\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12208815813064575  to: 0.12205690145492554\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12205690145492554  to: 0.12202553749084473\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12202553749084473  to: 0.12199403047561645\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12199403047561645  to: 0.12196239233016967\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12196239233016967  to: 0.12193067073822021\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12193067073822021  to: 0.12189886569976807\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12189886569976807  to: 0.12186696529388427\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12186696529388427  to: 0.12183499336242676\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12183499336242676  to: 0.12180299758911133\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12180299758911133  to: 0.12177015542984009\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12177015542984009  to: 0.12173656225204468\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12173656225204468  to: 0.12170225381851196\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12170225381851196  to: 0.12166732549667358\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12166732549667358  to: 0.1216318130493164\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.1216318130493164  to: 0.12159576416015624\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12159576416015624  to: 0.1215593695640564\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.1215593695640564  to: 0.12152347564697266\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12152347564697266  to: 0.12148811817169189\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12148811817169189  to: 0.1214531660079956\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1214531660079956  to: 0.1214186429977417\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.1214186429977417  to: 0.12138447761535645\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12138447761535645  to: 0.12135064601898193\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12135064601898193  to: 0.12131712436676026\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.12131712436676026  to: 0.12128413915634155\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.12128413915634155  to: 0.12125072479248047\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12125072479248047  to: 0.12121690511703491\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12121690511703491  to: 0.12118269205093384\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12118269205093384  to: 0.12114827632904053\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12114827632904053  to: 0.12111455202102661\n",
      "Training iteration: 679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12111455202102661  to: 0.12108138799667359\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.12108138799667359  to: 0.12104873657226563\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.12104873657226563  to: 0.12101653814315796\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12101653814315796  to: 0.12098472118377686\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.12098472118377686  to: 0.12095223665237427\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12095223665237427  to: 0.12091913223266601\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12091913223266601  to: 0.12088549137115479\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12088549137115479  to: 0.1208510160446167\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.1208510160446167  to: 0.12081680297851563\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12081680297851563  to: 0.12078280448913574\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.12078280448913574  to: 0.12074897289276124\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12074897289276124  to: 0.12071534395217895\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12071534395217895  to: 0.12068073749542237\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12068073749542237  to: 0.12064526081085206\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12064526081085206  to: 0.1206089735031128\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.1206089735031128  to: 0.12057306766510009\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12057306766510009  to: 0.12053744792938233\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.12053744792938233  to: 0.1205021858215332\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.1205021858215332  to: 0.12046636343002319\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12046636343002319  to: 0.1204300045967102\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.1204300045967102  to: 0.12039315700531006\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12039315700531006  to: 0.12035586833953857\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.12035586833953857  to: 0.12031937837600708\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.12031937837600708  to: 0.12028360366821289\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12028360366821289  to: 0.12024810314178466\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12024810314178466  to: 0.12021280527114868\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12021280527114868  to: 0.12017644643783569\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12017644643783569  to: 0.12013909816741944\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12013909816741944  to: 0.12010082006454467\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12010082006454467  to: 0.12006174325942993\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12006174325942993  to: 0.12002322673797608\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12002322673797608  to: 0.11998552083969116\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.11998552083969116  to: 0.11994802951812744\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.11994802951812744  to: 0.11990982294082642\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.11990982294082642  to: 0.11987092494964599\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.11987092494964599  to: 0.1198310136795044\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.1198310136795044  to: 0.11979020833969116\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.11979020833969116  to: 0.11974996328353882\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.11974996328353882  to: 0.11971021890640259\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.11971021890640259  to: 0.11967089176177978\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.11967089176177978  to: 0.11963093280792236\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.11963093280792236  to: 0.11959037780761719\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.11959037780761719  to: 0.1195488691329956\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.1195488691329956  to: 0.11950647830963135\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.11950647830963135  to: 0.119465172290802\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.119465172290802  to: 0.11942445039749146\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.11942445039749146  to: 0.11938272714614868\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.11938272714614868  to: 0.11934044361114501\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.11934044361114501  to: 0.11929886341094971\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.11929886341094971  to: 0.11925632953643799\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.11925632953643799  to: 0.11921329498291015\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.11921329498291015  to: 0.11917144060134888\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.11917144060134888  to: 0.1191285490989685\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.1191285490989685  to: 0.11908470392227173\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.11908470392227173  to: 0.11904165744781495\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.11904165744781495  to: 0.11899932622909545\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.11899932622909545  to: 0.11895593404769897\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.11895593404769897  to: 0.11891199350357055\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.11891199350357055  to: 0.11886756420135498\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.11886756420135498  to: 0.11882274150848389\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.11882274150848389  to: 0.11877700090408325\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.11877700090408325  to: 0.11873226165771485\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.11873226165771485  to: 0.11868841648101806\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.11868841648101806  to: 0.1186435341835022\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.1186435341835022  to: 0.11859773397445679\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.11859773397445679  to: 0.11855103969573974\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.11855103969573974  to: 0.11850398778915405\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.11850398778915405  to: 0.11845852136611938\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.11845852136611938  to: 0.11841403245925904\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.11841403245925904  to: 0.11836845874786377\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.11836845874786377  to: 0.1183218240737915\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.1183218240737915  to: 0.11827472448349\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.11827472448349  to: 0.11822671890258789\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.11822671890258789  to: 0.11817786693572999\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.11817786693572999  to: 0.11812868118286132\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.11812868118286132  to: 0.11808080673217773\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.11808080673217773  to: 0.11803200244903564\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.11803200244903564  to: 0.11798498630523682\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.11798498630523682  to: 0.11793695688247681\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.11793695688247681  to: 0.11788794994354249\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.11788794994354249  to: 0.11783803701400757\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.11783803701400757  to: 0.117787766456604\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.117787766456604  to: 0.11773717403411865\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.11773717403411865  to: 0.11768577098846436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 763\n",
      "Improved validation loss from: 0.11768577098846436  to: 0.11763589382171631\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.11763589382171631  to: 0.117587411403656\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.117587411403656  to: 0.11753785610198975\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.11753785610198975  to: 0.11748726367950439\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.11748726367950439  to: 0.11743571758270263\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.11743571758270263  to: 0.11738382577896118\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.11738382577896118  to: 0.11733160018920899\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.11733160018920899  to: 0.1172785997390747\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.1172785997390747  to: 0.1172248125076294\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.1172248125076294  to: 0.11717035770416259\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.11717035770416259  to: 0.11711523532867432\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.11711523532867432  to: 0.11705951690673828\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.11705951690673828  to: 0.11700642108917236\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.11700642108917236  to: 0.11695575714111328\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.11695575714111328  to: 0.11690404415130615\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.11690404415130615  to: 0.11685137748718262\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.11685137748718262  to: 0.11679776906967163\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.11679776906967163  to: 0.11674327850341797\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.11674327850341797  to: 0.1166879653930664\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1166879653930664  to: 0.11663186550140381\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.11663186550140381  to: 0.11657576560974121\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.11657576560974121  to: 0.11651959419250488\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.11651959419250488  to: 0.11646274328231812\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.11646274328231812  to: 0.1164052963256836\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.1164052963256836  to: 0.11634724140167237\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.11634724140167237  to: 0.1162886381149292\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.1162886381149292  to: 0.11622951030731202\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.11622951030731202  to: 0.11616990566253663\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.11616990566253663  to: 0.11611047983169556\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.11611047983169556  to: 0.11605119705200195\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.11605119705200195  to: 0.11599137783050537\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.11599137783050537  to: 0.11593124866485596\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.11593124866485596  to: 0.11587064266204834\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.11587064266204834  to: 0.1158094048500061\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.1158094048500061  to: 0.11574825048446655\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.11574825048446655  to: 0.11568753719329834\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.11568753719329834  to: 0.11562716960906982\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.11562716960906982  to: 0.11556774377822876\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.11556774377822876  to: 0.11550840139389038\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.11550840139389038  to: 0.115449059009552\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.115449059009552  to: 0.11538969278335572\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.11538969278335572  to: 0.11532970666885375\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.11532970666885375  to: 0.11526939868927003\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.11526939868927003  to: 0.11520880460739136\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.11520880460739136  to: 0.11514781713485718\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.11514781713485718  to: 0.11508640050888061\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.11508640050888061  to: 0.1150252103805542\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.1150252103805542  to: 0.11496340036392212\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.11496340036392212  to: 0.11490093469619751\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.11490093469619751  to: 0.11483780145645142\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.11483780145645142  to: 0.11477396488189698\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.11477396488189698  to: 0.11470941305160523\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.11470941305160523  to: 0.11464416980743408\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.11464416980743408  to: 0.11457901000976563\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.11457901000976563  to: 0.1145130753517151\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.1145130753517151  to: 0.11444637775421143\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.11444637775421143  to: 0.11437907218933105\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.11437907218933105  to: 0.11431113481521607\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.11431113481521607  to: 0.1142425775527954\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.1142425775527954  to: 0.11417429447174073\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.11417429447174073  to: 0.1141053557395935\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.1141053557395935  to: 0.11403584480285645\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.11403584480285645  to: 0.11396576166152954\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.11396576166152954  to: 0.1138951301574707\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.1138951301574707  to: 0.11382397413253784\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.11382397413253784  to: 0.11375319957733154\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.11375319957733154  to: 0.11368186473846435\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.11368186473846435  to: 0.11360998153686523\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.11360998153686523  to: 0.11353757381439208\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.11353757381439208  to: 0.11346462965011597\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.11346462965011597  to: 0.1133920669555664\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1133920669555664  to: 0.11331888437271118\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.11331888437271118  to: 0.11324511766433716\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.11324511766433716  to: 0.11316733360290528\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.11316733360290528  to: 0.11308597326278687\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.11308597326278687  to: 0.11300144195556641\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.11300144195556641  to: 0.11291420459747314\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.11291420459747314  to: 0.1128246545791626\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1128246545791626  to: 0.11273422241210937\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.11273422241210937  to: 0.11264324188232422\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.11264324188232422  to: 0.1125516414642334\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.1125516414642334  to: 0.11245931386947632\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.11245931386947632  to: 0.11236655712127686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 846\n",
      "Improved validation loss from: 0.11236655712127686  to: 0.11227365732192993\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.11227365732192993  to: 0.11218085289001464\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.11218085289001464  to: 0.1120883584022522\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.1120883584022522  to: 0.11199629306793213\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11199629306793213  to: 0.11190478801727295\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11190478801727295  to: 0.11181386709213256\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11181386709213256  to: 0.11172351837158204\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.11172351837158204  to: 0.1116337537765503\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.1116337537765503  to: 0.11154438257217407\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.11154438257217407  to: 0.1114553689956665\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.1114553689956665  to: 0.11136761903762818\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11136761903762818  to: 0.1112798810005188\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.1112798810005188  to: 0.11119202375411988\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.11119202375411988  to: 0.11110396385192871\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.11110396385192871  to: 0.11101655960083008\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.11101655960083008  to: 0.11092854738235473\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.11092854738235473  to: 0.11083983182907105\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.11083983182907105  to: 0.11075035333633423\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.11075035333633423  to: 0.11066009998321533\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.11066009998321533  to: 0.1105690360069275\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1105690360069275  to: 0.11047718524932862\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.11047718524932862  to: 0.11038455963134766\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.11038455963134766  to: 0.11029115915298462\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.11029115915298462  to: 0.11019842624664307\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.11019842624664307  to: 0.11010489463806153\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.11010489463806153  to: 0.11001060009002686\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.11001060009002686  to: 0.10991522073745727\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.10991522073745727  to: 0.10981873273849488\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.10981873273849488  to: 0.1097216010093689\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.1097216010093689  to: 0.1096238374710083\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.1096238374710083  to: 0.10951920747756957\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.10951920747756957  to: 0.10940718650817871\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.10940718650817871  to: 0.10928864479064941\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.10928864479064941  to: 0.10916452407836914\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.10916452407836914  to: 0.10903581380844116\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.10903581380844116  to: 0.10890340805053711\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.10890340805053711  to: 0.10877746343612671\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.10877746343612671  to: 0.10865644216537476\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.10865644216537476  to: 0.10854008197784423\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.10854008197784423  to: 0.10842797756195069\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.10842797756195069  to: 0.10831964015960693\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.10831964015960693  to: 0.10821439027786255\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.10821439027786255  to: 0.10810314416885376\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.10810314416885376  to: 0.10798757076263428\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.10798757076263428  to: 0.10786640644073486\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.10786640644073486  to: 0.1077400803565979\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.1077400803565979  to: 0.10760912895202637\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.10760912895202637  to: 0.10748294591903687\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.10748294591903687  to: 0.10736031532287597\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.10736031532287597  to: 0.10723419189453125\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.10723419189453125  to: 0.10710353851318359\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.10710353851318359  to: 0.10696883201599121\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.10696883201599121  to: 0.10684001445770264\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.10684001445770264  to: 0.1067167043685913\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.1067167043685913  to: 0.10658895969390869\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.10658895969390869  to: 0.10645883083343506\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.10645883083343506  to: 0.10632503032684326\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.10632503032684326  to: 0.10618804693222046\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.10618804693222046  to: 0.10604841709136963\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.10604841709136963  to: 0.10590856075286866\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.10590856075286866  to: 0.10576870441436767\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.10576870441436767  to: 0.1056288719177246\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.1056288719177246  to: 0.10550115108489991\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.10550115108489991  to: 0.10537198781967164\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.10537198781967164  to: 0.10524104833602906\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.10524104833602906  to: 0.10510834455490112\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.10510834455490112  to: 0.10497350692749023\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.10497350692749023  to: 0.10483641624450683\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.10483641624450683  to: 0.10469709634780884\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.10469709634780884  to: 0.10455567836761474\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.10455567836761474  to: 0.1044122338294983\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.1044122338294983  to: 0.10426702499389648\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.10426702499389648  to: 0.10412039756774902\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.10412039756774902  to: 0.10397268533706665\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.10397268533706665  to: 0.10382421016693115\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.10382421016693115  to: 0.10367752313613891\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.10367752313613891  to: 0.10353038311004639\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.10353038311004639  to: 0.10338295698165893\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.10338295698165893  to: 0.10323531627655029\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.10323531627655029  to: 0.10308984518051148\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.10308984518051148  to: 0.10294380187988281\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.10294380187988281  to: 0.10279704332351684\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.10279704332351684  to: 0.10264934301376342\n",
      "Training iteration: 929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10264934301376342  to: 0.10250064134597778\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.10250064134597778  to: 0.10235059261322021\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.10235059261322021  to: 0.10219895839691162\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.10219895839691162  to: 0.10204554796218872\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.10204554796218872  to: 0.10189293622970581\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.10189293622970581  to: 0.10172988176345825\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.10172988176345825  to: 0.10155508518218995\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.10155508518218995  to: 0.10137064456939697\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.10137064456939697  to: 0.1011777639389038\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.1011777639389038  to: 0.10097935199737548\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.10097935199737548  to: 0.10077807903289795\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.10077807903289795  to: 0.10058797597885132\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.10058797597885132  to: 0.10040884017944336\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.10040884017944336  to: 0.1002392053604126\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.1002392053604126  to: 0.10006424188613891\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.10006424188613891  to: 0.09988580942153931\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.09988580942153931  to: 0.09970291852951049\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.09970291852951049  to: 0.09951521754264832\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.09951521754264832  to: 0.09931970834732055\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.09931970834732055  to: 0.09911759495735169\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.09911759495735169  to: 0.09891059994697571\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.09891059994697571  to: 0.0987007737159729\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.0987007737159729  to: 0.09849015474319459\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.09849015474319459  to: 0.09829480051994324\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.09829480051994324  to: 0.09811355471611023\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.09811355471611023  to: 0.09792881011962891\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.09792881011962891  to: 0.09773911237716675\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.09773911237716675  to: 0.09754344224929809\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.09754344224929809  to: 0.09734135866165161\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.09734135866165161  to: 0.09713681936264038\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.09713681936264038  to: 0.09693022966384887\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.09693022966384887  to: 0.09672238230705262\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.09672238230705262  to: 0.09651004672050476\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.09651004672050476  to: 0.09629438519477844\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.09629438519477844  to: 0.09607637524604798\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.09607637524604798  to: 0.09585660099983215\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.09585660099983215  to: 0.09563518762588501\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.09563518762588501  to: 0.09541177749633789\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.09541177749633789  to: 0.09518556594848633\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.09518556594848633  to: 0.09495571255683899\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.09495571255683899  to: 0.09473956227302552\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.09473956227302552  to: 0.09453459978103637\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.09453459978103637  to: 0.09431511759757996\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.09431511759757996  to: 0.09409815669059754\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.09409815669059754  to: 0.09388208389282227\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.09388208389282227  to: 0.09366544485092163\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.09366544485092163  to: 0.09333505630493164\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.09333505630493164  to: 0.09294999837875366\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.09294999837875366  to: 0.09259361028671265\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.09259361028671265  to: 0.09228585958480835\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.09228585958480835  to: 0.09203402400016784\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.09203402400016784  to: 0.0918270468711853\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.0918270468711853  to: 0.091648530960083\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.091648530960083  to: 0.09142716526985169\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.09142716526985169  to: 0.09114986658096313\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.09114986658096313  to: 0.09083337783813476\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.09083337783813476  to: 0.09050424695014954\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.09050424695014954  to: 0.09023550152778625\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.09023550152778625  to: 0.09003530740737915\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.09003530740737915  to: 0.08987401127815246\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.08987401127815246  to: 0.08972780108451843\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.08972780108451843  to: 0.08953806161880493\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.08953806161880493  to: 0.0893060326576233\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.0893060326576233  to: 0.08906108140945435\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.08906108140945435  to: 0.08883028030395508\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.08883028030395508  to: 0.08861544728279114\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.08861544728279114  to: 0.08847463726997376\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.08847463726997376  to: 0.08838585615158082\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.08838585615158082  to: 0.08827247619628906\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.08827247619628906  to: 0.0881166160106659\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.0881166160106659  to: 0.08791028261184693\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.08791028261184693  to: 0.0876686692237854\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.0876686692237854  to: 0.08744891881942748\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.08744891881942748  to: 0.08725430369377137\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.08725430369377137  to: 0.08706073760986328\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.08706073760986328  to: 0.0868315875530243\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.0868315875530243  to: 0.08656932115554809\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.08656932115554809  to: 0.0862930178642273\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.0862930178642273  to: 0.08576689958572388\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.08576689958572388  to: 0.08539649844169617\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.08539649844169617  to: 0.08519374132156372\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.08519374132156372  to: 0.08506854772567748\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.08506854772567748  to: 0.08463510274887084\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.08463510274887084  to: 0.08426668047904969\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.08426668047904969  to: 0.08399409055709839\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.08399409055709839  to: 0.08382357358932495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.08382357358932495  to: 0.08342432975769043\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.08342432975769043  to: 0.08294988870620727\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.08294988870620727  to: 0.08281723260879517\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.08281723260879517  to: 0.08260784149169922\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.08260784149169922  to: 0.08230264782905579\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.08230264782905579  to: 0.08223762512207031\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.08223762512207031  to: 0.08188316226005554\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.08188316226005554  to: 0.08143890500068665\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.08143890500068665  to: 0.08134523630142212\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.08134523630142212  to: 0.08119531869888305\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.08119531869888305  to: 0.08095749020576477\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.08095749020576477  to: 0.08064582943916321\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.08064582943916321  to: 0.08031121492385865\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.08031121492385865  to: 0.08000634908676148\n",
      "Training iteration: 1029\n",
      "Validation loss (no improvement): 0.08010471463203431\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.08000634908676148  to: 0.0799398422241211\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.0799398422241211  to: 0.07952948808670043\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.07952948808670043  to: 0.0791736900806427\n",
      "Training iteration: 1033\n",
      "Validation loss (no improvement): 0.07929446697235107\n",
      "Training iteration: 1034\n",
      "Validation loss (no improvement): 0.07938666939735413\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.0791736900806427  to: 0.07915576100349427\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.07915576100349427  to: 0.07873884439468384\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.07873884439468384  to: 0.07842401266098023\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.07842401266098023  to: 0.07826045155525208\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.07826045155525208  to: 0.07823243141174316\n",
      "Training iteration: 1040\n",
      "Validation loss (no improvement): 0.07824013233184815\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.07823243141174316  to: 0.07798526883125305\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.07798526883125305  to: 0.07791245579719544\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.07791245579719544  to: 0.07764261960983276\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.07764261960983276  to: 0.07745914459228516\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.07745914459228516  to: 0.07740033268928528\n",
      "Training iteration: 1046\n",
      "Validation loss (no improvement): 0.07743463516235352\n",
      "Training iteration: 1047\n",
      "Validation loss (no improvement): 0.07745654582977295\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.07740033268928528  to: 0.0772449791431427\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.0772449791431427  to: 0.07690547108650207\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.07690547108650207  to: 0.07671024203300476\n",
      "Training iteration: 1051\n",
      "Validation loss (no improvement): 0.07703351378440856\n",
      "Training iteration: 1052\n",
      "Validation loss (no improvement): 0.07713485956192016\n",
      "Training iteration: 1053\n",
      "Validation loss (no improvement): 0.07687233686447144\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.07671024203300476  to: 0.07646852135658264\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.07646852135658264  to: 0.07623980641365051\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.07623980641365051  to: 0.07619485259056091\n",
      "Training iteration: 1057\n",
      "Validation loss (no improvement): 0.07629746198654175\n",
      "Training iteration: 1058\n",
      "Validation loss (no improvement): 0.07627907991409302\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.07619485259056091  to: 0.07603099942207336\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.07603099942207336  to: 0.07601627111434936\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.07601627111434936  to: 0.0758343756198883\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.0758343756198883  to: 0.07569987773895263\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.07569987773895263  to: 0.07564136981964112\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.07564136981964112  to: 0.07563603520393372\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.07563603520393372  to: 0.07561880350112915\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.07561880350112915  to: 0.0753588318824768\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.0753588318824768  to: 0.07513664960861206\n",
      "Training iteration: 1068\n",
      "Validation loss (no improvement): 0.07537323236465454\n",
      "Training iteration: 1069\n",
      "Validation loss (no improvement): 0.07525054216384888\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.07513664960861206  to: 0.07492126822471619\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.07492126822471619  to: 0.0747308075428009\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.0747308075428009  to: 0.07469096183776855\n",
      "Training iteration: 1073\n",
      "Validation loss (no improvement): 0.07476717233657837\n",
      "Training iteration: 1074\n",
      "Validation loss (no improvement): 0.07476879954338074\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.07469096183776855  to: 0.07428473234176636\n",
      "Training iteration: 1076\n",
      "Validation loss (no improvement): 0.07430159449577331\n",
      "Training iteration: 1077\n",
      "Validation loss (no improvement): 0.07436865568161011\n",
      "Training iteration: 1078\n",
      "Validation loss (no improvement): 0.07434417009353637\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.07428473234176636  to: 0.07409195899963379\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.07409195899963379  to: 0.07390339970588684\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.07390339970588684  to: 0.07380991578102111\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.07380991578102111  to: 0.07379058599472046\n",
      "Training iteration: 1083\n",
      "Validation loss (no improvement): 0.07396329045295716\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.07379058599472046  to: 0.07361035346984864\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.07361035346984864  to: 0.07338107824325561\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.07338107824325561  to: 0.07329702377319336\n",
      "Training iteration: 1087\n",
      "Validation loss (no improvement): 0.07333776354789734\n",
      "Training iteration: 1088\n",
      "Validation loss (no improvement): 0.07342625260353089\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.07329702377319336  to: 0.07301907539367676\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.07301907539367676  to: 0.0729859709739685\n",
      "Training iteration: 1091\n",
      "Validation loss (no improvement): 0.07328975200653076\n",
      "Training iteration: 1092\n",
      "Validation loss (no improvement): 0.0731045126914978\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.0729859709739685  to: 0.07293960452079773\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.07293960452079773  to: 0.07281016111373902\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.07281016111373902  to: 0.07271665334701538\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.07271665334701538  to: 0.07264769673347474\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.07264769673347474  to: 0.07220055460929871\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.07220055460929871  to: 0.07213150262832642\n",
      "Training iteration: 1099\n",
      "Validation loss (no improvement): 0.07227077484130859\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.07213150262832642  to: 0.07203182578086853\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.07203182578086853  to: 0.07186774611473083\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.07186774611473083  to: 0.07176735997200012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.07176735997200012  to: 0.07172167301177979\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.07172167301177979  to: 0.0715284526348114\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.0715284526348114  to: 0.07115620374679565\n",
      "Training iteration: 1106\n",
      "Validation loss (no improvement): 0.07119182348251343\n",
      "Training iteration: 1107\n",
      "Validation loss (no improvement): 0.07137924432754517\n",
      "Training iteration: 1108\n",
      "Validation loss (no improvement): 0.07118515968322754\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.07115620374679565  to: 0.07100550532341003\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.07100550532341003  to: 0.07082445025444031\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.07082445025444031  to: 0.07068634629249573\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.07068634629249573  to: 0.07059940695762634\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.07059940695762634  to: 0.07019070386886597\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.07019070386886597  to: 0.07006719708442688\n",
      "Training iteration: 1115\n",
      "Validation loss (no improvement): 0.0702020287513733\n",
      "Training iteration: 1116\n",
      "Validation loss (no improvement): 0.07007015943527221\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.07006719708442688  to: 0.06998619437217712\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.06998619437217712  to: 0.0698777973651886\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.0698777973651886  to: 0.06974791288375855\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.06974791288375855  to: 0.06955901384353638\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.06955901384353638  to: 0.0691865086555481\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.0691865086555481  to: 0.06917570233345031\n",
      "Training iteration: 1123\n",
      "Validation loss (no improvement): 0.06944811940193177\n",
      "Training iteration: 1124\n",
      "Validation loss (no improvement): 0.06936627626419067\n",
      "Training iteration: 1125\n",
      "Validation loss (no improvement): 0.06920642852783203\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.06917570233345031  to: 0.06909884214401245\n",
      "Training iteration: 1127\n",
      "Validation loss (no improvement): 0.06914830207824707\n",
      "Training iteration: 1128\n",
      "Validation loss (no improvement): 0.06930945515632629\n",
      "Training iteration: 1129\n",
      "Validation loss (no improvement): 0.06910213828086853\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.06909884214401245  to: 0.06866329312324523\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.06866329312324523  to: 0.0686036229133606\n",
      "Training iteration: 1132\n",
      "Validation loss (no improvement): 0.06876531839370728\n",
      "Training iteration: 1133\n",
      "Validation loss (no improvement): 0.06886169910430909\n",
      "Training iteration: 1134\n",
      "Validation loss (no improvement): 0.06872475147247314\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.0686036229133606  to: 0.06856021881103516\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.06856021881103516  to: 0.06837456822395324\n",
      "Training iteration: 1137\n",
      "Validation loss (no improvement): 0.0683832049369812\n",
      "Training iteration: 1138\n",
      "Validation loss (no improvement): 0.0683802306652069\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.06837456822395324  to: 0.06820130348205566\n",
      "Training iteration: 1140\n",
      "Validation loss (no improvement): 0.0682057797908783\n",
      "Training iteration: 1141\n",
      "Validation loss (no improvement): 0.0682080864906311\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.06820130348205566  to: 0.0681115984916687\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.0681115984916687  to: 0.0679540753364563\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.0679540753364563  to: 0.06779940128326416\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.06779940128326416  to: 0.06777998208999633\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.06777998208999633  to: 0.06768400073051453\n",
      "Training iteration: 1147\n",
      "Validation loss (no improvement): 0.06776940822601318\n",
      "Training iteration: 1148\n",
      "Validation loss (no improvement): 0.06776152849197388\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.06768400073051453  to: 0.0676717460155487\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.0676717460155487  to: 0.06749868392944336\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.06749868392944336  to: 0.06728429198265076\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.06728429198265076  to: 0.06726064682006835\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.06726064682006835  to: 0.06719225645065308\n",
      "Training iteration: 1154\n",
      "Validation loss (no improvement): 0.06725821495056153\n",
      "Training iteration: 1155\n",
      "Validation loss (no improvement): 0.06727336049079895\n",
      "Training iteration: 1156\n",
      "Validation loss (no improvement): 0.06720737218856812\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.06719225645065308  to: 0.06707146763801575\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.06707146763801575  to: 0.0669420063495636\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.0669420063495636  to: 0.06671239137649536\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.06671239137649536  to: 0.06652019619941711\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.06652019619941711  to: 0.06646395325660706\n",
      "Training iteration: 1162\n",
      "Validation loss (no improvement): 0.06665312647819518\n",
      "Training iteration: 1163\n",
      "Validation loss (no improvement): 0.06669909358024598\n",
      "Training iteration: 1164\n",
      "Validation loss (no improvement): 0.06666836738586426\n",
      "Training iteration: 1165\n",
      "Validation loss (no improvement): 0.06650038361549378\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.06646395325660706  to: 0.0661102592945099\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.0661102592945099  to: 0.06606457233428956\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.06606457233428956  to: 0.0659708023071289\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.0659708023071289  to: 0.06593924760818481\n",
      "Training iteration: 1170\n",
      "Validation loss (no improvement): 0.06594763398170471\n",
      "Training iteration: 1171\n",
      "Validation loss (no improvement): 0.06614222526550292\n",
      "Training iteration: 1172\n",
      "Validation loss (no improvement): 0.06625444293022156\n",
      "Training iteration: 1173\n",
      "Validation loss (no improvement): 0.06624405384063721\n",
      "Training iteration: 1174\n",
      "Validation loss (no improvement): 0.06608438491821289\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.06593924760818481  to: 0.06583458781242371\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.06583458781242371  to: 0.06574720740318299\n",
      "Training iteration: 1177\n",
      "Validation loss (no improvement): 0.06581679582595826\n",
      "Training iteration: 1178\n",
      "Validation loss (no improvement): 0.06576812267303467\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.06574720740318299  to: 0.06572351455688477\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.06572351455688477  to: 0.0657104194164276\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.0657104194164276  to: 0.06568908095359802\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.06568908095359802  to: 0.06550568342208862\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.06550568342208862  to: 0.06533941030502319\n",
      "Training iteration: 1184\n",
      "Validation loss (no improvement): 0.0653476059436798\n",
      "Training iteration: 1185\n",
      "Validation loss (no improvement): 0.06546488404273987\n",
      "Training iteration: 1186\n",
      "Validation loss (no improvement): 0.06545608639717101\n",
      "Training iteration: 1187\n",
      "Validation loss (no improvement): 0.06537955403327941\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.06533941030502319  to: 0.06520143747329712\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.06520143747329712  to: 0.06504135727882385\n",
      "Training iteration: 1190\n",
      "Validation loss (no improvement): 0.06509805321693421\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.06516398191452026\n",
      "Training iteration: 1192\n",
      "Validation loss (no improvement): 0.065095454454422\n",
      "Training iteration: 1193\n",
      "Validation loss (no improvement): 0.06512576937675477\n",
      "Training iteration: 1194\n",
      "Validation loss (no improvement): 0.06513803601264953\n",
      "Training iteration: 1195\n",
      "Validation loss (no improvement): 0.06520886421203613\n",
      "Training iteration: 1196\n",
      "Validation loss (no improvement): 0.0651407539844513\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.06504135727882385  to: 0.06491427421569824\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.06491427421569824  to: 0.06480005979537964\n",
      "Training iteration: 1199\n",
      "Validation loss (no improvement): 0.06481146812438965\n",
      "Training iteration: 1200\n",
      "Validation loss (no improvement): 0.06508877277374267\n",
      "Training iteration: 1201\n",
      "Validation loss (no improvement): 0.06511141061782837\n",
      "Training iteration: 1202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06500402688980103\n",
      "Training iteration: 1203\n",
      "Validation loss (no improvement): 0.06499158143997193\n",
      "Training iteration: 1204\n",
      "Validation loss (no improvement): 0.06500815153121949\n",
      "Training iteration: 1205\n",
      "Validation loss (no improvement): 0.06513248682022095\n",
      "Training iteration: 1206\n",
      "Validation loss (no improvement): 0.06503850221633911\n",
      "Training iteration: 1207\n",
      "Validation loss (no improvement): 0.06489199995994568\n",
      "Training iteration: 1208\n",
      "Validation loss (no improvement): 0.0648722231388092\n",
      "Training iteration: 1209\n",
      "Validation loss (no improvement): 0.06496747136116028\n",
      "Training iteration: 1210\n",
      "Validation loss (no improvement): 0.06503952145576478\n",
      "Training iteration: 1211\n",
      "Validation loss (no improvement): 0.06501759886741638\n",
      "Training iteration: 1212\n",
      "Validation loss (no improvement): 0.06492763757705688\n",
      "Training iteration: 1213\n",
      "Validation loss (no improvement): 0.06485344171524048\n",
      "Training iteration: 1214\n",
      "Validation loss (no improvement): 0.06483235955238342\n",
      "Training iteration: 1215\n",
      "Validation loss (no improvement): 0.0650115430355072\n",
      "Training iteration: 1216\n",
      "Validation loss (no improvement): 0.06505777835845947\n",
      "Training iteration: 1217\n",
      "Validation loss (no improvement): 0.06497095823287964\n",
      "Training iteration: 1218\n",
      "Validation loss (no improvement): 0.06489574313163757\n",
      "Training iteration: 1219\n",
      "Validation loss (no improvement): 0.06495679020881653\n",
      "Training iteration: 1220\n",
      "Validation loss (no improvement): 0.06501795053482055\n",
      "Training iteration: 1221\n",
      "Validation loss (no improvement): 0.06487349271774293\n",
      "Training iteration: 1222\n",
      "Validation loss (no improvement): 0.06488175392150879\n",
      "Training iteration: 1223\n",
      "Validation loss (no improvement): 0.06505830883979798\n",
      "Training iteration: 1224\n",
      "Validation loss (no improvement): 0.0652266800403595\n",
      "Training iteration: 1225\n",
      "Validation loss (no improvement): 0.06518553495407105\n",
      "Training iteration: 1226\n",
      "Validation loss (no improvement): 0.06506437659263611\n",
      "Training iteration: 1227\n",
      "Validation loss (no improvement): 0.06492814421653748\n",
      "Training iteration: 1228\n",
      "Validation loss (no improvement): 0.06498888731002808\n",
      "Training iteration: 1229\n",
      "Validation loss (no improvement): 0.0652492880821228\n",
      "Training iteration: 1230\n",
      "Validation loss (no improvement): 0.06514651775360107\n",
      "Training iteration: 1231\n",
      "Validation loss (no improvement): 0.06506675481796265\n",
      "Training iteration: 1232\n",
      "Validation loss (no improvement): 0.06516515016555786\n",
      "Training iteration: 1233\n",
      "Validation loss (no improvement): 0.06506454348564147\n",
      "Training iteration: 1234\n",
      "Validation loss (no improvement): 0.06504439115524292\n",
      "Training iteration: 1235\n",
      "Validation loss (no improvement): 0.06494020223617554\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.06480005979537964  to: 0.06470076441764831\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.06470076441764831  to: 0.06462780833244323\n",
      "Training iteration: 1238\n",
      "Validation loss (no improvement): 0.06476098299026489\n",
      "Training iteration: 1239\n",
      "Validation loss (no improvement): 0.0651294469833374\n",
      "Training iteration: 1240\n",
      "Validation loss (no improvement): 0.06533225774765014\n",
      "Training iteration: 1241\n",
      "Validation loss (no improvement): 0.06530043482780457\n",
      "Training iteration: 1242\n",
      "Validation loss (no improvement): 0.06528559923171998\n",
      "Training iteration: 1243\n",
      "Validation loss (no improvement): 0.06505149602890015\n",
      "Training iteration: 1244\n",
      "Validation loss (no improvement): 0.06505910158157349\n",
      "Training iteration: 1245\n",
      "Validation loss (no improvement): 0.0651275098323822\n",
      "Training iteration: 1246\n",
      "Validation loss (no improvement): 0.06500008702278137\n",
      "Training iteration: 1247\n",
      "Validation loss (no improvement): 0.06499755382537842\n",
      "Training iteration: 1248\n",
      "Validation loss (no improvement): 0.06512021422386169\n",
      "Training iteration: 1249\n",
      "Validation loss (no improvement): 0.0651725172996521\n",
      "Training iteration: 1250\n",
      "Validation loss (no improvement): 0.06515220403671265\n",
      "Training iteration: 1251\n",
      "Validation loss (no improvement): 0.06510525345802307\n",
      "Training iteration: 1252\n",
      "Validation loss (no improvement): 0.06508127450942994\n",
      "Training iteration: 1253\n",
      "Validation loss (no improvement): 0.06508344411849976\n",
      "Training iteration: 1254\n",
      "Validation loss (no improvement): 0.06498861312866211\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.06504877209663391\n",
      "Training iteration: 1256\n",
      "Validation loss (no improvement): 0.06512606739997864\n",
      "Training iteration: 1257\n",
      "Validation loss (no improvement): 0.06505048871040345\n",
      "Training iteration: 1258\n",
      "Validation loss (no improvement): 0.06494933366775513\n",
      "Training iteration: 1259\n",
      "Validation loss (no improvement): 0.06506701707839965\n",
      "Training iteration: 1260\n",
      "Validation loss (no improvement): 0.06497951745986938\n",
      "Training iteration: 1261\n",
      "Validation loss (no improvement): 0.06496804356575012\n",
      "Training iteration: 1262\n",
      "Validation loss (no improvement): 0.0651328444480896\n",
      "Training iteration: 1263\n",
      "Validation loss (no improvement): 0.06520600318908691\n",
      "Training iteration: 1264\n",
      "Validation loss (no improvement): 0.0650824785232544\n",
      "Training iteration: 1265\n",
      "Validation loss (no improvement): 0.06492283940315247\n",
      "Training iteration: 1266\n",
      "Validation loss (no improvement): 0.06476541757583618\n",
      "Training iteration: 1267\n",
      "Validation loss (no improvement): 0.06482468843460083\n",
      "Training iteration: 1268\n",
      "Validation loss (no improvement): 0.064903724193573\n",
      "Training iteration: 1269\n",
      "Validation loss (no improvement): 0.06491233706474304\n",
      "Training iteration: 1270\n",
      "Validation loss (no improvement): 0.06483209729194642\n",
      "Training iteration: 1271\n",
      "Validation loss (no improvement): 0.06473996043205262\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.0648460030555725\n",
      "Training iteration: 1273\n",
      "Validation loss (no improvement): 0.06503595113754272\n",
      "Training iteration: 1274\n",
      "Validation loss (no improvement): 0.06505169868469238\n",
      "Training iteration: 1275\n",
      "Validation loss (no improvement): 0.06506720185279846\n",
      "Training iteration: 1276\n",
      "Validation loss (no improvement): 0.06501949429512024\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.06489326953887939\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.06495351791381836\n",
      "Training iteration: 1279\n",
      "Validation loss (no improvement): 0.06492014527320862\n",
      "Training iteration: 1280\n",
      "Validation loss (no improvement): 0.06474426984786988\n",
      "Training iteration: 1281\n",
      "Validation loss (no improvement): 0.06490777730941773\n",
      "Training iteration: 1282\n",
      "Validation loss (no improvement): 0.06506000757217408\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.06520727276802063\n",
      "Training iteration: 1284\n",
      "Validation loss (no improvement): 0.06516866683959961\n",
      "Training iteration: 1285\n",
      "Validation loss (no improvement): 0.06501021981239319\n",
      "Training iteration: 1286\n",
      "Validation loss (no improvement): 0.06493948698043824\n",
      "Training iteration: 1287\n",
      "Validation loss (no improvement): 0.06490407586097717\n",
      "Training iteration: 1288\n",
      "Validation loss (no improvement): 0.06503779292106629\n",
      "Training iteration: 1289\n",
      "Validation loss (no improvement): 0.06510799527168273\n",
      "Training iteration: 1290\n",
      "Validation loss (no improvement): 0.0650069534778595\n",
      "Training iteration: 1291\n",
      "Validation loss (no improvement): 0.06497442126274108\n",
      "Training iteration: 1292\n",
      "Validation loss (no improvement): 0.06495716571807861\n",
      "Training iteration: 1293\n",
      "Validation loss (no improvement): 0.0648815929889679\n",
      "Training iteration: 1294\n",
      "Validation loss (no improvement): 0.06491302847862243\n",
      "Training iteration: 1295\n",
      "Validation loss (no improvement): 0.06474575996398926\n",
      "Training iteration: 1296\n",
      "Validation loss (no improvement): 0.06469661593437195\n",
      "Training iteration: 1297\n",
      "Validation loss (no improvement): 0.06485581398010254\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.06494015455245972\n",
      "Training iteration: 1299\n",
      "Validation loss (no improvement): 0.0649196207523346\n",
      "Training iteration: 1300\n",
      "Validation loss (no improvement): 0.06481984853744507\n",
      "Training iteration: 1301\n",
      "Validation loss (no improvement): 0.06489654779434204\n",
      "Training iteration: 1302\n",
      "Validation loss (no improvement): 0.06497746109962463\n",
      "Training iteration: 1303\n",
      "Validation loss (no improvement): 0.06493948698043824\n",
      "Training iteration: 1304\n",
      "Validation loss (no improvement): 0.06475781202316284\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.06473990082740784\n",
      "Training iteration: 1306\n",
      "Validation loss (no improvement): 0.06483302116394044\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.06486385464668273\n",
      "Training iteration: 1308\n",
      "Validation loss (no improvement): 0.06480540037155151\n",
      "Training iteration: 1309\n",
      "Validation loss (no improvement): 0.06476683616638183\n",
      "Training iteration: 1310\n",
      "Validation loss (no improvement): 0.06481798887252807\n",
      "Training iteration: 1311\n",
      "Validation loss (no improvement): 0.06488136053085328\n",
      "Training iteration: 1312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06485449075698853\n",
      "Training iteration: 1313\n",
      "Validation loss (no improvement): 0.06491144895553588\n",
      "Training iteration: 1314\n",
      "Validation loss (no improvement): 0.06492396593093872\n",
      "Training iteration: 1315\n",
      "Validation loss (no improvement): 0.0647284984588623\n",
      "Training iteration: 1316\n",
      "Validation loss (no improvement): 0.06475776433944702\n",
      "Training iteration: 1317\n",
      "Validation loss (no improvement): 0.06497785449028015\n",
      "Training iteration: 1318\n",
      "Validation loss (no improvement): 0.06515081524848938\n",
      "Training iteration: 1319\n",
      "Validation loss (no improvement): 0.06512258648872375\n",
      "Training iteration: 1320\n",
      "Validation loss (no improvement): 0.06495755910873413\n",
      "Training iteration: 1321\n",
      "Validation loss (no improvement): 0.06469290256500244\n",
      "Training iteration: 1322\n",
      "Validation loss (no improvement): 0.06472684144973755\n",
      "Training iteration: 1323\n",
      "Validation loss (no improvement): 0.06485646367073059\n",
      "Training iteration: 1324\n",
      "Validation loss (no improvement): 0.06489975452423095\n",
      "Training iteration: 1325\n",
      "Validation loss (no improvement): 0.06491917371749878\n",
      "Training iteration: 1326\n",
      "Validation loss (no improvement): 0.06493247747421264\n",
      "Training iteration: 1327\n",
      "Validation loss (no improvement): 0.06487513780593872\n",
      "Training iteration: 1328\n",
      "Validation loss (no improvement): 0.0648323655128479\n",
      "Training iteration: 1329\n",
      "Validation loss (no improvement): 0.06471724510192871\n",
      "Training iteration: 1330\n",
      "Validation loss (no improvement): 0.06486587524414063\n",
      "Training iteration: 1331\n",
      "Validation loss (no improvement): 0.06503934860229492\n",
      "Training iteration: 1332\n",
      "Validation loss (no improvement): 0.06503790020942687\n",
      "Training iteration: 1333\n",
      "Validation loss (no improvement): 0.06485791802406311\n",
      "Training iteration: 1334\n",
      "Validation loss (no improvement): 0.06469925642013549\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.06462780833244323  to: 0.06460368037223815\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.06460368037223815  to: 0.06458494067192078\n",
      "Training iteration: 1337\n",
      "Validation loss (no improvement): 0.0648323655128479\n",
      "Training iteration: 1338\n",
      "Validation loss (no improvement): 0.06496877074241639\n",
      "Training iteration: 1339\n",
      "Validation loss (no improvement): 0.06489508152008057\n",
      "Training iteration: 1340\n",
      "Validation loss (no improvement): 0.06480680108070373\n",
      "Training iteration: 1341\n",
      "Validation loss (no improvement): 0.0647149920463562\n",
      "Training iteration: 1342\n",
      "Validation loss (no improvement): 0.06463936567306519\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.06458494067192078  to: 0.06457222700119018\n",
      "Training iteration: 1344\n",
      "Validation loss (no improvement): 0.06465612649917603\n",
      "Training iteration: 1345\n",
      "Validation loss (no improvement): 0.06467536687850953\n",
      "Training iteration: 1346\n",
      "Validation loss (no improvement): 0.06500771641731262\n",
      "Training iteration: 1347\n",
      "Validation loss (no improvement): 0.06515480279922485\n",
      "Training iteration: 1348\n",
      "Validation loss (no improvement): 0.06506375074386597\n",
      "Training iteration: 1349\n",
      "Validation loss (no improvement): 0.06480867862701416\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.06457222700119018  to: 0.06448221802711487\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.06448221802711487  to: 0.06441913843154908\n",
      "Training iteration: 1352\n",
      "Validation loss (no improvement): 0.06454727053642273\n",
      "Training iteration: 1353\n",
      "Validation loss (no improvement): 0.06485074758529663\n",
      "Training iteration: 1354\n",
      "Validation loss (no improvement): 0.06511207818984985\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.06525734663009644\n",
      "Training iteration: 1356\n",
      "Validation loss (no improvement): 0.06511693000793457\n",
      "Training iteration: 1357\n",
      "Validation loss (no improvement): 0.06474011540412902\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.06468800306320191\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.06465398073196411\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.06441913843154908  to: 0.06439847350120545\n",
      "Training iteration: 1361\n",
      "Validation loss (no improvement): 0.06447418928146362\n",
      "Training iteration: 1362\n",
      "Validation loss (no improvement): 0.06485087275505066\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.06525255441665649\n",
      "Training iteration: 1364\n",
      "Validation loss (no improvement): 0.06539567708969116\n",
      "Training iteration: 1365\n",
      "Validation loss (no improvement): 0.06523528695106506\n",
      "Training iteration: 1366\n",
      "Validation loss (no improvement): 0.06488450169563294\n",
      "Training iteration: 1367\n",
      "Validation loss (no improvement): 0.06463637351989746\n",
      "Training iteration: 1368\n",
      "Validation loss (no improvement): 0.06458530426025391\n",
      "Training iteration: 1369\n",
      "Validation loss (no improvement): 0.06473931670188904\n",
      "Training iteration: 1370\n",
      "Validation loss (no improvement): 0.06494771838188171\n",
      "Training iteration: 1371\n",
      "Validation loss (no improvement): 0.06496812105178833\n",
      "Training iteration: 1372\n",
      "Validation loss (no improvement): 0.0647936463356018\n",
      "Training iteration: 1373\n",
      "Validation loss (no improvement): 0.0645425260066986\n",
      "Training iteration: 1374\n",
      "Validation loss (no improvement): 0.06444917917251587\n",
      "Training iteration: 1375\n",
      "Validation loss (no improvement): 0.06460744142532349\n",
      "Training iteration: 1376\n",
      "Validation loss (no improvement): 0.06468960046768188\n",
      "Training iteration: 1377\n",
      "Validation loss (no improvement): 0.06473973393440247\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.06477335095405579\n",
      "Training iteration: 1379\n",
      "Validation loss (no improvement): 0.0647621750831604\n",
      "Training iteration: 1380\n",
      "Validation loss (no improvement): 0.06470198035240174\n",
      "Training iteration: 1381\n",
      "Validation loss (no improvement): 0.06460105180740357\n",
      "Training iteration: 1382\n",
      "Validation loss (no improvement): 0.06445406675338745\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.06456599831581115\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.06470271348953247\n",
      "Training iteration: 1385\n",
      "Validation loss (no improvement): 0.06469703912734985\n",
      "Training iteration: 1386\n",
      "Validation loss (no improvement): 0.06462051272392273\n",
      "Training iteration: 1387\n",
      "Validation loss (no improvement): 0.06444416046142579\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.06449307799339295\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.0645721435546875\n",
      "Training iteration: 1390\n",
      "Validation loss (no improvement): 0.06451613306999207\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.06439847350120545  to: 0.06435943841934204\n",
      "Training iteration: 1392\n",
      "Validation loss (no improvement): 0.0643992304801941\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.06435943841934204  to: 0.06428712010383605\n",
      "Training iteration: 1394\n",
      "Validation loss (no improvement): 0.06435462236404418\n",
      "Training iteration: 1395\n",
      "Validation loss (no improvement): 0.06474340558052064\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.06493310928344727\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.06485158801078797\n",
      "Training iteration: 1398\n",
      "Validation loss (no improvement): 0.0645790934562683\n",
      "Training iteration: 1399\n",
      "Validation loss (no improvement): 0.06436858177185059\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.06438971757888794\n",
      "Training iteration: 1401\n",
      "Validation loss (no improvement): 0.06462569832801819\n",
      "Training iteration: 1402\n",
      "Validation loss (no improvement): 0.06466559171676636\n",
      "Training iteration: 1403\n",
      "Validation loss (no improvement): 0.06452075839042663\n",
      "Training iteration: 1404\n",
      "Validation loss (no improvement): 0.06447504162788391\n",
      "Training iteration: 1405\n",
      "Validation loss (no improvement): 0.06444491147994995\n",
      "Training iteration: 1406\n",
      "Validation loss (no improvement): 0.06439476013183594\n",
      "Training iteration: 1407\n",
      "Validation loss (no improvement): 0.06436020135879517\n",
      "Training iteration: 1408\n",
      "Validation loss (no improvement): 0.06455576419830322\n",
      "Training iteration: 1409\n",
      "Validation loss (no improvement): 0.0645963728427887\n",
      "Training iteration: 1410\n",
      "Validation loss (no improvement): 0.0644111156463623\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.06428712010383605  to: 0.0642598807811737\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.0642598807811737  to: 0.06425286531448364\n",
      "Training iteration: 1413\n",
      "Validation loss (no improvement): 0.06429917216300965\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.06425286531448364  to: 0.06423619985580445\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.06423619985580445  to: 0.06408103108406067\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.06408103108406067  to: 0.06401168704032897\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.06417728662490844\n",
      "Training iteration: 1418\n",
      "Validation loss (no improvement): 0.0642942488193512\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.0642475426197052\n",
      "Training iteration: 1420\n",
      "Validation loss (no improvement): 0.06435810327529908\n",
      "Training iteration: 1421\n",
      "Validation loss (no improvement): 0.06428632736206055\n",
      "Training iteration: 1422\n",
      "Validation loss (no improvement): 0.0643013596534729\n",
      "Training iteration: 1423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06424574255943298\n",
      "Training iteration: 1424\n",
      "Validation loss (no improvement): 0.06421546936035157\n",
      "Training iteration: 1425\n",
      "Validation loss (no improvement): 0.06442995667457581\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.06449065208435059\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.06443851590156555\n",
      "Training iteration: 1428\n",
      "Validation loss (no improvement): 0.06433175802230835\n",
      "Training iteration: 1429\n",
      "Validation loss (no improvement): 0.06440481543540955\n",
      "Training iteration: 1430\n",
      "Validation loss (no improvement): 0.06452473998069763\n",
      "Training iteration: 1431\n",
      "Validation loss (no improvement): 0.0644591987133026\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.06435962915420532\n",
      "Training iteration: 1433\n",
      "Validation loss (no improvement): 0.0641785979270935\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.06424713134765625\n",
      "Training iteration: 1435\n",
      "Validation loss (no improvement): 0.06457058191299439\n",
      "Training iteration: 1436\n",
      "Validation loss (no improvement): 0.06481618285179139\n",
      "Training iteration: 1437\n",
      "Validation loss (no improvement): 0.0648349642753601\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.06464253067970276\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.06429358720779418\n",
      "Training iteration: 1440\n",
      "Validation loss (no improvement): 0.06424298286437988\n",
      "Training iteration: 1441\n",
      "Validation loss (no improvement): 0.06454107165336609\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.06441647410392762\n",
      "Training iteration: 1443\n",
      "Validation loss (no improvement): 0.06447910070419312\n",
      "Training iteration: 1444\n",
      "Validation loss (no improvement): 0.06459128260612487\n",
      "Training iteration: 1445\n",
      "Validation loss (no improvement): 0.06456907391548157\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.0646370530128479\n",
      "Training iteration: 1447\n",
      "Validation loss (no improvement): 0.06467193365097046\n",
      "Training iteration: 1448\n",
      "Validation loss (no improvement): 0.06450411081314086\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.06435538530349731\n",
      "Training iteration: 1450\n",
      "Validation loss (no improvement): 0.06442291140556336\n",
      "Training iteration: 1451\n",
      "Validation loss (no improvement): 0.06440178155899048\n",
      "Training iteration: 1452\n",
      "Validation loss (no improvement): 0.06441062688827515\n",
      "Training iteration: 1453\n",
      "Validation loss (no improvement): 0.06446070671081543\n",
      "Training iteration: 1454\n",
      "Validation loss (no improvement): 0.0644187569618225\n",
      "Training iteration: 1455\n",
      "Validation loss (no improvement): 0.0642401933670044\n",
      "Training iteration: 1456\n",
      "Validation loss (no improvement): 0.0641555905342102\n",
      "Training iteration: 1457\n",
      "Validation loss (no improvement): 0.0641567587852478\n",
      "Training iteration: 1458\n",
      "Validation loss (no improvement): 0.06430699825286865\n",
      "Training iteration: 1459\n",
      "Validation loss (no improvement): 0.06434057950973511\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.06426380276679992\n",
      "Training iteration: 1461\n",
      "Validation loss (no improvement): 0.06413483023643493\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.06404043436050415\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.0640416145324707\n",
      "Training iteration: 1464\n",
      "Validation loss (no improvement): 0.06403950452804566\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.06401168704032897  to: 0.0638422966003418\n",
      "Training iteration: 1466\n",
      "Validation loss (no improvement): 0.06401405930519104\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.06430314183235168\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.06444536447525025\n",
      "Training iteration: 1469\n",
      "Validation loss (no improvement): 0.06437124013900757\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.06423766016960145\n",
      "Training iteration: 1471\n",
      "Validation loss (no improvement): 0.0643523871898651\n",
      "Training iteration: 1472\n",
      "Validation loss (no improvement): 0.06446935534477234\n",
      "Training iteration: 1473\n",
      "Validation loss (no improvement): 0.0645426332950592\n",
      "Training iteration: 1474\n",
      "Validation loss (no improvement): 0.0643936812877655\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.0642815351486206\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.06406769752502442\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.06414114236831665\n",
      "Training iteration: 1478\n",
      "Validation loss (no improvement): 0.06447237730026245\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.0646872103214264\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.06472407579421997\n",
      "Training iteration: 1481\n",
      "Validation loss (no improvement): 0.06454632878303528\n",
      "Training iteration: 1482\n",
      "Validation loss (no improvement): 0.06433240175247193\n",
      "Training iteration: 1483\n",
      "Validation loss (no improvement): 0.06420270204544068\n",
      "Training iteration: 1484\n",
      "Validation loss (no improvement): 0.06414173841476441\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.06390774846076966\n",
      "Training iteration: 1486\n",
      "Validation loss (no improvement): 0.06403691172599793\n",
      "Training iteration: 1487\n",
      "Validation loss (no improvement): 0.06448711156845092\n",
      "Training iteration: 1488\n",
      "Validation loss (no improvement): 0.06473318338394166\n",
      "Training iteration: 1489\n",
      "Validation loss (no improvement): 0.06467796564102173\n",
      "Training iteration: 1490\n",
      "Validation loss (no improvement): 0.06437872052192688\n",
      "Training iteration: 1491\n",
      "Validation loss (no improvement): 0.06397716403007507\n",
      "Training iteration: 1492\n",
      "Validation loss (no improvement): 0.063895845413208\n",
      "Training iteration: 1493\n",
      "Validation loss (no improvement): 0.06408873796463013\n",
      "Training iteration: 1494\n",
      "Validation loss (no improvement): 0.06419765353202819\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.06419438719749451\n",
      "Training iteration: 1496\n",
      "Validation loss (no improvement): 0.0644142210483551\n",
      "Training iteration: 1497\n",
      "Validation loss (no improvement): 0.0646161675453186\n",
      "Training iteration: 1498\n",
      "Validation loss (no improvement): 0.06459506750106811\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.06436823606491089\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.06396383047103882\n",
      "Training iteration: 1501\n",
      "Validation loss (no improvement): 0.06385608911514282\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.0639892578125\n",
      "Training iteration: 1503\n",
      "Validation loss (no improvement): 0.06437053084373474\n",
      "Training iteration: 1504\n",
      "Validation loss (no improvement): 0.06447547078132629\n",
      "Training iteration: 1505\n",
      "Validation loss (no improvement): 0.06435664892196655\n",
      "Training iteration: 1506\n",
      "Validation loss (no improvement): 0.0642142653465271\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.06410056948661805\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.06388779878616332\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.06397920846939087\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.06418164372444153\n",
      "Training iteration: 1511\n",
      "Validation loss (no improvement): 0.0641376793384552\n",
      "Training iteration: 1512\n",
      "Validation loss (no improvement): 0.06415127515792847\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.06396535634994507\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.06386600732803345\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.06400275230407715\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.06402552723884583\n",
      "Training iteration: 1517\n",
      "Validation loss (no improvement): 0.06405319571495056\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.06399815678596496\n",
      "Training iteration: 1519\n",
      "Validation loss (no improvement): 0.06388264298439025\n",
      "Training iteration: 1520\n",
      "Validation loss (no improvement): 0.06385412812232971\n",
      "Training iteration: 1521\n",
      "Validation loss (no improvement): 0.06409489512443542\n",
      "Training iteration: 1522\n",
      "Validation loss (no improvement): 0.0638468861579895\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.0638422966003418  to: 0.06379131078720093\n",
      "Training iteration: 1524\n",
      "Validation loss (no improvement): 0.0639161765575409\n",
      "Training iteration: 1525\n",
      "Validation loss (no improvement): 0.06410096883773804\n",
      "Training iteration: 1526\n",
      "Validation loss (no improvement): 0.06417472958564759\n",
      "Training iteration: 1527\n",
      "Validation loss (no improvement): 0.06413087844848633\n",
      "Training iteration: 1528\n",
      "Validation loss (no improvement): 0.06390677094459533\n",
      "Training iteration: 1529\n",
      "Validation loss (no improvement): 0.06381207704544067\n",
      "Training iteration: 1530\n",
      "Validation loss (no improvement): 0.06400346755981445\n",
      "Training iteration: 1531\n",
      "Validation loss (no improvement): 0.06408985257148743\n",
      "Training iteration: 1532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06427081823348998\n",
      "Training iteration: 1533\n",
      "Validation loss (no improvement): 0.06446687579154968\n",
      "Training iteration: 1534\n",
      "Validation loss (no improvement): 0.06437260508537293\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.06406705379486084\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.06386491060256957\n",
      "Training iteration: 1537\n",
      "Validation loss (no improvement): 0.06394434571266175\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.0642924427986145\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.06459460258483887\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.06466552019119262\n",
      "Training iteration: 1541\n",
      "Validation loss (no improvement): 0.06445918679237365\n",
      "Training iteration: 1542\n",
      "Validation loss (no improvement): 0.06409622430801391\n",
      "Training iteration: 1543\n",
      "Validation loss (no improvement): 0.06399928331375122\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.06379441022872925\n",
      "Training iteration: 1545\n",
      "Validation loss (no improvement): 0.06382091045379638\n",
      "Training iteration: 1546\n",
      "Validation loss (no improvement): 0.06411949396133423\n",
      "Training iteration: 1547\n",
      "Validation loss (no improvement): 0.06451185941696166\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.06460763216018676\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.06436538696289062\n",
      "Training iteration: 1550\n",
      "Validation loss (no improvement): 0.06401305198669434\n",
      "Training iteration: 1551\n",
      "Validation loss (no improvement): 0.06382853388786316\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.06379131078720093  to: 0.06377292275428773\n",
      "Training iteration: 1553\n",
      "Validation loss (no improvement): 0.06394767761230469\n",
      "Training iteration: 1554\n",
      "Validation loss (no improvement): 0.06438785791397095\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.06446428298950195\n",
      "Training iteration: 1556\n",
      "Validation loss (no improvement): 0.06426112055778503\n",
      "Training iteration: 1557\n",
      "Validation loss (no improvement): 0.06386878490447997\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.06377292275428773  to: 0.06365255117416382\n",
      "Training iteration: 1559\n",
      "Validation loss (no improvement): 0.06374874711036682\n",
      "Training iteration: 1560\n",
      "Validation loss (no improvement): 0.06380757093429565\n",
      "Training iteration: 1561\n",
      "Validation loss (no improvement): 0.06397234201431275\n",
      "Training iteration: 1562\n",
      "Validation loss (no improvement): 0.06418152451515198\n",
      "Training iteration: 1563\n",
      "Validation loss (no improvement): 0.06403985023498535\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.06365255117416382  to: 0.0636437714099884\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.0636437714099884  to: 0.06350471377372742\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.06367822289466858\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.06350471377372742  to: 0.06350197196006775\n",
      "Training iteration: 1568\n",
      "Validation loss (no improvement): 0.06359379887580871\n",
      "Training iteration: 1569\n",
      "Validation loss (no improvement): 0.06375197768211364\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.06396714448928834\n",
      "Training iteration: 1571\n",
      "Validation loss (no improvement): 0.06390963792800904\n",
      "Training iteration: 1572\n",
      "Validation loss (no improvement): 0.06376657485961915\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.06373251676559448\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.06378926634788513\n",
      "Training iteration: 1575\n",
      "Validation loss (no improvement): 0.06351991891860961\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.06356525421142578\n",
      "Training iteration: 1577\n",
      "Validation loss (no improvement): 0.06376176476478576\n",
      "Training iteration: 1578\n",
      "Validation loss (no improvement): 0.06406614184379578\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.06405993103981018\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.0639686942100525\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.06394832730293273\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.06403098702430725\n",
      "Training iteration: 1583\n",
      "Validation loss (no improvement): 0.06420994997024536\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.06414013504981994\n",
      "Training iteration: 1585\n",
      "Validation loss (no improvement): 0.06401039361953735\n",
      "Training iteration: 1586\n",
      "Validation loss (no improvement): 0.06396994590759278\n",
      "Training iteration: 1587\n",
      "Validation loss (no improvement): 0.06391558647155762\n",
      "Training iteration: 1588\n",
      "Validation loss (no improvement): 0.06402257680892945\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.06398348808288574\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.06384775638580323\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.06386680603027343\n",
      "Training iteration: 1592\n",
      "Validation loss (no improvement): 0.06367096900939942\n",
      "Training iteration: 1593\n",
      "Validation loss (no improvement): 0.06381263136863709\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.06425848603248596\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.06437296867370605\n",
      "Training iteration: 1596\n",
      "Validation loss (no improvement): 0.06412093043327331\n",
      "Training iteration: 1597\n",
      "Validation loss (no improvement): 0.0637063980102539\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.06367145776748658\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.06367765665054322\n",
      "Training iteration: 1600\n",
      "Validation loss (no improvement): 0.06396206021308899\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.06397987008094788\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.06380632519721985\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.06352048516273498\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.06358746886253357\n",
      "Training iteration: 1605\n",
      "Validation loss (no improvement): 0.06365007758140565\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.0635479748249054\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.063788902759552\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.06393500566482543\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.06375147104263305\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.06350197196006775  to: 0.0634117841720581\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.0634117841720581  to: 0.06330315470695495\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.06339915990829467\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.06348249316215515\n",
      "Training iteration: 1614\n",
      "Validation loss (no improvement): 0.06371351480484008\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.0638257086277008\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.06385330557823181\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.06358464956283569\n",
      "Training iteration: 1618\n",
      "Validation loss (no improvement): 0.06350075602531433\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.06330315470695495  to: 0.063141930103302\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.06322936415672302\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.06363101005554199\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.06404959559440612\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.0639962375164032\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.06377925276756287\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.0634713351726532\n",
      "Training iteration: 1626\n",
      "Validation loss (no improvement): 0.06367544531822204\n",
      "Training iteration: 1627\n",
      "Validation loss (no improvement): 0.06360961794853211\n",
      "Training iteration: 1628\n",
      "Validation loss (no improvement): 0.06369576454162598\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.06384607553482055\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.06395306587219238\n",
      "Training iteration: 1631\n",
      "Validation loss (no improvement): 0.06369100213050842\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.06354208588600159\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.06370238065719605\n",
      "Training iteration: 1634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.063918936252594\n",
      "Training iteration: 1635\n",
      "Validation loss (no improvement): 0.06392456889152527\n",
      "Training iteration: 1636\n",
      "Validation loss (no improvement): 0.06368542909622192\n",
      "Training iteration: 1637\n",
      "Validation loss (no improvement): 0.06353129744529724\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.06364869475364685\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.06379649043083191\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.06383348107337952\n",
      "Training iteration: 1641\n",
      "Validation loss (no improvement): 0.06369760632514954\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.06356569528579711\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.06354468464851379\n",
      "Training iteration: 1644\n",
      "Validation loss (no improvement): 0.06354742646217346\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.063634192943573\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.06354084014892578\n",
      "Training iteration: 1647\n",
      "Validation loss (no improvement): 0.0634501338005066\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.06337980031967164\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.0635434091091156\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.06347869634628296\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.0634163498878479\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.06336241364479064\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.063141930103302  to: 0.06314132809638977\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.06314132809638977  to: 0.06307233572006225\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.06307233572006225  to: 0.06304320096969604\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.06304320096969604  to: 0.06297939419746398\n",
      "Training iteration: 1657\n",
      "Validation loss (no improvement): 0.06308769583702087\n",
      "Training iteration: 1658\n",
      "Validation loss (no improvement): 0.06321855783462524\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.06328156590461731\n",
      "Training iteration: 1660\n",
      "Validation loss (no improvement): 0.0631814181804657\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.06314781308174133\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.06319062113761902\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.06329333782196045\n",
      "Training iteration: 1664\n",
      "Validation loss (no improvement): 0.06324437260627747\n",
      "Training iteration: 1665\n",
      "Validation loss (no improvement): 0.06329420804977418\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.06344372034072876\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.06333950757980347\n",
      "Training iteration: 1668\n",
      "Validation loss (no improvement): 0.06346585154533387\n",
      "Training iteration: 1669\n",
      "Validation loss (no improvement): 0.06335717439651489\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.06331070661544799\n",
      "Training iteration: 1671\n",
      "Validation loss (no improvement): 0.0633176863193512\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.06313241720199585\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.0634803295135498\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.06378992795944213\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.06387624740600586\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.06361445784568787\n",
      "Training iteration: 1677\n",
      "Validation loss (no improvement): 0.06328795552253723\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.06321340799331665\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.06332699656486511\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.06328889727592468\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.06340955495834351\n",
      "Training iteration: 1682\n",
      "Validation loss (no improvement): 0.06397047638893127\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.06436476707458497\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.0642673134803772\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.06378695368766785\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.06339174509048462\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.06334381699562072\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.06316758990287781\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.06328178644180298\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.06377568244934081\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.06437733173370361\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.06456215977668762\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.06397968530654907\n",
      "Training iteration: 1694\n",
      "Validation loss (no improvement): 0.06326789855957031\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.0633081316947937\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.06297939419746398  to: 0.06290960311889648\n",
      "Training iteration: 1697\n",
      "Validation loss (no improvement): 0.06294880509376526\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.06340945959091186\n",
      "Training iteration: 1699\n",
      "Validation loss (no improvement): 0.06385785937309266\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.0637683093547821\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.063299560546875\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.06290960311889648  to: 0.06280825734138488\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.06280825734138488  to: 0.0626291275024414\n",
      "Training iteration: 1704\n",
      "Validation loss (no improvement): 0.06301996111869812\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.0632210373878479\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.06297990083694457\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.06359630823135376\n",
      "Training iteration: 1708\n",
      "Validation loss (no improvement): 0.06402384042739868\n",
      "Training iteration: 1709\n",
      "Validation loss (no improvement): 0.06412479877471924\n",
      "Training iteration: 1710\n",
      "Validation loss (no improvement): 0.0639644742012024\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.06338716745376587\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.0628740906715393\n",
      "Training iteration: 1713\n",
      "Validation loss (no improvement): 0.06278371810913086\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.06307729482650756\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.06337224245071411\n",
      "Training iteration: 1716\n",
      "Validation loss (no improvement): 0.0633762001991272\n",
      "Training iteration: 1717\n",
      "Validation loss (no improvement): 0.06380208134651184\n",
      "Training iteration: 1718\n",
      "Validation loss (no improvement): 0.06400075554847717\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.06390321254730225\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.06348307728767395\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.06319719552993774\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.06326464414596558\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.06366861462593079\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.06378372311592102\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.06352348923683167\n",
      "Training iteration: 1726\n",
      "Validation loss (no improvement): 0.06326066851615905\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.06313727498054504\n",
      "Training iteration: 1728\n",
      "Validation loss (no improvement): 0.06309552788734436\n",
      "Training iteration: 1729\n",
      "Validation loss (no improvement): 0.06309658885002137\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.06319723129272461\n",
      "Training iteration: 1731\n",
      "Validation loss (no improvement): 0.06361175179481507\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.06379201412200927\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.06352121233940125\n",
      "Training iteration: 1734\n",
      "Validation loss (no improvement): 0.06311672925949097\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.06282383799552918\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.06273815035820007\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.0626291275024414  to: 0.06258735060691833\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.06273393630981446\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.06306262016296386\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.06317006349563599\n",
      "Training iteration: 1741\n",
      "Validation loss (no improvement): 0.06302624344825744\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.06276530623435975\n",
      "Training iteration: 1743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06275714635848999\n",
      "Training iteration: 1744\n",
      "Validation loss (no improvement): 0.0628343939781189\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.06292204856872559\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.06303036808967591\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.06309540271759033\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.06304318308830262\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.06311233639717102\n",
      "Training iteration: 1750\n",
      "Validation loss (no improvement): 0.06299854516983032\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.06286667585372925\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.06265201568603515\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.06286026239395141\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.0628635048866272\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.0629349946975708\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.062979656457901\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.06293944120407105\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.0630183219909668\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.06298642754554748\n",
      "Training iteration: 1760\n",
      "Validation loss (no improvement): 0.06312953233718872\n",
      "Training iteration: 1761\n",
      "Validation loss (no improvement): 0.0630871295928955\n",
      "Training iteration: 1762\n",
      "Validation loss (no improvement): 0.06292556524276734\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.062784743309021\n",
      "Training iteration: 1764\n",
      "Validation loss (no improvement): 0.0631324291229248\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.06316579580307007\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.0630917489528656\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.06285781860351562\n",
      "Training iteration: 1768\n",
      "Validation loss (no improvement): 0.062922602891922\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.0631171464920044\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.06319876909255981\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.06308435201644898\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.06286521553993225\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.06305480003356934\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.06303918957710267\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.06289072036743164\n",
      "Training iteration: 1776\n",
      "Validation loss (no improvement): 0.06267484426498413\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.06289929151535034\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.06299457550048829\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.06308034658432007\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.06300511360168456\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.06269409656524658\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.06269928216934204\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.0626745879650116\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.06273863911628723\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.06280471682548523\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.0627661645412445\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.06258735060691833  to: 0.06253251433372498\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.06253251433372498  to: 0.062298285961151126\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.06237267255783081\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.06254299879074096\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.06269433498382568\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.0625499963760376\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.062298285961151126  to: 0.06220937967300415\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.06220937967300415  to: 0.06220786571502686\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.062492644786834715\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.06239182353019714\n",
      "Training iteration: 1797\n",
      "Validation loss (no improvement): 0.06235474348068237\n",
      "Training iteration: 1798\n",
      "Validation loss (no improvement): 0.06223706007003784\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.06223374605178833\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.06221080422401428\n",
      "Training iteration: 1801\n",
      "Validation loss (no improvement): 0.06240125298500061\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.06255542635917663\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.06248987317085266\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.062341153621673584\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.06220786571502686  to: 0.0622065544128418\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.062422990798950195\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.06267002820968628\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.06295841336250305\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.0629723846912384\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.06288361549377441\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.06250038146972656\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.06227007508277893\n",
      "Training iteration: 1813\n",
      "Validation loss (no improvement): 0.0624195396900177\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.06274367570877075\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.06283549070358277\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.06254565119743347\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.06251012086868286\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.06273124217987061\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.06294758915901184\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.06282451748847961\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.062427079677581786\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.06247028112411499\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.06252216100692749\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.06268226504325866\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.06278925538063049\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.06261506080627441\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.0622065544128418  to: 0.06217266917228699\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.06217266917228699  to: 0.062056368589401244\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.062056368589401244  to: 0.062021851539611816\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.0620577871799469\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.06235779523849487\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.0626172423362732\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.0627637267112732\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.06257155537605286\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.0621848464012146\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.062021851539611816  to: 0.061905431747436526\n",
      "Training iteration: 1837\n",
      "Validation loss (no improvement): 0.06200602650642395\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.06245764493942261\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.06240854263305664\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.0620597243309021\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.061905431747436526  to: 0.06165647506713867\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.0617976188659668\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.06183813810348511\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.06186330318450928\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.06245632171630859\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.06285737752914429\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.06277457475662232\n",
      "Training iteration: 1848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06226484179496765\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.061725187301635745\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.06173201203346253\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.06216270327568054\n",
      "Training iteration: 1852\n",
      "Validation loss (no improvement): 0.062166464328765866\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.06259856820106506\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.06286594271659851\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.06268439292907715\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.06237534284591675\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.062024646997451784\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.06205548048019409\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.062415856122970584\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.06301745176315307\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.06320891380310059\n",
      "Training iteration: 1862\n",
      "Validation loss (no improvement): 0.06291326284408569\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.06228877305984497\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.062032902240753175\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.06193897724151611\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.06202298402786255\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.06249682307243347\n",
      "Training iteration: 1868\n",
      "Validation loss (no improvement): 0.06270114183425904\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.06248151659965515\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.06204807758331299\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.06171673536300659\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.0616735577583313\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.0618837833404541\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.06208310723304748\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.061979615688323976\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.06165647506713867  to: 0.0616119384765625\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.0616119384765625  to: 0.061600446701049805\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.06160487532615662\n",
      "Training iteration: 1879\n",
      "Validation loss (no improvement): 0.061789965629577635\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.061887156963348386\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.06171369552612305\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.061600446701049805  to: 0.06135302782058716\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.0613551139831543\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.06165750622749329\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.06202607154846192\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.06211534738540649\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.061886394023895265\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.06157534122467041\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.061477559804916385\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.06164695024490356\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.0621293842792511\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.06284947395324707\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.06302396059036255\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.06251522302627563\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.061806333065032956\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.06140487790107727\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.061439627408981325\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.061626899242401126\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.06189229488372803\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.06204525828361511\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.06200555562973022\n",
      "Training iteration: 1902\n",
      "Validation loss (no improvement): 0.061590451002120974\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.061515706777572635\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.0617388129234314\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.062004053592681886\n",
      "Training iteration: 1906\n",
      "Validation loss (no improvement): 0.06243563890457153\n",
      "Training iteration: 1907\n",
      "Validation loss (no improvement): 0.06240438222885132\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.06201189160346985\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.0615229070186615\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.06135302782058716  to: 0.061329877376556395\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.061542284488677976\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.06190563440322876\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.06242520213127136\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.06242548227310181\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.06191301345825195\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.06142865419387818\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.061329877376556395  to: 0.06132760047912598\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.06132760047912598  to: 0.06125335693359375\n",
      "Training iteration: 1919\n",
      "Validation loss (no improvement): 0.06160935163497925\n",
      "Training iteration: 1920\n",
      "Validation loss (no improvement): 0.062115222215652466\n",
      "Training iteration: 1921\n",
      "Validation loss (no improvement): 0.06220706105232239\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.061872446537017824\n",
      "Training iteration: 1923\n",
      "Validation loss (no improvement): 0.06132490038871765\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.06129722595214844\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.06136932373046875\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.06152814030647278\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.062175416946411134\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.06248627901077271\n",
      "Training iteration: 1929\n",
      "Validation loss (no improvement): 0.062199336290359494\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.061541962623596194\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.06125335693359375  to: 0.06119896769523621\n",
      "Training iteration: 1932\n",
      "Validation loss (no improvement): 0.061325854063034056\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.06143218874931335\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.061595183610916135\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.06154585480690002\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.061345213651657106\n",
      "Training iteration: 1937\n",
      "Validation loss (no improvement): 0.06128174066543579\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.06119896769523621  to: 0.06112501621246338\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.06112501621246338  to: 0.06097358465194702\n",
      "Training iteration: 1940\n",
      "Validation loss (no improvement): 0.06119029521942139\n",
      "Training iteration: 1941\n",
      "Validation loss (no improvement): 0.06133564114570618\n",
      "Training iteration: 1942\n",
      "Validation loss (no improvement): 0.061286437511444095\n",
      "Training iteration: 1943\n",
      "Validation loss (no improvement): 0.061088913679122926\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.06109410524368286\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.06126033663749695\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.061329185962677\n",
      "Training iteration: 1947\n",
      "Validation loss (no improvement): 0.0612854540348053\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.06134618520736694\n",
      "Training iteration: 1949\n",
      "Validation loss (no improvement): 0.06159670948982239\n",
      "Training iteration: 1950\n",
      "Validation loss (no improvement): 0.061604040861129764\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.06134806871414185\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.061105775833129886\n",
      "Training iteration: 1953\n",
      "Validation loss (no improvement): 0.06121528744697571\n",
      "Training iteration: 1954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06166092753410339\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.06223224401473999\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.06225796937942505\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.061834442615509036\n",
      "Training iteration: 1958\n",
      "Validation loss (no improvement): 0.061236721277236936\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.06113542318344116\n",
      "Training iteration: 1960\n",
      "Validation loss (no improvement): 0.06103583574295044\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.0613680899143219\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.06163698434829712\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.061658543348312375\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.061708229780197146\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.06171829104423523\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.0611878514289856\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.06103399395942688\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.06097358465194702  to: 0.06083106994628906\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.060995787382125854\n",
      "Training iteration: 1970\n",
      "Validation loss (no improvement): 0.061667656898498534\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.06198223829269409\n",
      "Training iteration: 1972\n",
      "Validation loss (no improvement): 0.061808550357818605\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.06126060485839844\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.06083106994628906  to: 0.0608102023601532\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.0608102023601532  to: 0.06056641936302185\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.06056641936302185  to: 0.06047778129577637\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.06067610383033752\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.060985791683197024\n",
      "Training iteration: 1979\n",
      "Validation loss (no improvement): 0.06139217615127564\n",
      "Training iteration: 1980\n",
      "Validation loss (no improvement): 0.0613994300365448\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.06113001108169556\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.06088520288467407\n",
      "Training iteration: 1983\n",
      "Validation loss (no improvement): 0.06057076454162598\n",
      "Training iteration: 1984\n",
      "Validation loss (no improvement): 0.060743510723114014\n",
      "Training iteration: 1985\n",
      "Validation loss (no improvement): 0.0611663281917572\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.06130642294883728\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.061124670505523684\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.060947763919830325\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.060742241144180295\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.060963279008865355\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.06126679182052612\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.061596852540969846\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.06157705783843994\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.06110208630561829\n",
      "Training iteration: 1995\n",
      "Validation loss (no improvement): 0.060823100805282596\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.06047778129577637  to: 0.06040353178977966\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.06042893528938294\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.060946881771087646\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.06179733276367187\n",
      "Training iteration: 2000\n",
      "Validation loss (no improvement): 0.06205852627754212\n",
      "Training iteration: 2001\n",
      "Validation loss (no improvement): 0.061490494012832644\n",
      "Training iteration: 2002\n",
      "Validation loss (no improvement): 0.06071869134902954\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.06046454310417175\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.06040353178977966  to: 0.060388374328613284\n",
      "Training iteration: 2005\n",
      "Validation loss (no improvement): 0.060504406690597534\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.06083090901374817\n",
      "Training iteration: 2007\n",
      "Validation loss (no improvement): 0.061747580766677856\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.062193548679351805\n",
      "Training iteration: 2009\n",
      "Validation loss (no improvement): 0.061741650104522705\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.06086423993110657\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.060388374328613284  to: 0.06034975051879883\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.06034975051879883  to: 0.06032052040100098\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.06032052040100098  to: 0.06016297340393066\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.06029964685440063\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.0607047975063324\n",
      "Training iteration: 2016\n",
      "Validation loss (no improvement): 0.0613541305065155\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.06147851347923279\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.060998845100402835\n",
      "Training iteration: 2019\n",
      "Validation loss (no improvement): 0.060564517974853516\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.06016297340393066  to: 0.06016040444374084\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.06023436784744263\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.060748302936553956\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.06139010190963745\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.06158411502838135\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.061224377155303954\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.060593140125274655\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.06044268608093262\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.060356706380844116\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.06043294668197632\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.06105203628540039\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.061375749111175534\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.06109069585800171\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.06040343642234802\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.060176366567611696\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.060385113954544066\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.060813939571380614\n",
      "Training iteration: 2037\n",
      "Validation loss (no improvement): 0.06102319955825806\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.06168354749679565\n",
      "Training iteration: 2039\n",
      "Validation loss (no improvement): 0.06154493689537048\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.060971629619598386\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.0604649007320404\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.06016040444374084  to: 0.059960675239562986\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.060018062591552734\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.06059336066246033\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.061485910415649415\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.06162795424461365\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.06062402725219727\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.059960675239562986  to: 0.0599493682384491\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.0599493682384491  to: 0.05984758734703064\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.05984758734703064  to: 0.05954378843307495\n",
      "Training iteration: 2051\n",
      "Validation loss (no improvement): 0.05963281393051147\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.06028519868850708\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.061374300718307497\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.062138569355010984\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.06184049248695374\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.060772436857223514\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.0600838303565979\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.06000329852104187\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.05974167585372925\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.06001019477844238\n",
      "Training iteration: 2061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06064783334732056\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): 0.06152779459953308\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): 0.06153074502944946\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.0606975257396698\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.05984616279602051\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.059863603115081786\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.05990862250328064\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.05979718565940857\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.06058437824249267\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.06106637716293335\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.06087457537651062\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.060154682397842406\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): 0.059748375415802\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): 0.059664130210876465\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.05992948412895203\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.06022928357124328\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): 0.060792040824890134\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.061271488666534424\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): 0.061285513639450076\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): 0.06063786745071411\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.05995398759841919\n",
      "Training iteration: 2082\n",
      "Validation loss (no improvement): 0.059745460748672485\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): 0.05973002314567566\n",
      "Training iteration: 2084\n",
      "Validation loss (no improvement): 0.05973564386367798\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.059956413507461545\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): 0.06045770049095154\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): 0.06064302921295166\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): 0.060229694843292235\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): 0.05967874526977539\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.059727811813354494\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): 0.05984074473381042\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): 0.06007518768310547\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.06064308881759643\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.06059341430664063\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.05997830629348755\n",
      "Training iteration: 2096\n",
      "Validation loss (no improvement): 0.05981115102767944\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): 0.06002992391586304\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.060369682312011716\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.060304248332977296\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.05997553467750549\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.05981391668319702\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.059951788187026976\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.05995911359786987\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.06008235216140747\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.05979922413825989\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.05968301892280579\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.05962268114089966\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.059877610206604\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.05967183113098144\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.05954378843307495  to: 0.05948864221572876\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.05948864221572876  to: 0.059245365858078006\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.05924919843673706\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.05929812788963318\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.059245365858078006  to: 0.059139972925186156\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.059139972925186156  to: 0.058924371004104616\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.058924371004104616  to: 0.05884207487106323\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.05889654755592346\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): 0.05905653834342957\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.05933597683906555\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.059234988689422605\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.059017503261566163\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.059125316143035886\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.05934551954269409\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.05986366868019104\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.05983102321624756\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.059395444393157956\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.05960586667060852\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.05954083204269409\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.060127007961273196\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.06028097867965698\n",
      "Training iteration: 2131\n",
      "Validation loss (no improvement): 0.05982964038848877\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.05952436327934265\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.05954948663711548\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.05959600210189819\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.05948403477668762\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.059638512134552\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.059487450122833255\n",
      "Training iteration: 2138\n",
      "Validation loss (no improvement): 0.05987681150436401\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.0600339412689209\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.0597595751285553\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.059178495407104494\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.0593206524848938\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.05952017903327942\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.059726905822753903\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.05973128080368042\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.05958911180496216\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.0593964159488678\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.059411078691482544\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.05929104685783386\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): 0.05947690010070801\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): 0.05962987542152405\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.059430825710296634\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.059322237968444824\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): 0.059031689167022706\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.059351098537445066\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): 0.05991863012313843\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.059745460748672485\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.05912085771560669\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.05884207487106323  to: 0.05869326591491699\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.05884903073310852\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.05924259424209595\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.05942489504814148\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.05936884880065918\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.05896069407463074\n",
      "Training iteration: 2165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.05870100259780884\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.05869326591491699  to: 0.05811060070991516\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.0582350492477417\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.05894623994827271\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.0596615195274353\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.05971206426620483\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.05914679765701294\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.05859563946723938\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): 0.05816774964332581\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): 0.05833503007888794\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.05910062789916992\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.05989113450050354\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.06014012694358826\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.05946008563041687\n",
      "Training iteration: 2179\n",
      "Validation loss (no improvement): 0.05871005654335022\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): 0.05850540399551392\n",
      "Training iteration: 2181\n",
      "Validation loss (no improvement): 0.05846530199050903\n",
      "Training iteration: 2182\n",
      "Validation loss (no improvement): 0.058854812383651735\n",
      "Training iteration: 2183\n",
      "Validation loss (no improvement): 0.05954238176345825\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.059864270687103274\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): 0.05942241549491882\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): 0.05875560641288757\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): 0.05825453996658325\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): 0.058164775371551514\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.05872023105621338\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): 0.05938664674758911\n",
      "Training iteration: 2191\n",
      "Validation loss (no improvement): 0.05943223834037781\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): 0.059053099155426024\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.05869503021240234\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.05817528367042542\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.058292829990386964\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.05875698924064636\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.05933564305305481\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): 0.059385502338409425\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): 0.05873531103134155\n",
      "Training iteration: 2200\n",
      "Validation loss (no improvement): 0.05856606960296631\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): 0.058856618404388425\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): 0.05901805758476257\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.059075462818145755\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): 0.058820325136184695\n",
      "Training iteration: 2205\n",
      "Validation loss (no improvement): 0.05886876583099365\n",
      "Training iteration: 2206\n",
      "Validation loss (no improvement): 0.05889943242073059\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.05844836235046387\n",
      "Training iteration: 2208\n",
      "Validation loss (no improvement): 0.05846463441848755\n",
      "Training iteration: 2209\n",
      "Validation loss (no improvement): 0.05822535753250122\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.058466780185699466\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.058724188804626466\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.05820867419242859\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.05811060070991516  to: 0.05810598134994507\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.058685201406478885\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.05871432423591614\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.05827654600143432\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.05810598134994507  to: 0.05797014236450195\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.05804163217544556\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.05847225785255432\n",
      "Training iteration: 2220\n",
      "Validation loss (no improvement): 0.059040027856826785\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): 0.05900000333786011\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.05848506689071655\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.05797773599624634\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.05797014236450195  to: 0.05779203176498413\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.05779203176498413  to: 0.05764531493186951\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.058210241794586184\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.05898572206497192\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.05863258242607117\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.057827317714691163\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.057756507396698\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.05780501365661621\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.05794416069984436\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.058376312255859375\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.0582869291305542\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): 0.0577982246875763\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): 0.05771748423576355\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.05773727893829346\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.05817270278930664\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.058533257246017455\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.0581570565700531\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.05794088244438171\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.05798349380493164\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.057956695556640625\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): 0.05809202194213867\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.05831292867660522\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.05793795585632324\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.05764531493186951  to: 0.05724424719810486\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.05724424719810486  to: 0.05681421160697937\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.05707365274429321\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): 0.057542169094085695\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): 0.057920044660568236\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.058027875423431394\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): 0.0577316164970398\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.057183265686035156\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.05743422508239746\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.05841470956802368\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.058747798204422\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.05798091292381287\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.05721307396888733\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.057083940505981444\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.05746127367019653\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.05777240991592407\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.058132731914520265\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.05807605981826782\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): 0.05808044672012329\n",
      "Training iteration: 2266\n",
      "Validation loss (no improvement): 0.05775930285453797\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): 0.05713905096054077\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.05760272741317749\n",
      "Training iteration: 2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.05765010714530945\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.05848132967948914\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.05853952765464783\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): 0.05782582759857178\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): 0.05704976916313172\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.05681421160697937  to: 0.05652462244033814\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): 0.0571239173412323\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): 0.05737133622169495\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): 0.05693461894989014\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.057694852352142334\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.058027487993240354\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.057928466796875\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.05758669972419739\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.05710510015487671\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): 0.056808799505233765\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): 0.057210326194763184\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): 0.05844153761863709\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): 0.05841898322105408\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): 0.05736057758331299\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.05701355934143067\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.05652462244033814  to: 0.056506043672561644\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): 0.05685056447982788\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.05671663880348206\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.05671801567077637\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): 0.05743039846420288\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): 0.05793282985687256\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): 0.057492798566818236\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): 0.05666312575340271\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.05661093592643738\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.05674562454223633\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): 0.05694453120231628\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.057629901170730594\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): 0.05783231854438782\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.05696329474449158\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.056506043672561644  to: 0.0559843897819519\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.0559843897819519  to: 0.055901634693145755\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.05660285353660584\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): 0.05761556029319763\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): 0.057389968633651735\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): 0.056564366817474364\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.055901634693145755  to: 0.05576952695846558\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): 0.05600849390029907\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): 0.0562130331993103\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.05609519481658935\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.056889313459396365\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.057034820318222046\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.056817591190338135\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.05614849328994751\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.05576952695846558  to: 0.05568538904190064\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.056152713298797605\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.05662161707878113\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): 0.05674370527267456\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.05622665286064148\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): 0.05616217851638794\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.05590367317199707\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.05621490478515625\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): 0.05661365985870361\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): 0.05666464567184448\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.05648956298828125\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.0563144862651825\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.055986320972442626\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.05568538904190064  to: 0.05509430766105652\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): 0.055154949426651\n",
      "Training iteration: 2332\n",
      "Validation loss (no improvement): 0.05593100786209106\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 0.05681528449058533\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): 0.05641295909881592\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): 0.055424457788467406\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): 0.055293208360671996\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): 0.055290162563323975\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.055743622779846194\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): 0.056625211238861085\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): 0.056970030069351196\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): 0.056329536437988284\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.05552583932876587\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.055101430416107176\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.05509430766105652  to: 0.05465575456619263\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.05502594709396362\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.0560416579246521\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.05607683658599853\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.055233466625213626\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.05465575456619263  to: 0.054474031925201415\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.054521262645721436\n",
      "Training iteration: 2351\n",
      "Improved validation loss from: 0.054474031925201415  to: 0.05430980920791626\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.05435092449188232\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): 0.055487406253814694\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.05618007779121399\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.05549073815345764\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.054575031995773314\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.05487132668495178\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.05441320538520813\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.054368406534194946\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): 0.05527490973472595\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.056010544300079346\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.055445021390914916\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.05430980920791626  to: 0.05417352914810181\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.05417352914810181  to: 0.05386863946914673\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.05430417656898499\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.055337166786193846\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.05572654008865356\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.055368489027023314\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): 0.05457823276519776\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.05457030534744263\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.054601925611495974\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.05456728935241699\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.05448293685913086\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.054182004928588864\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.05430096387863159\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.0541149914264679\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.05452531576156616\n",
      "Training iteration: 2378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.05433270931243896\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.053941857814788816\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.05386863946914673  to: 0.05340084433555603\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.05368232727050781\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.05476733446121216\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.055764639377593996\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.05524559617042542\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.054135262966156006\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.053676438331604\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.05406476259231567\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.05340084433555603  to: 0.053150475025177\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.05348268747329712\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.054780930280685425\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): 0.05484186410903931\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): 0.0536805272102356\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.053150475025177  to: 0.05294139981269837\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): 0.05342345237731934\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): 0.053055965900421144\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.05294139981269837  to: 0.05289154648780823\n",
      "Training iteration: 2397\n",
      "Validation loss (no improvement): 0.05356810688972473\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): 0.05488467216491699\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.05544187426567078\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.0550132155418396\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.0544553279876709\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.05387549996376038\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): 0.05357598066329956\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.05405530333518982\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.054870384931564334\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.05487877130508423\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): 0.05404579043388367\n",
      "Training iteration: 2408\n",
      "Validation loss (no improvement): 0.05324673652648926\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.05289154648780823  to: 0.0527921199798584\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.0527921199798584  to: 0.05267402529716492\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.053317928314208986\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.053659290075302124\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.0536231517791748\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.05288609266281128\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.05293444991111755\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: 0.05267402529716492  to: 0.052600085735321045\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.05309035181999207\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.05372706055641174\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.053450047969818115\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.053113728761672974\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.052600085735321045  to: 0.05241007804870605\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.05241007804870605  to: 0.052105510234832765\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.05268456339836121\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.053251230716705324\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.05274777412414551\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.052145540714263916\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.052105510234832765  to: 0.051709812879562375\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.05179901123046875\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.052281391620635984\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.05253320932388306\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.05329968333244324\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.05324295163154602\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.05283697247505188\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.05225778818130493\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.051936328411102295\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.051776325702667235\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.05230684876441956\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.052696245908737185\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.05239605903625488\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.051709812879562375  to: 0.05168336629867554\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.05168336629867554  to: 0.051486289501190184\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.051700246334075925\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.052064359188079834\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.05204542875289917\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.0517998218536377\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): 0.05199288129806519\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.05222301483154297\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.05237998962402344\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.052601009607315063\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.05271261930465698\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.052308350801467896\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): 0.05232328772544861\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.05268077254295349\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): 0.052161562442779544\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.051486289501190184  to: 0.05098045468330383\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.05098045468330383  to: 0.050733530521392824\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.05122560262680054\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.05171830654144287\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.05132585167884827\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.05080329775810242\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.050733530521392824  to: 0.05062878131866455\n",
      "Training iteration: 2462\n",
      "Improved validation loss from: 0.05062878131866455  to: 0.050501853227615356\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.050975024700164795\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.05192652940750122\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.0520933985710144\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.051358604431152345\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.05078443884849548\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.05065873861312866\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.050501853227615356  to: 0.050247478485107425\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.05078384280204773\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.050989854335784915\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.05099811553955078\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.05041548013687134\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): 0.05045969486236572\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.05117209553718567\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.05112435817718506\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.050350844860076904\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.050304090976715087\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.05082606077194214\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.05146916508674622\n",
      "Training iteration: 2481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.050905823707580566\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.050247478485107425  to: 0.04995892941951752\n",
      "Training iteration: 2483\n",
      "Validation loss (no improvement): 0.05000779628753662\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.04995892941951752  to: 0.04994568824768066\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.050320541858673094\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.05144814848899841\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.05124125480651855\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): 0.050304049253463747\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.050084376335144044\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.04994568824768066  to: 0.04987302720546723\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.05045265555381775\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.05114442706108093\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.050711429119110106\n",
      "Training iteration: 2494\n",
      "Improved validation loss from: 0.04987302720546723  to: 0.04965656399726868\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.04965656399726868  to: 0.04944941997528076\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.04984590411186218\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): 0.050469970703125\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): 0.05057936906814575\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.05071720480918884\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.050942516326904295\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.050590455532073975\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.050772345066070555\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.05058853626251221\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.0502855122089386\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.0495879739522934\n",
      "Training iteration: 2506\n",
      "Improved validation loss from: 0.04944941997528076  to: 0.04914845526218414\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.04959717392921448\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.05079302191734314\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.050757914781570435\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.04980306625366211\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.0496835857629776\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.0503156840801239\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.050570666790008545\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.05039948225021362\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.04969386160373688\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): 0.049249070882797244\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.04914845526218414  to: 0.049119243025779726\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.049708428978919986\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.049376565217971805\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.04915989339351654\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.049119243025779726  to: 0.048894685506820676\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.049336212873458865\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.049694308638572694\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.04965737462043762\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.049141731858253476\n",
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.04896579682826996\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): 0.0492436945438385\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.049687701463699344\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): 0.04991470277309418\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.049028795957565305\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.048894685506820676  to: 0.04878474175930023\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.04930324554443359\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.04965415894985199\n",
      "Training iteration: 2534\n",
      "Validation loss (no improvement): 0.049517661333084106\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.04910246729850769\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.04878474175930023  to: 0.048691684007644655\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.04905895292758942\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): 0.04951254427433014\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.04940753877162933\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.049096998572349546\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.048691684007644655  to: 0.048632994294166565\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.04897735118865967\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): 0.04968964457511902\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.0493901789188385\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: 0.048632994294166565  to: 0.04848683476448059\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.04848683476448059  to: 0.04833051562309265\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.04833051562309265  to: 0.047977256774902347\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.04836650788784027\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.04856148362159729\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.0480391263961792\n",
      "Training iteration: 2551\n",
      "Improved validation loss from: 0.047977256774902347  to: 0.04796293675899506\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.04857549071311951\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.04889039993286133\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.048835930228233335\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.04879961907863617\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.04813186228275299\n",
      "Training iteration: 2557\n",
      "Improved validation loss from: 0.04796293675899506  to: 0.04765680432319641\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: 0.04765680432319641  to: 0.04743452668190003\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.048160681128501893\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.04869810938835144\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.04910589158535004\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.04899543225765228\n",
      "Training iteration: 2563\n",
      "Validation loss (no improvement): 0.04881802499294281\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.04834546148777008\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.048261824250221255\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.048158034682273865\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.048127397894859314\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.04781952500343323\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.048182496428489686\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.04876483976840973\n",
      "Training iteration: 2571\n",
      "Validation loss (no improvement): 0.048147445917129515\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: 0.04743452668190003  to: 0.04729216694831848\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.047634029388427736\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.04766212105751037\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.04791940152645111\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.04900437891483307\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.04901436269283295\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.04836179614067078\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.047443097829818724\n",
      "Training iteration: 2580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.04729216694831848  to: 0.047167882323265076\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.04758756160736084\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.0477547824382782\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.04760354459285736\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.0472283661365509\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.047167882323265076  to: 0.04658507406711578\n",
      "Training iteration: 2586\n",
      "Improved validation loss from: 0.04658507406711578  to: 0.04641245007514953\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.04714175760746002\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.04809257984161377\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.04825800359249115\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.048075932264328006\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.04797050952911377\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.04697306752204895\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.046672219038009645\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): 0.04722529947757721\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): 0.0483278751373291\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.04809110164642334\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.04680213928222656\n",
      "Training iteration: 2598\n",
      "Improved validation loss from: 0.04641245007514953  to: 0.0463507741689682\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.04658549427986145\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.04748251438140869\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): 0.048007544875144956\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): 0.04786567091941833\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.04694594442844391\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): 0.046762028336524965\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.047248554229736325\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): 0.04641890525817871\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.04651699066162109\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.047618833184242246\n",
      "Training iteration: 2609\n",
      "Validation loss (no improvement): 0.04845630526542664\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): 0.04815018773078918\n",
      "Training iteration: 2611\n",
      "Validation loss (no improvement): 0.046944618225097656\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): 0.04654203057289123\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.04679201245307922\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): 0.04744805693626404\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.04801223278045654\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.047808870673179626\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.04725168347358703\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.04683238565921784\n",
      "Training iteration: 2619\n",
      "Improved validation loss from: 0.0463507741689682  to: 0.04625414311885834\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.046535664796829225\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.04682205319404602\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.04677821695804596\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.04625414311885834  to: 0.046241527795791625\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.04649074971675873\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.04702414572238922\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.04643692076206207\n",
      "Training iteration: 2627\n",
      "Improved validation loss from: 0.046241527795791625  to: 0.046012312173843384\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.04632090628147125\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.04633276462554932\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.046012312173843384  to: 0.045727604627609254\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.04576719701290131\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.04613194465637207\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.045903491973876956\n",
      "Training iteration: 2634\n",
      "Improved validation loss from: 0.045727604627609254  to: 0.04562244415283203\n",
      "Training iteration: 2635\n",
      "Improved validation loss from: 0.04562244415283203  to: 0.04502999186515808\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.04557076394557953\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.04573567807674408\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.04622464179992676\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.04654908776283264\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.04662483334541321\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.046203446388244626\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.04604242742061615\n",
      "Training iteration: 2643\n",
      "Validation loss (no improvement): 0.04652159214019776\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.0469083696603775\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.046459078788757324\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.04562823176383972\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): 0.04559423327445984\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.04622216820716858\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.046511679887771606\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.04731639325618744\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.047330302000045774\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.04640037417411804\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.04554189145565033\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.04546736776828766\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.046114292740821836\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.046675854921340944\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.04673438966274261\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.04583729803562164\n",
      "Training iteration: 2659\n",
      "Improved validation loss from: 0.04502999186515808  to: 0.044699454307556154\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.04483411908149719\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.04577214121818542\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.04541938900947571\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.04608987867832184\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.046244746446609496\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.04564592242240906\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.04630933403968811\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.04593661725521088\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.04569733738899231\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.045867925882339476\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.0459301620721817\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.04649997651576996\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): 0.04619522094726562\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): 0.0455629289150238\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.04599982798099518\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.04572869837284088\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.04595401883125305\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.04597658216953278\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.04526877403259277\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.04554259181022644\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.046550384163856505\n",
      "Training iteration: 2681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.046936559677124026\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.046386712789535524\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.04510559141635895\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.044699454307556154  to: 0.04463814795017242\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.04481683671474457\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.04546899795532226\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.04604739546775818\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.04571760594844818\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.04503285884857178\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.045244607329368594\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.0449704259634018\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.0450293630361557\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.045149344205856326\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.04539063572883606\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.0451684296131134\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.04502331614494324\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.04463814795017242  to: 0.04458325505256653\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.04481735825538635\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.045159950852394104\n",
      "Training iteration: 2700\n",
      "Improved validation loss from: 0.04458325505256653  to: 0.044576770067214964\n",
      "Training iteration: 2701\n",
      "Improved validation loss from: 0.044576770067214964  to: 0.04410502016544342\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.04436405599117279\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.04438463151454926\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.044861888885498045\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.04472907185554505\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.04454420506954193\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.04423573613166809\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.04460446834564209\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.044491586089134214\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.04439525008201599\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.04417139887809753\n",
      "Training iteration: 2712\n",
      "Improved validation loss from: 0.04410502016544342  to: 0.04401565492153168\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.04402879774570465\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.04401565492153168  to: 0.04385086596012115\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.044047707319259645\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.04436735212802887\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.044404792785644534\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.04507438540458679\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.04534450173377991\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.04501543939113617\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.04458571970462799\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): 0.044796648621559146\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): 0.044907492399215695\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.044700294733047485\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.04419875144958496\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.04390843510627747\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.044168314337730406\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): 0.04443215429782867\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.04466769695281982\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.044246286153793335\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.044159072637557986\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.04476277232170105\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.0447425901889801\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.044251665472984314\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.04385086596012115  to: 0.04382807612419128\n",
      "Training iteration: 2736\n",
      "Improved validation loss from: 0.04382807612419128  to: 0.0431846559047699\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.04323203563690185\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.043898454308509825\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.044586768746376036\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.04445446133613586\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.04404817521572113\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.04360150694847107\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.04393504559993744\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.04414847791194916\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.044188499450683594\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.043882593512535095\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.043605613708496097\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.04364208579063415\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.043470603227615354\n",
      "Training iteration: 2750\n",
      "Improved validation loss from: 0.0431846559047699  to: 0.04316523969173432\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.043559154868125914\n",
      "Training iteration: 2752\n",
      "Validation loss (no improvement): 0.04367838799953461\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.04316523969173432  to: 0.04312664568424225\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.04312664568424225  to: 0.04308463633060455\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.043605643510818484\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.04398962557315826\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.04380932748317719\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.043479880690574645\n",
      "Training iteration: 2759\n",
      "Improved validation loss from: 0.04308463633060455  to: 0.042835989594459535\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.04316989481449127\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.04325973391532898\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.04394770562648773\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.044315490126609805\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.04381337761878967\n",
      "Training iteration: 2765\n",
      "Improved validation loss from: 0.042835989594459535  to: 0.04278186857700348\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.043096524477005\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.043252366781234744\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.04318976402282715\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.043559932708740236\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): 0.0433399498462677\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.04322587847709656\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.04308741092681885\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.043461781740188596\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.04375756680965424\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.04416041374206543\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.043653401732444766\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.043755316734313966\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.04380491375923157\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.04365600645542145\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.043633061647415164\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.04332647323608398\n",
      "Training iteration: 2782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0437249094247818\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.04359138906002045\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.043599176406860354\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.04387694001197815\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.04373624324798584\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.04328381419181824\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.04286056160926819\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.04294114112854004\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.04367852210998535\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.04318578243255615\n",
      "Training iteration: 2792\n",
      "Improved validation loss from: 0.04278186857700348  to: 0.04239205718040466\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.04243965744972229\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.04249878525733948\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.04290954172611237\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.043021750450134275\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): 0.04317975044250488\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): 0.042648258805274966\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.04287742078304291\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.042780667543411255\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.04254972338676453\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.04298922419548035\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.04303105473518372\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.042706018686294554\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.042802834510803224\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.04245107173919678\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): 0.04250832498073578\n",
      "Training iteration: 2808\n",
      "Improved validation loss from: 0.04239205718040466  to: 0.04191790521144867\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.042129459977149966\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.04296249747276306\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.04279746115207672\n",
      "Training iteration: 2812\n",
      "Improved validation loss from: 0.04191790521144867  to: 0.04181398749351502\n",
      "Training iteration: 2813\n",
      "Improved validation loss from: 0.04181398749351502  to: 0.04167348742485046\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): 0.042488521337509154\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): 0.04305044710636139\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.0432073175907135\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.04309106469154358\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.0428438276052475\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.04221658706665039\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.0423815906047821\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.042902231216430664\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): 0.042321643233299254\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.04189671576023102\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.042222538590431215\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.043330717086791995\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.0433335155248642\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.042553442716598514\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.04187029004096985\n",
      "Training iteration: 2829\n",
      "Validation loss (no improvement): 0.04192075729370117\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.04184650480747223\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): 0.042296433448791505\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): 0.042470541596412656\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.04237362742424011\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.0419215202331543\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.04199906289577484\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.04235185980796814\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): 0.04240811467170715\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): 0.0419361412525177\n",
      "Training iteration: 2839\n",
      "Improved validation loss from: 0.04167348742485046  to: 0.041583937406539914\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.04193390905857086\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.04279223382472992\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.04230470061302185\n",
      "Training iteration: 2843\n",
      "Improved validation loss from: 0.041583937406539914  to: 0.04146861433982849\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.04158439636230469\n",
      "Training iteration: 2845\n",
      "Improved validation loss from: 0.04146861433982849  to: 0.04142413735389709\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.04185404181480408\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.041983413696289065\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.04173785746097565\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.04186230599880218\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.0414558082818985\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.04153857827186584\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.04202362895011902\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.041806396842002866\n",
      "Training iteration: 2854\n",
      "Validation loss (no improvement): 0.04176645278930664\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.04142413735389709  to: 0.04113103747367859\n",
      "Training iteration: 2856\n",
      "Improved validation loss from: 0.04113103747367859  to: 0.04112101197242737\n",
      "Training iteration: 2857\n",
      "Improved validation loss from: 0.04112101197242737  to: 0.04075618386268616\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): 0.0413156658411026\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.042756134271621705\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.04290131032466889\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): 0.04203318655490875\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): 0.0414398193359375\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.04161528944969177\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.041166433691978456\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.04179774224758148\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): 0.04183414578437805\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.0417806327342987\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.041103726625442503\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.04124861657619476\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): 0.042164430022239685\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): 0.041943758726119995\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.041213250160217284\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.04118915498256683\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.041327446699142456\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.041373211145401004\n",
      "Training iteration: 2876\n",
      "Validation loss (no improvement): 0.04091236591339111\n",
      "Training iteration: 2877\n",
      "Validation loss (no improvement): 0.04112076759338379\n",
      "Training iteration: 2878\n",
      "Validation loss (no improvement): 0.04110310077667236\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.040931296348571775\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.04107557237148285\n",
      "Training iteration: 2881\n",
      "Improved validation loss from: 0.04075618386268616  to: 0.0406975120306015\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.04104422926902771\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.04086982309818268\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.04112313389778137\n",
      "Training iteration: 2885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.04109221994876862\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.040921860933303834\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.04102231860160828\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): 0.04080381393432617\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.04122939705848694\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): 0.04103131294250488\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): 0.040983691811561584\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.04077713489532471\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.041054001450538634\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.04116918444633484\n",
      "Training iteration: 2895\n",
      "Improved validation loss from: 0.0406975120306015  to: 0.040667420625686644\n",
      "Training iteration: 2896\n",
      "Improved validation loss from: 0.040667420625686644  to: 0.040397587418556216\n",
      "Training iteration: 2897\n",
      "Improved validation loss from: 0.040397587418556216  to: 0.040312346816062924\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.04068913459777832\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.041052085161209104\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.04056290686130524\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.0404921680688858\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): 0.040347108244895936\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.040312346816062924  to: 0.04008418619632721\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.04053492546081543\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.04083307683467865\n",
      "Training iteration: 2906\n",
      "Validation loss (no improvement): 0.040391817688941956\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.04008418619632721  to: 0.03997337818145752\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.03997337818145752  to: 0.03914355933666229\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): 0.03932226598262787\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.040647190809249875\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.04098680913448334\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): 0.039963850378990175\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): 0.03917751014232636\n",
      "Training iteration: 2914\n",
      "Validation loss (no improvement): 0.03922012746334076\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): 0.039413100481033324\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.040217190980911255\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.040042394399642946\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.039988577365875244\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.04025111198425293\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): 0.04024699330329895\n",
      "Training iteration: 2921\n",
      "Validation loss (no improvement): 0.04030216634273529\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.040172427892684937\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.03984197080135345\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.03937103748321533\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.03945707380771637\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.04007435441017151\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.04046553671360016\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.04000162184238434\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.039331752061843875\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.03992690443992615\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.0399757444858551\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.0398862898349762\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.03966314196586609\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.039153876900672915\n",
      "Training iteration: 2935\n",
      "Improved validation loss from: 0.03914355933666229  to: 0.03873124718666077\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): 0.03904135525226593\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.03999939858913422\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.041648516058921815\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.04172776341438293\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.041443973779678345\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.04001211524009705\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.039157363772392276\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.040464550256729126\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.03925272822380066\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.03898253440856934\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.03999224603176117\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.039762872457504275\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.0416430413722992\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.04223768711090088\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.04151879847049713\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.04104113578796387\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.03974784016609192\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.03926683366298676\n",
      "Training iteration: 2954\n",
      "Improved validation loss from: 0.03873124718666077  to: 0.038572490215301514\n",
      "Training iteration: 2955\n",
      "Improved validation loss from: 0.038572490215301514  to: 0.037954181432724\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.03825618028640747\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.03953907787799835\n",
      "Training iteration: 2958\n",
      "Validation loss (no improvement): 0.039459827542304995\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.03802473247051239\n",
      "Training iteration: 2960\n",
      "Improved validation loss from: 0.037954181432724  to: 0.0376468688249588\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.03831217288970947\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.03935697674751282\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.04078556001186371\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): 0.04088407158851624\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.03934716284275055\n",
      "Training iteration: 2966\n",
      "Improved validation loss from: 0.0376468688249588  to: 0.037435227632522584\n",
      "Training iteration: 2967\n",
      "Improved validation loss from: 0.037435227632522584  to: 0.03627142310142517\n",
      "Training iteration: 2968\n",
      "Improved validation loss from: 0.03627142310142517  to: 0.03594607710838318\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.036619645357131955\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.038271921873092654\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.04024698138237\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.040997165441513064\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.04001637995243072\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): 0.038745191693305966\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.038091796636581424\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.03760106861591339\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.0385036051273346\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.038637498021125795\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.03785194158554077\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.03714537918567658\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.03691678047180176\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.03749385476112366\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.03912869989871979\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.04002535343170166\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.039123982191085815\n",
      "Training iteration: 2986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03746078610420227\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.036997812986373904\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.0372649759054184\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.037773546576499936\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.03816901743412018\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.037381240725517274\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.03688760399818421\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.036297541856765744\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.03657802939414978\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.038280326128005984\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.038329610228538515\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.03654761016368866\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.036003601551055905\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.03617836236953735\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.03709260821342468\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.03748762607574463\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.03660380244255066\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.03655027747154236\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.03677880167961121\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.03775604367256165\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.0372910350561142\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.03611336052417755\n",
      "Training iteration: 3008\n",
      "Improved validation loss from: 0.03594607710838318  to: 0.03518600463867187\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.035618957877159116\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.03687692284584045\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): 0.03643162846565247\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): 0.036216247081756595\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.036813920736312865\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): 0.03647322654724121\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): 0.03720800876617432\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.03751132488250732\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.03711184561252594\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.035802847146987914\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.03522571921348572\n",
      "Training iteration: 3020\n",
      "Improved validation loss from: 0.03518600463867187  to: 0.03462409973144531\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): 0.03561903834342957\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): 0.0373572826385498\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.038519033789634706\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.03726575374603271\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.036520805954933164\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.03529604375362396\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.035291582345962524\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.036812272667884824\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.03750998079776764\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.03887686729431152\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.03810777068138123\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.03634961247444153\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.036781424283981325\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.03544153273105621\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.03611612617969513\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.03618206381797791\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.03511385321617126\n",
      "Training iteration: 3038\n",
      "Improved validation loss from: 0.03462409973144531  to: 0.03447455167770386\n",
      "Training iteration: 3039\n",
      "Improved validation loss from: 0.03447455167770386  to: 0.03423615992069244\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.034685945510864256\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.03582601547241211\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.038154810667037964\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.037705710530281066\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.03580750823020935\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: 0.03423615992069244  to: 0.03352771401405334\n",
      "Training iteration: 3046\n",
      "Improved validation loss from: 0.03352771401405334  to: 0.033131268620491025\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.0340073436498642\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.034450370073318484\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.03570966124534607\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.03521483242511749\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.034546884894371035\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.03410603702068329\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.033338361978530885\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.03360847532749176\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.03459518253803253\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.0357823520898819\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.03660854995250702\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.03537904024124146\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): 0.03385220170021057\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.033833152055740355\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.03480188250541687\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.03560490012168884\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.03766916394233703\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.03760653138160706\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.03647486567497253\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): 0.03526382446289063\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): 0.0333004891872406\n",
      "Training iteration: 3068\n",
      "Improved validation loss from: 0.033131268620491025  to: 0.032836800813674925\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.03368266224861145\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.03586276173591614\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.037040242552757265\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.03603680729866028\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.03487760126590729\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.034422746300697325\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.03440687656402588\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.03522805571556091\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.03617145121097565\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.03719015717506409\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.03660408854484558\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.03498845100402832\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.034599405527114865\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.03413384556770325\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.03485211730003357\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.03579498827457428\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.03516081273555756\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.0345650851726532\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.03478596806526184\n",
      "Training iteration: 3088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03481626510620117\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.03401624858379364\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.03414700627326965\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.03483613133430481\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.03579993546009064\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.03516880869865417\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.03389274477958679\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.03321625590324402\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.03356100916862488\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.03516225516796112\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.036418408155441284\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.03511672616004944\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.03331224918365479\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.033106905221939084\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.03294919431209564\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.03432055115699768\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.03628199696540833\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.035565230250358584\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.03376550674438476\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.03330963551998138\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.033160734176635745\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.03411866426467895\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.03568128943443298\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.03481068015098572\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.03432709574699402\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.034121114015579226\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.034497231245040894\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.03609094023704529\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.03609771132469177\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.03426609933376312\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.03323708474636078\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.03357789218425751\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.035162228345870974\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.03556875288486481\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.034145528078079225\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.033413705229759214\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.03376310467720032\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.03499112725257873\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.03509507179260254\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.0346958190202713\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.034858503937721254\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.03369187116622925\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.03406822085380554\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.03386642634868622\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.034325534105300905\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.034211665391922\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.03413607180118561\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.03359038233757019\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.033783406019210815\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.03365355134010315\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.034025371074676514\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.03425669074058533\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.033488431572914125\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.03372339606285095\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.033852297067642215\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.03342607915401459\n",
      "Training iteration: 3144\n",
      "Improved validation loss from: 0.032836800813674925  to: 0.03281548619270325\n",
      "Training iteration: 3145\n",
      "Improved validation loss from: 0.03281548619270325  to: 0.0320672482252121\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.03226673007011414\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.03312881588935852\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.03381134569644928\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.033049005270004275\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.03305080235004425\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.033837854862213135\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.03367975652217865\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.032730066776275636\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.03259085714817047\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.03319461643695831\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.033812513947486876\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.0329633891582489\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.03273400664329529\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.03302692770957947\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.034156006574630735\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.033888009190559384\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.03259105086326599\n",
      "Training iteration: 3163\n",
      "Improved validation loss from: 0.0320672482252121  to: 0.03185439705848694\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.03195061683654785\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.033127936720848086\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.032987958192825316\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.031909826397895816\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.03200535774230957\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.03272396922111511\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.033763918280601504\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.033154231309890744\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.032096919417381284\n",
      "Training iteration: 3173\n",
      "Improved validation loss from: 0.03185439705848694  to: 0.030988305807113647\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.031032124161720277\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.032406100630760194\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.03387385308742523\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.033097770810127256\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.03141787052154541\n",
      "Training iteration: 3179\n",
      "Improved validation loss from: 0.030988305807113647  to: 0.030955290794372557\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.03147499561309815\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.032915666699409485\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.03422655165195465\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.03292872905731201\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.031333121657371524\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.03181284368038177\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.031004446744918823\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.03183873295783997\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.03377242386341095\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.032876390218734744\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.03136977553367615\n",
      "Training iteration: 3191\n",
      "Improved validation loss from: 0.030955290794372557  to: 0.03058677613735199\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.030889853835105896\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.03213649690151214\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.03429795503616333\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.03477235734462738\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.03341540098190308\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.031065922975540162\n",
      "Training iteration: 3198\n",
      "Improved validation loss from: 0.03058677613735199  to: 0.030516570806503295\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.030677929520606995\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.030832314491271974\n",
      "Training iteration: 3201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.032879704236984254\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.032258611917495725\n",
      "Training iteration: 3203\n",
      "Improved validation loss from: 0.030516570806503295  to: 0.030366122722625732\n",
      "Training iteration: 3204\n",
      "Improved validation loss from: 0.030366122722625732  to: 0.029165631532669066\n",
      "Training iteration: 3205\n",
      "Improved validation loss from: 0.029165631532669066  to: 0.029117390513420105\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.030274537205696107\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.03231734931468964\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.032171493768692015\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.03022363781929016\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.029539036750793456\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.029739096760749817\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.030875831842422485\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.03209637999534607\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.031058719754219054\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.029425162076950073\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.02913505434989929\n",
      "Training iteration: 3217\n",
      "Improved validation loss from: 0.029117390513420105  to: 0.029058876633644103\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.029794520139694212\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.031280338764190674\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.030654829740524293\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.030462926626205443\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.031161671876907347\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.031184965372085573\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.030514943599700927\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.030003446340560912\n",
      "Training iteration: 3226\n",
      "Improved validation loss from: 0.029058876633644103  to: 0.02903175950050354\n",
      "Training iteration: 3227\n",
      "Improved validation loss from: 0.02903175950050354  to: 0.028974875807762146\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.029080542922019958\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): 0.029815950989723207\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.02951258420944214\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.02947256863117218\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.029964104294776917\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.02953244149684906\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.02979215681552887\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.030015891790390013\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.029115757346153258\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.029210901260375975\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.029281195998191834\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.03009670376777649\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.02985113263130188\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.02996874749660492\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.029212352633476258\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.029396671056747436\n",
      "Training iteration: 3244\n",
      "Improved validation loss from: 0.028974875807762146  to: 0.028692847490310668\n",
      "Training iteration: 3245\n",
      "Improved validation loss from: 0.028692847490310668  to: 0.028378796577453614\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.028960955142974854\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.028962621092796327\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.028386521339416503\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.028745996952056884\n",
      "Training iteration: 3250\n",
      "Improved validation loss from: 0.028378796577453614  to: 0.027879461646080017\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.02860366702079773\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.02887294888496399\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.030018523335456848\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.029721438884735107\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.029161721467971802\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.027884823083877564\n",
      "Training iteration: 3257\n",
      "Improved validation loss from: 0.027879461646080017  to: 0.027593827247619628\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.028191226720809936\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.029439395666122435\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.02907324433326721\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.028623777627944946\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.027824729681015015\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.028108063340187072\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.02950558364391327\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.029152494668960572\n",
      "Training iteration: 3266\n",
      "Improved validation loss from: 0.027593827247619628  to: 0.02739286720752716\n",
      "Training iteration: 3267\n",
      "Improved validation loss from: 0.02739286720752716  to: 0.026846489310264586\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.027258506417274474\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.028298228979110718\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.02949807047843933\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.02925633192062378\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.02800845205783844\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.027237701416015624\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.027020978927612304\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.02792605459690094\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.02778436541557312\n",
      "Training iteration: 3277\n",
      "Improved validation loss from: 0.026846489310264586  to: 0.026499122381210327\n",
      "Training iteration: 3278\n",
      "Improved validation loss from: 0.026499122381210327  to: 0.025962907075881957\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.026163762807846068\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.02729958891868591\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.028352832794189452\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): 0.027680209279060362\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.026484888792037965\n",
      "Training iteration: 3284\n",
      "Improved validation loss from: 0.025962907075881957  to: 0.02572125494480133\n",
      "Training iteration: 3285\n",
      "Improved validation loss from: 0.02572125494480133  to: 0.02558407187461853\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.0261669397354126\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.026802214980125427\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.02610536217689514\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.026169484853744505\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.027057436108589173\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.027553096413612366\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.026366066932678223\n",
      "Training iteration: 3293\n",
      "Improved validation loss from: 0.02558407187461853  to: 0.02526976466178894\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.025384807586669923\n",
      "Training iteration: 3295\n",
      "Improved validation loss from: 0.02526976466178894  to: 0.02490139901638031\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.025192707777023315\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.026124945282936095\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.027493947744369508\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.026606088876724242\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.025583454966545106\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.025484460592269897\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.025781136751174927\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.026345905661582947\n",
      "Training iteration: 3304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.026048102974891664\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.025461500883102416\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.02497941702604294\n",
      "Training iteration: 3307\n",
      "Improved validation loss from: 0.02490139901638031  to: 0.024855795502662658\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.02564775347709656\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.027594584226608276\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.027707251906394958\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.025834837555885316\n",
      "Training iteration: 3312\n",
      "Improved validation loss from: 0.024855795502662658  to: 0.02456302344799042\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.02484738826751709\n",
      "Training iteration: 3314\n",
      "Improved validation loss from: 0.02456302344799042  to: 0.024149489402770997\n",
      "Training iteration: 3315\n",
      "Improved validation loss from: 0.024149489402770997  to: 0.023749470710754395\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.02494693100452423\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.025154432654380797\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.02507200837135315\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.02475479543209076\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.024873144924640656\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.025042003393173216\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.024720919132232667\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.024146147072315216\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.024126653373241425\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.024369868636131286\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.02499815970659256\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.025284337997436523\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.025232562422752382\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.02484159767627716\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.025057101249694826\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.025330975651741028\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.024767808616161346\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.024731478095054625\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.025275331735610963\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.024512800574302673\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.02390970289707184\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.023963551223278045\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.023890864849090577\n",
      "Training iteration: 3339\n",
      "Improved validation loss from: 0.023749470710754395  to: 0.02374156713485718\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.023810894787311555\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.02402283698320389\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.02505840063095093\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.02539820075035095\n",
      "Training iteration: 3344\n",
      "Improved validation loss from: 0.02374156713485718  to: 0.023663893342018127\n",
      "Training iteration: 3345\n",
      "Improved validation loss from: 0.023663893342018127  to: 0.023051810264587403\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.02320225238800049\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.0237216517329216\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.023846769332885744\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.02396596670150757\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.024508571624755858\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.02476060837507248\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.02428073137998581\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.024111182987689973\n",
      "Training iteration: 3354\n",
      "Improved validation loss from: 0.023051810264587403  to: 0.023024335503578186\n",
      "Training iteration: 3355\n",
      "Improved validation loss from: 0.023024335503578186  to: 0.0227769136428833\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.02371007651090622\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.023405751585960387\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.023088626563549042\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.02366809844970703\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.024993796646595002\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.02486008107662201\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.023727062344551086\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.023376643657684326\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.02327282875776291\n",
      "Training iteration: 3365\n",
      "Validation loss (no improvement): 0.023384885489940645\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.02409796416759491\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.023714032769203187\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.023039047420024873\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.023244528472423552\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.023157474398612977\n",
      "Training iteration: 3371\n",
      "Improved validation loss from: 0.0227769136428833  to: 0.022488899528980255\n",
      "Training iteration: 3372\n",
      "Improved validation loss from: 0.022488899528980255  to: 0.021415571868419647\n",
      "Training iteration: 3373\n",
      "Validation loss (no improvement): 0.0215855211019516\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.0222372367978096\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.022111043334007263\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.02192533016204834\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.02254256308078766\n",
      "Training iteration: 3378\n",
      "Validation loss (no improvement): 0.022427888214588167\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.022850051522254944\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.023714356124401093\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.02355211228132248\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.022888147830963136\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.02302359342575073\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.02330240309238434\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): 0.02349807471036911\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.023628857731819154\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.023350103199481963\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): 0.02394600659608841\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.02271222323179245\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.0222158282995224\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.022103993594646452\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.023118117451667787\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.023522663116455077\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.022733113169670104\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.02214531898498535\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.023006387054920197\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.02308361977338791\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.023450088500976563\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.022617089748382568\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.022706952691078187\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): 0.02227012664079666\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.021631550788879395\n",
      "Training iteration: 3403\n",
      "Improved validation loss from: 0.021415571868419647  to: 0.021377408504486085\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.02245049923658371\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.022857442498207092\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.021815328299999236\n",
      "Training iteration: 3407\n",
      "Improved validation loss from: 0.021377408504486085  to: 0.021111936867237092\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.021142983436584474\n",
      "Training iteration: 3409\n",
      "Improved validation loss from: 0.021111936867237092  to: 0.021015577018260956\n",
      "Training iteration: 3410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0210430309176445\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.021346493065357207\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.02192879617214203\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.022485527396202087\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.022343799471855164\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.021590356528759003\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.021283218264579774\n",
      "Training iteration: 3417\n",
      "Improved validation loss from: 0.021015577018260956  to: 0.020340855419635772\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.020476667582988738\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.020686575770378114\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.021899740397930145\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): 0.02192823439836502\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): 0.02177930325269699\n",
      "Training iteration: 3423\n",
      "Validation loss (no improvement): 0.021759907901287078\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): 0.021902294456958772\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.021914657950401307\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.021363143622875214\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.02161722183227539\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.020886142551898957\n",
      "Training iteration: 3429\n",
      "Improved validation loss from: 0.020340855419635772  to: 0.020075114071369173\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.020402376353740693\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.021897728741168975\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.021844594180583952\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.020366498827934267\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.020075571537017823\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.020242126286029817\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.020494981110095976\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.020834827423095705\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.02048414498567581\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.020891399681568147\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.020222583413124086\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.020384566485881807\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.020801654458045958\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.02168363630771637\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.020590277016162874\n",
      "Training iteration: 3445\n",
      "Improved validation loss from: 0.020075114071369173  to: 0.0199459046125412\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.02000281512737274\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.021248511970043182\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.020716074109077453\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): 0.020934739708900453\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.021409325301647186\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.0215388685464859\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.020410116016864776\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.0204444482922554\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.02081250846385956\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.02159784585237503\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.02101767361164093\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.020398247241973876\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.020996932685375214\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.020238932967185975\n",
      "Training iteration: 3460\n",
      "Improved validation loss from: 0.0199459046125412  to: 0.019068047404289246\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.019155852496623993\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.020418572425842284\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.02044021785259247\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.0192038431763649\n",
      "Training iteration: 3465\n",
      "Improved validation loss from: 0.019068047404289246  to: 0.018699018657207488\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.01892617642879486\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.02018103897571564\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.020775887370109557\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.019935745000839233\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.02004181146621704\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): 0.020988932251930235\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.020952849090099333\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.020061755180358888\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.020122294127941132\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.019675450026988985\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.019693125784397126\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.019626022875308992\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.02003433257341385\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.020027060806751252\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.018976525962352754\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.01904182732105255\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.019345192611217497\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.019519442319869997\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.019741563498973845\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.019627320766448974\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.019946031272411346\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.019172534346580505\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.01931006759405136\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): 0.019683603942394257\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.019036325812339782\n",
      "Training iteration: 3491\n",
      "Improved validation loss from: 0.018699018657207488  to: 0.018263521790504455\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): 0.018649016320705415\n",
      "Training iteration: 3493\n",
      "Validation loss (no improvement): 0.02008655071258545\n",
      "Training iteration: 3494\n",
      "Validation loss (no improvement): 0.020763087272644042\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.02087291032075882\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.020492203533649445\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): 0.019845160841941833\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.01867445409297943\n",
      "Training iteration: 3499\n",
      "Improved validation loss from: 0.018263521790504455  to: 0.0182038813829422\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): 0.018561942875385283\n",
      "Training iteration: 3501\n",
      "Validation loss (no improvement): 0.018864865601062774\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): 0.018930158019065856\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.01939208209514618\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.019762659072875978\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.019752587378025054\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): 0.01986592262983322\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): 0.019323694705963134\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.01887601912021637\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.020153161883354188\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.02091427743434906\n",
      "Training iteration: 3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.021016886830329894\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.021073870360851288\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.020782768726348877\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.02033555805683136\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.019226184487342833\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.019043585658073424\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.019665682315826417\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.019488659501075745\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.019235940277576448\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.019517961144447326\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.01911530941724777\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.01937533766031265\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): 0.01950405091047287\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): 0.018779221177101135\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.018861833214759826\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.019947361946105958\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.020399954915046693\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.018955107033252715\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.018712234497070313\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.01965401917695999\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.020113015174865724\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.01902584880590439\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.018482348322868346\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.020104889571666718\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.018844904005527498\n",
      "Training iteration: 3536\n",
      "Improved validation loss from: 0.0182038813829422  to: 0.01793014109134674\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.018267321586608886\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): 0.019190898537635802\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.018745410442352294\n",
      "Training iteration: 3540\n",
      "Improved validation loss from: 0.01793014109134674  to: 0.017619314789772033\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.017622821033000946\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.018515172600746154\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.01993558406829834\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.019219975173473357\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.01793084591627121\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.018291480839252472\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): 0.018921999633312224\n",
      "Training iteration: 3548\n",
      "Improved validation loss from: 0.017619314789772033  to: 0.01702943295240402\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.01710183173418045\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.018920114636421202\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.019518092274665833\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.017802348732948302\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.017353516817092896\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.018051263689994813\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.01911085844039917\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.01887010335922241\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.018380990624427794\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): 0.019157245755195618\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): 0.017680779099464417\n",
      "Training iteration: 3560\n",
      "Improved validation loss from: 0.01702943295240402  to: 0.016840313374996186\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.017550425231456758\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.01837826520204544\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.01742345988750458\n",
      "Training iteration: 3564\n",
      "Improved validation loss from: 0.016840313374996186  to: 0.01678209602832794\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): 0.017722086608409883\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): 0.01709195673465729\n",
      "Training iteration: 3567\n",
      "Improved validation loss from: 0.01678209602832794  to: 0.016408967971801757\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.01673329770565033\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.01725175082683563\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.016829301416873933\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): 0.016923703253269196\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.017191784083843233\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.017715467512607573\n",
      "Training iteration: 3574\n",
      "Validation loss (no improvement): 0.016930583119392394\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.016408967971801757  to: 0.016318507492542267\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): 0.016797669231891632\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.017702394723892213\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.01679006665945053\n",
      "Training iteration: 3579\n",
      "Improved validation loss from: 0.016318507492542267  to: 0.016076131165027617\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): 0.01633092314004898\n",
      "Training iteration: 3581\n",
      "Improved validation loss from: 0.016076131165027617  to: 0.016045670211315154\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.016790729761123658\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.016623689234256743\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.01665332019329071\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.016948828101158143\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.016952794790267945\n",
      "Training iteration: 3587\n",
      "Improved validation loss from: 0.016045670211315154  to: 0.015271766483783722\n",
      "Training iteration: 3588\n",
      "Improved validation loss from: 0.015271766483783722  to: 0.015233969688415528\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.01654101312160492\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.01658760607242584\n",
      "Training iteration: 3591\n",
      "Validation loss (no improvement): 0.01612684428691864\n",
      "Training iteration: 3592\n",
      "Validation loss (no improvement): 0.01655551642179489\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.017787139117717742\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.017166602611541747\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.01680290400981903\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.016299457848072053\n",
      "Training iteration: 3597\n",
      "Validation loss (no improvement): 0.01592247188091278\n",
      "Training iteration: 3598\n",
      "Validation loss (no improvement): 0.015863579511642457\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.01633494645357132\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.015882356464862822\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.016294178366661072\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.01620630770921707\n",
      "Training iteration: 3603\n",
      "Improved validation loss from: 0.015233969688415528  to: 0.014732161164283752\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.01544857770204544\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.01791985332965851\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.01628725975751877\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.015535669028759002\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.01565379649400711\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.01804840415716171\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.017522110044956206\n",
      "Training iteration: 3611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015142013132572175\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.015303106606006622\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.01667104661464691\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.01921701431274414\n",
      "Training iteration: 3615\n",
      "Validation loss (no improvement): 0.017394344508647918\n",
      "Training iteration: 3616\n",
      "Validation loss (no improvement): 0.01614566147327423\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.016027794778347017\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.018046489357948302\n",
      "Training iteration: 3619\n",
      "Validation loss (no improvement): 0.019664816558361053\n",
      "Training iteration: 3620\n",
      "Validation loss (no improvement): 0.01606802046298981\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.016446323692798616\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.01642354726791382\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.018189457058906556\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.01830871105194092\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.015639007091522217\n",
      "Training iteration: 3626\n",
      "Validation loss (no improvement): 0.015004360675811767\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.015389540791511535\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.017225196957588194\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.017888674139976503\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.016423574090003966\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.015446814894676208\n",
      "Training iteration: 3632\n",
      "Validation loss (no improvement): 0.015410891175270081\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.016062600910663603\n",
      "Training iteration: 3634\n",
      "Validation loss (no improvement): 0.01594580113887787\n",
      "Training iteration: 3635\n",
      "Improved validation loss from: 0.014732161164283752  to: 0.014606425166130066\n",
      "Training iteration: 3636\n",
      "Improved validation loss from: 0.014606425166130066  to: 0.014367322623729705\n",
      "Training iteration: 3637\n",
      "Validation loss (no improvement): 0.015093624591827393\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.014949432015419007\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.014463035762310028\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.014491842687129974\n",
      "Training iteration: 3641\n",
      "Validation loss (no improvement): 0.015395936369895936\n",
      "Training iteration: 3642\n",
      "Validation loss (no improvement): 0.015565887093544006\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.015204133093357086\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.015560635924339294\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.015346565842628479\n",
      "Training iteration: 3646\n",
      "Validation loss (no improvement): 0.014407578110694885\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.014515456557273865\n",
      "Training iteration: 3648\n",
      "Validation loss (no improvement): 0.014445479214191436\n",
      "Training iteration: 3649\n",
      "Improved validation loss from: 0.014367322623729705  to: 0.014228297770023346\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.014502699673175811\n",
      "Training iteration: 3651\n",
      "Validation loss (no improvement): 0.015135245025157928\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.015469835698604583\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.014751510322093963\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.014418159425258637\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.014295932650566102\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.014419053494930268\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.014243140816688538\n",
      "Training iteration: 3658\n",
      "Improved validation loss from: 0.014228297770023346  to: 0.01412566602230072\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.014227241277694702\n",
      "Training iteration: 3660\n",
      "Improved validation loss from: 0.01412566602230072  to: 0.014086459577083588\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.01432129591703415\n",
      "Training iteration: 3662\n",
      "Validation loss (no improvement): 0.014141044020652771\n",
      "Training iteration: 3663\n",
      "Improved validation loss from: 0.014086459577083588  to: 0.013134579360485076\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.01324724406003952\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.014102454483509063\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.014042070508003235\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.013828961551189423\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.014365467429161071\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.0161455437541008\n",
      "Training iteration: 3670\n",
      "Validation loss (no improvement): 0.014658115804195404\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.01372455656528473\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.01421954482793808\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.016240723431110382\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.014398983120918274\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.013303945958614349\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.013784801959991455\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.016243986785411835\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.01745820790529251\n",
      "Training iteration: 3679\n",
      "Validation loss (no improvement): 0.01466677337884903\n",
      "Training iteration: 3680\n",
      "Validation loss (no improvement): 0.0142354354262352\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.01483454704284668\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.016370004415512084\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.016206923127174377\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.015662924945354463\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.01552804708480835\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.016144795715808867\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.01486874520778656\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.013803717494010926\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.014220431447029114\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.01422169953584671\n",
      "Training iteration: 3691\n",
      "Improved validation loss from: 0.013134579360485076  to: 0.012727849185466766\n",
      "Training iteration: 3692\n",
      "Improved validation loss from: 0.012727849185466766  to: 0.012334676086902618\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.013623137772083283\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.014052633941173554\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.013843818008899689\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.013757091760635377\n",
      "Training iteration: 3697\n",
      "Validation loss (no improvement): 0.013816404342651366\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.013997606933116913\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.014783194661140442\n",
      "Training iteration: 3700\n",
      "Validation loss (no improvement): 0.014517292380332947\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.013846300542354584\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.014207637310028077\n",
      "Training iteration: 3703\n",
      "Validation loss (no improvement): 0.013883586227893829\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.014376649260520935\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.014355418086051942\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.013607697188854217\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.014318494498729706\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.014277343451976777\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.014194247126579285\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.014073804020881653\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.014142954349517822\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.013112838566303252\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.013243253529071807\n",
      "Training iteration: 3714\n",
      "Validation loss (no improvement): 0.014170531928539277\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.012532277405261994\n",
      "Training iteration: 3716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.012805439531803131\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.014398951828479768\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.013141734898090363\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.013151511549949646\n",
      "Training iteration: 3720\n",
      "Validation loss (no improvement): 0.014390358328819275\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.01332523375749588\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.013218870759010315\n",
      "Training iteration: 3723\n",
      "Validation loss (no improvement): 0.013548792898654937\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.013099689781665803\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.013593295216560363\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.013606004416942596\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.012532302737236023\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.013534899055957793\n",
      "Training iteration: 3729\n",
      "Validation loss (no improvement): 0.013298773765563964\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.012931379675865173\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.014290130138397217\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.013203854858875274\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.012897036969661713\n",
      "Training iteration: 3734\n",
      "Validation loss (no improvement): 0.014198720455169678\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.013164223730564117\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.012684619426727295\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.014121954143047333\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.013092558085918426\n",
      "Training iteration: 3739\n",
      "Validation loss (no improvement): 0.013429370522499085\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.014210352301597595\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.013040745258331298\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.013497176766395568\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.014943651854991913\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.012748494744300842\n",
      "Training iteration: 3745\n",
      "Improved validation loss from: 0.012334676086902618  to: 0.012260975688695908\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.013888731598854065\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.013065554201602936\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.012284207344055175\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.013092753291130067\n",
      "Training iteration: 3750\n",
      "Validation loss (no improvement): 0.013257011771202087\n",
      "Training iteration: 3751\n",
      "Validation loss (no improvement): 0.01300085186958313\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.01385270357131958\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.013346509635448455\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.013088086247444152\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.012828229367733002\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.01410638988018036\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.013253739476203919\n",
      "Training iteration: 3758\n",
      "Validation loss (no improvement): 0.012435822188854218\n",
      "Training iteration: 3759\n",
      "Validation loss (no improvement): 0.012276092916727066\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.013802018761634827\n",
      "Training iteration: 3761\n",
      "Validation loss (no improvement): 0.012596063315868378\n",
      "Training iteration: 3762\n",
      "Validation loss (no improvement): 0.013137222826480865\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.013982997834682464\n",
      "Training iteration: 3764\n",
      "Improved validation loss from: 0.012260975688695908  to: 0.012161626666784286\n",
      "Training iteration: 3765\n",
      "Validation loss (no improvement): 0.012771131098270416\n",
      "Training iteration: 3766\n",
      "Validation loss (no improvement): 0.014282232522964478\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.012858188152313233\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.012517708539962768\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.013477668166160583\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.013653086125850677\n",
      "Training iteration: 3771\n",
      "Validation loss (no improvement): 0.013274717330932616\n",
      "Training iteration: 3772\n",
      "Validation loss (no improvement): 0.013046829402446747\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.012888017296791076\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.01248818039894104\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.013192489743232727\n",
      "Training iteration: 3776\n",
      "Validation loss (no improvement): 0.012620799243450165\n",
      "Training iteration: 3777\n",
      "Validation loss (no improvement): 0.012988176941871644\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.013226442039012909\n",
      "Training iteration: 3779\n",
      "Improved validation loss from: 0.012161626666784286  to: 0.01193486899137497\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.012847886979579925\n",
      "Training iteration: 3781\n",
      "Validation loss (no improvement): 0.012668666243553162\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.012654712796211243\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.012906581163406372\n",
      "Training iteration: 3784\n",
      "Validation loss (no improvement): 0.01261463463306427\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.01290132999420166\n",
      "Training iteration: 3786\n",
      "Validation loss (no improvement): 0.013636532425880431\n",
      "Training iteration: 3787\n",
      "Improved validation loss from: 0.01193486899137497  to: 0.011609311401844024\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.012679803371429443\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.013948997855186463\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.012210621684789657\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.01249150037765503\n",
      "Training iteration: 3792\n",
      "Validation loss (no improvement): 0.01434236764907837\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.012941281497478484\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.01203724518418312\n",
      "Training iteration: 3795\n",
      "Validation loss (no improvement): 0.013103196024894714\n",
      "Training iteration: 3796\n",
      "Validation loss (no improvement): 0.01282014548778534\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.012114965915679931\n",
      "Training iteration: 3798\n",
      "Validation loss (no improvement): 0.012836052477359772\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.013341504335403442\n",
      "Training iteration: 3800\n",
      "Validation loss (no improvement): 0.013321638107299805\n",
      "Training iteration: 3801\n",
      "Validation loss (no improvement): 0.012561753392219543\n",
      "Training iteration: 3802\n",
      "Validation loss (no improvement): 0.012758111953735352\n",
      "Training iteration: 3803\n",
      "Validation loss (no improvement): 0.01515994518995285\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.01451520025730133\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.013352084159851074\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.012306139618158341\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.012154097855091094\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.012969450652599334\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.012639255821704864\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.012172539532184602\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.013090448081493377\n",
      "Training iteration: 3812\n",
      "Validation loss (no improvement): 0.012364605814218521\n",
      "Training iteration: 3813\n",
      "Validation loss (no improvement): 0.012678305804729461\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.013562867045402527\n",
      "Training iteration: 3815\n",
      "Validation loss (no improvement): 0.013041362166404724\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.012972292304039002\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.01285226047039032\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.011779878288507462\n",
      "Training iteration: 3819\n",
      "Validation loss (no improvement): 0.012438166141510009\n",
      "Training iteration: 3820\n",
      "Validation loss (no improvement): 0.013439366221427917\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.011620674282312393\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.012000572681427003\n",
      "Training iteration: 3823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.012423869222402573\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.013526776432991027\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.01310606747865677\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.012479984760284423\n",
      "Training iteration: 3827\n",
      "Validation loss (no improvement): 0.012805633246898651\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.013073745369911193\n",
      "Training iteration: 3829\n",
      "Validation loss (no improvement): 0.012346035242080689\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.013160377740859985\n",
      "Training iteration: 3831\n",
      "Validation loss (no improvement): 0.012733116745948792\n",
      "Training iteration: 3832\n",
      "Improved validation loss from: 0.011609311401844024  to: 0.011223609745502471\n",
      "Training iteration: 3833\n",
      "Validation loss (no improvement): 0.012822799384593964\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.01328585296869278\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.012196367979049683\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.01265956461429596\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.011706922948360444\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.012360341846942902\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.012314581871032714\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.0121485136449337\n",
      "Training iteration: 3841\n",
      "Improved validation loss from: 0.011223609745502471  to: 0.0109380804002285\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.012117370218038558\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.013739290833473205\n",
      "Training iteration: 3844\n",
      "Improved validation loss from: 0.0109380804002285  to: 0.010839402675628662\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.01112740784883499\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.01339929699897766\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.012119922786951065\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.012459252029657364\n",
      "Training iteration: 3849\n",
      "Validation loss (no improvement): 0.014321941137313842\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.012512490153312683\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.011278774589300156\n",
      "Training iteration: 3852\n",
      "Validation loss (no improvement): 0.013071425259113312\n",
      "Training iteration: 3853\n",
      "Validation loss (no improvement): 0.012801374495029449\n",
      "Training iteration: 3854\n",
      "Validation loss (no improvement): 0.011585847288370133\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.013232365250587463\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.014445145428180695\n",
      "Training iteration: 3857\n",
      "Validation loss (no improvement): 0.013452987372875213\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.011543543636798858\n",
      "Training iteration: 3859\n",
      "Validation loss (no improvement): 0.013049748539924622\n",
      "Training iteration: 3860\n",
      "Validation loss (no improvement): 0.015087535977363587\n",
      "Training iteration: 3861\n",
      "Validation loss (no improvement): 0.01497320830821991\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.01334422528743744\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.011651398986577988\n",
      "Training iteration: 3864\n",
      "Validation loss (no improvement): 0.01215483769774437\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.013221895694732666\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.012374714761972428\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.011689834296703339\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.012610633671283723\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.01298350840806961\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.012303884327411651\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.013362398743629456\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.013285191357135772\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.01179284080862999\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.012517476081848144\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.012431831657886505\n",
      "Training iteration: 3876\n",
      "Validation loss (no improvement): 0.011360611766576767\n",
      "Training iteration: 3877\n",
      "Validation loss (no improvement): 0.012104413658380508\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.012797912955284119\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.01211598888039589\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.01101774200797081\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.01253412663936615\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.014244559407234191\n",
      "Training iteration: 3883\n",
      "Validation loss (no improvement): 0.012142455577850342\n",
      "Training iteration: 3884\n",
      "Validation loss (no improvement): 0.01102650910615921\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.012614777684211731\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.013025470077991486\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.011377296596765517\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.012226678431034088\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.013642039895057679\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.014612594246864319\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.011456196010112763\n",
      "Training iteration: 3892\n",
      "Validation loss (no improvement): 0.01158483251929283\n",
      "Training iteration: 3893\n",
      "Validation loss (no improvement): 0.014522083103656769\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.012838058173656464\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.011675743758678437\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.012014110386371613\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.013921710848808288\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.012941539287567139\n",
      "Training iteration: 3899\n",
      "Improved validation loss from: 0.010839402675628662  to: 0.010623462498188019\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.011589989811182023\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.015233874320983887\n",
      "Training iteration: 3902\n",
      "Validation loss (no improvement): 0.015776917338371277\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.011764933168888093\n",
      "Training iteration: 3904\n",
      "Validation loss (no improvement): 0.011090125888586044\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.01242026910185814\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.014519624412059784\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.013194163143634797\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.012112919986248017\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.012895385921001434\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.014198830723762513\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.012461750209331513\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.011336489021778107\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.013245847821235657\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.014611992239952087\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.01300351619720459\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.011033773422241211\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.011541350185871125\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.013328474760055543\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.011850563436746597\n",
      "Training iteration: 3920\n",
      "Validation loss (no improvement): 0.011165324598550797\n",
      "Training iteration: 3921\n",
      "Validation loss (no improvement): 0.01182275041937828\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.013522990047931671\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.012334997951984405\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.010848578065633774\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.01188710480928421\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.014207956194877625\n",
      "Training iteration: 3927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01503770649433136\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.011894466727972031\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.01106337085366249\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.012820400297641754\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.013380280137062073\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.01108175963163376\n",
      "Training iteration: 3933\n",
      "Validation loss (no improvement): 0.011331621557474136\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.013747745752334594\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.012054884433746338\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.01128273457288742\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.012455277144908905\n",
      "Training iteration: 3938\n",
      "Validation loss (no improvement): 0.011662448942661285\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.01218704953789711\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.01082921475172043\n",
      "Training iteration: 3941\n",
      "Validation loss (no improvement): 0.012188176065683365\n",
      "Training iteration: 3942\n",
      "Validation loss (no improvement): 0.011736978590488435\n",
      "Training iteration: 3943\n",
      "Improved validation loss from: 0.010623462498188019  to: 0.01010705679655075\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.011848652362823486\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.013790412247180939\n",
      "Training iteration: 3946\n",
      "Validation loss (no improvement): 0.011592700332403182\n",
      "Training iteration: 3947\n",
      "Validation loss (no improvement): 0.01140914112329483\n",
      "Training iteration: 3948\n",
      "Validation loss (no improvement): 0.01349242627620697\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.012033138424158096\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.011509253084659577\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.012992483377456666\n",
      "Training iteration: 3952\n",
      "Validation loss (no improvement): 0.01074334755539894\n",
      "Training iteration: 3953\n",
      "Validation loss (no improvement): 0.011532758176326752\n",
      "Training iteration: 3954\n",
      "Validation loss (no improvement): 0.012534210085868835\n",
      "Training iteration: 3955\n",
      "Validation loss (no improvement): 0.010802338272333145\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.01132703796029091\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.013338075578212738\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.010148920863866807\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.010936667770147323\n",
      "Training iteration: 3960\n",
      "Validation loss (no improvement): 0.016038787364959717\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.012971155345439911\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.010742752254009247\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.011890296638011933\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.01399528533220291\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.011811108887195587\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.011697876453399658\n",
      "Training iteration: 3967\n",
      "Validation loss (no improvement): 0.01386459618806839\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.013487455248832703\n",
      "Training iteration: 3969\n",
      "Validation loss (no improvement): 0.010622593015432358\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.012565401196479798\n",
      "Training iteration: 3971\n",
      "Validation loss (no improvement): 0.014870655536651612\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.01471184492111206\n",
      "Training iteration: 3973\n",
      "Validation loss (no improvement): 0.012523174285888672\n",
      "Training iteration: 3974\n",
      "Validation loss (no improvement): 0.010521328449249268\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.011525607109069825\n",
      "Training iteration: 3976\n",
      "Validation loss (no improvement): 0.012904702126979828\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.012606345117092133\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.01169806271791458\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.012434549629688263\n",
      "Training iteration: 3980\n",
      "Validation loss (no improvement): 0.013098430633544923\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.011345674842596054\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.012225042283535003\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.013307145237922669\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.01222778782248497\n",
      "Training iteration: 3985\n",
      "Validation loss (no improvement): 0.010753442347049714\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.012329463660717011\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.012849342823028565\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.011135368049144745\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.01220858320593834\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.013483241200447083\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.011068884283304214\n",
      "Training iteration: 3992\n",
      "Validation loss (no improvement): 0.011590931564569473\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.01357187032699585\n",
      "Training iteration: 3994\n",
      "Validation loss (no improvement): 0.011760188639163971\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.010710705816745759\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.01309424191713333\n",
      "Training iteration: 3997\n",
      "Validation loss (no improvement): 0.010704479366540908\n",
      "Training iteration: 3998\n",
      "Validation loss (no improvement): 0.011982852220535278\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.013406181335449218\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_2 = toy_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_2.train_model()\n",
    "ensemble_model_2.model_inference()\n",
    "\n",
    "ensemble_mean_2 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_2 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_2 = np.sqrt(np.exp(ensemble_logvar_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 35.180999755859375\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 35.180999755859375  to: 24.882426452636718\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 24.882426452636718  to: 17.835128784179688\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 17.835128784179688  to: 13.084822082519532\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 13.084822082519532  to: 9.80785675048828\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 9.80785675048828  to: 7.466618347167969\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 7.466618347167969  to: 5.770670700073242\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 5.770670700073242  to: 4.525684356689453\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 4.525684356689453  to: 3.600556564331055\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 3.600556564331055  to: 2.904195785522461\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 2.904195785522461  to: 2.3738723754882813\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 2.3738723754882813  to: 1.9652734756469727\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.9652734756469727  to: 1.647134017944336\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 1.647134017944336  to: 1.3969204902648926\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 1.3969204902648926  to: 1.1982518196105958\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 1.1982518196105958  to: 1.0390594482421875\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 1.0390594482421875  to: 0.9103941917419434\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.9103941917419434  to: 0.8055253982543945\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.8055253982543945  to: 0.7195168972015381\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.7195168972015381  to: 0.6484350681304931\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.6484350681304931  to: 0.5892617225646972\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.5892617225646972  to: 0.5396076679229737\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.5396076679229737  to: 0.4977253437042236\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.4977253437042236  to: 0.4621490478515625\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.4621490478515625  to: 0.43176956176757814\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.43176956176757814  to: 0.4056055545806885\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.4056055545806885  to: 0.38297333717346194\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.38297333717346194  to: 0.36327261924743653\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.36327261924743653  to: 0.34604663848876954\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.34604663848876954  to: 0.33091857433319094\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.33091857433319094  to: 0.31757340431213377\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.31757340431213377  to: 0.3057518720626831\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.3057518720626831  to: 0.2952388048171997\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.2952388048171997  to: 0.28585362434387207\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.28585362434387207  to: 0.27744503021240235\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.27744503021240235  to: 0.2698850631713867\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.2698850631713867  to: 0.26306512355804446\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.26306512355804446  to: 0.25689215660095216\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.25689215660095216  to: 0.25128698348999023\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.25128698348999023  to: 0.24618191719055177\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.24618191719055177  to: 0.2415172576904297\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.2415172576904297  to: 0.23724110126495362\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.23724110126495362  to: 0.2333094596862793\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.2333094596862793  to: 0.2296825885772705\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.2296825885772705  to: 0.22632966041564942\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.22632966041564942  to: 0.2232222080230713\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.2232222080230713  to: 0.22033467292785644\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.22033467292785644  to: 0.21764421463012695\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.21764421463012695  to: 0.2151310920715332\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.2151310920715332  to: 0.2127784013748169\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.2127784013748169  to: 0.2105705738067627\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.2105705738067627  to: 0.20849380493164063\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.20849380493164063  to: 0.2065361499786377\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.2065361499786377  to: 0.2046868085861206\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.2046868085861206  to: 0.20293617248535156\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.20293617248535156  to: 0.20127577781677247\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.20127577781677247  to: 0.1996978521347046\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1996978521347046  to: 0.19819538593292235\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.19819538593292235  to: 0.19676214456558228\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.19676214456558228  to: 0.195392644405365\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.195392644405365  to: 0.19408191442489625\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.19408191442489625  to: 0.19282573461532593\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.19282573461532593  to: 0.191619873046875\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.191619873046875  to: 0.19046056270599365\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.19046056270599365  to: 0.1893445372581482\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.1893445372581482  to: 0.18826782703399658\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.18826782703399658  to: 0.18722785711288453\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.18722785711288453  to: 0.1862229585647583\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.1862229585647583  to: 0.18525089025497438\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.18525089025497438  to: 0.1843096375465393\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.1843096375465393  to: 0.1833972692489624\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.1833972692489624  to: 0.1825121521949768\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.1825121521949768  to: 0.18165271282196044\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.18165271282196044  to: 0.18081754446029663\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.18081754446029663  to: 0.18000538349151612\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.18000538349151612  to: 0.17921504974365235\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.17921504974365235  to: 0.1784454584121704\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.1784454584121704  to: 0.17769550085067748\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.17769550085067748  to: 0.1769644021987915\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.1769644021987915  to: 0.176251220703125\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.176251220703125  to: 0.17555520534515381\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.17555520534515381  to: 0.17487573623657227\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.17487573623657227  to: 0.17421261072158814\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.17421261072158814  to: 0.1735645890235901\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.1735645890235901  to: 0.17293113470077515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 85\n",
      "Improved validation loss from: 0.17293113470077515  to: 0.17231171131134032\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.17231171131134032  to: 0.17170579433441163\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.17170579433441163  to: 0.17111295461654663\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.17111295461654663  to: 0.17053276300430298\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.17053276300430298  to: 0.1699647545814514\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1699647545814514  to: 0.16940858364105224\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.16940858364105224  to: 0.16886390447616578\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.16886390447616578  to: 0.16833035945892333\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.16833035945892333  to: 0.16780760288238525\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.16780760288238525  to: 0.16729532480239867\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.16729532480239867  to: 0.16679327487945556\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.16679327487945556  to: 0.16630109548568725\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.16630109548568725  to: 0.16581863164901733\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.16581863164901733  to: 0.1653455376625061\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.1653455376625061  to: 0.1648816704750061\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.1648816704750061  to: 0.16442668437957764\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.16442668437957764  to: 0.1639804482460022\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.1639804482460022  to: 0.1635427474975586\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.1635427474975586  to: 0.16311333179473878\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.16311333179473878  to: 0.16269214153289796\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.16269214153289796  to: 0.1622789740562439\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.1622789740562439  to: 0.16187366247177123\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.16187366247177123  to: 0.16147588491439818\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.16147588491439818  to: 0.16108548641204834\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.16108548641204834  to: 0.16070210933685303\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.16070210933685303  to: 0.16032575368881224\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.16032575368881224  to: 0.15995639562606812\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.15995639562606812  to: 0.15959389209747316\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.15959389209747316  to: 0.15923808813095092\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.15923808813095092  to: 0.158888840675354\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.158888840675354  to: 0.15854603052139282\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.15854603052139282  to: 0.158208966255188\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.158208966255188  to: 0.1578779935836792\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1578779935836792  to: 0.15755302906036378\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.15755302906036378  to: 0.15723394155502318\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.15723394155502318  to: 0.15692065954208373\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.15692065954208373  to: 0.15661296844482422\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.15661296844482422  to: 0.15631078481674193\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.15631078481674193  to: 0.15601418018341065\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.15601418018341065  to: 0.1557229995727539\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.1557229995727539  to: 0.15543727874755858\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.15543727874755858  to: 0.15515687465667724\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.15515687465667724  to: 0.15488178730010987\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.15488178730010987  to: 0.15461199283599852\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.15461199283599852  to: 0.15434737205505372\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.15434737205505372  to: 0.15408774614334106\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.15408774614334106  to: 0.15383297204971313\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.15383297204971313  to: 0.15358324050903321\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.15358324050903321  to: 0.15333836078643798\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.15333836078643798  to: 0.15309818983078002\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.15309818983078002  to: 0.1528625726699829\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.1528625726699829  to: 0.15263149738311768\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.15263149738311768  to: 0.1524048328399658\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1524048328399658  to: 0.15218240022659302\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.15218240022659302  to: 0.15196411609649657\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.15196411609649657  to: 0.151749849319458\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.151749849319458  to: 0.1515394926071167\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1515394926071167  to: 0.15133291482925415\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.15133291482925415  to: 0.1511300563812256\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.1511300563812256  to: 0.15093082189559937\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.15093082189559937  to: 0.15073509216308595\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.15073509216308595  to: 0.1505427598953247\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1505427598953247  to: 0.15035380125045777\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.15035380125045777  to: 0.15016807317733766\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.15016807317733766  to: 0.14998553991317748\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14998553991317748  to: 0.14980623722076417\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14980623722076417  to: 0.14963009357452392\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14963009357452392  to: 0.1494571328163147\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.1494571328163147  to: 0.1492872953414917\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.1492872953414917  to: 0.1491204619407654\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.1491204619407654  to: 0.148956561088562\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.148956561088562  to: 0.148795485496521\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.148795485496521  to: 0.14863722324371337\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14863722324371337  to: 0.1484816312789917\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.1484816312789917  to: 0.14832881689071656\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14832881689071656  to: 0.14817867279052735\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14817867279052735  to: 0.14803112745285035\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14803112745285035  to: 0.14788612127304077\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14788612127304077  to: 0.14774357080459594\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14774357080459594  to: 0.1476034164428711\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1476034164428711  to: 0.14746558666229248\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14746558666229248  to: 0.14733003377914428\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.14733003377914428  to: 0.1471966862678528\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1471966862678528  to: 0.14706552028656006\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.14706552028656006  to: 0.14693645238876343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14693645238876343  to: 0.14680944681167601\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14680944681167601  to: 0.14668445587158202\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14668445587158202  to: 0.14656143188476561\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14656143188476561  to: 0.14644033908843995\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14644033908843995  to: 0.14632112979888917\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14632112979888917  to: 0.14620373249053956\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14620373249053956  to: 0.1460881471633911\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.1460881471633911  to: 0.14597434997558595\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.14597434997558595  to: 0.14586225748062134\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14586225748062134  to: 0.14575185775756835\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14575185775756835  to: 0.14564311504364014\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14564311504364014  to: 0.1455359935760498\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.1455359935760498  to: 0.14543046951293945\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14543046951293945  to: 0.14532649517059326\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14532649517059326  to: 0.14522404670715333\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14522404670715333  to: 0.14512310028076172\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.14512310028076172  to: 0.1450236201286316\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.1450236201286316  to: 0.14492559432983398\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.14492559432983398  to: 0.14482895135879517\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.14482895135879517  to: 0.144733726978302\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.144733726978302  to: 0.1446398377418518\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.1446398377418518  to: 0.14454706907272338\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.14454706907272338  to: 0.14445550441741944\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.14445550441741944  to: 0.1443652629852295\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.1443652629852295  to: 0.14427634477615356\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.14427634477615356  to: 0.1441887378692627\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.1441887378692627  to: 0.14410240650177003\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14410240650177003  to: 0.14401733875274658\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.14401733875274658  to: 0.1439334750175476\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1439334750175476  to: 0.1438508152961731\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.1438508152961731  to: 0.14376933574676515\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.14376933574676515  to: 0.14368898868560792\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14368898868560792  to: 0.14360973834991456\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.14360973834991456  to: 0.14353159666061402\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.14353159666061402  to: 0.14345451593399047\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.14345451593399047  to: 0.14337847232818604\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.14337847232818604  to: 0.1433034658432007\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.1433034658432007  to: 0.14322946071624756\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.14322946071624756  to: 0.14315640926361084\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.14315640926361084  to: 0.1430843472480774\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.1430843472480774  to: 0.1430133104324341\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.1430133104324341  to: 0.14294331073760985\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.14294331073760985  to: 0.14287426471710205\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.14287426471710205  to: 0.14280622005462645\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.14280622005462645  to: 0.14273906946182252\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.14273906946182252  to: 0.14267282485961913\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.14267282485961913  to: 0.14260746240615846\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.14260746240615846  to: 0.14254295825958252\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.14254295825958252  to: 0.1424793004989624\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1424793004989624  to: 0.14241645336151124\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.14241645336151124  to: 0.142354416847229\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.142354416847229  to: 0.14229313135147095\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.14229313135147095  to: 0.14223262071609497\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.14223262071609497  to: 0.14217283725738525\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.14217283725738525  to: 0.1421138048171997\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.1421138048171997  to: 0.14205548763275147\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.14205548763275147  to: 0.14199784994125367\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.14199784994125367  to: 0.14194087982177733\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.14194087982177733  to: 0.14188460111618043\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.14188460111618043  to: 0.14182895421981812\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.14182895421981812  to: 0.14177393913269043\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.14177393913269043  to: 0.14171965122222902\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.14171965122222902  to: 0.1416660189628601\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.1416660189628601  to: 0.14161309003829955\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.14161309003829955  to: 0.14156079292297363\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.14156079292297363  to: 0.14150911569595337\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.14150911569595337  to: 0.14145805835723876\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.14145805835723876  to: 0.141407573223114\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.141407573223114  to: 0.141357684135437\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.141357684135437  to: 0.14130834341049195\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.14130834341049195  to: 0.1412595510482788\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.1412595510482788  to: 0.14121129512786865\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.14121129512786865  to: 0.14116355180740356\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.14116355180740356  to: 0.14111629724502564\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.14111629724502564  to: 0.14106953144073486\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.14106953144073486  to: 0.14102327823638916\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.14102327823638916  to: 0.14097745418548585\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.14097745418548585  to: 0.14093211889266968\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.14093211889266968  to: 0.14088722467422485\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.14088722467422485  to: 0.1408427596092224\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.1408427596092224  to: 0.1407987356185913\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.1407987356185913  to: 0.14075511693954468\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.14075511693954468  to: 0.14071191549301149\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.14071191549301149  to: 0.14066911935806276\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.14066911935806276  to: 0.14062671661376952\n",
      "Training iteration: 255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14062671661376952  to: 0.14058465957641603\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.14058465957641603  to: 0.1405429482460022\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1405429482460022  to: 0.14050160646438598\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.14050160646438598  to: 0.14046061038970947\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.14046061038970947  to: 0.14041999578475953\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.14041999578475953  to: 0.1403797149658203\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.1403797149658203  to: 0.14033979177474976\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.14033979177474976  to: 0.14030020236968993\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.14030020236968993  to: 0.14026092290878295\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.14026092290878295  to: 0.1402219772338867\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.1402219772338867  to: 0.14018336534500123\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.14018336534500123  to: 0.14014503955841065\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.14014503955841065  to: 0.1401070237159729\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.1401070237159729  to: 0.140069317817688\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.140069317817688  to: 0.14003188610076905\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.14003188610076905  to: 0.1399947762489319\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.1399947762489319  to: 0.1399579167366028\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1399579167366028  to: 0.1399213433265686\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.1399213433265686  to: 0.1398850440979004\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.1398850440979004  to: 0.13984901905059816\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13984901905059816  to: 0.13981324434280396\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13981324434280396  to: 0.1397777318954468\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.1397777318954468  to: 0.13974246978759766\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13974246978759766  to: 0.13970744609832764\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.13970744609832764  to: 0.13967269659042358\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13967269659042358  to: 0.13963817358016967\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13963817358016967  to: 0.13960386514663697\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13960386514663697  to: 0.1395698070526123\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.1395698070526123  to: 0.1395359754562378\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.1395359754562378  to: 0.13950235843658448\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13950235843658448  to: 0.13946895599365233\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13946895599365233  to: 0.1394357919692993\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1394357919692993  to: 0.13940280675888062\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13940280675888062  to: 0.13937003612518312\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13937003612518312  to: 0.13933746814727782\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13933746814727782  to: 0.13930510282516478\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13930510282516478  to: 0.13927295207977294\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13927295207977294  to: 0.13924095630645753\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13924095630645753  to: 0.13920917510986328\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13920917510986328  to: 0.13917757272720338\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13917757272720338  to: 0.13914613723754882\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13914613723754882  to: 0.1391149044036865\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.1391149044036865  to: 0.1390838384628296\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1390838384628296  to: 0.13905292749404907\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13905292749404907  to: 0.13902219533920288\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13902219533920288  to: 0.1389916181564331\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.1389916181564331  to: 0.13896123170852662\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13896123170852662  to: 0.13893097639083862\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13893097639083862  to: 0.13890091180801392\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13890091180801392  to: 0.1388709783554077\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.1388709783554077  to: 0.13884122371673585\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13884122371673585  to: 0.1388116240501404\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.1388116240501404  to: 0.13878217935562134\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13878217935562134  to: 0.1387528657913208\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.1387528657913208  to: 0.13872371912002562\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13872371912002562  to: 0.13869471549987794\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13869471549987794  to: 0.13866584300994872\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13866584300994872  to: 0.13863708972930908\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13863708972930908  to: 0.13860849142074586\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13860849142074586  to: 0.13858002424240112\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13858002424240112  to: 0.1385516881942749\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.1385516881942749  to: 0.13852345943450928\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13852345943450928  to: 0.1384953737258911\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.1384953737258911  to: 0.13846739530563354\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13846739530563354  to: 0.13843953609466553\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13843953609466553  to: 0.13841181993484497\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13841181993484497  to: 0.1383841872215271\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1383841872215271  to: 0.1383566975593567\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.1383566975593567  to: 0.13832929134368896\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13832929134368896  to: 0.13830201625823973\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13830201625823973  to: 0.13827483654022216\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13827483654022216  to: 0.13824775218963622\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13824775218963622  to: 0.1382207989692688\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.1382207989692688  to: 0.1381939172744751\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1381939172744751  to: 0.138167142868042\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.138167142868042  to: 0.13814046382904052\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13814046382904052  to: 0.1381138563156128\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.1381138563156128  to: 0.13808730840682984\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13808730840682984  to: 0.13806087970733644\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13806087970733644  to: 0.13803451061248778\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13803451061248778  to: 0.1380082607269287\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.1380082607269287  to: 0.13798208236694337\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13798208236694337  to: 0.1379559874534607\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.1379559874534607  to: 0.1379299759864807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 339\n",
      "Improved validation loss from: 0.1379299759864807  to: 0.13790405988693238\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13790405988693238  to: 0.13787821531295777\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13787821531295777  to: 0.13785245418548583\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13785245418548583  to: 0.13782676458358764\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13782676458358764  to: 0.13780115842819213\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13780115842819213  to: 0.13777563571929932\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13777563571929932  to: 0.13775016069412233\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13775016069412233  to: 0.13772478103637695\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13772478103637695  to: 0.13769948482513428\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13769948482513428  to: 0.13767426013946532\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13767426013946532  to: 0.1376490831375122\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.1376490831375122  to: 0.13762397766113282\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13762397766113282  to: 0.1375989556312561\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1375989556312561  to: 0.1375739812850952\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.1375739812850952  to: 0.13754907846450806\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13754907846450806  to: 0.13752423524856566\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13752423524856566  to: 0.13749940395355226\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.13749940395355226  to: 0.1374746084213257\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.1374746084213257  to: 0.13744988441467285\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13744988441467285  to: 0.13742520809173583\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13742520809173583  to: 0.13740060329437256\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.13740060329437256  to: 0.1373760461807251\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1373760461807251  to: 0.13735154867172242\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13735154867172242  to: 0.1373271107673645\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.1373271107673645  to: 0.13730270862579347\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13730270862579347  to: 0.1372783899307251\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.1372783899307251  to: 0.13725409507751465\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13725409507751465  to: 0.13722987174987794\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13722987174987794  to: 0.13720568418502807\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13720568418502807  to: 0.137181556224823\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.137181556224823  to: 0.13715746402740478\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13715746402740478  to: 0.1371334433555603\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1371334433555603  to: 0.13710944652557372\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.13710944652557372  to: 0.13708553314208985\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13708553314208985  to: 0.13706164360046386\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13706164360046386  to: 0.1370378017425537\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.1370378017425537  to: 0.13701399564743041\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13701399564743041  to: 0.1369902491569519\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1369902491569519  to: 0.13696653842926027\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13696653842926027  to: 0.13694287538528443\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13694287538528443  to: 0.13691924810409545\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13691924810409545  to: 0.13689566850662233\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.13689566850662233  to: 0.13687212467193605\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13687212467193605  to: 0.13684862852096558\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13684862852096558  to: 0.13682515621185304\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13682515621185304  to: 0.13680174350738525\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.13680174350738525  to: 0.1367783546447754\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.1367783546447754  to: 0.13675501346588134\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13675501346588134  to: 0.13673179149627684\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13673179149627684  to: 0.13670871257781983\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.13670871257781983  to: 0.13668572902679443\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.13668572902679443  to: 0.13666285276412965\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.13666285276412965  to: 0.13664008378982545\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.13664008378982545  to: 0.1366173505783081\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.1366173505783081  to: 0.13659465312957764\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13659465312957764  to: 0.136572003364563\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.136572003364563  to: 0.1365493655204773\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.1365493655204773  to: 0.13652679920196534\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13652679920196534  to: 0.1365042209625244\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1365042209625244  to: 0.13648170232772827\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.13648170232772827  to: 0.13645920753479004\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.13645920753479004  to: 0.13643676042556763\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13643676042556763  to: 0.13641433715820311\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13641433715820311  to: 0.13639192581176757\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.13639192581176757  to: 0.1363695740699768\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.1363695740699768  to: 0.13634722232818602\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13634722232818602  to: 0.1363249182701111\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.1363249182701111  to: 0.1363026261329651\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.1363026261329651  to: 0.13628036975860597\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13628036975860597  to: 0.13625814914703369\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.13625814914703369  to: 0.13623594045639037\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13623594045639037  to: 0.13621375560760499\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13621375560760499  to: 0.1361916184425354\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1361916184425354  to: 0.1361694812774658\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.1361694812774658  to: 0.1361473798751831\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1361473798751831  to: 0.1361253023147583\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1361253023147583  to: 0.13610326051712035\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13610326051712035  to: 0.13608121871948242\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13608121871948242  to: 0.1360592246055603\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.1360592246055603  to: 0.13603724241256715\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.13603724241256715  to: 0.13601528406143187\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13601528406143187  to: 0.13599334955215453\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.13599334955215453  to: 0.13597143888473512\n",
      "Training iteration: 422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13597143888473512  to: 0.13594954013824462\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13594954013824462  to: 0.13592767715454102\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.13592767715454102  to: 0.13590583801269532\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13590583801269532  to: 0.13588402271270753\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13588402271270753  to: 0.13586221933364867\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13586221933364867  to: 0.13584043979644775\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.13584043979644775  to: 0.1358186960220337\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1358186960220337  to: 0.13579695224761962\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.13579695224761962  to: 0.13577525615692138\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.13577525615692138  to: 0.1357535719871521\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.1357535719871521  to: 0.13573188781738282\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.13573188781738282  to: 0.1357102394104004\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.1357102394104004  to: 0.13568862676620483\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13568862676620483  to: 0.13566701412200927\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13566701412200927  to: 0.13564543724060057\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13564543724060057  to: 0.13562387228012085\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13562387228012085  to: 0.13560233116149903\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13560233116149903  to: 0.1355808138847351\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.1355808138847351  to: 0.1355593204498291\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1355593204498291  to: 0.135537850856781\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.135537850856781  to: 0.13551638126373292\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13551638126373292  to: 0.13549495935440065\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.13549495935440065  to: 0.13547353744506835\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.13547353744506835  to: 0.13545213937759398\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13545213937759398  to: 0.1354307770729065\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.1354307770729065  to: 0.13540942668914796\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.13540942668914796  to: 0.13538811206817628\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13538811206817628  to: 0.13536678552627562\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13536678552627562  to: 0.13534551858901978\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.13534551858901978  to: 0.135324227809906\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.135324227809906  to: 0.13530298471450805\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.13530298471450805  to: 0.13528177738189698\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13528177738189698  to: 0.13526055812835694\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.13526055812835694  to: 0.13523937463760377\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13523937463760377  to: 0.1352182149887085\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.1352182149887085  to: 0.1351970911026001\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.1351970911026001  to: 0.13517595529556276\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13517595529556276  to: 0.1351548671722412\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1351548671722412  to: 0.13513377904891968\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.13513377904891968  to: 0.13511273860931397\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13511273860931397  to: 0.1350916862487793\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1350916862487793  to: 0.13507068157196045\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13507068157196045  to: 0.13504968881607055\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13504968881607055  to: 0.13502873182296754\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13502873182296754  to: 0.13500778675079345\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13500778675079345  to: 0.13498685359954835\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.13498685359954835  to: 0.13496594429016112\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.13496594429016112  to: 0.13494505882263183\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13494505882263183  to: 0.13492419719696044\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13492419719696044  to: 0.13490335941314696\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.13490335941314696  to: 0.1348825454711914\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.1348825454711914  to: 0.13486175537109374\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.13486175537109374  to: 0.13484097719192506\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.13484097719192506  to: 0.1348202109336853\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.1348202109336853  to: 0.13479948043823242\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.13479948043823242  to: 0.1347787857055664\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1347787857055664  to: 0.13475810289382933\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13475810289382933  to: 0.13473743200302124\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13473743200302124  to: 0.13471678495407105\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.13471678495407105  to: 0.1346961736679077\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.1346961736679077  to: 0.13467557430267335\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.13467557430267335  to: 0.13465502262115478\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.13465502262115478  to: 0.13463451862335205\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13463451862335205  to: 0.13461406230926515\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.13461406230926515  to: 0.13459365367889403\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.13459365367889403  to: 0.13457326889038085\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13457326889038085  to: 0.1345529317855835\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.1345529317855835  to: 0.13453264236450196\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.13453264236450196  to: 0.13451237678527833\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.13451237678527833  to: 0.13449214696884154\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.13449214696884154  to: 0.13447192907333375\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.13447192907333375  to: 0.13445175886154176\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13445175886154176  to: 0.13443162441253662\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13443162441253662  to: 0.13441150188446044\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.13441150188446044  to: 0.13439143896102906\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.13439143896102906  to: 0.1343713879585266\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.1343713879585266  to: 0.13435136079788207\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.13435136079788207  to: 0.13433135747909547\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.13433135747909547  to: 0.13431140184402465\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.13431140184402465  to: 0.13429144620895386\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13429144620895386  to: 0.13427152633666992\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.13427152633666992  to: 0.1342516541481018\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.1342516541481018  to: 0.13423179388046264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 505\n",
      "Improved validation loss from: 0.13423179388046264  to: 0.13421194553375243\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.13421194553375243  to: 0.134192156791687\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.134192156791687  to: 0.13417236804962157\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.13417236804962157  to: 0.134152615070343\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.134152615070343  to: 0.13413289785385132\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.13413289785385132  to: 0.13411319255828857\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.13411319255828857  to: 0.1340935230255127\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.1340935230255127  to: 0.13407386541366578\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.13407386541366578  to: 0.13405425548553468\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.13405425548553468  to: 0.13403465747833251\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.13403465747833251  to: 0.13401508331298828\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.13401508331298828  to: 0.13399553298950195\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.13399553298950195  to: 0.13397598266601562\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.13397598266601562  to: 0.1339564561843872\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.1339564561843872  to: 0.1339369535446167\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.1339369535446167  to: 0.133917498588562\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.133917498588562  to: 0.13389803171157838\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13389803171157838  to: 0.1338786244392395\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.1338786244392395  to: 0.1338592290878296\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.1338592290878296  to: 0.13383986949920654\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.13383986949920654  to: 0.1338205575942993\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.1338205575942993  to: 0.13380129337310792\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.13380129337310792  to: 0.13378201723098754\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.13378201723098754  to: 0.13376280069351196\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.13376280069351196  to: 0.13374359607696534\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.13374359607696534  to: 0.13372442722320557\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.13372442722320557  to: 0.13370527029037477\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13370527029037477  to: 0.1336861491203308\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.1336861491203308  to: 0.13366706371307374\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13366706371307374  to: 0.13364799022674562\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.13364799022674562  to: 0.1336289405822754\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1336289405822754  to: 0.13360996246337892\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.13360996246337892  to: 0.13359103202819825\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.13359103202819825  to: 0.13357217311859132\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.13357217311859132  to: 0.1335533380508423\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.1335533380508423  to: 0.13353456258773805\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13353456258773805  to: 0.1335158348083496\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.1335158348083496  to: 0.13349720239639282\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13349720239639282  to: 0.13347861766815186\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.13347861766815186  to: 0.13346010446548462\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.13346010446548462  to: 0.13344166278839112\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13344166278839112  to: 0.13342326879501343\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.13342326879501343  to: 0.13340494632720948\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.13340494632720948  to: 0.1333866834640503\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.1333866834640503  to: 0.13336846828460694\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.13336846828460694  to: 0.13335030078887938\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13335030078887938  to: 0.13333218097686766\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.13333218097686766  to: 0.13331410884857178\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13331410884857178  to: 0.1332960844039917\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.1332960844039917  to: 0.13327810764312745\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.13327810764312745  to: 0.133260178565979\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.133260178565979  to: 0.13324226140975953\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.13324226140975953  to: 0.13322440385818482\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.13322440385818482  to: 0.13320658206939698\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.13320658206939698  to: 0.133188796043396\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.133188796043396  to: 0.13317105770111085\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.13317105770111085  to: 0.13315335512161255\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13315335512161255  to: 0.13313567638397217\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.13313567638397217  to: 0.13311804533004762\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.13311804533004762  to: 0.133100426197052\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.133100426197052  to: 0.13308286666870117\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.13308286666870117  to: 0.1330653190612793\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.1330653190612793  to: 0.1330478310585022\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.1330478310585022  to: 0.13303035497665405\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.13303035497665405  to: 0.13301292657852173\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.13301292657852173  to: 0.13299552202224732\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13299552202224732  to: 0.13297815322875978\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.13297815322875978  to: 0.13296080827713014\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.13296080827713014  to: 0.13294349908828734\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.13294349908828734  to: 0.1329262375831604\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.1329262375831604  to: 0.1329089879989624\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.1329089879989624  to: 0.1328919291496277\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.1328919291496277  to: 0.13287506103515626\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.13287506103515626  to: 0.13285835981369018\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.13285835981369018  to: 0.13284181356430053\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.13284181356430053  to: 0.1328253984451294\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.1328253984451294  to: 0.1328091263771057\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.1328091263771057  to: 0.13279297351837158\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.13279297351837158  to: 0.13277695178985596\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.13277695178985596  to: 0.13276100158691406\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.13276100158691406  to: 0.1327451705932617\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.1327451705932617  to: 0.13272942304611207\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13272942304611207  to: 0.13271375894546508\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.13271375894546508  to: 0.13269816637039183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 589\n",
      "Improved validation loss from: 0.13269816637039183  to: 0.13268264532089233\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.13268264532089233  to: 0.13266719579696656\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.13266719579696656  to: 0.1326518177986145\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.1326518177986145  to: 0.13263648748397827\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.13263648748397827  to: 0.1326212167739868\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.1326212167739868  to: 0.13260600566864014\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.13260600566864014  to: 0.1325908422470093\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.1325908422470093  to: 0.13257571458816528\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.13257571458816528  to: 0.13256064653396607\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.13256064653396607  to: 0.13254562616348267\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.13254562616348267  to: 0.1325306177139282\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.1325306177139282  to: 0.13251569271087646\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.13251569271087646  to: 0.1325007677078247\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.1325007677078247  to: 0.13248589038848876\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.13248589038848876  to: 0.13247106075286866\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.13247106075286866  to: 0.1324562430381775\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.1324562430381775  to: 0.13244147300720216\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.13244147300720216  to: 0.1324267268180847\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.1324267268180847  to: 0.13241200447082518\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.13241200447082518  to: 0.1323973298072815\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.1323973298072815  to: 0.13238266706466675\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.13238266706466675  to: 0.1323680281639099\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.1323680281639099  to: 0.13235342502593994\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.13235342502593994  to: 0.13233884572982788\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.13233884572982788  to: 0.13232429027557374\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.13232429027557374  to: 0.1323097825050354\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.1323097825050354  to: 0.13229526281356813\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.13229526281356813  to: 0.13228079080581664\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.13228079080581664  to: 0.13226635456085206\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.13226635456085206  to: 0.13225193023681642\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.13225193023681642  to: 0.13223754167556762\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.13223754167556762  to: 0.1322231650352478\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.1322231650352478  to: 0.13220884799957275\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.13220884799957275  to: 0.13219454288482665\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.13219454288482665  to: 0.13218027353286743\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.13218027353286743  to: 0.1321660280227661\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.1321660280227661  to: 0.1321518301963806\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.1321518301963806  to: 0.13213763236999512\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.13213763236999512  to: 0.13212348222732545\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.13212348222732545  to: 0.13210933208465575\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.13210933208465575  to: 0.13209522962570192\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.13209522962570192  to: 0.13208115100860596\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.13208115100860596  to: 0.13206708431243896\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.13206708431243896  to: 0.13205305337905884\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.13205305337905884  to: 0.13203904628753663\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.13203904628753663  to: 0.13202506303787231\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.13202506303787231  to: 0.1320111036300659\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.1320111036300659  to: 0.1319971799850464\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.1319971799850464  to: 0.13198325634002686\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.13198325634002686  to: 0.13196938037872313\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.13196938037872313  to: 0.13195550441741943\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.13195550441741943  to: 0.13194167613983154\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.13194167613983154  to: 0.13192784786224365\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.13192784786224365  to: 0.13191406726837157\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.13191406726837157  to: 0.13190028667449952\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.13190028667449952  to: 0.13188655376434327\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.13188655376434327  to: 0.13187280893325806\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.13187280893325806  to: 0.13185912370681763\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.13185912370681763  to: 0.1318454384803772\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.1318454384803772  to: 0.1318317770957947\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.1318317770957947  to: 0.13181816339492797\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.13181816339492797  to: 0.13180453777313234\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.13180453777313234  to: 0.13179095983505248\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.13179095983505248  to: 0.13177740573883057\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.13177740573883057  to: 0.1317638635635376\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1317638635635376  to: 0.13175039291381835\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.13175039291381835  to: 0.13173698186874389\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.13173698186874389  to: 0.13172366619110107\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.13172366619110107  to: 0.13171038627624512\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.13171038627624512  to: 0.13169715404510499\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.13169715404510499  to: 0.13168399333953856\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.13168399333953856  to: 0.13167089223861694\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.13167089223861694  to: 0.13165781497955323\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.13165781497955323  to: 0.13164478540420532\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.13164478540420532  to: 0.13163182735443116\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.13163182735443116  to: 0.13161888122558593\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.13161888122558593  to: 0.13160600662231445\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.13160600662231445  to: 0.13159315586090087\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.13159315586090087  to: 0.13158032894134522\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.13158032894134522  to: 0.13156754970550538\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.13156754970550538  to: 0.13155481815338135\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.13155481815338135  to: 0.1315420985221863\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.1315420985221863  to: 0.13152942657470704\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.13152942657470704  to: 0.13151676654815675\n",
      "Training iteration: 673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13151676654815675  to: 0.13150415420532227\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.13150415420532227  to: 0.13149157762527466\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.13149157762527466  to: 0.131479012966156\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.131479012966156  to: 0.13146648406982422\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.13146648406982422  to: 0.1314539909362793\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.1314539909362793  to: 0.13144150972366334\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.13144150972366334  to: 0.13142907619476318\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.13142907619476318  to: 0.13141666650772094\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.13141666650772094  to: 0.13140428066253662\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.13140428066253662  to: 0.13139193058013915\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.13139193058013915  to: 0.13137959241867064\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.13137959241867064  to: 0.1313673138618469\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.1313673138618469  to: 0.13135502338409424\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.13135502338409424  to: 0.13134279251098632\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.13134279251098632  to: 0.13133058547973633\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.13133058547973633  to: 0.13131839036941528\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.13131839036941528  to: 0.1313062310218811\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.1313062310218811  to: 0.13129409551620483\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.13129409551620483  to: 0.13128198385238649\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.13128198385238649  to: 0.13126989603042602\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.13126989603042602  to: 0.13125784397125245\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.13125784397125245  to: 0.13124579191207886\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.13124579191207886  to: 0.1312337875366211\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1312337875366211  to: 0.13122179508209228\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.13122179508209228  to: 0.1312098264694214\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.1312098264694214  to: 0.13119789361953735\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.13119789361953735  to: 0.13118597269058227\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.13118597269058227  to: 0.13117406368255616\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.13117406368255616  to: 0.13116214275360108\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.13116214275360108  to: 0.1311502456665039\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.1311502456665039  to: 0.1311383605003357\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.1311383605003357  to: 0.13112648725509643\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.13112648725509643  to: 0.13111464977264403\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.13111464977264403  to: 0.1311028480529785\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.1311028480529785  to: 0.13109104633331298\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.13109104633331298  to: 0.13107926845550538\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.13107926845550538  to: 0.13106753826141357\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.13106753826141357  to: 0.13105580806732178\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.13105580806732178  to: 0.1310441017150879\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.1310441017150879  to: 0.13103243112564086\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.13103243112564086  to: 0.13102079629898072\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.13102079629898072  to: 0.13100917339324952\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.13100917339324952  to: 0.13099758625030516\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.13099758625030516  to: 0.13098602294921874\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.13098602294921874  to: 0.13097448348999025\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.13097448348999025  to: 0.1309629797935486\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.1309629797935486  to: 0.13095149993896485\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.13095149993896485  to: 0.130940043926239\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.130940043926239  to: 0.1309286117553711\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.1309286117553711  to: 0.13091723918914794\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.13091723918914794  to: 0.13090591430664061\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.13090591430664061  to: 0.13089461326599122\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.13089461326599122  to: 0.1308833360671997\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.1308833360671997  to: 0.13087209463119506\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.13087209463119506  to: 0.1308608889579773\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1308608889579773  to: 0.1308497190475464\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.1308497190475464  to: 0.13083854913711548\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.13083854913711548  to: 0.13082740306854249\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.13082740306854249  to: 0.1308163046836853\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.1308163046836853  to: 0.13080523014068604\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.13080523014068604  to: 0.13079416751861572\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.13079416751861572  to: 0.1307831287384033\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.1307831287384033  to: 0.13077211380004883\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.13077211380004883  to: 0.13076112270355225\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.13076112270355225  to: 0.13075015544891358\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.13075015544891358  to: 0.13073920011520385\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.13073920011520385  to: 0.13072829246520995\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.13072829246520995  to: 0.13071739673614502\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.13071739673614502  to: 0.130706524848938\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.130706524848938  to: 0.13069567680358887\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.13069567680358887  to: 0.13068482875823975\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.13068482875823975  to: 0.13067398071289063\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.13067398071289063  to: 0.13066315650939941\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.13066315650939941  to: 0.13065234422683716\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.13065234422683716  to: 0.13064157962799072\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.13064157962799072  to: 0.13063080310821534\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.13063080310821534  to: 0.1306200623512268\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.1306200623512268  to: 0.13060929775238037\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.13060929775238037  to: 0.13059858083724976\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.13059858083724976  to: 0.13058786392211913\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.13058786392211913  to: 0.1305771589279175\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.1305771589279175  to: 0.13056650161743164\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.13056650161743164  to: 0.13055588006973268\n",
      "Training iteration: 756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13055588006973268  to: 0.1305452823638916\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.1305452823638916  to: 0.13053470849990845\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.13053470849990845  to: 0.1305241346359253\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.1305241346359253  to: 0.130513596534729\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.130513596534729  to: 0.13050308227539062\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.13050308227539062  to: 0.13049256801605225\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.13049256801605225  to: 0.13048208951950074\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.13048208951950074  to: 0.13047162294387818\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.13047162294387818  to: 0.13046118021011352\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.13046118021011352  to: 0.1304507374763489\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.1304507374763489  to: 0.1304403305053711\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.1304403305053711  to: 0.13042994737625122\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.13042994737625122  to: 0.1304195761680603\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.1304195761680603  to: 0.13040921688079835\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.13040921688079835  to: 0.1303988814353943\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.1303988814353943  to: 0.13038856983184816\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.13038856983184816  to: 0.13037827014923095\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.13037827014923095  to: 0.13036799430847168\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.13036799430847168  to: 0.1303577184677124\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.1303577184677124  to: 0.13034746646881104\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.13034746646881104  to: 0.1303372263908386\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.1303372263908386  to: 0.13032702207565308\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.13032702207565308  to: 0.13031680583953859\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.13031680583953859  to: 0.1303066372871399\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1303066372871399  to: 0.13029645681381224\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.13029645681381224  to: 0.1302863121032715\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1302863121032715  to: 0.1302761673927307\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.1302761673927307  to: 0.1302660346031189\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.1302660346031189  to: 0.13025592565536498\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.13025592565536498  to: 0.130245840549469\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.130245840549469  to: 0.13023574352264405\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.13023574352264405  to: 0.13022568225860595\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.13022568225860595  to: 0.13021562099456788\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.13021562099456788  to: 0.13020559549331664\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.13020559549331664  to: 0.13019555807113647\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.13019555807113647  to: 0.130185866355896\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.130185866355896  to: 0.13017647266387938\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.13017647266387938  to: 0.1301673412322998\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.1301673412322998  to: 0.1301584482192993\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.1301584482192993  to: 0.13014980554580688\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.13014980554580688  to: 0.13014132976531984\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.13014132976531984  to: 0.13013304471969606\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.13013304471969606  to: 0.13012492656707764\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.13012492656707764  to: 0.13011693954467773\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.13011693954467773  to: 0.1301090955734253\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.1301090955734253  to: 0.13010135889053345\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.13010135889053345  to: 0.1300937294960022\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.1300937294960022  to: 0.13008617162704467\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.13008617162704467  to: 0.1300787091255188\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.1300787091255188  to: 0.13007131814956666\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.13007131814956666  to: 0.13006397485733032\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.13006397485733032  to: 0.13005669116973878\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.13005669116973878  to: 0.1300494432449341\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.1300494432449341  to: 0.1300422430038452\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.1300422430038452  to: 0.1300350546836853\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.1300350546836853  to: 0.1300279140472412\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.1300279140472412  to: 0.13002078533172606\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.13002078533172606  to: 0.13001365661621095\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.13001365661621095  to: 0.13000656366348268\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.13000656366348268  to: 0.12999944686889647\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.12999944686889647  to: 0.1299923539161682\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.1299923539161682  to: 0.12998526096343993\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12998526096343993  to: 0.12997815608978272\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12997815608978272  to: 0.12997105121612548\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12997105121612548  to: 0.1299639344215393\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.1299639344215393  to: 0.12995680570602416\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.12995680570602416  to: 0.12994966506958008\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12994966506958008  to: 0.12994253635406494\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12994253635406494  to: 0.129935359954834\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.129935359954834  to: 0.1299281597137451\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.1299281597137451  to: 0.12992093563079835\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12992093563079835  to: 0.12991371154785156\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12991371154785156  to: 0.12990645170211793\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12990645170211793  to: 0.12989919185638427\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12989919185638427  to: 0.12989189624786376\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.12989189624786376  to: 0.1298845887184143\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.1298845887184143  to: 0.12987724542617798\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12987724542617798  to: 0.12986990213394164\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.12986990213394164  to: 0.1298625349998474\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.1298625349998474  to: 0.12985513210296631\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12985513210296631  to: 0.12984771728515626\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12984771728515626  to: 0.12984027862548828\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12984027862548828  to: 0.1298328161239624\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.1298328161239624  to: 0.12982532978057862\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12982532978057862  to: 0.12981784343719482\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12981784343719482  to: 0.12981035709381103\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12981035709381103  to: 0.1298028349876404\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.1298028349876404  to: 0.12979530096054076\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12979530096054076  to: 0.12978774309158325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12978774309158325  to: 0.12978014945983887\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12978014945983887  to: 0.12977253198623656\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.12977253198623656  to: 0.1297649025917053\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.1297649025917053  to: 0.12975724935531616\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12975724935531616  to: 0.12974953651428223\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.12974953651428223  to: 0.12974174022674562\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12974174022674562  to: 0.12973392009735107\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.12973392009735107  to: 0.1297260642051697\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.1297260642051697  to: 0.12971820831298828\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.12971820831298828  to: 0.12971031665802002\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.12971031665802002  to: 0.12970240116119386\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.12970240116119386  to: 0.12969446182250977\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12969446182250977  to: 0.12968652248382567\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12968652248382567  to: 0.12967852354049683\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.12967852354049683  to: 0.12967052459716796\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.12967052459716796  to: 0.1296625018119812\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.1296625018119812  to: 0.12965446710586548\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12965446710586548  to: 0.1296463966369629\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.1296463966369629  to: 0.12963831424713135\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12963831424713135  to: 0.129630184173584\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.129630184173584  to: 0.12962205410003663\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.12962205410003663  to: 0.12961390018463134\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12961390018463134  to: 0.1296057105064392\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.1296057105064392  to: 0.12959753274917601\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12959753274917601  to: 0.129589307308197\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.129589307308197  to: 0.12958106994628907\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12958106994628907  to: 0.1295728087425232\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.1295728087425232  to: 0.12956453561782838\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12956453561782838  to: 0.12955623865127563\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.12955623865127563  to: 0.12954790592193605\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12954790592193605  to: 0.1295395612716675\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.1295395612716675  to: 0.12953120470046997\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.12953120470046997  to: 0.12952282428741455\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12952282428741455  to: 0.12951443195343018\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12951443195343018  to: 0.1295056700706482\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.1295056700706482  to: 0.12949661016464234\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.12949661016464234  to: 0.12948726415634154\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.12948726415634154  to: 0.12947765588760377\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.12947765588760377  to: 0.12946780920028686\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12946780920028686  to: 0.12945775985717772\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12945775985717772  to: 0.129447865486145\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.129447865486145  to: 0.12943809032440184\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12943809032440184  to: 0.1294284462928772\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.1294284462928772  to: 0.12941888570785523\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12941888570785523  to: 0.1294094443321228\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.1294094443321228  to: 0.12940008640289308\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12940008640289308  to: 0.12939083576202393\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.12939083576202393  to: 0.12938165664672852\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.12938165664672852  to: 0.12937254905700685\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.12937254905700685  to: 0.12936350107192993\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.12936350107192993  to: 0.12935450077056884\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.12935450077056884  to: 0.12934520244598388\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12934520244598388  to: 0.12933565378189088\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.12933565378189088  to: 0.12932586669921875\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.12932586669921875  to: 0.12931619882583617\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12931619882583617  to: 0.1293066382408142\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.1293066382408142  to: 0.12929718494415282\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.12929718494415282  to: 0.12928780317306518\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.12928780317306518  to: 0.12927851676940919\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12927851676940919  to: 0.12926895618438722\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12926895618438722  to: 0.1292591691017151\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.1292591691017151  to: 0.1292495012283325\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.1292495012283325  to: 0.12923991680145264\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.12923991680145264  to: 0.12923047542572022\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12923047542572022  to: 0.12922115325927735\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.12922115325927735  to: 0.12921155691146852\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12921155691146852  to: 0.12920207977294923\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12920207977294923  to: 0.1291923403739929\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.1291923403739929  to: 0.12918273210525513\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12918273210525513  to: 0.1291734218597412\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.1291734218597412  to: 0.12916418313980102\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.12916418313980102  to: 0.12915506362915039\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.12915506362915039  to: 0.1291460633277893\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.1291460633277893  to: 0.1291368007659912\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.1291368007659912  to: 0.12912768125534058\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12912768125534058  to: 0.1291183352470398\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.1291183352470398  to: 0.1291091561317444\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.1291091561317444  to: 0.12910008430480957\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.12910008430480957  to: 0.12909114360809326\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12909114360809326  to: 0.12908195257186889\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.12908195257186889  to: 0.12907254695892334\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12907254695892334  to: 0.12906330823898315\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12906330823898315  to: 0.12905418872833252\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12905418872833252  to: 0.1290452241897583\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.1290452241897583  to: 0.129036021232605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 930\n",
      "Improved validation loss from: 0.129036021232605  to: 0.1290265917778015\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.1290265917778015  to: 0.12901732921600342\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.12901732921600342  to: 0.1290082335472107\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.1290082335472107  to: 0.12899924516677858\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12899924516677858  to: 0.12899001836776733\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12899001836776733  to: 0.12898061275482178\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12898061275482178  to: 0.1289713501930237\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.1289713501930237  to: 0.12896223068237306\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12896223068237306  to: 0.12895290851593016\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.12895290851593016  to: 0.1289433717727661\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.1289433717727661  to: 0.12893401384353637\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12893401384353637  to: 0.12892478704452515\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12892478704452515  to: 0.12891554832458496\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12891554832458496  to: 0.12890608310699464\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12890608310699464  to: 0.12889641523361206\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.12889641523361206  to: 0.12888693809509277\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12888693809509277  to: 0.12887758016586304\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12887758016586304  to: 0.12886803150177\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12886803150177  to: 0.1288582682609558\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.1288582682609558  to: 0.12884869575500488\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.12884869575500488  to: 0.12883927822113037\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.12883927822113037  to: 0.12882964611053466\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.12882964611053466  to: 0.12881982326507568\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.12881982326507568  to: 0.12881017923355104\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12881017923355104  to: 0.1288007140159607\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.1288007140159607  to: 0.1287910223007202\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.1287910223007202  to: 0.12878116369247436\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.12878116369247436  to: 0.12877148389816284\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12877148389816284  to: 0.12876198291778565\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12876198291778565  to: 0.1287522554397583\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.1287522554397583  to: 0.12874234914779664\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.12874234914779664  to: 0.1287322759628296\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.1287322759628296  to: 0.1287224054336548\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.1287224054336548  to: 0.12871272563934327\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.12871272563934327  to: 0.12870323657989502\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12870323657989502  to: 0.12869350910186766\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12869350910186766  to: 0.12868359088897705\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12868359088897705  to: 0.12867351770401\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.12867351770401  to: 0.12866365909576416\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12866365909576416  to: 0.12865395545959474\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12865395545959474  to: 0.12864407300949096\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12864407300949096  to: 0.1286340117454529\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.1286340117454529  to: 0.1286237955093384\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.1286237955093384  to: 0.12861379384994506\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12861379384994506  to: 0.128603994846344\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.128603994846344  to: 0.12859437465667725\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12859437465667725  to: 0.12858455181121825\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12858455181121825  to: 0.12857453823089598\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12857453823089598  to: 0.12856435775756836\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12856435775756836  to: 0.12855401039123535\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.12855401039123535  to: 0.12854353189468384\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12854353189468384  to: 0.12853330373764038\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12853330373764038  to: 0.12852329015731812\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12852329015731812  to: 0.12851349115371705\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12851349115371705  to: 0.12850347757339478\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12850347757339478  to: 0.1284932851791382\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.1284932851791382  to: 0.12848296165466308\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12848296165466308  to: 0.12847248315811158\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12847248315811158  to: 0.12846187353134156\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12846187353134156  to: 0.12845151424407958\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.12845151424407958  to: 0.12844139337539673\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12844139337539673  to: 0.1284311056137085\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.1284311056137085  to: 0.12842068672180176\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12842068672180176  to: 0.12841008901596068\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.12841008901596068  to: 0.12839963436126708\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12839963436126708  to: 0.12838938236236572\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.12838938236236572  to: 0.12837897539138793\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12837897539138793  to: 0.12836840152740478\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12836840152740478  to: 0.12835768461227418\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12835768461227418  to: 0.128346848487854\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.128346848487854  to: 0.12833590507507325\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.12833590507507325  to: 0.1283252239227295\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.1283252239227295  to: 0.12831480503082277\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12831480503082277  to: 0.12830419540405275\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12830419540405275  to: 0.12829344272613524\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12829344272613524  to: 0.1282825231552124\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.1282825231552124  to: 0.1282715082168579\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.1282715082168579  to: 0.12826038599014283\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12826038599014283  to: 0.12824952602386475\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12824952602386475  to: 0.12823857069015504\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12823857069015504  to: 0.1282278776168823\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.1282278776168823  to: 0.12821705341339112\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.12821705341339112  to: 0.1282061219215393\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.1282061219215393  to: 0.12819507122039794\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.12819507122039794  to: 0.12818394899368285\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12818394899368285  to: 0.12817270755767823\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12817270755767823  to: 0.12816140651702881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.12816140651702881  to: 0.12815002202987671\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.12815002202987671  to: 0.12813860177993774\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.12813860177993774  to: 0.12812747955322265\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12812747955322265  to: 0.12811626195907594\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.12811626195907594  to: 0.12810497283935546\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.12810497283935546  to: 0.12809360027313232\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.12809360027313232  to: 0.12808215618133545\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.12808215618133545  to: 0.12807066440582277\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.12807066440582277  to: 0.12805911302566528\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.12805911302566528  to: 0.12804787158966063\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.12804787158966063  to: 0.12803657054901124\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.12803657054901124  to: 0.12802518606185914\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.12802518606185914  to: 0.12801371812820433\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.12801371812820433  to: 0.12800219058990478\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.12800219058990478  to: 0.12799060344696045\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.12799060344696045  to: 0.12797896862030028\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.12797896862030028  to: 0.12796727418899537\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.12796727418899537  to: 0.12795554399490355\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12795554399490355  to: 0.1279437780380249\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1279437780380249  to: 0.12793195247650146\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.12793195247650146  to: 0.12792011499404907\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.12792011499404907  to: 0.12790824174880983\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12790824174880983  to: 0.12789634466171265\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.12789634466171265  to: 0.12788442373275757\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.12788442373275757  to: 0.12787249088287353\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12787249088287353  to: 0.12786052227020264\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.12786052227020264  to: 0.1278485655784607\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.1278485655784607  to: 0.12783695459365846\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.12783695459365846  to: 0.12782528400421142\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12782528400421142  to: 0.1278137445449829\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.1278137445449829  to: 0.1278023362159729\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.1278023362159729  to: 0.12779098749160767\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12779098749160767  to: 0.12777974605560302\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12777974605560302  to: 0.12776858806610109\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.12776858806610109  to: 0.1277574896812439\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.1277574896812439  to: 0.12774645090103148\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.12774645090103148  to: 0.1277354836463928\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.1277354836463928  to: 0.127724552154541\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.127724552154541  to: 0.12771365642547608\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.12771365642547608  to: 0.12770278453826905\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.12770278453826905  to: 0.12769197225570678\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12769197225570678  to: 0.12768118381500243\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.12768118381500243  to: 0.12767039537429808\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12767039537429808  to: 0.1276596426963806\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.1276596426963806  to: 0.12764889001846313\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12764889001846313  to: 0.12763816118240356\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.12763816118240356  to: 0.127627432346344\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.127627432346344  to: 0.12761670351028442\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.12761670351028442  to: 0.12760599851608276\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12760599851608276  to: 0.1275952935218811\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.1275952935218811  to: 0.12758458852767945\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.12758458852767945  to: 0.12757385969161988\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12757385969161988  to: 0.12756315469741822\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.12756315469741822  to: 0.12755242586135865\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.12755242586135865  to: 0.12754169702529908\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12754169702529908  to: 0.1275309443473816\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.1275309443473816  to: 0.1275201916694641\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.1275201916694641  to: 0.1275094270706177\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.1275094270706177  to: 0.12749865055084228\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.12749865055084228  to: 0.1274878740310669\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.1274878740310669  to: 0.1274770975112915\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.1274770975112915  to: 0.1274663209915161\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.1274663209915161  to: 0.12745553255081177\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.12745553255081177  to: 0.12744473218917846\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12744473218917846  to: 0.1274339437484741\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.1274339437484741  to: 0.12742313146591186\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12742313146591186  to: 0.12741230726242064\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12741230726242064  to: 0.1274014949798584\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.1274014949798584  to: 0.12739064693450927\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12739064693450927  to: 0.1273797869682312\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.1273797869682312  to: 0.12736891508102416\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.12736891508102416  to: 0.12735801935195923\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12735801935195923  to: 0.12734711170196533\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.12734711170196533  to: 0.12733618021011353\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12733618021011353  to: 0.12732523679733276\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12732523679733276  to: 0.12731425762176513\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12731425762176513  to: 0.12730327844619752\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.12730327844619752  to: 0.1272922396659851\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.1272922396659851  to: 0.12728118896484375\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12728118896484375  to: 0.12727012634277343\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.12727012634277343  to: 0.12725902795791627\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.12725902795791627  to: 0.1272478938102722\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.1272478938102722  to: 0.12723673582077027\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12723673582077027  to: 0.12722556591033934\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.12722556591033934  to: 0.12721457481384277\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12721457481384277  to: 0.12720375061035155\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12720375061035155  to: 0.1271931052207947\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.1271931052207947  to: 0.12718261480331422\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.12718261480331422  to: 0.1271722435951233\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.1271722435951233  to: 0.127161967754364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.127161967754364  to: 0.12715179920196534\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12715179920196534  to: 0.12714169025421143\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.12714169025421143  to: 0.1271316647529602\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.1271316647529602  to: 0.12712167501449584\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.12712167501449584  to: 0.12711174488067628\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.12711174488067628  to: 0.12710185050964357\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.12710185050964357  to: 0.12709200382232666\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12709200382232666  to: 0.12708215713500975\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.12708215713500975  to: 0.12707233428955078\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.12707233428955078  to: 0.1270625352859497\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.1270625352859497  to: 0.12705272436141968\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12705272436141968  to: 0.12704293727874755\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.12704293727874755  to: 0.12703313827514648\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.12703313827514648  to: 0.12702332735061644\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.12702332735061644  to: 0.12701351642608644\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.12701351642608644  to: 0.12700369358062744\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.12700369358062744  to: 0.12699384689331056\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12699384689331056  to: 0.12698400020599365\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.12698400020599365  to: 0.1269741177558899\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.1269741177558899  to: 0.12696421146392822\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.12696421146392822  to: 0.1269542932510376\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.1269542932510376  to: 0.1269443392753601\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.1269443392753601  to: 0.12693436145782472\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.12693436145782472  to: 0.1269243597984314\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.1269243597984314  to: 0.12691432237625122\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.12691432237625122  to: 0.12690426111221315\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.12690426111221315  to: 0.12689416408538817\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.12689416408538817  to: 0.12688403129577636\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.12688403129577636  to: 0.12687389850616454\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.12687389850616454  to: 0.12686374187469482\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.12686374187469482  to: 0.1268535614013672\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.1268535614013672  to: 0.1268433451652527\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.1268433451652527  to: 0.12683309316635133\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.12683309316635133  to: 0.1268228054046631\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.1268228054046631  to: 0.12681248188018798\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.12681248188018798  to: 0.12680211067199706\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.12680211067199706  to: 0.1267917275428772\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.1267917275428772  to: 0.12678129673004152\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.12678129673004152  to: 0.12677080631256105\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.12677080631256105  to: 0.12676031589508058\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.12676031589508058  to: 0.12674980163574218\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.12674980163574218  to: 0.12673918008804322\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.12673918008804322  to: 0.12672845125198365\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.12672845125198365  to: 0.1267176866531372\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.1267176866531372  to: 0.12670687437057496\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.12670687437057496  to: 0.12669603824615477\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.12669603824615477  to: 0.12668516635894775\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.12668516635894775  to: 0.1266742467880249\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.1266742467880249  to: 0.12666331529617308\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.12666331529617308  to: 0.12665233612060547\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.12665233612060547  to: 0.12664130926132203\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.12664130926132203  to: 0.1266302704811096\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.1266302704811096  to: 0.12661919593811036\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.12661919593811036  to: 0.1266080617904663\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.1266080617904663  to: 0.12659692764282227\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.12659692764282227  to: 0.1265857458114624\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.1265857458114624  to: 0.12657454013824462\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.12657454013824462  to: 0.12656329870223998\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.12656329870223998  to: 0.12655203342437743\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.12655203342437743  to: 0.1265407085418701\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.1265407085418701  to: 0.12652937173843384\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.12652937173843384  to: 0.12651799917221068\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.12651799917221068  to: 0.12650659084320068\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.12650659084320068  to: 0.1264951467514038\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.1264951467514038  to: 0.12648365497589112\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.12648365497589112  to: 0.1264721632003784\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.1264721632003784  to: 0.1264606237411499\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.1264606237411499  to: 0.1264490604400635\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.1264490604400635  to: 0.12643747329711913\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.12643747329711913  to: 0.1264258623123169\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.1264258623123169  to: 0.12641420364379882\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.12641420364379882  to: 0.1264025092124939\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.1264025092124939  to: 0.12639080286026\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.12639080286026  to: 0.1263790488243103\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.1263790488243103  to: 0.12636728286743165\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.12636728286743165  to: 0.12635548114776612\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.12635548114776612  to: 0.12634364366531373\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.12634364366531373  to: 0.12633177042007446\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.12633177042007446  to: 0.12631988525390625\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.12631988525390625  to: 0.1263079524040222\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.1263079524040222  to: 0.12629600763320922\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.12629600763320922  to: 0.12628400325775146\n",
      "Training iteration: 1189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12628400325775146  to: 0.12627198696136474\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.12627198696136474  to: 0.1262599468231201\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.1262599468231201  to: 0.12624785900115967\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.12624785900115967  to: 0.1262357473373413\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.1262357473373413  to: 0.1262235999107361\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.1262235999107361  to: 0.1262114405632019\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.1262114405632019  to: 0.12619922161102295\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.12619922161102295  to: 0.126187002658844\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.126187002658844  to: 0.12617472410202027\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.12617472410202027  to: 0.12616244554519654\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.12616244554519654  to: 0.12615010738372803\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.12615010738372803  to: 0.12613776922225953\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.12613776922225953  to: 0.12612537145614625\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.12612537145614625  to: 0.126112961769104\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.126112961769104  to: 0.12610052824020385\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.12610052824020385  to: 0.12608804702758789\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.12608804702758789  to: 0.12607555389404296\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.12607555389404296  to: 0.1260630249977112\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.1260630249977112  to: 0.12605044841766358\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.12605044841766358  to: 0.126037859916687\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.126037859916687  to: 0.12602522373199462\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.12602522373199462  to: 0.12601258754730224\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.12601258754730224  to: 0.12599993944168092\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.12599993944168092  to: 0.12598726749420167\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.12598726749420167  to: 0.12597460746765138\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.12597460746765138  to: 0.12596192359924316\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.12596192359924316  to: 0.12594921588897706\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.12594921588897706  to: 0.12593650817871094\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.12593650817871094  to: 0.12592378854751587\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.12592378854751587  to: 0.12591104507446288\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.12591104507446288  to: 0.12589808702468872\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.12589808702468872  to: 0.12588484287261964\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.12588484287261964  to: 0.12587156295776367\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.12587156295776367  to: 0.12585824728012085\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.12585824728012085  to: 0.12584491968154907\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.12584491968154907  to: 0.12583156824111938\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.12583156824111938  to: 0.12581818103790282\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.12581818103790282  to: 0.12580478191375732\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.12580478191375732  to: 0.125791335105896\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.125791335105896  to: 0.12577786445617675\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.12577786445617675  to: 0.12576438188552858\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.12576438188552858  to: 0.12575085163116456\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.12575085163116456  to: 0.12573726177215577\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.12573726177215577  to: 0.12572368383407592\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.12572368383407592  to: 0.12571005821228026\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.12571005821228026  to: 0.1256964087486267\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.1256964087486267  to: 0.12568275928497313\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.12568275928497313  to: 0.1256691813468933\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.1256691813468933  to: 0.12565559148788452\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.12565559148788452  to: 0.12564194202423096\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.12564194202423096  to: 0.1256282925605774\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.1256282925605774  to: 0.12561459541320802\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.12561459541320802  to: 0.1256007432937622\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.1256007432937622  to: 0.12558678388595582\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.12558678388595582  to: 0.1255728483200073\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.1255728483200073  to: 0.1255589485168457\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.1255589485168457  to: 0.1255450129508972\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.1255450129508972  to: 0.12553102970123292\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.12553102970123292  to: 0.1255170464515686\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.1255170464515686  to: 0.1255030632019043\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.1255030632019043  to: 0.12548904418945311\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.12548904418945311  to: 0.1254749894142151\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.1254749894142151  to: 0.1254608988761902\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.1254608988761902  to: 0.12544677257537842\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.12544677257537842  to: 0.12543261051177979\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.12543261051177979  to: 0.12541842460632324\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.12541842460632324  to: 0.1254041910171509\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.1254041910171509  to: 0.12538989782333373\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.12538989782333373  to: 0.1253756284713745\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.1253756284713745  to: 0.12536131143569945\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.12536131143569945  to: 0.12534697055816652\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.12534697055816652  to: 0.12533260583877565\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.12533260583877565  to: 0.1253182053565979\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.1253182053565979  to: 0.12530380487442017\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.12530380487442017  to: 0.12528934478759765\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.12528934478759765  to: 0.1252748727798462\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.1252748727798462  to: 0.1252603530883789\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.1252603530883789  to: 0.12524583339691162\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.12524583339691162  to: 0.12523126602172852\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.12523126602172852  to: 0.12521665096282958\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.12521665096282958  to: 0.12520203590393067\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.12520203590393067  to: 0.12518737316131592\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.12518737316131592  to: 0.1251726746559143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.1251726746559143  to: 0.12515795230865479\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.12515795230865479  to: 0.12514320611953736\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.12514320611953736  to: 0.12512842416763306\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.12512842416763306  to: 0.125113582611084\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.125113582611084  to: 0.12509872913360595\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.12509872913360595  to: 0.12508385181427\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.12508385181427  to: 0.12506892681121826\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.12506892681121826  to: 0.1250539779663086\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.1250539779663086  to: 0.1250389814376831\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.1250389814376831  to: 0.1250239610671997\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.1250239610671997  to: 0.1250089168548584\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.1250089168548584  to: 0.12499383687973023\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.12499383687973023  to: 0.12497870922088623\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.12497870922088623  to: 0.12496354579925537\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.12496354579925537  to: 0.12494838237762451\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.12494838237762451  to: 0.12493314743041992\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.12493314743041992  to: 0.12491790056228638\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.12491790056228638  to: 0.12490262985229492\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.12490262985229492  to: 0.12488728761672974\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.12488728761672974  to: 0.12487194538116456\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.12487194538116456  to: 0.12485654354095459\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.12485654354095459  to: 0.12484114170074463\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.12484114170074463  to: 0.12482568025588989\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.12482568025588989  to: 0.12481019496917725\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.12481019496917725  to: 0.12479467391967773\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.12479467391967773  to: 0.12477916479110718\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.12477916479110718  to: 0.12476361989974975\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.12476361989974975  to: 0.1247481107711792\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.1247481107711792  to: 0.12473256587982177\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.12473256587982177  to: 0.12471702098846435\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.12471702098846435  to: 0.12470146417617797\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.12470146417617797  to: 0.12468588352203369\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.12468588352203369  to: 0.12467027902603149\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.12467027902603149  to: 0.12465465068817139\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.12465465068817139  to: 0.12463902235031128\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.12463902235031128  to: 0.12462337017059326\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.12462337017059326  to: 0.12460768222808838\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.12460768222808838  to: 0.12459198236465455\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.12459198236465455  to: 0.12457624673843384\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.12457624673843384  to: 0.12456048727035522\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.12456048727035522  to: 0.12454469203948974\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.12454469203948974  to: 0.12452889680862426\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.12452889680862426  to: 0.12451304197311401\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.12451304197311401  to: 0.1244971752166748\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.1244971752166748  to: 0.12448126077651978\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.12448126077651978  to: 0.12446532249450684\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.12446532249450684  to: 0.12444936037063599\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.12444936037063599  to: 0.12443335056304931\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.12443335056304931  to: 0.12441731691360473\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.12441731691360473  to: 0.1244012475013733\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.1244012475013733  to: 0.12438514232635497\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.12438514232635497  to: 0.12436900138854981\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.12436900138854981  to: 0.12435281276702881\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.12435281276702881  to: 0.12433669567108155\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.12433669567108155  to: 0.12432057857513427\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.12432057857513427  to: 0.12430444955825806\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.12430444955825806  to: 0.12428824901580811\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.12428824901580811  to: 0.1242720365524292\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.1242720365524292  to: 0.12425577640533447\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.12425577640533447  to: 0.12423946857452392\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.12423946857452392  to: 0.12422316074371338\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.12422316074371338  to: 0.124206805229187\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.124206805229187  to: 0.12419041395187377\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.12419041395187377  to: 0.12417399883270264\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.12417399883270264  to: 0.12415754795074463\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.12415754795074463  to: 0.1241410493850708\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.1241410493850708  to: 0.12412455081939697\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.12412455081939697  to: 0.12410799264907837\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.12410799264907837  to: 0.1240913987159729\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.1240913987159729  to: 0.12407476902008056\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.12407476902008056  to: 0.12405811548233033\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.12405811548233033  to: 0.1240414023399353\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.1240414023399353  to: 0.12402466535568238\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.12402466535568238  to: 0.12400788068771362\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.12400788068771362  to: 0.12399108409881592\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.12399108409881592  to: 0.1239742636680603\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.1239742636680603  to: 0.12395737171173096\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.12395737171173096  to: 0.12394049167633056\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.12394049167633056  to: 0.1239235520362854\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.1239235520362854  to: 0.12390658855438233\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.12390658855438233  to: 0.12388960123062134\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.12388960123062134  to: 0.12387256622314453\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.12387256622314453  to: 0.12385549545288085\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.12385549545288085  to: 0.12383840084075928\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.12383840084075928  to: 0.12382124662399292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.12382124662399292  to: 0.12380406856536866\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.12380406856536866  to: 0.12378685474395752\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.12378685474395752  to: 0.12376960515975952\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.12376960515975952  to: 0.1237523078918457\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.1237523078918457  to: 0.12373497486114501\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.12373497486114501  to: 0.12371759414672852\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.12371759414672852  to: 0.1237001895904541\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.1237001895904541  to: 0.12368272542953491\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.12368272542953491  to: 0.12366523742675781\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.12366523742675781  to: 0.12364771366119384\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.12364771366119384  to: 0.12363011837005615\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.12363011837005615  to: 0.1236125111579895\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.1236125111579895  to: 0.12359483242034912\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.12359483242034912  to: 0.12357714176177978\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.12357714176177978  to: 0.12355940341949463\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.12355940341949463  to: 0.1235416054725647\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.1235416054725647  to: 0.12352378368377685\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.12352378368377685  to: 0.12350591421127319\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.12350591421127319  to: 0.12348799705505371\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.12348799705505371  to: 0.12347004413604737\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.12347004413604737  to: 0.1234520673751831\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.1234520673751831  to: 0.12343404293060303\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.12343404293060303  to: 0.123416006565094\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.123416006565094  to: 0.1233979344367981\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.1233979344367981  to: 0.12337981462478638\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.12337981462478638  to: 0.12336165904998779\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.12336165904998779  to: 0.12334349155426025\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.12334349155426025  to: 0.1233252763748169\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.1233252763748169  to: 0.12330702543258668\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.12330702543258668  to: 0.12328873872756958\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.12328873872756958  to: 0.12327040433883667\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.12327040433883667  to: 0.1232520341873169\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.1232520341873169  to: 0.12323365211486817\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.12323365211486817  to: 0.12321525812149048\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.12321525812149048  to: 0.12319686412811279\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.12319686412811279  to: 0.12317845821380616\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.12317845821380616  to: 0.12316005229949951\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.12316005229949951  to: 0.12314162254333497\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.12314162254333497  to: 0.1231231927871704\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.1231231927871704  to: 0.12310473918914795\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.12310473918914795  to: 0.12308626174926758\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.12308626174926758  to: 0.12306776046752929\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.12306776046752929  to: 0.12304923534393311\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.12304923534393311  to: 0.123030686378479\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.123030686378479  to: 0.12301210165023804\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.12301210165023804  to: 0.12299350500106812\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.12299350500106812  to: 0.12297484874725342\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.12297484874725342  to: 0.12295618057250976\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.12295618057250976  to: 0.12293747663497925\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.12293747663497925  to: 0.12291872501373291\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.12291872501373291  to: 0.12289994955062866\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.12289994955062866  to: 0.12288115024566651\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.12288115024566651  to: 0.12286231517791749\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.12286231517791749  to: 0.12284343242645264\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.12284343242645264  to: 0.12282456159591675\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.12282456159591675  to: 0.122805655002594\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.122805655002594  to: 0.12278671264648437\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.12278671264648437  to: 0.12276774644851685\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.12276774644851685  to: 0.1227487564086914\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.1227487564086914  to: 0.1227297306060791\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.1227297306060791  to: 0.12271068096160889\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.12271068096160889  to: 0.1226915955543518\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.1226915955543518  to: 0.12267248630523682\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.12267248630523682  to: 0.12265335321426392\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.12265335321426392  to: 0.12263416051864624\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.12263416051864624  to: 0.12261496782302857\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.12261496782302857  to: 0.12259571552276612\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.12259571552276612  to: 0.1225764274597168\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.1225764274597168  to: 0.12255710363388062\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.12255710363388062  to: 0.12253774404525757\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.12253774404525757  to: 0.1225183367729187\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.1225183367729187  to: 0.12249889373779296\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.12249889373779296  to: 0.12247940301895141\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.12247940301895141  to: 0.12245991230010986\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.12245991230010986  to: 0.12244067192077637\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.12244067192077637  to: 0.122421395778656\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.122421395778656  to: 0.12240211963653565\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.12240211963653565  to: 0.1223827838897705\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.1223827838897705  to: 0.12236343622207642\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.12236343622207642  to: 0.12234405279159546\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.12234405279159546  to: 0.12232463359832764\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.12232463359832764  to: 0.12230517864227294\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.12230517864227294  to: 0.12228567600250244\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.12228567600250244  to: 0.12226613759994506\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.12226613759994506  to: 0.12224658727645873\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.12224658727645873  to: 0.12222700119018555\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.12222700119018555  to: 0.1222073793411255\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.1222073793411255  to: 0.12218787670135497\n",
      "Training iteration: 1445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12218787670135497  to: 0.1221684455871582\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.1221684455871582  to: 0.12214896678924561\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.12214896678924561  to: 0.12212946414947509\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.12212946414947509  to: 0.12210990190505981\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.12210990190505981  to: 0.12209030389785766\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.12209030389785766  to: 0.12207067012786865\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.12207067012786865  to: 0.12205097675323487\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.12205097675323487  to: 0.12203123569488525\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.12203123569488525  to: 0.12201144695281982\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.12201144695281982  to: 0.12199159860610961\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.12199159860610961  to: 0.1219717025756836\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.1219717025756836  to: 0.1219517707824707\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.1219517707824707  to: 0.12193176746368409\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.12193176746368409  to: 0.12191171646118164\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.12191171646118164  to: 0.12189161777496338\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.12189161777496338  to: 0.12187144756317139\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.12187144756317139  to: 0.12185124158859253\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.12185124158859253  to: 0.12183084487915039\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.12183084487915039  to: 0.12180979251861572\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.12180979251861572  to: 0.12178870439529418\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.12178870439529418  to: 0.12176755666732789\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.12176755666732789  to: 0.1217463731765747\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.1217463731765747  to: 0.12172511816024781\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.12172511816024781  to: 0.12170382738113403\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.12170382738113403  to: 0.12168247699737549\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.12168247699737549  to: 0.12166106700897217\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.12166106700897217  to: 0.12163960933685303\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.12163960933685303  to: 0.12161809206008911\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.12161809206008911  to: 0.12159651517868042\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.12159651517868042  to: 0.12157491445541382\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.12157491445541382  to: 0.12155325412750244\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.12155325412750244  to: 0.12153152227401734\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.12153152227401734  to: 0.12150976657867432\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.12150976657867432  to: 0.12148795127868653\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.12148795127868653  to: 0.12146610021591187\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.12146610021591187  to: 0.12144418954849243\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.12144418954849243  to: 0.12142223119735718\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.12142223119735718  to: 0.12140023708343506\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.12140023708343506  to: 0.12137817144393921\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.12137817144393921  to: 0.1213560700416565\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.1213560700416565  to: 0.121333909034729\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.121333909034729  to: 0.12131164073944092\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.12131164073944092  to: 0.12128932476043701\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.12128932476043701  to: 0.12126696109771729\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.12126696109771729  to: 0.12124452590942383\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.12124452590942383  to: 0.12122204303741455\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.12122204303741455  to: 0.12119951248168945\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.12119951248168945  to: 0.12117695808410645\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.12117695808410645  to: 0.12115437984466552\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.12115437984466552  to: 0.1211317539215088\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.1211317539215088  to: 0.12110909223556518\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.12110909223556518  to: 0.12108640670776367\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.12108640670776367  to: 0.1210636854171753\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.1210636854171753  to: 0.12104092836380005\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.12104092836380005  to: 0.1210181474685669\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.1210181474685669  to: 0.12099530696868896\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.12099530696868896  to: 0.12097245454788208\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.12097245454788208  to: 0.12094959020614623\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.12094959020614623  to: 0.12092673778533936\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.12092673778533936  to: 0.1209038496017456\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.1209038496017456  to: 0.1208809494972229\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.1208809494972229  to: 0.12085802555084228\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.12085802555084228  to: 0.12083508968353271\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.12083508968353271  to: 0.12081215381622315\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.12081215381622315  to: 0.12078920602798462\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.12078920602798462  to: 0.12076622247695923\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.12076622247695923  to: 0.1207432508468628\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.1207432508468628  to: 0.12072021961212158\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.12072021961212158  to: 0.12069718837738037\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.12069718837738037  to: 0.1206741452217102\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.1206741452217102  to: 0.12065105438232422\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.12065105438232422  to: 0.12062795162200927\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.12062795162200927  to: 0.12060482501983642\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.12060482501983642  to: 0.12058165073394775\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.12058165073394775  to: 0.12055845260620117\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.12055845260620117  to: 0.12053520679473877\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.12053520679473877  to: 0.12051194906234741\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.12051194906234741  to: 0.12048865556716919\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.12048865556716919  to: 0.12046530246734619\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.12046530246734619  to: 0.12044190168380738\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.12044190168380738  to: 0.12041847705841065\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.12041847705841065  to: 0.1203950047492981\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.1203950047492981  to: 0.12037149667739869\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.12037149667739869  to: 0.12034795284271241\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.12034795284271241  to: 0.12032457590103149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.12032457590103149  to: 0.1203012466430664\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.1203012466430664  to: 0.12027788162231445\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.12027788162231445  to: 0.12025448083877563\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.12025448083877563  to: 0.12023100852966309\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.12023100852966309  to: 0.12020748853683472\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.12020748853683472  to: 0.12018395662307739\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.12018395662307739  to: 0.12016035318374634\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.12016035318374634  to: 0.12013671398162842\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.12013671398162842  to: 0.12011301517486572\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.12011301517486572  to: 0.12008926868438721\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.12008926868438721  to: 0.12006547451019287\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.12006547451019287  to: 0.12004187107086181\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.12004187107086181  to: 0.12001827955245972\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.12001827955245972  to: 0.1199946403503418\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.1199946403503418  to: 0.11997095346450806\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.11997095346450806  to: 0.11994720697402954\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.11994720697402954  to: 0.11992342472076416\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.11992342472076416  to: 0.11989959478378295\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.11989959478378295  to: 0.11987574100494384\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.11987574100494384  to: 0.11985181570053101\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.11985181570053101  to: 0.11982786655426025\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.11982786655426025  to: 0.11980385780334472\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.11980385780334472  to: 0.11977980136871338\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.11977980136871338  to: 0.11975570917129516\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.11975570917129516  to: 0.1197315812110901\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.1197315812110901  to: 0.11970738172531128\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.11970738172531128  to: 0.1196831464767456\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.1196831464767456  to: 0.11965889930725097\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.11965889930725097  to: 0.11963456869125366\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.11963456869125366  to: 0.1196102261543274\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.1196102261543274  to: 0.11958582401275634\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.11958582401275634  to: 0.11956138610839843\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.11956138610839843  to: 0.11953688859939575\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.11953688859939575  to: 0.11951239109039306\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.11951239109039306  to: 0.11948788166046143\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.11948788166046143  to: 0.11946344375610352\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.11946344375610352  to: 0.11943919658660888\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.11943919658660888  to: 0.11941490173339844\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.11941490173339844  to: 0.11939058303833008\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.11939058303833008  to: 0.1193662405014038\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.1193662405014038  to: 0.11934186220169067\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.11934186220169067  to: 0.11931751966476441\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.11931751966476441  to: 0.1192931890487671\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.1192931890487671  to: 0.11926882266998291\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.11926882266998291  to: 0.11924440860748291\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.11924440860748291  to: 0.11921994686126709\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.11921994686126709  to: 0.11919567584991456\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.11919567584991456  to: 0.1191713571548462\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.1191713571548462  to: 0.11914697885513306\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.11914697885513306  to: 0.11912250518798828\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.11912250518798828  to: 0.11909795999526977\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.11909795999526977  to: 0.11907335519790649\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.11907335519790649  to: 0.11904869079589844\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.11904869079589844  to: 0.11902396678924561\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.11902396678924561  to: 0.1189991593360901\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.1189991593360901  to: 0.11897428035736084\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.11897428035736084  to: 0.11894935369491577\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.11894935369491577  to: 0.11892433166503906\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.11892433166503906  to: 0.11889922618865967\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.11889922618865967  to: 0.11887403726577758\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.11887403726577758  to: 0.11884920597076416\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.11884920597076416  to: 0.11882469654083253\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.11882469654083253  to: 0.11880043745040894\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.11880043745040894  to: 0.11877640485763549\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.11877640485763549  to: 0.11875256299972534\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.11875256299972534  to: 0.1187288999557495\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.1187288999557495  to: 0.11870536804199219\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.11870536804199219  to: 0.1186819314956665\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.1186819314956665  to: 0.11865856647491455\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.11865856647491455  to: 0.11863527297973633\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.11863527297973633  to: 0.11861200332641601\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.11861200332641601  to: 0.11858875751495361\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.11858875751495361  to: 0.11856546401977539\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.11856546401977539  to: 0.11854215860366821\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.11854215860366821  to: 0.11851880550384522\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.11851880550384522  to: 0.11849541664123535\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.11849541664123535  to: 0.1184719443321228\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.1184719443321228  to: 0.11844837665557861\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.11844837665557861  to: 0.11842483282089233\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.11842483282089233  to: 0.11840136051177978\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.11840136051177978  to: 0.11837780475616455\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.11837780475616455  to: 0.11835414171218872\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.11835414171218872  to: 0.11833035945892334\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.11833035945892334  to: 0.11830649375915528\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.11830649375915528  to: 0.1182824969291687\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.1182824969291687  to: 0.11825836896896362\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.11825836896896362  to: 0.11823412179946899\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.11823412179946899  to: 0.11820971965789795\n",
      "Training iteration: 1618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11820971965789795  to: 0.11818516254425049\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.11818516254425049  to: 0.11816041469573975\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.11816041469573975  to: 0.11813553571701049\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.11813553571701049  to: 0.11811034679412842\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.11811034679412842  to: 0.11808481216430664\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.11808481216430664  to: 0.11805909872055054\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.11805909872055054  to: 0.11803323030471802\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.11803323030471802  to: 0.11800718307495117\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.11800718307495117  to: 0.11798095703125\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.11798095703125  to: 0.11795458793640137\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.11795458793640137  to: 0.11792808771133423\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.11792808771133423  to: 0.11790143251419068\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.11790143251419068  to: 0.11787464618682861\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.11787464618682861  to: 0.11784770488739013\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.11784770488739013  to: 0.1178206443786621\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.1178206443786621  to: 0.11779344081878662\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.11779344081878662  to: 0.11776607036590576\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.11776607036590576  to: 0.11773856878280639\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.11773856878280639  to: 0.11771143674850464\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.11771143674850464  to: 0.11768471002578736\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.11768471002578736  to: 0.11765787601470948\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.11765787601470948  to: 0.11763231754302979\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.11763231754302979  to: 0.11760790348052978\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.11760790348052978  to: 0.11758439540863037\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.11758439540863037  to: 0.1175616979598999\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.1175616979598999  to: 0.11753971576690674\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.11753971576690674  to: 0.11751831769943237\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.11751831769943237  to: 0.11749740839004516\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.11749740839004516  to: 0.11747553348541259\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.11747553348541259  to: 0.11745271682739258\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.11745271682739258  to: 0.11743040084838867\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.11743040084838867  to: 0.1174085259437561\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.1174085259437561  to: 0.11738697290420533\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.11738697290420533  to: 0.11736568212509155\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.11736568212509155  to: 0.11734462976455688\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.11734462976455688  to: 0.11732367277145386\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.11732367277145386  to: 0.11730281114578248\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.11730281114578248  to: 0.11728196144104004\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.11728196144104004  to: 0.1172610878944397\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.1172610878944397  to: 0.11724011898040772\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.11724011898040772  to: 0.11721900701522828\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.11721900701522828  to: 0.11719769239425659\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.11719769239425659  to: 0.11717621088027955\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.11717621088027955  to: 0.11715447902679443\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.11715447902679443  to: 0.11713107824325561\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.11713107824325561  to: 0.11710615158081054\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.11710615158081054  to: 0.11707980632781982\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.11707980632781982  to: 0.11705213785171509\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.11705213785171509  to: 0.11702325344085693\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.11702325344085693  to: 0.11699470281600952\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.11699470281600952  to: 0.11696640253067017\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.11696640253067017  to: 0.116938316822052\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.116938316822052  to: 0.11691038608551026\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.11691038608551026  to: 0.1168825387954712\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.1168825387954712  to: 0.11685478687286377\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.11685478687286377  to: 0.11682714223861694\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.11682714223861694  to: 0.11679959297180176\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.11679959297180176  to: 0.1167720913887024\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.1167720913887024  to: 0.11674458980560302\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.11674458980560302  to: 0.11671711206436157\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.11671711206436157  to: 0.11668958663940429\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.11668958663940429  to: 0.11666343212127686\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.11666343212127686  to: 0.11663846969604492\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.11663846969604492  to: 0.11661455631256104\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.11661455631256104  to: 0.11659152507781982\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.11659152507781982  to: 0.11656925678253174\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.11656925678253174  to: 0.1165475845336914\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.1165475845336914  to: 0.11652636528015137\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.11652636528015137  to: 0.1165055513381958\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.1165055513381958  to: 0.11648499965667725\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.11648499965667725  to: 0.11646465063095093\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.11646465063095093  to: 0.11644437313079833\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.11644437313079833  to: 0.11642266511917114\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.11642266511917114  to: 0.11639959812164306\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.11639959812164306  to: 0.11637519598007202\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.11637519598007202  to: 0.1163495659828186\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.1163495659828186  to: 0.11632273197174073\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.11632273197174073  to: 0.11629482507705688\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.11629482507705688  to: 0.11626589298248291\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.11626589298248291  to: 0.11623603105545044\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.11623603105545044  to: 0.11620528697967529\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.11620528697967529  to: 0.11617376804351806\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.11617376804351806  to: 0.11614147424697877\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.11614147424697877  to: 0.11610852479934693\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.11610852479934693  to: 0.11607494354248046\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.11607494354248046  to: 0.11604080200195313\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.11604080200195313  to: 0.11600615978240966\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.11600615978240966  to: 0.11597106456756592\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.11597106456756592  to: 0.11593551635742187\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.11593551635742187  to: 0.11589963436126709\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.11589963436126709  to: 0.1158633828163147\n",
      "Training iteration: 1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1158633828163147  to: 0.11582739353179931\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.11582739353179931  to: 0.11579124927520752\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.11579124927520752  to: 0.11575490236282349\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.11575490236282349  to: 0.11571781635284424\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.11571781635284424  to: 0.11568059921264648\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.11568059921264648  to: 0.11564329862594605\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.11564329862594605  to: 0.11560592651367188\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.11560592651367188  to: 0.11556850671768189\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.11556850671768189  to: 0.11553099155426025\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.11553099155426025  to: 0.1154935121536255\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.1154935121536255  to: 0.11545602083206177\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.11545602083206177  to: 0.11541855335235596\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.11541855335235596  to: 0.11538110971450806\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.11538110971450806  to: 0.11534371376037597\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.11534371376037597  to: 0.11530636548995972\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.11530636548995972  to: 0.11526916027069092\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.11526916027069092  to: 0.11523201465606689\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.11523201465606689  to: 0.11519492864608764\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.11519492864608764  to: 0.115159010887146\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.115159010887146  to: 0.1151241421699524\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.1151241421699524  to: 0.11508935689926147\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.11508935689926147  to: 0.11505465507507324\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.11505465507507324  to: 0.11501998901367187\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.11501998901367187  to: 0.11498537063598632\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.11498537063598632  to: 0.11495072841644287\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.11495072841644287  to: 0.11491608619689941\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.11491608619689941  to: 0.11488142013549804\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.11488142013549804  to: 0.11484677791595459\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.11484677791595459  to: 0.11481209993362426\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.11481209993362426  to: 0.11477892398834229\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.11477892398834229  to: 0.11474705934524536\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.11474705934524536  to: 0.1147163987159729\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.1147163987159729  to: 0.11468681097030639\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.11468681097030639  to: 0.11465816497802735\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.11465816497802735  to: 0.11463029384613037\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.11463029384613037  to: 0.11460305452346801\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.11460305452346801  to: 0.11457639932632446\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.11457639932632446  to: 0.11455020904541016\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.11455020904541016  to: 0.11452440023422242\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.11452440023422242  to: 0.11449882984161378\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.11449882984161378  to: 0.11447347402572632\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.11447347402572632  to: 0.11444816589355469\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.11444816589355469  to: 0.11442283391952515\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.11442283391952515  to: 0.11439743041992187\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.11439743041992187  to: 0.114371919631958\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.114371919631958  to: 0.11434626579284668\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.11434626579284668  to: 0.11431891918182373\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.11431891918182373  to: 0.1142899751663208\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.1142899751663208  to: 0.11425958871841431\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.11425958871841431  to: 0.11422783136367798\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.11422783136367798  to: 0.11419481039047241\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.11419481039047241  to: 0.11416060924530029\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.11416060924530029  to: 0.1141253113746643\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.1141253113746643  to: 0.1140890121459961\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.1140890121459961  to: 0.1140519142150879\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.1140519142150879  to: 0.1140140414237976\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.1140140414237976  to: 0.11397554874420165\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.11397554874420165  to: 0.11393646001815796\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.11393646001815796  to: 0.11389691829681396\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.11389691829681396  to: 0.1138569474220276\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.1138569474220276  to: 0.11381658315658569\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.11381658315658569  to: 0.11377590894699097\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.11377590894699097  to: 0.11373497247695923\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.11373497247695923  to: 0.11369390487670898\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.11369390487670898  to: 0.11365272998809814\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.11365272998809814  to: 0.11361153125762939\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.11361153125762939  to: 0.11357030868530274\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.11357030868530274  to: 0.11352910995483398\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.11352910995483398  to: 0.11348793506622315\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.11348793506622315  to: 0.11344677209854126\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.11344677209854126  to: 0.11340569257736206\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.11340569257736206  to: 0.1133648157119751\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.1133648157119751  to: 0.11332412958145141\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.11332412958145141  to: 0.11328365802764892\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.11328365802764892  to: 0.11324341297149658\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.11324341297149658  to: 0.1132046937942505\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.1132046937942505  to: 0.11316736936569213\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.11316736936569213  to: 0.11313135623931884\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.11313135623931884  to: 0.1130964994430542\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.1130964994430542  to: 0.11306264400482177\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.11306264400482177  to: 0.11302963495254517\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.11302963495254517  to: 0.11299742460250854\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.11299742460250854  to: 0.11296591758728028\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.11296591758728028  to: 0.11293495893478393\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.11293495893478393  to: 0.11290439367294311\n",
      "Training iteration: 1794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11290439367294311  to: 0.11287410259246826\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.11287410259246826  to: 0.11284406185150146\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.11284406185150146  to: 0.11281419992446899\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.11281419992446899  to: 0.11278442144393921\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.11278442144393921  to: 0.1127547025680542\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.1127547025680542  to: 0.11272495985031128\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.11272495985031128  to: 0.11269512176513671\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.11269512176513671  to: 0.11266508102416992\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.11266508102416992  to: 0.11263477802276611\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.11263477802276611  to: 0.11260420083999634\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.11260420083999634  to: 0.11257331371307373\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.11257331371307373  to: 0.11254215240478516\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.11254215240478516  to: 0.11250936985015869\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.11250936985015869  to: 0.11247513294219971\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.11247513294219971  to: 0.11243951320648193\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.11243951320648193  to: 0.11240264177322387\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.11240264177322387  to: 0.112364661693573\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.112364661693573  to: 0.11232559680938721\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.11232559680938721  to: 0.11228559017181397\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.11228559017181397  to: 0.11224472522735596\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.11224472522735596  to: 0.11220433712005615\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.11220433712005615  to: 0.11216447353363038\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.11216447353363038  to: 0.11212506294250488\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.11212506294250488  to: 0.11208608150482177\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.11208608150482177  to: 0.11204746961593628\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.11204746961593628  to: 0.11200917959213257\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.11200917959213257  to: 0.11197118759155274\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.11197118759155274  to: 0.11193335056304932\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.11193335056304932  to: 0.11189566850662232\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.11189566850662232  to: 0.11185808181762695\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.11185808181762695  to: 0.11182057857513428\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.11182057857513428  to: 0.11178315877914428\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.11178315877914428  to: 0.11174581050872803\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.11174581050872803  to: 0.11170852184295654\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.11170852184295654  to: 0.11167123317718505\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.11167123317718505  to: 0.11163387298583985\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.11163387298583985  to: 0.11159641742706299\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.11159641742706299  to: 0.11155889034271241\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.11155889034271241  to: 0.111521315574646\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.111521315574646  to: 0.11148359775543212\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.11148359775543212  to: 0.1114457368850708\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.1114457368850708  to: 0.11140779256820679\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.11140779256820679  to: 0.11136854887008667\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.11136854887008667  to: 0.11132811307907105\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.11132811307907105  to: 0.11128653287887573\n",
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.11128653287887573  to: 0.11124515533447266\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.11124515533447266  to: 0.11120402812957764\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.11120402812957764  to: 0.11116315126419067\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.11116315126419067  to: 0.11112239360809326\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.11112239360809326  to: 0.11108186244964599\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.11108186244964599  to: 0.11104141473770142\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.11104141473770142  to: 0.11100107431411743\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.11100107431411743  to: 0.11096088886260987\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.11096088886260987  to: 0.11092084646224976\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.11092084646224976  to: 0.11088094711303711\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.11088094711303711  to: 0.1108411431312561\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.1108411431312561  to: 0.110801362991333\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.110801362991333  to: 0.11076154708862304\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.11076154708862304  to: 0.11072173118591308\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.11072173118591308  to: 0.11068187952041626\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.11068187952041626  to: 0.11064084768295288\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.11064084768295288  to: 0.1105987548828125\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.1105987548828125  to: 0.11055572032928467\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.11055572032928467  to: 0.11051304340362549\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.11051304340362549  to: 0.11047065258026123\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.11047065258026123  to: 0.11042852401733398\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.11042852401733398  to: 0.11038663387298583\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.11038663387298583  to: 0.11034501791000366\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.11034501791000366  to: 0.11030366420745849\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.11030366420745849  to: 0.11026252508163452\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.11026252508163452  to: 0.11022151708602905\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.11022151708602905  to: 0.11017935276031494\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.11017935276031494  to: 0.11013616323471069\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.11013616323471069  to: 0.11009336709976196\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.11009336709976196  to: 0.11005089282989503\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.11005089282989503  to: 0.11000750064849854\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.11000750064849854  to: 0.10996440649032593\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.10996440649032593  to: 0.1099216103553772\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.1099216103553772  to: 0.10987913608551025\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.10987913608551025  to: 0.10983574390411377\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.10983574390411377  to: 0.10979268550872803\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.10979268550872803  to: 0.10974867343902588\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.10974867343902588  to: 0.10970504283905029\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.10970504283905029  to: 0.10966188907623291\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.10966188907623291  to: 0.10961787700653076\n",
      "Training iteration: 1879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10961787700653076  to: 0.10957313776016235\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.10957313776016235  to: 0.10952895879745483\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.10952895879745483  to: 0.10948526859283447\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.10948526859283447  to: 0.10944197177886963\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.10944197177886963  to: 0.1093977928161621\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.1093977928161621  to: 0.10935283899307251\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.10935283899307251  to: 0.10930719375610351\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.10930719375610351  to: 0.10926101207733155\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.10926101207733155  to: 0.10921438932418823\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.10921438932418823  to: 0.10916739702224731\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.10916739702224731  to: 0.1091200828552246\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.1091200828552246  to: 0.10907250642776489\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.10907250642776489  to: 0.1090246319770813\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.1090246319770813  to: 0.10897649526596069\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.10897649526596069  to: 0.10892820358276367\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.10892820358276367  to: 0.10887975692749023\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.10887975692749023  to: 0.10883125066757202\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.10883125066757202  to: 0.10878270864486694\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.10878270864486694  to: 0.10873429775238037\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.10873429775238037  to: 0.10868602991104126\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.10868602991104126  to: 0.10863792896270752\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.10863792896270752  to: 0.10859009027481079\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.10859009027481079  to: 0.10854252576828002\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.10854252576828002  to: 0.1084951639175415\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.1084951639175415  to: 0.10844801664352417\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.10844801664352417  to: 0.10840113162994384\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.10840113162994384  to: 0.10835450887680054\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.10835450887680054  to: 0.10830821990966796\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.10830821990966796  to: 0.10826222896575928\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.10826222896575928  to: 0.10821672677993774\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.10821672677993774  to: 0.10817171335220337\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.10817171335220337  to: 0.10812586545944214\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.10812586545944214  to: 0.1080790638923645\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.1080790638923645  to: 0.1080314040184021\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.1080314040184021  to: 0.10798304080963135\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.10798304080963135  to: 0.10793404579162598\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.10793404579162598  to: 0.10788455009460449\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.10788455009460449  to: 0.10783476829528808\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.10783476829528808  to: 0.10778477191925048\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.10778477191925048  to: 0.10773612260818481\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.10773612260818481  to: 0.10768859386444092\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.10768859386444092  to: 0.10764214992523194\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.10764214992523194  to: 0.10759668350219727\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.10759668350219727  to: 0.10755212306976318\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.10755212306976318  to: 0.10750842094421387\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.10750842094421387  to: 0.10746560096740723\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.10746560096740723  to: 0.10742356777191162\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.10742356777191162  to: 0.1073805809020996\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.1073805809020996  to: 0.10733669996261597\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.10733669996261597  to: 0.10729200839996338\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.10729200839996338  to: 0.10724656581878662\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.10724656581878662  to: 0.10720212459564209\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.10720212459564209  to: 0.10715855360031128\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.10715855360031128  to: 0.10711566209793091\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.10711566209793091  to: 0.10707335472106934\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.10707335472106934  to: 0.10703184604644775\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.10703184604644775  to: 0.10699089765548705\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.10699089765548705  to: 0.10695040225982666\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.10695040225982666  to: 0.10690867900848389\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.10690867900848389  to: 0.1068656325340271\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.1068656325340271  to: 0.10682300329208375\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.10682300329208375  to: 0.10678075551986695\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.10678075551986695  to: 0.10673878192901612\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.10673878192901612  to: 0.10669760704040528\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.10669760704040528  to: 0.10665545463562012\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.10665545463562012  to: 0.10661354064941406\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.10661354064941406  to: 0.10657180547714233\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.10657180547714233  to: 0.10653021335601806\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.10653021335601806  to: 0.10648696422576905\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.10648696422576905  to: 0.10644403696060181\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.10644403696060181  to: 0.10640137195587158\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.10640137195587158  to: 0.10635877847671509\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.10635877847671509  to: 0.10631619691848755\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.10631619691848755  to: 0.1062735915184021\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.1062735915184021  to: 0.1062310814857483\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.1062310814857483  to: 0.106186842918396\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.106186842918396  to: 0.10614264011383057\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.10614264011383057  to: 0.10609841346740723\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.10609841346740723  to: 0.10605418682098389\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.10605418682098389  to: 0.10600988864898682\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.10600988864898682  to: 0.10596565008163453\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.10596565008163453  to: 0.10591962337493896\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.10591962337493896  to: 0.10587377548217773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.10587377548217773  to: 0.105827796459198\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.105827796459198  to: 0.10578172206878662\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.10578172206878662  to: 0.10573551654815674\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.10573551654815674  to: 0.10568928718566895\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.10568928718566895  to: 0.10564310550689697\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.10564310550689697  to: 0.10559691190719604\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.10559691190719604  to: 0.10555073022842407\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.10555073022842407  to: 0.1055044412612915\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.1055044412612915  to: 0.1054579496383667\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.1054579496383667  to: 0.10541119575500488\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.10541119575500488  to: 0.10536420345306396\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.10536420345306396  to: 0.10531699657440186\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.10531699657440186  to: 0.1052676796913147\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.1052676796913147  to: 0.10521845817565918\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.10521845817565918  to: 0.10516936779022217\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.10516936779022217  to: 0.1051203727722168\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.1051203727722168  to: 0.1050714373588562\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.1050714373588562  to: 0.10502243041992188\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.10502243041992188  to: 0.10497331619262695\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.10497331619262695  to: 0.10492409467697143\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.10492409467697143  to: 0.10487476587295533\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.10487476587295533  to: 0.10482534170150756\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.10482534170150756  to: 0.10477578639984131\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.10477578639984131  to: 0.10472663640975952\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.10472663640975952  to: 0.10467758178710937\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.10467758178710937  to: 0.10462853908538819\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.10462853908538819  to: 0.10457936525344849\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.10457936525344849  to: 0.10452998876571655\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.10452998876571655  to: 0.1044804334640503\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.1044804334640503  to: 0.10443071126937867\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.10443071126937867  to: 0.10438096523284912\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.10438096523284912  to: 0.10433118343353272\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.10433118343353272  to: 0.10428133010864257\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.10428133010864257  to: 0.10423123836517334\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.10423123836517334  to: 0.104180908203125\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.104180908203125  to: 0.1041303277015686\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.1041303277015686  to: 0.10407949686050415\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.10407949686050415  to: 0.10402836799621581\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.10402836799621581  to: 0.1039771556854248\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.1039771556854248  to: 0.10392582416534424\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.10392582416534424  to: 0.10387438535690308\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.10387438535690308  to: 0.10382283926010132\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.10382283926010132  to: 0.1037710189819336\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.1037710189819336  to: 0.10371890068054199\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.10371890068054199  to: 0.10366652011871338\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.10366652011871338  to: 0.10361387729644775\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.10361387729644775  to: 0.10356097221374512\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.10356097221374512  to: 0.1035078763961792\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.1035078763961792  to: 0.10345475673675537\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.10345475673675537  to: 0.10340158939361573\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.10340158939361573  to: 0.10334837436676025\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.10334837436676025  to: 0.10329509973526001\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.10329509973526001  to: 0.1032415509223938\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.1032415509223938  to: 0.10318775177001953\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.10318775177001953  to: 0.1031337022781372\n",
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.1031337022781372  to: 0.10307939052581787\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.10307939052581787  to: 0.10302484035491943\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.10302484035491943  to: 0.10297024250030518\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.10297024250030518  to: 0.1029155969619751\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.1029155969619751  to: 0.10286086797714233\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.10286086797714233  to: 0.10280587673187255\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.10280587673187255  to: 0.10275059938430786\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.10275059938430786  to: 0.10269505977630615\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.10269505977630615  to: 0.10263946056365966\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.10263946056365966  to: 0.10258380174636841\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.10258380174636841  to: 0.10252783298492432\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.10252783298492432  to: 0.10247160196304321\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.10247160196304321  to: 0.10241508483886719\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.10241508483886719  to: 0.10235850811004639\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.10235850811004639  to: 0.10230187177658082\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.10230187177658082  to: 0.10224515199661255\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.10224515199661255  to: 0.1021881341934204\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.1021881341934204  to: 0.10213079452514648\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.10213079452514648  to: 0.10207316875457764\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.10207316875457764  to: 0.10201526880264282\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.10201526880264282  to: 0.10195729732513428\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.10195729732513428  to: 0.10189926624298096\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.10189926624298096  to: 0.10184112787246705\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.10184112787246705  to: 0.10178267955780029\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.10178267955780029  to: 0.10172393321990966\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.10172393321990966  to: 0.10166487693786622\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.10166487693786622  to: 0.10160573720932006\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.10160573720932006  to: 0.10154653787612915\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.10154653787612915  to: 0.10148565769195557\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.10148565769195557  to: 0.10142323970794678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.10142323970794678  to: 0.10135968923568725\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.10135968923568725  to: 0.10129512548446655\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.10129512548446655  to: 0.10122967958450317\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.10122967958450317  to: 0.10116344690322876\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.10116344690322876  to: 0.10109657049179077\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.10109657049179077  to: 0.10102912187576293\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.10102912187576293  to: 0.10096096992492676\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.10096096992492676  to: 0.10089223384857178\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.10089223384857178  to: 0.10082268714904785\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.10082268714904785  to: 0.10075267553329467\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.10075267553329467  to: 0.1006822943687439\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.1006822943687439  to: 0.10061166286468506\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.10061166286468506  to: 0.1005408525466919\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.1005408525466919  to: 0.10046989917755127\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.10046989917755127  to: 0.10039891004562378\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.10039891004562378  to: 0.10032765865325928\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.10032765865325928  to: 0.10025632381439209\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.10025632381439209  to: 0.10018538236618042\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.10018538236618042  to: 0.10011463165283203\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.10011463165283203  to: 0.10004405975341797\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.10004405975341797  to: 0.09997371435165406\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.09997371435165406  to: 0.09990354776382446\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.09990354776382446  to: 0.09983360171318054\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.09983360171318054  to: 0.09976357221603394\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.09976357221603394  to: 0.09969350695610046\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.09969350695610046  to: 0.0996234118938446\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.0996234118938446  to: 0.09955328702926636\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.09955328702926636  to: 0.09948339462280273\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.09948339462280273  to: 0.09941375851631165\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.09941375851631165  to: 0.09934433102607727\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.09934433102607727  to: 0.09927511215209961\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.09927511215209961  to: 0.09920603632926941\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.09920603632926941  to: 0.09913656115531921\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.09913656115531921  to: 0.09906673431396484\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.09906673431396484  to: 0.0989965558052063\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.0989965558052063  to: 0.09892594218254089\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.09892594218254089  to: 0.09885520935058593\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.09885520935058593  to: 0.09878438711166382\n",
      "Training iteration: 2085\n",
      "Improved validation loss from: 0.09878438711166382  to: 0.09871344566345215\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.09871344566345215  to: 0.09864240884780884\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.09864240884780884  to: 0.09857100248336792\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.09857100248336792  to: 0.09849926233291625\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.09849926233291625  to: 0.09842721819877624\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.09842721819877624  to: 0.09835489988327026\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.09835489988327026  to: 0.09828264117240906\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.09828264117240906  to: 0.09821049571037292\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.09821049571037292  to: 0.09813838005065918\n",
      "Training iteration: 2094\n",
      "Improved validation loss from: 0.09813838005065918  to: 0.0980663001537323\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.0980663001537323  to: 0.09799396395683288\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.09799396395683288  to: 0.09792140126228333\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.09792140126228333  to: 0.09784863591194153\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.09784863591194153  to: 0.09777568578720093\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.09777568578720093  to: 0.09770288467407226\n",
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.09770288467407226  to: 0.09763019680976867\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.09763019680976867  to: 0.09755762219429016\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.09755762219429016  to: 0.0974848747253418\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.0974848747253418  to: 0.09741195440292358\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.09741195440292358  to: 0.09733888506889343\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.09733888506889343  to: 0.09726611971855163\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.09726611971855163  to: 0.09719361066818237\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.09719361066818237  to: 0.09712125658988953\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.09712125658988953  to: 0.09704872965812683\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.09704872965812683  to: 0.09697604179382324\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.09697604179382324  to: 0.0969032108783722\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.0969032108783722  to: 0.09683025479316712\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.09683025479316712  to: 0.0967572033405304\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.0967572033405304  to: 0.09668434262275696\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.09668434262275696  to: 0.09661170244216918\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.09661170244216918  to: 0.09653922915458679\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.09653922915458679  to: 0.09646692276000976\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.09646692276000976  to: 0.09639445543289185\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.09639445543289185  to: 0.09632208943367004\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.09632208943367004  to: 0.09624933004379273\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.09624933004379273  to: 0.09617568850517273\n",
      "Training iteration: 2121\n",
      "Improved validation loss from: 0.09617568850517273  to: 0.09610173106193542\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.09610173106193542  to: 0.09602783322334289\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.09602783322334289  to: 0.09595394134521484\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.09595394134521484  to: 0.0958800494670868\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.0958800494670868  to: 0.09580610394477844\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.09580610394477844  to: 0.09573175311088562\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.09573175311088562  to: 0.09565699696540833\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.09565699696540833  to: 0.09558186531066895\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.09558186531066895  to: 0.09550637006759644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.09550637006759644  to: 0.0954305350780487\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.0954305350780487  to: 0.09535435438156128\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.09535435438156128  to: 0.09527819752693176\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.09527819752693176  to: 0.09520204663276673\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.09520204663276673  to: 0.09512587785720825\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.09512587785720825  to: 0.095049649477005\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.095049649477005  to: 0.09497336149215699\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.09497336149215699  to: 0.09489662051200867\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.09489662051200867  to: 0.09481900930404663\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.09481900930404663  to: 0.09474056959152222\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.09474056959152222  to: 0.09466137886047363\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.09466137886047363  to: 0.09458149075508118\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.09458149075508118  to: 0.09450095295906066\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.09450095295906066  to: 0.09441982507705689\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: 0.09441982507705689  to: 0.0943385124206543\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.0943385124206543  to: 0.09425705075263976\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.09425705075263976  to: 0.09417551755905151\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.09417551755905151  to: 0.09409383535385132\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.09409383535385132  to: 0.09401200413703918\n",
      "Training iteration: 2149\n",
      "Improved validation loss from: 0.09401200413703918  to: 0.09392965435981751\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.09392965435981751  to: 0.09384679794311523\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.09384679794311523  to: 0.09376350641250611\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.09376350641250611  to: 0.09367979168891907\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.09367979168891907  to: 0.09359566569328308\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.09359566569328308  to: 0.09351117014884949\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.09351117014884949  to: 0.09342634081840515\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.09342634081840515  to: 0.09334158897399902\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.09334158897399902  to: 0.0932568907737732\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.0932568907737732  to: 0.09317222833633423\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.09317222833633423  to: 0.09308758974075318\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.09308758974075318  to: 0.09300251007080078\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.09300251007080078  to: 0.09291704297065735\n",
      "Training iteration: 2162\n",
      "Improved validation loss from: 0.09291704297065735  to: 0.09283117055892945\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.09283117055892945  to: 0.09274494051933288\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.09274494051933288  to: 0.09265767335891724\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.09265767335891724  to: 0.09256946444511413\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.09256946444511413  to: 0.09248083829879761\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.09248083829879761  to: 0.09239192008972168\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.09239192008972168  to: 0.0923023521900177\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.0923023521900177  to: 0.0922119140625\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.0922119140625  to: 0.09212117195129395\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: 0.09212117195129395  to: 0.0920298457145691\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.0920298457145691  to: 0.09193804860115051\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.09193804860115051  to: 0.09184630513191223\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.09184630513191223  to: 0.09175426363945008\n",
      "Training iteration: 2175\n",
      "Improved validation loss from: 0.09175426363945008  to: 0.09166204333305358\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.09166204333305358  to: 0.09156970977783203\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.09156970977783203  to: 0.09147781133651733\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.09147781133651733  to: 0.09138630032539367\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.09138630032539367  to: 0.09129474759101867\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.09129474759101867  to: 0.09120138287544251\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.09120138287544251  to: 0.09110645055770875\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.09110645055770875  to: 0.0910101592540741\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.0910101592540741  to: 0.09091315269470215\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.09091315269470215  to: 0.09081559181213379\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.09081559181213379  to: 0.09071758389472961\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.09071758389472961  to: 0.09061880111694336\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.09061880111694336  to: 0.09051984548568726\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.09051984548568726  to: 0.09042035937309265\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.09042035937309265  to: 0.09032041430473328\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.09032041430473328  to: 0.0902206301689148\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.0902206301689148  to: 0.09012103080749512\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.09012103080749512  to: 0.09002116918563843\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.09002116918563843  to: 0.08992111086845397\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.08992111086845397  to: 0.08982089757919312\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.08982089757919312  to: 0.08972107768058776\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.08972107768058776  to: 0.08962160348892212\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.08962160348892212  to: 0.08952198028564454\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.08952198028564454  to: 0.08942223787307739\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.08942223787307739  to: 0.0893223762512207\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.0893223762512207  to: 0.08922290802001953\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.08922290802001953  to: 0.08912330865859985\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.08912330865859985  to: 0.08902405500411988\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.08902405500411988  to: 0.08892461657524109\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.08892461657524109  to: 0.08882501721382141\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.08882501721382141  to: 0.08872524499893189\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.08872524499893189  to: 0.08862582445144654\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.08862582445144654  to: 0.08852623105049133\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.08852623105049133  to: 0.08842645883560181\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.08842645883560181  to: 0.0883270263671875\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.0883270263671875  to: 0.08822738528251647\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.08822738528251647  to: 0.08812756538391113\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.08812756538391113  to: 0.08802756071090698\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.08802756071090698  to: 0.08792793154716491\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.08792793154716491  to: 0.08782811164855957\n",
      "Training iteration: 2215\n",
      "Improved validation loss from: 0.08782811164855957  to: 0.08772813081741333\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.08772813081741333  to: 0.08762852549552917\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.08762852549552917  to: 0.08752873539924622\n",
      "Training iteration: 2218\n",
      "Improved validation loss from: 0.08752873539924622  to: 0.08742879033088684\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.08742879033088684  to: 0.08732872009277344\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.08732872009277344  to: 0.0872300922870636\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.0872300922870636  to: 0.08713297843933106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2222\n",
      "Improved validation loss from: 0.08713297843933106  to: 0.08703585863113403\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.08703585863113403  to: 0.08693822026252747\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.08693822026252747  to: 0.08683918118476867\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.08683918118476867  to: 0.08673860430717469\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.08673860430717469  to: 0.08663676977157593\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.08663676977157593  to: 0.0865369200706482\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.0865369200706482  to: 0.08643924593925476\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.08643924593925476  to: 0.08633947372436523\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.08633947372436523  to: 0.08623711466789245\n",
      "Training iteration: 2231\n",
      "Improved validation loss from: 0.08623711466789245  to: 0.08613203167915344\n",
      "Training iteration: 2232\n",
      "Improved validation loss from: 0.08613203167915344  to: 0.08602498769760132\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.08602498769760132  to: 0.08592122197151184\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.08592122197151184  to: 0.08582012057304382\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.08582012057304382  to: 0.08572195172309875\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.08572195172309875  to: 0.0856199860572815\n",
      "Training iteration: 2237\n",
      "Improved validation loss from: 0.0856199860572815  to: 0.08551370501518249\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.08551370501518249  to: 0.08540328145027161\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.08540328145027161  to: 0.08529592752456665\n",
      "Training iteration: 2240\n",
      "Improved validation loss from: 0.08529592752456665  to: 0.08519102334976196\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.08519102334976196  to: 0.08508869409561157\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.08508869409561157  to: 0.08498016595840455\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.08498016595840455  to: 0.08486601710319519\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.08486601710319519  to: 0.0847557544708252\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.0847557544708252  to: 0.08464854955673218\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.08464854955673218  to: 0.08454442024230957\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.08454442024230957  to: 0.0844325840473175\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.0844325840473175  to: 0.08431336283683777\n",
      "Training iteration: 2249\n",
      "Improved validation loss from: 0.08431336283683777  to: 0.0841982364654541\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.0841982364654541  to: 0.08408687710762024\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.08408687710762024  to: 0.08397752642631531\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.08397752642631531  to: 0.08387001156806946\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.08387001156806946  to: 0.08376411199569703\n",
      "Training iteration: 2254\n",
      "Improved validation loss from: 0.08376411199569703  to: 0.08364639282226563\n",
      "Training iteration: 2255\n",
      "Improved validation loss from: 0.08364639282226563  to: 0.08353073000907899\n",
      "Training iteration: 2256\n",
      "Improved validation loss from: 0.08353073000907899  to: 0.08341758847236633\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.08341758847236633  to: 0.08330672979354858\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.08330672979354858  to: 0.08319795727729798\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.08319795727729798  to: 0.08309094309806823\n",
      "Training iteration: 2260\n",
      "Improved validation loss from: 0.08309094309806823  to: 0.08298540115356445\n",
      "Training iteration: 2261\n",
      "Improved validation loss from: 0.08298540115356445  to: 0.08286404609680176\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.08286404609680176  to: 0.08274433016777039\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.08274433016777039  to: 0.08262627720832824\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.08262627720832824  to: 0.08250986337661743\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.08250986337661743  to: 0.08239505887031555\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.08239505887031555  to: 0.08228179812431335\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.08228179812431335  to: 0.08216997385025024\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: 0.08216997385025024  to: 0.08205941915512086\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.08205941915512086  to: 0.0819499671459198\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.0819499671459198  to: 0.08184139132499695\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: 0.08184139132499695  to: 0.08173343539237976\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.08173343539237976  to: 0.08162641525268555\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.08162641525268555  to: 0.08151999711990357\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.08151999711990357  to: 0.08139079213142394\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.08139079213142394  to: 0.0812645435333252\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.0812645435333252  to: 0.08114148378372192\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.08114148378372192  to: 0.0810217022895813\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.0810217022895813  to: 0.08090521693229676\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.08090521693229676  to: 0.08079193234443664\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.08079193234443664  to: 0.08068169355392456\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: 0.08068169355392456  to: 0.08057435750961303\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.08057435750961303  to: 0.0804697334766388\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.0804697334766388  to: 0.08036737442016602\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.08036737442016602  to: 0.08026691675186157\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.08026691675186157  to: 0.08016794919967651\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.08016794919967651  to: 0.08007003664970398\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.08007003664970398  to: 0.07997273206710816\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.07997273206710816  to: 0.079875648021698\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.079875648021698  to: 0.07977834939956666\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.07977834939956666  to: 0.07968052625656127\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.07968052625656127  to: 0.07958194017410278\n",
      "Training iteration: 2292\n",
      "Improved validation loss from: 0.07958194017410278  to: 0.0794824242591858\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.0794824242591858  to: 0.07938193082809449\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.07938193082809449  to: 0.07928052544593811\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.07928052544593811  to: 0.07917956709861755\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.07917956709861755  to: 0.07907902598381042\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.07907902598381042  to: 0.07897881269454957\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.07897881269454957  to: 0.07887874841690064\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.07887874841690064  to: 0.07877870798110961\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.07877870798110961  to: 0.07867860198020935\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.07867860198020935  to: 0.07857831716537475\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.07857831716537475  to: 0.07847779989242554\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.07847779989242554  to: 0.07837670445442199\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.07837670445442199  to: 0.07827507257461548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.07827507257461548  to: 0.07817298173904419\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.07817298173904419  to: 0.07807053327560425\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.07807053327560425  to: 0.07796787619590759\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.07796787619590759  to: 0.07786518335342407\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.07786518335342407  to: 0.07776261568069458\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.07776261568069458  to: 0.07766036987304688\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.07766036987304688  to: 0.07755841016769409\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.07755841016769409  to: 0.07745693922042847\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.07745693922042847  to: 0.07735612392425537\n",
      "Training iteration: 2314\n",
      "Improved validation loss from: 0.07735612392425537  to: 0.07725611925125123\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.07725611925125123  to: 0.07715701460838317\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.07715701460838317  to: 0.07705889344215393\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.07705889344215393  to: 0.07696177363395691\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.07696177363395691  to: 0.07686564326286316\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.07686564326286316  to: 0.07677044868469238\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.07677044868469238  to: 0.07667611837387085\n",
      "Training iteration: 2321\n",
      "Improved validation loss from: 0.07667611837387085  to: 0.07658255696296692\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.07658255696296692  to: 0.07648965120315551\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.07648965120315551  to: 0.07639726400375366\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.07639726400375366  to: 0.07630528211593628\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.07630528211593628  to: 0.07621359229087829\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.07621359229087829  to: 0.0761220932006836\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.0761220932006836  to: 0.07603071928024292\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.07603071928024292  to: 0.07593852877616883\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.07593852877616883  to: 0.07584561109542846\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.07584561109542846  to: 0.07575209736824036\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.07575209736824036  to: 0.07565810084342957\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.07565810084342957  to: 0.0755638062953949\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.0755638062953949  to: 0.07546935081481934\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.07546935081481934  to: 0.07536848187446595\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.07536848187446595  to: 0.07526900768280029\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.07526900768280029  to: 0.0751712441444397\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.0751712441444397  to: 0.07507532835006714\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.07507532835006714  to: 0.07498133182525635\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.07498133182525635  to: 0.07488913536071777\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.07488913536071777  to: 0.074798583984375\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.074798583984375  to: 0.07470942735671997\n",
      "Training iteration: 2342\n",
      "Improved validation loss from: 0.07470942735671997  to: 0.07462111115455627\n",
      "Training iteration: 2343\n",
      "Improved validation loss from: 0.07462111115455627  to: 0.07452728748321533\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.07452728748321533  to: 0.07442887425422669\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.07442887425422669  to: 0.07432736158370971\n",
      "Training iteration: 2346\n",
      "Improved validation loss from: 0.07432736158370971  to: 0.07422438859939576\n",
      "Training iteration: 2347\n",
      "Improved validation loss from: 0.07422438859939576  to: 0.07412147521972656\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.07412147521972656  to: 0.0740243673324585\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.0740243673324585  to: 0.0739323377609253\n",
      "Training iteration: 2350\n",
      "Improved validation loss from: 0.0739323377609253  to: 0.07384487390518188\n",
      "Training iteration: 2351\n",
      "Improved validation loss from: 0.07384487390518188  to: 0.07375664114952088\n",
      "Training iteration: 2352\n",
      "Improved validation loss from: 0.07375664114952088  to: 0.07366746664047241\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.07366746664047241  to: 0.0735772728919983\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.0735772728919983  to: 0.0734860897064209\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.0734860897064209  to: 0.0733940601348877\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.0733940601348877  to: 0.07330144047737122\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.07330144047737122  to: 0.0732085108757019\n",
      "Training iteration: 2358\n",
      "Improved validation loss from: 0.0732085108757019  to: 0.07311559319496155\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.07311559319496155  to: 0.07301486730575561\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.07301486730575561  to: 0.07290903329849244\n",
      "Training iteration: 2361\n",
      "Improved validation loss from: 0.07290903329849244  to: 0.07280755043029785\n",
      "Training iteration: 2362\n",
      "Improved validation loss from: 0.07280755043029785  to: 0.07270966768264771\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.07270966768264771  to: 0.07261493802070618\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.07261493802070618  to: 0.0725231111049652\n",
      "Training iteration: 2365\n",
      "Improved validation loss from: 0.0725231111049652  to: 0.0724270761013031\n",
      "Training iteration: 2366\n",
      "Improved validation loss from: 0.0724270761013031  to: 0.07232736349105835\n",
      "Training iteration: 2367\n",
      "Improved validation loss from: 0.07232736349105835  to: 0.07222825288772583\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.07222825288772583  to: 0.07213009595870971\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.07213009595870971  to: 0.07203315496444702\n",
      "Training iteration: 2370\n",
      "Improved validation loss from: 0.07203315496444702  to: 0.07193752527236938\n",
      "Training iteration: 2371\n",
      "Improved validation loss from: 0.07193752527236938  to: 0.07184329032897949\n",
      "Training iteration: 2372\n",
      "Improved validation loss from: 0.07184329032897949  to: 0.07175041437149048\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.07175041437149048  to: 0.07165890336036682\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.07165890336036682  to: 0.07156875133514404\n",
      "Training iteration: 2375\n",
      "Improved validation loss from: 0.07156875133514404  to: 0.07147995233535767\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.07147995233535767  to: 0.07139250040054321\n",
      "Training iteration: 2377\n",
      "Improved validation loss from: 0.07139250040054321  to: 0.07130638360977173\n",
      "Training iteration: 2378\n",
      "Improved validation loss from: 0.07130638360977173  to: 0.07122151255607605\n",
      "Training iteration: 2379\n",
      "Improved validation loss from: 0.07122151255607605  to: 0.0711377739906311\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.0711377739906311  to: 0.07105503678321838\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.07105503678321838  to: 0.07097322344779969\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.07097322344779969  to: 0.07089234590530395\n",
      "Training iteration: 2383\n",
      "Improved validation loss from: 0.07089234590530395  to: 0.07081246972084046\n",
      "Training iteration: 2384\n",
      "Improved validation loss from: 0.07081246972084046  to: 0.0707336962223053\n",
      "Training iteration: 2385\n",
      "Improved validation loss from: 0.0707336962223053  to: 0.07065619826316834\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.07065619826316834  to: 0.07058022618293762\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.07058022618293762  to: 0.0705059289932251\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.0705059289932251  to: 0.07043336629867554\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.07043336629867554  to: 0.07036253213882446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2390\n",
      "Improved validation loss from: 0.07036253213882446  to: 0.07029334902763366\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.07029334902763366  to: 0.0702256977558136\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.0702256977558136  to: 0.07015937566757202\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.07015937566757202  to: 0.07009416818618774\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.07009416818618774  to: 0.07002981901168823\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.07002981901168823  to: 0.06996604204177856\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.06996604204177856  to: 0.06990252733230591\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.06990252733230591  to: 0.06983900666236878\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.06983900666236878  to: 0.06977214813232421\n",
      "Training iteration: 2399\n",
      "Improved validation loss from: 0.06977214813232421  to: 0.06970213055610656\n",
      "Training iteration: 2400\n",
      "Improved validation loss from: 0.06970213055610656  to: 0.06962924003601074\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: 0.06962924003601074  to: 0.06955376863479615\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.06955376863479615  to: 0.06947096586227416\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.06947096586227416  to: 0.06938706040382385\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: 0.06938706040382385  to: 0.06930245161056518\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: 0.06930245161056518  to: 0.06921755075454712\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.06921755075454712  to: 0.06913261413574219\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.06913261413574219  to: 0.06904792189598083\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.06904792189598083  to: 0.0689636528491974\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.0689636528491974  to: 0.06887992024421692\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.06887992024421692  to: 0.0687968373298645\n",
      "Training iteration: 2411\n",
      "Improved validation loss from: 0.0687968373298645  to: 0.06871442794799805\n",
      "Training iteration: 2412\n",
      "Improved validation loss from: 0.06871442794799805  to: 0.06863274574279785\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.06863274574279785  to: 0.06855185627937317\n",
      "Training iteration: 2414\n",
      "Improved validation loss from: 0.06855185627937317  to: 0.06847184300422668\n",
      "Training iteration: 2415\n",
      "Improved validation loss from: 0.06847184300422668  to: 0.06839281916618348\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: 0.06839281916618348  to: 0.06831492185592651\n",
      "Training iteration: 2417\n",
      "Improved validation loss from: 0.06831492185592651  to: 0.06823827624320984\n",
      "Training iteration: 2418\n",
      "Improved validation loss from: 0.06823827624320984  to: 0.06816301941871643\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.06816301941871643  to: 0.06808916926383972\n",
      "Training iteration: 2420\n",
      "Improved validation loss from: 0.06808916926383972  to: 0.06801677346229554\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.06801677346229554  to: 0.06794575452804566\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.06794575452804566  to: 0.06787605285644531\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.06787605285644531  to: 0.06780753135681153\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.06780753135681153  to: 0.06774004697799682\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.06774004697799682  to: 0.06767431497573853\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: 0.06767431497573853  to: 0.06761009097099305\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.06761009097099305  to: 0.06754713654518127\n",
      "Training iteration: 2428\n",
      "Improved validation loss from: 0.06754713654518127  to: 0.0674852192401886\n",
      "Training iteration: 2429\n",
      "Improved validation loss from: 0.0674852192401886  to: 0.06742414236068725\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: 0.06742414236068725  to: 0.06736376285552978\n",
      "Training iteration: 2431\n",
      "Improved validation loss from: 0.06736376285552978  to: 0.06729776263237\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: 0.06729776263237  to: 0.06722687482833863\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.06722687482833863  to: 0.0671518623828888\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: 0.0671518623828888  to: 0.06707956790924072\n",
      "Training iteration: 2435\n",
      "Improved validation loss from: 0.06707956790924072  to: 0.06700971722602844\n",
      "Training iteration: 2436\n",
      "Improved validation loss from: 0.06700971722602844  to: 0.06694207191467286\n",
      "Training iteration: 2437\n",
      "Improved validation loss from: 0.06694207191467286  to: 0.06687637567520141\n",
      "Training iteration: 2438\n",
      "Improved validation loss from: 0.06687637567520141  to: 0.06680583953857422\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.06680583953857422  to: 0.06673070788383484\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.06673070788383484  to: 0.06665031313896179\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.06665031313896179  to: 0.06656618714332581\n",
      "Training iteration: 2442\n",
      "Improved validation loss from: 0.06656618714332581  to: 0.06647909879684448\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.06647909879684448  to: 0.06638687252998351\n",
      "Training iteration: 2444\n",
      "Improved validation loss from: 0.06638687252998351  to: 0.06629517674446106\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.06629517674446106  to: 0.06620984077453614\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.06620984077453614  to: 0.06612493991851806\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.06612493991851806  to: 0.06604159474372864\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.06604159474372864  to: 0.06595730185508727\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.06595730185508727  to: 0.06587045788764953\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.06587045788764953  to: 0.0657811164855957\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.0657811164855957  to: 0.06569061279296876\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.06569061279296876  to: 0.065598863363266\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.065598863363266  to: 0.0655052125453949\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.0655052125453949  to: 0.06540371179580688\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.06540371179580688  to: 0.06529959440231323\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.06529959440231323  to: 0.06519872546195984\n",
      "Training iteration: 2457\n",
      "Improved validation loss from: 0.06519872546195984  to: 0.06509977579116821\n",
      "Training iteration: 2458\n",
      "Improved validation loss from: 0.06509977579116821  to: 0.06500066518783569\n",
      "Training iteration: 2459\n",
      "Improved validation loss from: 0.06500066518783569  to: 0.06490186452865601\n",
      "Training iteration: 2460\n",
      "Improved validation loss from: 0.06490186452865601  to: 0.06480512022972107\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.06480512022972107  to: 0.06471213102340698\n",
      "Training iteration: 2462\n",
      "Improved validation loss from: 0.06471213102340698  to: 0.06462159752845764\n",
      "Training iteration: 2463\n",
      "Improved validation loss from: 0.06462159752845764  to: 0.06453483700752258\n",
      "Training iteration: 2464\n",
      "Improved validation loss from: 0.06453483700752258  to: 0.0644547998905182\n",
      "Training iteration: 2465\n",
      "Improved validation loss from: 0.0644547998905182  to: 0.06437841653823853\n",
      "Training iteration: 2466\n",
      "Improved validation loss from: 0.06437841653823853  to: 0.06430207490921021\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.06430207490921021  to: 0.06422544717788696\n",
      "Training iteration: 2468\n",
      "Improved validation loss from: 0.06422544717788696  to: 0.06415090560913086\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.06415090560913086  to: 0.06408101916313172\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.06408101916313172  to: 0.06401603817939758\n",
      "Training iteration: 2471\n",
      "Improved validation loss from: 0.06401603817939758  to: 0.06395472288131714\n",
      "Training iteration: 2472\n",
      "Improved validation loss from: 0.06395472288131714  to: 0.06389952301979065\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.06389952301979065  to: 0.06384602785110474\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.06384602785110474  to: 0.06378967165946961\n",
      "Training iteration: 2475\n",
      "Improved validation loss from: 0.06378967165946961  to: 0.06372939348220825\n",
      "Training iteration: 2476\n",
      "Improved validation loss from: 0.06372939348220825  to: 0.0636675238609314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2477\n",
      "Improved validation loss from: 0.0636675238609314  to: 0.06360958814620972\n",
      "Training iteration: 2478\n",
      "Improved validation loss from: 0.06360958814620972  to: 0.06355428695678711\n",
      "Training iteration: 2479\n",
      "Improved validation loss from: 0.06355428695678711  to: 0.0634964108467102\n",
      "Training iteration: 2480\n",
      "Improved validation loss from: 0.0634964108467102  to: 0.06343671679496765\n",
      "Training iteration: 2481\n",
      "Improved validation loss from: 0.06343671679496765  to: 0.06336475014686585\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.06336475014686585  to: 0.06328018903732299\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.06328018903732299  to: 0.06318938136100768\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.06318938136100768  to: 0.06310150027275085\n",
      "Training iteration: 2485\n",
      "Improved validation loss from: 0.06310150027275085  to: 0.06301928758621216\n",
      "Training iteration: 2486\n",
      "Improved validation loss from: 0.06301928758621216  to: 0.06294296383857727\n",
      "Training iteration: 2487\n",
      "Improved validation loss from: 0.06294296383857727  to: 0.06286724805831909\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.06286724805831909  to: 0.06278997659683228\n",
      "Training iteration: 2489\n",
      "Improved validation loss from: 0.06278997659683228  to: 0.06270560026168823\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.06270560026168823  to: 0.06262081861495972\n",
      "Training iteration: 2491\n",
      "Improved validation loss from: 0.06262081861495972  to: 0.06253539323806763\n",
      "Training iteration: 2492\n",
      "Improved validation loss from: 0.06253539323806763  to: 0.0624430239200592\n",
      "Training iteration: 2493\n",
      "Improved validation loss from: 0.0624430239200592  to: 0.062345093488693236\n",
      "Training iteration: 2494\n",
      "Improved validation loss from: 0.062345093488693236  to: 0.062250882387161255\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.062250882387161255  to: 0.06216467022895813\n",
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.06216467022895813  to: 0.062086641788482666\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.062086641788482666  to: 0.06201070547103882\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.06201070547103882  to: 0.061934030055999754\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.061934030055999754  to: 0.06185731291770935\n",
      "Training iteration: 2500\n",
      "Improved validation loss from: 0.06185731291770935  to: 0.06178323626518249\n",
      "Training iteration: 2501\n",
      "Improved validation loss from: 0.06178323626518249  to: 0.06171563267707825\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.06171563267707825  to: 0.06165406107902527\n",
      "Training iteration: 2503\n",
      "Improved validation loss from: 0.06165406107902527  to: 0.06159213185310364\n",
      "Training iteration: 2504\n",
      "Improved validation loss from: 0.06159213185310364  to: 0.06152656674385071\n",
      "Training iteration: 2505\n",
      "Improved validation loss from: 0.06152656674385071  to: 0.061457931995391846\n",
      "Training iteration: 2506\n",
      "Improved validation loss from: 0.061457931995391846  to: 0.06138936877250671\n",
      "Training iteration: 2507\n",
      "Improved validation loss from: 0.06138936877250671  to: 0.061325520277023315\n",
      "Training iteration: 2508\n",
      "Improved validation loss from: 0.061325520277023315  to: 0.061266422271728516\n",
      "Training iteration: 2509\n",
      "Improved validation loss from: 0.061266422271728516  to: 0.06120542287826538\n",
      "Training iteration: 2510\n",
      "Improved validation loss from: 0.06120542287826538  to: 0.06113976836204529\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.06113976836204529  to: 0.06107136607170105\n",
      "Training iteration: 2512\n",
      "Improved validation loss from: 0.06107136607170105  to: 0.06100444197654724\n",
      "Training iteration: 2513\n",
      "Improved validation loss from: 0.06100444197654724  to: 0.06094123125076294\n",
      "Training iteration: 2514\n",
      "Improved validation loss from: 0.06094123125076294  to: 0.06088171005249023\n",
      "Training iteration: 2515\n",
      "Improved validation loss from: 0.06088171005249023  to: 0.06082764267921448\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.06082764267921448  to: 0.06076842546463013\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.06076842546463013  to: 0.06070515513420105\n",
      "Training iteration: 2518\n",
      "Improved validation loss from: 0.06070515513420105  to: 0.06064162254333496\n",
      "Training iteration: 2519\n",
      "Improved validation loss from: 0.06064162254333496  to: 0.06057990193367004\n",
      "Training iteration: 2520\n",
      "Improved validation loss from: 0.06057990193367004  to: 0.060517793893814086\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.060517793893814086  to: 0.06045465469360352\n",
      "Training iteration: 2522\n",
      "Improved validation loss from: 0.06045465469360352  to: 0.06039040684700012\n",
      "Training iteration: 2523\n",
      "Improved validation loss from: 0.06039040684700012  to: 0.06032899022102356\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: 0.06032899022102356  to: 0.06026543378829956\n",
      "Training iteration: 2525\n",
      "Improved validation loss from: 0.06026543378829956  to: 0.060196316242218016\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.060196316242218016  to: 0.06012290120124817\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.06012290120124817  to: 0.0600490391254425\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: 0.0600490391254425  to: 0.059977734088897706\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.059977734088897706  to: 0.059883284568786624\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.059883284568786624  to: 0.059768950939178465\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.059768950939178465  to: 0.059639155864715576\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: 0.059639155864715576  to: 0.059529060125350954\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.059529060125350954  to: 0.059396564960479736\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.059396564960479736  to: 0.05924275517463684\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.05924275517463684  to: 0.0591183602809906\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.0591183602809906  to: 0.05902141332626343\n",
      "Training iteration: 2537\n",
      "Improved validation loss from: 0.05902141332626343  to: 0.05889230966567993\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.05889230966567993  to: 0.058747225999832155\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: 0.058747225999832155  to: 0.058639132976531984\n",
      "Training iteration: 2540\n",
      "Improved validation loss from: 0.058639132976531984  to: 0.05857752561569214\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.05857752561569214  to: 0.05848418474197388\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.05848418474197388  to: 0.05835153460502625\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.05835153460502625  to: 0.058253628015518186\n",
      "Training iteration: 2544\n",
      "Improved validation loss from: 0.058253628015518186  to: 0.05818712115287781\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: 0.05818712115287781  to: 0.05817142128944397\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.05817142128944397  to: 0.0581208348274231\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.0581208348274231  to: 0.058017891645431516\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: 0.058017891645431516  to: 0.05792857408523559\n",
      "Training iteration: 2549\n",
      "Improved validation loss from: 0.05792857408523559  to: 0.05787097215652466\n",
      "Training iteration: 2550\n",
      "Improved validation loss from: 0.05787097215652466  to: 0.05783554315567017\n",
      "Training iteration: 2551\n",
      "Improved validation loss from: 0.05783554315567017  to: 0.05779742002487183\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: 0.05779742002487183  to: 0.057722413539886476\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: 0.057722413539886476  to: 0.05763726830482483\n",
      "Training iteration: 2554\n",
      "Improved validation loss from: 0.05763726830482483  to: 0.05755928158760071\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: 0.05755928158760071  to: 0.0575030505657196\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: 0.0575030505657196  to: 0.05743927955627441\n",
      "Training iteration: 2557\n",
      "Improved validation loss from: 0.05743927955627441  to: 0.057345062494277954\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: 0.057345062494277954  to: 0.057247626781463626\n",
      "Training iteration: 2559\n",
      "Improved validation loss from: 0.057247626781463626  to: 0.05718650817871094\n",
      "Training iteration: 2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.05718650817871094  to: 0.05713414549827576\n",
      "Training iteration: 2561\n",
      "Improved validation loss from: 0.05713414549827576  to: 0.05703323483467102\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: 0.05703323483467102  to: 0.05694222450256348\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.05694222450256348  to: 0.05688400268554687\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.05688400268554687  to: 0.05683058500289917\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.05683058500289917  to: 0.05677331686019897\n",
      "Training iteration: 2566\n",
      "Improved validation loss from: 0.05677331686019897  to: 0.05668427348136902\n",
      "Training iteration: 2567\n",
      "Improved validation loss from: 0.05668427348136902  to: 0.05662051439285278\n",
      "Training iteration: 2568\n",
      "Improved validation loss from: 0.05662051439285278  to: 0.056569254398345946\n",
      "Training iteration: 2569\n",
      "Improved validation loss from: 0.056569254398345946  to: 0.05652564764022827\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.05652564764022827  to: 0.05644768476486206\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.05644768476486206  to: 0.05636295080184937\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: 0.05636295080184937  to: 0.05627967715263367\n",
      "Training iteration: 2573\n",
      "Improved validation loss from: 0.05627967715263367  to: 0.05622433423995972\n",
      "Training iteration: 2574\n",
      "Improved validation loss from: 0.05622433423995972  to: 0.05614625215530396\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.05614625215530396  to: 0.05604954957962036\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.05604954957962036  to: 0.05597618818283081\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: 0.05597618818283081  to: 0.05592736005783081\n",
      "Training iteration: 2578\n",
      "Improved validation loss from: 0.05592736005783081  to: 0.05585139393806458\n",
      "Training iteration: 2579\n",
      "Improved validation loss from: 0.05585139393806458  to: 0.05575724840164185\n",
      "Training iteration: 2580\n",
      "Improved validation loss from: 0.05575724840164185  to: 0.05569028854370117\n",
      "Training iteration: 2581\n",
      "Improved validation loss from: 0.05569028854370117  to: 0.055656802654266355\n",
      "Training iteration: 2582\n",
      "Improved validation loss from: 0.055656802654266355  to: 0.055504602193832395\n",
      "Training iteration: 2583\n",
      "Improved validation loss from: 0.055504602193832395  to: 0.055403441190719604\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.055403441190719604  to: 0.05531657338142395\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.05531657338142395  to: 0.055258524417877194\n",
      "Training iteration: 2586\n",
      "Improved validation loss from: 0.055258524417877194  to: 0.0551052451133728\n",
      "Training iteration: 2587\n",
      "Improved validation loss from: 0.0551052451133728  to: 0.05499611496925354\n",
      "Training iteration: 2588\n",
      "Improved validation loss from: 0.05499611496925354  to: 0.054921519756317136\n",
      "Training iteration: 2589\n",
      "Improved validation loss from: 0.054921519756317136  to: 0.05486955046653748\n",
      "Training iteration: 2590\n",
      "Improved validation loss from: 0.05486955046653748  to: 0.054709500074386595\n",
      "Training iteration: 2591\n",
      "Improved validation loss from: 0.054709500074386595  to: 0.054636812210083006\n",
      "Training iteration: 2592\n",
      "Improved validation loss from: 0.054636812210083006  to: 0.05462226867675781\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.054645806550979614\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.05462226867675781  to: 0.054516923427581784\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.054516923427581784  to: 0.05446792840957641\n",
      "Training iteration: 2596\n",
      "Improved validation loss from: 0.05446792840957641  to: 0.054411137104034425\n",
      "Training iteration: 2597\n",
      "Improved validation loss from: 0.054411137104034425  to: 0.05441086292266846\n",
      "Training iteration: 2598\n",
      "Improved validation loss from: 0.05441086292266846  to: 0.054309523105621337\n",
      "Training iteration: 2599\n",
      "Improved validation loss from: 0.054309523105621337  to: 0.05416854023933411\n",
      "Training iteration: 2600\n",
      "Improved validation loss from: 0.05416854023933411  to: 0.054076659679412845\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.054076659679412845  to: 0.054052716493606566\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.054052716493606566  to: 0.05398783683776855\n",
      "Training iteration: 2603\n",
      "Improved validation loss from: 0.05398783683776855  to: 0.05384305715560913\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.05384305715560913  to: 0.05378035306930542\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.05378132462501526\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.05378035306930542  to: 0.05374700427055359\n",
      "Training iteration: 2607\n",
      "Improved validation loss from: 0.05374700427055359  to: 0.05360342860221863\n",
      "Training iteration: 2608\n",
      "Improved validation loss from: 0.05360342860221863  to: 0.05354796648025513\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.05354796648025513  to: 0.05353800058364868\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.05353800058364868  to: 0.05351572036743164\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.05351572036743164  to: 0.053411030769348146\n",
      "Training iteration: 2612\n",
      "Improved validation loss from: 0.053411030769348146  to: 0.0533488392829895\n",
      "Training iteration: 2613\n",
      "Improved validation loss from: 0.0533488392829895  to: 0.05331762433052063\n",
      "Training iteration: 2614\n",
      "Improved validation loss from: 0.05331762433052063  to: 0.053303873538970946\n",
      "Training iteration: 2615\n",
      "Improved validation loss from: 0.053303873538970946  to: 0.0531660795211792\n",
      "Training iteration: 2616\n",
      "Improved validation loss from: 0.0531660795211792  to: 0.053091681003570555\n",
      "Training iteration: 2617\n",
      "Improved validation loss from: 0.053091681003570555  to: 0.05306477546691894\n",
      "Training iteration: 2618\n",
      "Improved validation loss from: 0.05306477546691894  to: 0.05288066864013672\n",
      "Training iteration: 2619\n",
      "Improved validation loss from: 0.05288066864013672  to: 0.052747714519500735\n",
      "Training iteration: 2620\n",
      "Improved validation loss from: 0.052747714519500735  to: 0.05268450975418091\n",
      "Training iteration: 2621\n",
      "Improved validation loss from: 0.05268450975418091  to: 0.05260695815086365\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.05260695815086365  to: 0.05253632068634033\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.05253632068634033  to: 0.05247029662132263\n",
      "Training iteration: 2624\n",
      "Improved validation loss from: 0.05247029662132263  to: 0.05241117477416992\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.0524178683757782\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.05243946313858032\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.05252484083175659\n",
      "Training iteration: 2628\n",
      "Improved validation loss from: 0.05241117477416992  to: 0.05240131616592407\n",
      "Training iteration: 2629\n",
      "Improved validation loss from: 0.05240131616592407  to: 0.052334898710250856\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.052334898710250856  to: 0.052274465560913086\n",
      "Training iteration: 2631\n",
      "Improved validation loss from: 0.052274465560913086  to: 0.05219570398330688\n",
      "Training iteration: 2632\n",
      "Improved validation loss from: 0.05219570398330688  to: 0.05210908651351929\n",
      "Training iteration: 2633\n",
      "Improved validation loss from: 0.05210908651351929  to: 0.05204890370368957\n",
      "Training iteration: 2634\n",
      "Improved validation loss from: 0.05204890370368957  to: 0.05198031663894653\n",
      "Training iteration: 2635\n",
      "Improved validation loss from: 0.05198031663894653  to: 0.05188910961151123\n",
      "Training iteration: 2636\n",
      "Improved validation loss from: 0.05188910961151123  to: 0.051803338527679446\n",
      "Training iteration: 2637\n",
      "Improved validation loss from: 0.051803338527679446  to: 0.051784247159957886\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.051805704832077026\n",
      "Training iteration: 2639\n",
      "Improved validation loss from: 0.051784247159957886  to: 0.05160707235336304\n",
      "Training iteration: 2640\n",
      "Improved validation loss from: 0.05160707235336304  to: 0.051553255319595336\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.05157790184020996\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.051553255319595336  to: 0.05152093768119812\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.05152093768119812  to: 0.051447951793670656\n",
      "Training iteration: 2644\n",
      "Improved validation loss from: 0.051447951793670656  to: 0.051369976997375486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.05138410329818725\n",
      "Training iteration: 2646\n",
      "Improved validation loss from: 0.051369976997375486  to: 0.05132803916931152\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.05132803916931152  to: 0.051253223419189455\n",
      "Training iteration: 2648\n",
      "Improved validation loss from: 0.051253223419189455  to: 0.0511214017868042\n",
      "Training iteration: 2649\n",
      "Improved validation loss from: 0.0511214017868042  to: 0.05110871195793152\n",
      "Training iteration: 2650\n",
      "Improved validation loss from: 0.05110871195793152  to: 0.05101844072341919\n",
      "Training iteration: 2651\n",
      "Improved validation loss from: 0.05101844072341919  to: 0.050931680202484134\n",
      "Training iteration: 2652\n",
      "Improved validation loss from: 0.050931680202484134  to: 0.050906187295913695\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.05092986822128296\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.050933510065078735\n",
      "Training iteration: 2655\n",
      "Improved validation loss from: 0.050906187295913695  to: 0.05088592767715454\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.0508950412273407\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.05098987817764282\n",
      "Training iteration: 2658\n",
      "Improved validation loss from: 0.05088592767715454  to: 0.05077996850013733\n",
      "Training iteration: 2659\n",
      "Improved validation loss from: 0.05077996850013733  to: 0.050640588998794554\n",
      "Training iteration: 2660\n",
      "Improved validation loss from: 0.050640588998794554  to: 0.05056530237197876\n",
      "Training iteration: 2661\n",
      "Improved validation loss from: 0.05056530237197876  to: 0.05048478841781616\n",
      "Training iteration: 2662\n",
      "Improved validation loss from: 0.05048478841781616  to: 0.05037773847579956\n",
      "Training iteration: 2663\n",
      "Improved validation loss from: 0.05037773847579956  to: 0.050279629230499265\n",
      "Training iteration: 2664\n",
      "Improved validation loss from: 0.050279629230499265  to: 0.05027546882629395\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.05037372708320618\n",
      "Training iteration: 2666\n",
      "Improved validation loss from: 0.05027546882629395  to: 0.05017629861831665\n",
      "Training iteration: 2667\n",
      "Improved validation loss from: 0.05017629861831665  to: 0.05014532208442688\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.05017839670181275\n",
      "Training iteration: 2669\n",
      "Improved validation loss from: 0.05014532208442688  to: 0.050139439105987546\n",
      "Training iteration: 2670\n",
      "Improved validation loss from: 0.050139439105987546  to: 0.05003127455711365\n",
      "Training iteration: 2671\n",
      "Improved validation loss from: 0.05003127455711365  to: 0.049937430024147036\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): 0.04996649622917175\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.049937430024147036  to: 0.049801602959632874\n",
      "Training iteration: 2674\n",
      "Improved validation loss from: 0.049801602959632874  to: 0.049704399704933164\n",
      "Training iteration: 2675\n",
      "Improved validation loss from: 0.049704399704933164  to: 0.04967848658561706\n",
      "Training iteration: 2676\n",
      "Improved validation loss from: 0.04967848658561706  to: 0.04965822696685791\n",
      "Training iteration: 2677\n",
      "Improved validation loss from: 0.04965822696685791  to: 0.049634695053100586\n",
      "Training iteration: 2678\n",
      "Improved validation loss from: 0.049634695053100586  to: 0.04961878657341003\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.04967354834079742\n",
      "Training iteration: 2680\n",
      "Improved validation loss from: 0.04961878657341003  to: 0.049547380208969115\n",
      "Training iteration: 2681\n",
      "Improved validation loss from: 0.049547380208969115  to: 0.049464407563209536\n",
      "Training iteration: 2682\n",
      "Improved validation loss from: 0.049464407563209536  to: 0.049443292617797854\n",
      "Training iteration: 2683\n",
      "Improved validation loss from: 0.049443292617797854  to: 0.04939001202583313\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.04939001202583313  to: 0.049296697974205016\n",
      "Training iteration: 2685\n",
      "Improved validation loss from: 0.049296697974205016  to: 0.04928716719150543\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.049394255876541136\n",
      "Training iteration: 2687\n",
      "Improved validation loss from: 0.04928716719150543  to: 0.04921444952487945\n",
      "Training iteration: 2688\n",
      "Improved validation loss from: 0.04921444952487945  to: 0.049133199453353885\n",
      "Training iteration: 2689\n",
      "Improved validation loss from: 0.049133199453353885  to: 0.04908910691738129\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.049133673310279846\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.04911320805549622\n",
      "Training iteration: 2692\n",
      "Improved validation loss from: 0.04908910691738129  to: 0.04898600578308106\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.04898600578308106  to: 0.048974251747131346\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.049058142304420474\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.049018746614456175\n",
      "Training iteration: 2696\n",
      "Improved validation loss from: 0.048974251747131346  to: 0.0488123744726181\n",
      "Training iteration: 2697\n",
      "Validation loss (no improvement): 0.04887681901454925\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.04891654849052429\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.0488669216632843\n",
      "Training iteration: 2700\n",
      "Improved validation loss from: 0.0488123744726181  to: 0.04870949685573578\n",
      "Training iteration: 2701\n",
      "Improved validation loss from: 0.04870949685573578  to: 0.04855145514011383\n",
      "Training iteration: 2702\n",
      "Improved validation loss from: 0.04855145514011383  to: 0.048517951369285585\n",
      "Training iteration: 2703\n",
      "Improved validation loss from: 0.048517951369285585  to: 0.04848583340644837\n",
      "Training iteration: 2704\n",
      "Improved validation loss from: 0.04848583340644837  to: 0.048447436094284056\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.048447436094284056  to: 0.04841124415397644\n",
      "Training iteration: 2706\n",
      "Improved validation loss from: 0.04841124415397644  to: 0.048388606309890746\n",
      "Training iteration: 2707\n",
      "Improved validation loss from: 0.048388606309890746  to: 0.04832179546356201\n",
      "Training iteration: 2708\n",
      "Improved validation loss from: 0.04832179546356201  to: 0.04826882481575012\n",
      "Training iteration: 2709\n",
      "Improved validation loss from: 0.04826882481575012  to: 0.048228129744529724\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.04824668765068054\n",
      "Training iteration: 2711\n",
      "Improved validation loss from: 0.048228129744529724  to: 0.047999757528305056\n",
      "Training iteration: 2712\n",
      "Improved validation loss from: 0.047999757528305056  to: 0.04794731140136719\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.0479595810174942\n",
      "Training iteration: 2714\n",
      "Validation loss (no improvement): 0.047954064607620236\n",
      "Training iteration: 2715\n",
      "Improved validation loss from: 0.04794731140136719  to: 0.04785700738430023\n",
      "Training iteration: 2716\n",
      "Improved validation loss from: 0.04785700738430023  to: 0.04785480499267578\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.04788138270378113\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.04793455004692078\n",
      "Training iteration: 2719\n",
      "Improved validation loss from: 0.04785480499267578  to: 0.04776869714260101\n",
      "Training iteration: 2720\n",
      "Improved validation loss from: 0.04776869714260101  to: 0.04762345850467682\n",
      "Training iteration: 2721\n",
      "Improved validation loss from: 0.04762345850467682  to: 0.04752492010593414\n",
      "Training iteration: 2722\n",
      "Improved validation loss from: 0.04752492010593414  to: 0.047514572739601135\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.047514572739601135  to: 0.04738015532493591\n",
      "Training iteration: 2724\n",
      "Improved validation loss from: 0.04738015532493591  to: 0.047276681661605834\n",
      "Training iteration: 2725\n",
      "Improved validation loss from: 0.047276681661605834  to: 0.04725071489810943\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.04734562039375305\n",
      "Training iteration: 2727\n",
      "Improved validation loss from: 0.04725071489810943  to: 0.0471371740102768\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.0471371740102768  to: 0.04703054428100586\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.04704707264900208\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.0470711886882782\n",
      "Training iteration: 2731\n",
      "Improved validation loss from: 0.04703054428100586  to: 0.04686852395534515\n",
      "Training iteration: 2732\n",
      "Improved validation loss from: 0.04686852395534515  to: 0.04684851765632629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.047014981508255005\n",
      "Training iteration: 2734\n",
      "Improved validation loss from: 0.04684851765632629  to: 0.04676049649715423\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.04676049649715423  to: 0.046689844131469725\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.046719294786453244\n",
      "Training iteration: 2737\n",
      "Improved validation loss from: 0.046689844131469725  to: 0.04662914872169495\n",
      "Training iteration: 2738\n",
      "Improved validation loss from: 0.04662914872169495  to: 0.04657377600669861\n",
      "Training iteration: 2739\n",
      "Improved validation loss from: 0.04657377600669861  to: 0.04656294882297516\n",
      "Training iteration: 2740\n",
      "Improved validation loss from: 0.04656294882297516  to: 0.046474489569664004\n",
      "Training iteration: 2741\n",
      "Improved validation loss from: 0.046474489569664004  to: 0.046345099806785583\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.04634747505187988\n",
      "Training iteration: 2743\n",
      "Improved validation loss from: 0.046345099806785583  to: 0.04630416333675384\n",
      "Training iteration: 2744\n",
      "Improved validation loss from: 0.04630416333675384  to: 0.04620567262172699\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.04622801840305328\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.04639685750007629\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.04621850848197937\n",
      "Training iteration: 2748\n",
      "Improved validation loss from: 0.04620567262172699  to: 0.04609928727149963\n",
      "Training iteration: 2749\n",
      "Improved validation loss from: 0.04609928727149963  to: 0.0460238516330719\n",
      "Training iteration: 2750\n",
      "Improved validation loss from: 0.0460238516330719  to: 0.04602024555206299\n",
      "Training iteration: 2751\n",
      "Improved validation loss from: 0.04602024555206299  to: 0.045995664596557614\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.045995664596557614  to: 0.04576937258243561\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.04576937258243561  to: 0.04564503133296967\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.04564503133296967  to: 0.04558983743190766\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.04563546776771545\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.0456214964389801\n",
      "Training iteration: 2757\n",
      "Improved validation loss from: 0.04558983743190766  to: 0.04555090963840484\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.04557221531867981\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.045597496628761294\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.04571083188056946\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.04578786790370941\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.04570865035057068\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.04569827914237976\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.045738449692726134\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.04574643075466156\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.04555116593837738\n",
      "Training iteration: 2767\n",
      "Improved validation loss from: 0.04555090963840484  to: 0.04538554549217224\n",
      "Training iteration: 2768\n",
      "Improved validation loss from: 0.04538554549217224  to: 0.045369958877563475\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.04544812142848968\n",
      "Training iteration: 2770\n",
      "Improved validation loss from: 0.045369958877563475  to: 0.04532428681850433\n",
      "Training iteration: 2771\n",
      "Improved validation loss from: 0.04532428681850433  to: 0.045288640260696414\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.045373272895812986\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.045432037115097045\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.04532319903373718\n",
      "Training iteration: 2775\n",
      "Improved validation loss from: 0.045288640260696414  to: 0.04524208605289459\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.04530495703220368\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.04535531997680664\n",
      "Training iteration: 2778\n",
      "Improved validation loss from: 0.04524208605289459  to: 0.045147189497947694\n",
      "Training iteration: 2779\n",
      "Improved validation loss from: 0.045147189497947694  to: 0.045136579871177675\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.045164471864700316\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.04514325261116028\n",
      "Training iteration: 2782\n",
      "Improved validation loss from: 0.045136579871177675  to: 0.045067507028579715\n",
      "Training iteration: 2783\n",
      "Improved validation loss from: 0.045067507028579715  to: 0.0449755996465683\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.04498295187950134\n",
      "Training iteration: 2785\n",
      "Improved validation loss from: 0.0449755996465683  to: 0.04486896097660065\n",
      "Training iteration: 2786\n",
      "Improved validation loss from: 0.04486896097660065  to: 0.04470624029636383\n",
      "Training iteration: 2787\n",
      "Improved validation loss from: 0.04470624029636383  to: 0.04469946920871735\n",
      "Training iteration: 2788\n",
      "Improved validation loss from: 0.04469946920871735  to: 0.044540539383888245\n",
      "Training iteration: 2789\n",
      "Improved validation loss from: 0.044540539383888245  to: 0.04448322355747223\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.04452723562717438\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.04450210630893707\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.04450541138648987\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.04455035626888275\n",
      "Training iteration: 2794\n",
      "Improved validation loss from: 0.04448322355747223  to: 0.04446338713169098\n",
      "Training iteration: 2795\n",
      "Improved validation loss from: 0.04446338713169098  to: 0.044453397393226624\n",
      "Training iteration: 2796\n",
      "Improved validation loss from: 0.044453397393226624  to: 0.04443268775939942\n",
      "Training iteration: 2797\n",
      "Improved validation loss from: 0.04443268775939942  to: 0.04431279301643372\n",
      "Training iteration: 2798\n",
      "Improved validation loss from: 0.04431279301643372  to: 0.04424118101596832\n",
      "Training iteration: 2799\n",
      "Improved validation loss from: 0.04424118101596832  to: 0.04419041275978088\n",
      "Training iteration: 2800\n",
      "Improved validation loss from: 0.04419041275978088  to: 0.04411386549472809\n",
      "Training iteration: 2801\n",
      "Improved validation loss from: 0.04411386549472809  to: 0.044111508131027224\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.044125279784202574\n",
      "Training iteration: 2803\n",
      "Improved validation loss from: 0.044111508131027224  to: 0.04407946467399597\n",
      "Training iteration: 2804\n",
      "Improved validation loss from: 0.04407946467399597  to: 0.044060739874839785\n",
      "Training iteration: 2805\n",
      "Improved validation loss from: 0.044060739874839785  to: 0.044031071662902835\n",
      "Training iteration: 2806\n",
      "Improved validation loss from: 0.044031071662902835  to: 0.04385649263858795\n",
      "Training iteration: 2807\n",
      "Improved validation loss from: 0.04385649263858795  to: 0.04380083084106445\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): 0.04383112788200379\n",
      "Training iteration: 2809\n",
      "Improved validation loss from: 0.04380083084106445  to: 0.043794688582420346\n",
      "Training iteration: 2810\n",
      "Improved validation loss from: 0.043794688582420346  to: 0.04373254179954529\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.043801575899124146\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): 0.0438487708568573\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.043742552399635315\n",
      "Training iteration: 2814\n",
      "Improved validation loss from: 0.04373254179954529  to: 0.04360329210758209\n",
      "Training iteration: 2815\n",
      "Improved validation loss from: 0.04360329210758209  to: 0.04357889592647553\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.0436274915933609\n",
      "Training iteration: 2817\n",
      "Improved validation loss from: 0.04357889592647553  to: 0.04353184700012207\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.043610531091690066\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.0436626136302948\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.0437141478061676\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.04359559416770935\n",
      "Training iteration: 2822\n",
      "Improved validation loss from: 0.04353184700012207  to: 0.04342443346977234\n",
      "Training iteration: 2823\n",
      "Improved validation loss from: 0.04342443346977234  to: 0.0433980792760849\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.04353383183479309\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.043509107828140256\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.043473029136657716\n",
      "Training iteration: 2827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.043507567048072814\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.04351857602596283\n",
      "Training iteration: 2829\n",
      "Validation loss (no improvement): 0.04350603520870209\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.0434147834777832\n",
      "Training iteration: 2831\n",
      "Improved validation loss from: 0.0433980792760849  to: 0.043385854363441466\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): 0.0434075266122818\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.04352113306522369\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.04347709715366364\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.04344924092292786\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.04342071413993835\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): 0.0434753268957138\n",
      "Training iteration: 2838\n",
      "Improved validation loss from: 0.043385854363441466  to: 0.04338434338569641\n",
      "Training iteration: 2839\n",
      "Improved validation loss from: 0.04338434338569641  to: 0.04324632585048675\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.04330773949623108\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.04348801076412201\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.0434177964925766\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.04341515600681305\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.04350493848323822\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.04346162676811218\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.043408203125\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.04333222508430481\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.043288740515708926\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.043264302611351016\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.0432835578918457\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.0432751327753067\n",
      "Training iteration: 2852\n",
      "Improved validation loss from: 0.04324632585048675  to: 0.04320954382419586\n",
      "Training iteration: 2853\n",
      "Improved validation loss from: 0.04320954382419586  to: 0.04315647482872009\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.04315647482872009  to: 0.043154916167259215\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.043154916167259215  to: 0.043064460158348083\n",
      "Training iteration: 2856\n",
      "Improved validation loss from: 0.043064460158348083  to: 0.043050751090049744\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.0430997759103775\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): 0.043089276552200316\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.043117040395736696\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.043120890855789185\n",
      "Training iteration: 2861\n",
      "Improved validation loss from: 0.043050751090049744  to: 0.043044295907020566\n",
      "Training iteration: 2862\n",
      "Improved validation loss from: 0.043044295907020566  to: 0.0428887277841568\n",
      "Training iteration: 2863\n",
      "Improved validation loss from: 0.0428887277841568  to: 0.04278661608695984\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.04279430508613587\n",
      "Training iteration: 2865\n",
      "Improved validation loss from: 0.04278661608695984  to: 0.042746105790138246\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): 0.0427478164434433\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.04280722737312317\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.04278443455696106\n",
      "Training iteration: 2869\n",
      "Improved validation loss from: 0.042746105790138246  to: 0.0426779180765152\n",
      "Training iteration: 2870\n",
      "Improved validation loss from: 0.0426779180765152  to: 0.04261215627193451\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): 0.04266331791877746\n",
      "Training iteration: 2872\n",
      "Improved validation loss from: 0.04261215627193451  to: 0.04255073070526123\n",
      "Training iteration: 2873\n",
      "Improved validation loss from: 0.04255073070526123  to: 0.042515867948532106\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.04252249598503113\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.042529338598251344\n",
      "Training iteration: 2876\n",
      "Validation loss (no improvement): 0.04252564311027527\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.042515867948532106  to: 0.04249675869941712\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.04249675869941712  to: 0.04245277941226959\n",
      "Training iteration: 2879\n",
      "Improved validation loss from: 0.04245277941226959  to: 0.04243769645690918\n",
      "Training iteration: 2880\n",
      "Improved validation loss from: 0.04243769645690918  to: 0.04234262406826019\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.04238302111625671\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.04241460263729095\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.04241707921028137\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.04250991344451904\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): 0.0424983412027359\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.04238898754119873\n",
      "Training iteration: 2887\n",
      "Improved validation loss from: 0.04234262406826019  to: 0.04221300184726715\n",
      "Training iteration: 2888\n",
      "Improved validation loss from: 0.04221300184726715  to: 0.042154303193092345\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.04220486581325531\n",
      "Training iteration: 2890\n",
      "Improved validation loss from: 0.042154303193092345  to: 0.04213944971561432\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): 0.04217448830604553\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.04230836927890778\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.042355161905288694\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.04229055345058441\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.04222866892814636\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.04223528504371643\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.042317891120910646\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.04218451380729675\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.04220709204673767\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.04237756133079529\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.0424147754907608\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): 0.042360329627990724\n",
      "Training iteration: 2903\n",
      "Validation loss (no improvement): 0.04226531982421875\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.04221982955932617\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.042159050703048706\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: 0.04213944971561432  to: 0.042130890488624576\n",
      "Training iteration: 2907\n",
      "Validation loss (no improvement): 0.04213905930519104\n",
      "Training iteration: 2908\n",
      "Validation loss (no improvement): 0.0421710342168808\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): 0.04214426577091217\n",
      "Training iteration: 2910\n",
      "Improved validation loss from: 0.042130890488624576  to: 0.042055445909500125\n",
      "Training iteration: 2911\n",
      "Improved validation loss from: 0.042055445909500125  to: 0.0419971376657486\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): 0.042018857598304746\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): 0.04215145707130432\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.0419971376657486  to: 0.0419764906167984\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): 0.04201036095619202\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.042204856872558594\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.04228649139404297\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.042124581336975095\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.041979193687438965\n",
      "Training iteration: 2920\n",
      "Improved validation loss from: 0.0419764906167984  to: 0.04189941883087158\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.04189941883087158  to: 0.041888228058815\n",
      "Training iteration: 2922\n",
      "Improved validation loss from: 0.041888228058815  to: 0.04178821444511414\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.041791719198226926\n",
      "Training iteration: 2924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.041814404726028445\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.041841381788253786\n",
      "Training iteration: 2926\n",
      "Improved validation loss from: 0.04178821444511414  to: 0.04175341725349426\n",
      "Training iteration: 2927\n",
      "Improved validation loss from: 0.04175341725349426  to: 0.04165036678314209\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.04166430532932282\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.041749849915504456\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.04175300598144531\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.04188486039638519\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.04198858141899109\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.041951870918273924\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.041837215423583984\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): 0.041755810379981995\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): 0.041709956526756284\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.04179219603538513\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.04180935323238373\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.041940274834632876\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.04195918440818787\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.04185847342014313\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.041803160309791566\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.0417116641998291\n",
      "Training iteration: 2944\n",
      "Improved validation loss from: 0.04165036678314209  to: 0.041634559631347656\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.04165186882019043\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.04176743924617767\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.04168502390384674\n",
      "Training iteration: 2948\n",
      "Improved validation loss from: 0.041634559631347656  to: 0.041602373123168945\n",
      "Training iteration: 2949\n",
      "Improved validation loss from: 0.041602373123168945  to: 0.04154574275016785\n",
      "Training iteration: 2950\n",
      "Improved validation loss from: 0.04154574275016785  to: 0.041526174545288085\n",
      "Training iteration: 2951\n",
      "Improved validation loss from: 0.041526174545288085  to: 0.041358575224876404\n",
      "Training iteration: 2952\n",
      "Improved validation loss from: 0.041358575224876404  to: 0.04135235846042633\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.04143487811088562\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): 0.04148949086666107\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.0415076732635498\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.04145425260066986\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.04141231179237366\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.04135235846042633  to: 0.04132557511329651\n",
      "Training iteration: 2959\n",
      "Improved validation loss from: 0.04132557511329651  to: 0.041265636682510376\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): 0.04132622182369232\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.041403016448020934\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.0413608729839325\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.04127930700778961\n",
      "Training iteration: 2964\n",
      "Improved validation loss from: 0.041265636682510376  to: 0.041236647963523866\n",
      "Training iteration: 2965\n",
      "Improved validation loss from: 0.041236647963523866  to: 0.041223841905593875\n",
      "Training iteration: 2966\n",
      "Improved validation loss from: 0.041223841905593875  to: 0.04109639525413513\n",
      "Training iteration: 2967\n",
      "Improved validation loss from: 0.04109639525413513  to: 0.041042456030845643\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.04108707010746002\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.04119469523429871\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.04118067622184753\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.04114920496940613\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.04119407534599304\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.04119485914707184\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): 0.041157832741737364\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.04107050895690918\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.04104827046394348\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.04107772707939148\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.04115461409091949\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.04111558496952057\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.04115313589572907\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.04120811522006988\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.04122104644775391\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.041144651174545285\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.041084423661231995\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.04109593033790589\n",
      "Training iteration: 2986\n",
      "Improved validation loss from: 0.041042456030845643  to: 0.04104134440422058\n",
      "Training iteration: 2987\n",
      "Improved validation loss from: 0.04104134440422058  to: 0.04097998738288879\n",
      "Training iteration: 2988\n",
      "Improved validation loss from: 0.04097998738288879  to: 0.04096410274505615\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.04101026952266693\n",
      "Training iteration: 2990\n",
      "Improved validation loss from: 0.04096410274505615  to: 0.04092814922332764\n",
      "Training iteration: 2991\n",
      "Improved validation loss from: 0.04092814922332764  to: 0.04083443284034729\n",
      "Training iteration: 2992\n",
      "Improved validation loss from: 0.04083443284034729  to: 0.040741586685180665\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.04083411693572998\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.040782013535499574\n",
      "Training iteration: 2995\n",
      "Improved validation loss from: 0.040741586685180665  to: 0.04066568911075592\n",
      "Training iteration: 2996\n",
      "Improved validation loss from: 0.04066568911075592  to: 0.04064812660217285\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.040710872411727904\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.04084938168525696\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.040702620148658754\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.04072224497795105\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.04078917503356934\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.04091086983680725\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.04085395336151123\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.040840435028076175\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.04082354605197906\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.04080283045768738\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.04081075191497803\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.040709248185157774\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.04070703387260437\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.040663138031959534\n",
      "Training iteration: 3011\n",
      "Improved validation loss from: 0.04064812660217285  to: 0.04058069586753845\n",
      "Training iteration: 3012\n",
      "Improved validation loss from: 0.04058069586753845  to: 0.040536928176879886\n",
      "Training iteration: 3013\n",
      "Improved validation loss from: 0.040536928176879886  to: 0.04052014946937561\n",
      "Training iteration: 3014\n",
      "Improved validation loss from: 0.04052014946937561  to: 0.04046249985694885\n",
      "Training iteration: 3015\n",
      "Improved validation loss from: 0.04046249985694885  to: 0.040377086400985716\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.04043227136135101\n",
      "Training iteration: 3017\n",
      "Improved validation loss from: 0.040377086400985716  to: 0.04033003747463226\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.04037100672721863\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.040449279546737674\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.040514954924583436\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): 0.04035507142543793\n",
      "Training iteration: 3022\n",
      "Improved validation loss from: 0.04033003747463226  to: 0.04019911885261536\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.04020836353302002\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.04027979969978333\n",
      "Training iteration: 3025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.04019911885261536  to: 0.040186652541160585\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.040213164687156674\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.040285739302635196\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.040343555808067325\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.0402164876461029\n",
      "Training iteration: 3030\n",
      "Improved validation loss from: 0.040186652541160585  to: 0.04010423123836517\n",
      "Training iteration: 3031\n",
      "Improved validation loss from: 0.04010423123836517  to: 0.040076452493667605\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.040125927329063414\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.04024105668067932\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.04016733169555664\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.040120044350624086\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.04017515778541565\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.04021940231323242\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.040193992853164676\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.040266042947769164\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.040255260467529294\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.040221118927001955\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.04008453488349915\n",
      "Training iteration: 3043\n",
      "Improved validation loss from: 0.040076452493667605  to: 0.03994880020618439\n",
      "Training iteration: 3044\n",
      "Improved validation loss from: 0.03994880020618439  to: 0.03989714086055755\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: 0.03989714086055755  to: 0.03988042771816254\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.03989641666412354\n",
      "Training iteration: 3047\n",
      "Improved validation loss from: 0.03988042771816254  to: 0.03987523019313812\n",
      "Training iteration: 3048\n",
      "Improved validation loss from: 0.03987523019313812  to: 0.03977667391300201\n",
      "Training iteration: 3049\n",
      "Improved validation loss from: 0.03977667391300201  to: 0.03970967233181\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.03974320888519287\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.039861848950386046\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.03998082280158997\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.040017041563987735\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.040000492334365846\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.0399720698595047\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.039985886216163634\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.039956015348434445\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.039843803644180296\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): 0.03980838358402252\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.0398379236459732\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.039774847030639646\n",
      "Training iteration: 3062\n",
      "Improved validation loss from: 0.03970967233181  to: 0.03961470127105713\n",
      "Training iteration: 3063\n",
      "Improved validation loss from: 0.03961470127105713  to: 0.03948402404785156\n",
      "Training iteration: 3064\n",
      "Improved validation loss from: 0.03948402404785156  to: 0.03947749733924866\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.03951990008354187\n",
      "Training iteration: 3066\n",
      "Improved validation loss from: 0.03947749733924866  to: 0.03945052623748779\n",
      "Training iteration: 3067\n",
      "Improved validation loss from: 0.03945052623748779  to: 0.039408379793167116\n",
      "Training iteration: 3068\n",
      "Improved validation loss from: 0.039408379793167116  to: 0.03934434950351715\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.03938156962394714\n",
      "Training iteration: 3070\n",
      "Improved validation loss from: 0.03934434950351715  to: 0.03929597735404968\n",
      "Training iteration: 3071\n",
      "Improved validation loss from: 0.03929597735404968  to: 0.03923436999320984\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.03928071558475495\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.039405021071434024\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.03954814076423645\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.039612767100334165\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.03964528441429138\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.03963554799556732\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.03962026536464691\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.039488834142684934\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.03944009244441986\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.03953061699867248\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.03964223563671112\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.03954537808895111\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.03944024443626404\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.03927265703678131\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.039324143528938295\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.039285597205162046\n",
      "Training iteration: 3088\n",
      "Improved validation loss from: 0.03923436999320984  to: 0.039135140180587766\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.0391731321811676\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.039225250482559204\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.039421486854553225\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.03928058445453644\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.03914065957069397\n",
      "Training iteration: 3094\n",
      "Improved validation loss from: 0.039135140180587766  to: 0.03913258612155914\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.03921849131584167\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.039406505227088925\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.03960350751876831\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.03967509865760803\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.039630341529846194\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.03951009213924408\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.039377883076667786\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.039346083998680115\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.03933078944683075\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.039302697777748107\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.039276623725891115\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.039141255617141726\n",
      "Training iteration: 3107\n",
      "Improved validation loss from: 0.03913258612155914  to: 0.03912044763565063\n",
      "Training iteration: 3108\n",
      "Improved validation loss from: 0.03912044763565063  to: 0.03904368579387665\n",
      "Training iteration: 3109\n",
      "Improved validation loss from: 0.03904368579387665  to: 0.0390110194683075\n",
      "Training iteration: 3110\n",
      "Improved validation loss from: 0.0390110194683075  to: 0.03891966044902802\n",
      "Training iteration: 3111\n",
      "Improved validation loss from: 0.03891966044902802  to: 0.03883433938026428\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.0388416588306427\n",
      "Training iteration: 3113\n",
      "Improved validation loss from: 0.03883433938026428  to: 0.038777786493301394\n",
      "Training iteration: 3114\n",
      "Improved validation loss from: 0.038777786493301394  to: 0.038710477948188785\n",
      "Training iteration: 3115\n",
      "Improved validation loss from: 0.038710477948188785  to: 0.03869069218635559\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.03872072696685791\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.038776612281799315\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.03881498277187347\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.03877471387386322\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.03881416916847229\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.03870820999145508\n",
      "Training iteration: 3122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03869069218635559  to: 0.03852720856666565\n",
      "Training iteration: 3123\n",
      "Improved validation loss from: 0.03852720856666565  to: 0.03847169280052185\n",
      "Training iteration: 3124\n",
      "Improved validation loss from: 0.03847169280052185  to: 0.03842667937278747\n",
      "Training iteration: 3125\n",
      "Improved validation loss from: 0.03842667937278747  to: 0.038412544131278994\n",
      "Training iteration: 3126\n",
      "Improved validation loss from: 0.038412544131278994  to: 0.03840020596981049\n",
      "Training iteration: 3127\n",
      "Improved validation loss from: 0.03840020596981049  to: 0.038376989960670474\n",
      "Training iteration: 3128\n",
      "Improved validation loss from: 0.038376989960670474  to: 0.0383037269115448\n",
      "Training iteration: 3129\n",
      "Improved validation loss from: 0.0383037269115448  to: 0.03829067349433899\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.038332438468933104\n",
      "Training iteration: 3131\n",
      "Improved validation loss from: 0.03829067349433899  to: 0.038268619775772096\n",
      "Training iteration: 3132\n",
      "Improved validation loss from: 0.038268619775772096  to: 0.038255196809768674\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.03827863931655884\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.03826246559619904\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.03834126889705658\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.03837482333183288\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.03841989636421204\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.038294950127601625\n",
      "Training iteration: 3139\n",
      "Improved validation loss from: 0.038255196809768674  to: 0.03815923631191254\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.038192841410636905\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.038281983137130736\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.038256695866584776\n",
      "Training iteration: 3143\n",
      "Improved validation loss from: 0.03815923631191254  to: 0.03813294172286987\n",
      "Training iteration: 3144\n",
      "Improved validation loss from: 0.03813294172286987  to: 0.03805265426635742\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.038125577569007876\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.038118472695350646\n",
      "Training iteration: 3147\n",
      "Improved validation loss from: 0.03805265426635742  to: 0.038033515214920044\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.038103288412094115\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.03817874789237976\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.03822432458400726\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.03819438815116882\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.03816610276699066\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.03821485042572022\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.03827730119228363\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.038266390562057495\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.03823247849941254\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.0382644921541214\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.038204577565193173\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.03817695379257202\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.03819213807582855\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.038130524754524234\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.038114359974861144\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.03806983232498169\n",
      "Training iteration: 3164\n",
      "Improved validation loss from: 0.038033515214920044  to: 0.03799955844879151\n",
      "Training iteration: 3165\n",
      "Improved validation loss from: 0.03799955844879151  to: 0.03785485327243805\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.03789590895175934\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.03803272843360901\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.038138529658317565\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.03807400465011597\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.0379355788230896\n",
      "Training iteration: 3171\n",
      "Improved validation loss from: 0.03785485327243805  to: 0.03783342242240906\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.03790346086025238\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.03790956735610962\n",
      "Training iteration: 3174\n",
      "Improved validation loss from: 0.03783342242240906  to: 0.03779056966304779\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.037909680604934694\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.038084942102432254\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.03813505172729492\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.03800312876701355\n",
      "Training iteration: 3179\n",
      "Improved validation loss from: 0.03779056966304779  to: 0.037759947776794436\n",
      "Training iteration: 3180\n",
      "Improved validation loss from: 0.037759947776794436  to: 0.03762698769569397\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.03778098821640015\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.03766900300979614\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.0376331627368927\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.03772689402103424\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.03779701292514801\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.03773287832736969\n",
      "Training iteration: 3187\n",
      "Improved validation loss from: 0.03762698769569397  to: 0.037610724568367004\n",
      "Training iteration: 3188\n",
      "Improved validation loss from: 0.037610724568367004  to: 0.03749493360519409\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.03751345574855804\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.03762880861759186\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.03772691488265991\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.03778497576713562\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.037802869081497194\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.03773294389247894\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.03768358826637268\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.03764169812202454\n",
      "Training iteration: 3197\n",
      "Improved validation loss from: 0.03749493360519409  to: 0.03748800158500672\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.03753472566604614\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.03762533068656922\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.037491613626480104\n",
      "Training iteration: 3201\n",
      "Improved validation loss from: 0.03748800158500672  to: 0.03743923008441925\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.03744472861289978\n",
      "Training iteration: 3203\n",
      "Improved validation loss from: 0.03743923008441925  to: 0.037415406107902525\n",
      "Training iteration: 3204\n",
      "Improved validation loss from: 0.037415406107902525  to: 0.0372994601726532\n",
      "Training iteration: 3205\n",
      "Improved validation loss from: 0.0372994601726532  to: 0.03728164434432983\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.037329083681106566\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.03742435872554779\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.0373207688331604\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.03731610178947449\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.03746197521686554\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.03760325312614441\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.03761212825775147\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.03761148750782013\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.037553015351295474\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.03751414120197296\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.03749213814735412\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.0374408096075058\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.037436109781265256\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.03746502697467804\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.03737278580665589\n",
      "Training iteration: 3221\n",
      "Improved validation loss from: 0.03728164434432983  to: 0.037276926636695865\n",
      "Training iteration: 3222\n",
      "Improved validation loss from: 0.037276926636695865  to: 0.03721717894077301\n",
      "Training iteration: 3223\n",
      "Improved validation loss from: 0.03721717894077301  to: 0.03720937371253967\n",
      "Training iteration: 3224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03729273676872254\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.037351357936859134\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.03737989068031311\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.03725575804710388\n",
      "Training iteration: 3228\n",
      "Improved validation loss from: 0.03720937371253967  to: 0.03716597855091095\n",
      "Training iteration: 3229\n",
      "Improved validation loss from: 0.03716597855091095  to: 0.037163949012756346\n",
      "Training iteration: 3230\n",
      "Improved validation loss from: 0.037163949012756346  to: 0.037122851610183714\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.03713774979114533\n",
      "Training iteration: 3232\n",
      "Improved validation loss from: 0.037122851610183714  to: 0.03708754479885101\n",
      "Training iteration: 3233\n",
      "Improved validation loss from: 0.03708754479885101  to: 0.03704651296138763\n",
      "Training iteration: 3234\n",
      "Improved validation loss from: 0.03704651296138763  to: 0.03694734573364258\n",
      "Training iteration: 3235\n",
      "Improved validation loss from: 0.03694734573364258  to: 0.03691949248313904\n",
      "Training iteration: 3236\n",
      "Improved validation loss from: 0.03691949248313904  to: 0.03679197430610657\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.03682032823562622\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.03697949349880218\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.036933472752571105\n",
      "Training iteration: 3240\n",
      "Improved validation loss from: 0.03679197430610657  to: 0.03674831986427307\n",
      "Training iteration: 3241\n",
      "Improved validation loss from: 0.03674831986427307  to: 0.036543482542037965\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.03658095598220825\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.0366401731967926\n",
      "Training iteration: 3244\n",
      "Improved validation loss from: 0.036543482542037965  to: 0.03646235466003418\n",
      "Training iteration: 3245\n",
      "Improved validation loss from: 0.03646235466003418  to: 0.036458954215049744\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.036559310555458066\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.036830300092697145\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.036752766370773314\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.03669455945491791\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.03673970103263855\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.03689318001270294\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.036980047821998596\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.03698625862598419\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.0370697945356369\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.03713709712028503\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.037125617265701294\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.0370307058095932\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.03686233162879944\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.03674748837947846\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.036846083402633664\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.03691011965274811\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.03688333034515381\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.036854088306427\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.03686690926551819\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.03687790334224701\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.03680315911769867\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.03678097426891327\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.036861634254455565\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.03695273399353027\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.036924776434898374\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.036864304542541505\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.03682698905467987\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.03679603934288025\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.03671726584434509\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.03666994571685791\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.03664589524269104\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.03663918375968933\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.036639684438705446\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.03655301928520203\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.03650456666946411\n",
      "Training iteration: 3281\n",
      "Improved validation loss from: 0.036458954215049744  to: 0.03645259439945221\n",
      "Training iteration: 3282\n",
      "Improved validation loss from: 0.03645259439945221  to: 0.03640754818916321\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.03648121654987335\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.0364135205745697\n",
      "Training iteration: 3285\n",
      "Improved validation loss from: 0.03640754818916321  to: 0.036370834708213805\n",
      "Training iteration: 3286\n",
      "Improved validation loss from: 0.036370834708213805  to: 0.036356741189956666\n",
      "Training iteration: 3287\n",
      "Improved validation loss from: 0.036356741189956666  to: 0.036299067735672\n",
      "Training iteration: 3288\n",
      "Improved validation loss from: 0.036299067735672  to: 0.03621454238891601\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.036215806007385255\n",
      "Training iteration: 3290\n",
      "Improved validation loss from: 0.03621454238891601  to: 0.03617851138114929\n",
      "Training iteration: 3291\n",
      "Improved validation loss from: 0.03617851138114929  to: 0.036134010553359984\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.03614526391029358\n",
      "Training iteration: 3293\n",
      "Improved validation loss from: 0.036134010553359984  to: 0.03610909581184387\n",
      "Training iteration: 3294\n",
      "Improved validation loss from: 0.03610909581184387  to: 0.03598138689994812\n",
      "Training iteration: 3295\n",
      "Improved validation loss from: 0.03598138689994812  to: 0.03587044179439545\n",
      "Training iteration: 3296\n",
      "Improved validation loss from: 0.03587044179439545  to: 0.035828900337219236\n",
      "Training iteration: 3297\n",
      "Improved validation loss from: 0.035828900337219236  to: 0.035823500156402587\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.03586270809173584\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.03593340218067169\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.036033958196640015\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.03596829175949097\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.03594142496585846\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.0358900249004364\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.0359264075756073\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.03597959876060486\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.03599245548248291\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.03598714768886566\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.03596561551094055\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.03587393164634704\n",
      "Training iteration: 3310\n",
      "Improved validation loss from: 0.035823500156402587  to: 0.035786384344100954\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.03580258786678314\n",
      "Training iteration: 3312\n",
      "Improved validation loss from: 0.035786384344100954  to: 0.03577347695827484\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.03580022156238556\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.035893294215202334\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.03583059310913086\n",
      "Training iteration: 3316\n",
      "Improved validation loss from: 0.03577347695827484  to: 0.035627371072769164\n",
      "Training iteration: 3317\n",
      "Improved validation loss from: 0.035627371072769164  to: 0.03552906811237335\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.03553459048271179\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.03563627004623413\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.03561971783638\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.0357810378074646\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.03598605692386627\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.03600393533706665\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.035806700587272644\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.03568286299705505\n",
      "Training iteration: 3326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.035765090584754945\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.035798221826553345\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.035832732915878296\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.03591935932636261\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.03609926104545593\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.03612833619117737\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.03599669933319092\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.03586375117301941\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.03584774732589722\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.03574368357658386\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.035744547843933105\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.03588785231113434\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.035987991094589236\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.03599320352077484\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.03586958944797516\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.03565896451473236\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.03566820621490478\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.03564295768737793\n",
      "Training iteration: 3344\n",
      "Improved validation loss from: 0.03552906811237335  to: 0.03548591434955597\n",
      "Training iteration: 3345\n",
      "Improved validation loss from: 0.03548591434955597  to: 0.03547928929328918\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.035489031672477724\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.03549673557281494\n",
      "Training iteration: 3348\n",
      "Improved validation loss from: 0.03547928929328918  to: 0.03546454906463623\n",
      "Training iteration: 3349\n",
      "Improved validation loss from: 0.03546454906463623  to: 0.03538934290409088\n",
      "Training iteration: 3350\n",
      "Improved validation loss from: 0.03538934290409088  to: 0.03536200821399689\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.03541615903377533\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.03551223874092102\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.03561452329158783\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.035645151138305665\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.035647517442703246\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.03561398088932037\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.03559688031673432\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.035472708940505984\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.03542380928993225\n",
      "Training iteration: 3360\n",
      "Improved validation loss from: 0.03536200821399689  to: 0.03536190092563629\n",
      "Training iteration: 3361\n",
      "Improved validation loss from: 0.03536190092563629  to: 0.035278183221817014\n",
      "Training iteration: 3362\n",
      "Improved validation loss from: 0.035278183221817014  to: 0.035240405797958375\n",
      "Training iteration: 3363\n",
      "Improved validation loss from: 0.035240405797958375  to: 0.03517795205116272\n",
      "Training iteration: 3364\n",
      "Improved validation loss from: 0.03517795205116272  to: 0.03508821129798889\n",
      "Training iteration: 3365\n",
      "Validation loss (no improvement): 0.03509139120578766\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.0351571649312973\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.03515743613243103\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.035170748829841614\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.03522233366966247\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.03521016538143158\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.03522316813468933\n",
      "Training iteration: 3372\n",
      "Validation loss (no improvement): 0.035272955894470215\n",
      "Training iteration: 3373\n",
      "Validation loss (no improvement): 0.03521100878715515\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.03518546223640442\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.035167986154556276\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.03517782986164093\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.035148507356643675\n",
      "Training iteration: 3378\n",
      "Improved validation loss from: 0.03508821129798889  to: 0.035005292296409606\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.035054188966751096\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.03511492609977722\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.03521654307842255\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.035237255692482\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.03516810834407806\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.0350338876247406\n",
      "Training iteration: 3385\n",
      "Improved validation loss from: 0.035005292296409606  to: 0.03495058119297027\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.035090678930282594\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.03512378334999085\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): 0.035136133432388306\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.03518217206001282\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.03517083823680878\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.03504909574985504\n",
      "Training iteration: 3392\n",
      "Improved validation loss from: 0.03495058119297027  to: 0.034920570254325864\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.03492079377174377\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.03499377965927124\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.03501890003681183\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.03507263660430908\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.035137027502059937\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.035180026292800905\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.03510806262493134\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.03499594330787659\n",
      "Training iteration: 3401\n",
      "Improved validation loss from: 0.034920570254325864  to: 0.03491710126399994\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.03501445651054382\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.03505691587924957\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.03511686623096466\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.03510006666183472\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.035009080171585084\n",
      "Training iteration: 3407\n",
      "Improved validation loss from: 0.03491710126399994  to: 0.034904545545578\n",
      "Training iteration: 3408\n",
      "Improved validation loss from: 0.034904545545578  to: 0.03480364084243774\n",
      "Training iteration: 3409\n",
      "Improved validation loss from: 0.03480364084243774  to: 0.03480214476585388\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): 0.034986072778701784\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.03504652380943298\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.03504534363746643\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.03498372733592987\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.03487921357154846\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.03482138216495514\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.03480565547943115\n",
      "Training iteration: 3417\n",
      "Improved validation loss from: 0.03480214476585388  to: 0.03476486802101135\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.034790074825286864\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.0348187655210495\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.03482969403266907\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): 0.03480846285820007\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): 0.034792378544807434\n",
      "Training iteration: 3423\n",
      "Improved validation loss from: 0.03476486802101135  to: 0.03472653031349182\n",
      "Training iteration: 3424\n",
      "Improved validation loss from: 0.03472653031349182  to: 0.03464711606502533\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.0346870481967926\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.0348330557346344\n",
      "Training iteration: 3427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.034931451082229614\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.03495542109012604\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): 0.034907680749893186\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.0348674476146698\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.03479402959346771\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.03473623394966126\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.034870243072509764\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.03500687181949615\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.03502397835254669\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.034956502914428714\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.03492172360420227\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.03489508330821991\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.03476017117500305\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.03469522595405579\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.03473619818687439\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.03480217754840851\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.03493250012397766\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.03479963839054108\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.0347268283367157\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.03481710255146027\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.03490809798240661\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.034879487752914426\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): 0.034960561990737916\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.03511387407779694\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.03520922064781189\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.03515871465206146\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.034950414299964906\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.03483770191669464\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.03488723933696747\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.03504571318626404\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.034963247179985044\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.035010480880737306\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.03520702719688416\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.03535820841789246\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.035468295216560364\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.03526464104652405\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.03503047823905945\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.03493916392326355\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.03503312468528748\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.035186567902565004\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.03513461947441101\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.0351771891117096\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.035137799382209775\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.03509513139724731\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): 0.035327115654945375\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.03527524471282959\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.0352098286151886\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.0353405088186264\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.03545314371585846\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.03541889190673828\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.03524664342403412\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.0350483238697052\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.034955072402954104\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.034936001896858214\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.03497872948646545\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.03507389724254608\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.03512735366821289\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.035039037466049194\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.03490729629993439\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.03478389680385589\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.03471609055995941\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.034710833430290224\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): 0.034654933214187625\n",
      "Training iteration: 3490\n",
      "Improved validation loss from: 0.03464711606502533  to: 0.03461107611656189\n",
      "Training iteration: 3491\n",
      "Improved validation loss from: 0.03461107611656189  to: 0.03457625508308411\n",
      "Training iteration: 3492\n",
      "Improved validation loss from: 0.03457625508308411  to: 0.03453629016876221\n",
      "Training iteration: 3493\n",
      "Improved validation loss from: 0.03453629016876221  to: 0.034427911043167114\n",
      "Training iteration: 3494\n",
      "Improved validation loss from: 0.034427911043167114  to: 0.03429227471351624\n",
      "Training iteration: 3495\n",
      "Improved validation loss from: 0.03429227471351624  to: 0.03425254225730896\n",
      "Training iteration: 3496\n",
      "Improved validation loss from: 0.03425254225730896  to: 0.03423182666301727\n",
      "Training iteration: 3497\n",
      "Improved validation loss from: 0.03423182666301727  to: 0.0341960608959198\n",
      "Training iteration: 3498\n",
      "Improved validation loss from: 0.0341960608959198  to: 0.034161001443862915\n",
      "Training iteration: 3499\n",
      "Improved validation loss from: 0.034161001443862915  to: 0.03411407768726349\n",
      "Training iteration: 3500\n",
      "Improved validation loss from: 0.03411407768726349  to: 0.03408175110816956\n",
      "Training iteration: 3501\n",
      "Improved validation loss from: 0.03408175110816956  to: 0.03401288390159607\n",
      "Training iteration: 3502\n",
      "Improved validation loss from: 0.03401288390159607  to: 0.03387055993080139\n",
      "Training iteration: 3503\n",
      "Improved validation loss from: 0.03387055993080139  to: 0.033832764625549315\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.033891627192497255\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.03406562209129334\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): 0.03424796760082245\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): 0.03431090712547302\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.03425192832946777\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.034172677993774415\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.03414812684059143\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.034221071004867556\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.034149378538131714\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.03422496914863586\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.03436588943004608\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.03437010645866394\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.03424747288227081\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.034102204442024234\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.03410521149635315\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.03402792811393738\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.03401133120059967\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.03409223258495331\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.03414716124534607\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): 0.034096845984458925\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): 0.033941406011581424\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.03384354114532471\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.03389870524406433\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.03400518000125885\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.03405970335006714\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.03416985869407654\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.03424191772937775\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.03419561386108398\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.03413979411125183\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.034103164076805116\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.03401867151260376\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.03406810760498047\n",
      "Training iteration: 3536\n",
      "Validation loss (no improvement): 0.03422683179378509\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.03424558639526367\n",
      "Training iteration: 3538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.034192842245101926\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.0341366708278656\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): 0.034088701009750366\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.03395805954933166\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.033903035521507266\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.033944505453109744\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.03393922448158264\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.033937233686447146\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.03390000760555267\n",
      "Training iteration: 3547\n",
      "Improved validation loss from: 0.033832764625549315  to: 0.0337815135717392\n",
      "Training iteration: 3548\n",
      "Improved validation loss from: 0.0337815135717392  to: 0.03374855518341065\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.033853039145469666\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.034023994207382204\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.033956319093704224\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.033940857648849486\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.03396535515785217\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.03400673866271973\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.03405962884426117\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.03403239548206329\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.034070524573326114\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): 0.03405132293701172\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): 0.03404262661933899\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): 0.03391621708869934\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.03381595015525818\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.03381037712097168\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.03390434384346008\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): 0.034041684865951535\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): 0.03412352800369263\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): 0.03412056863307953\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): 0.034083780646324155\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.034118443727493286\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.03406932353973389\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.03389813303947449\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): 0.03385992348194122\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.0338418185710907\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.03386220037937164\n",
      "Training iteration: 3574\n",
      "Validation loss (no improvement): 0.03381885588169098\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.03374855518341065  to: 0.033725929260253903\n",
      "Training iteration: 3576\n",
      "Improved validation loss from: 0.033725929260253903  to: 0.033662033081054685\n",
      "Training iteration: 3577\n",
      "Improved validation loss from: 0.033662033081054685  to: 0.0336516410112381\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.03372166752815246\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.03381633162498474\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): 0.03385666012763977\n",
      "Training iteration: 3581\n",
      "Validation loss (no improvement): 0.03381945192813873\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.03377864360809326\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.03380697965621948\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.03381567597389221\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.033848837018013\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.03393147587776184\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.03396287560462952\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.033918583393096925\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.03377393186092377\n",
      "Training iteration: 3590\n",
      "Improved validation loss from: 0.0336516410112381  to: 0.03362063467502594\n",
      "Training iteration: 3591\n",
      "Improved validation loss from: 0.03362063467502594  to: 0.03356351852416992\n",
      "Training iteration: 3592\n",
      "Improved validation loss from: 0.03356351852416992  to: 0.03352359235286713\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.033566609025001526\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.03358812928199768\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.03361872434616089\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.03357028067111969\n",
      "Training iteration: 3597\n",
      "Improved validation loss from: 0.03352359235286713  to: 0.033483749628067015\n",
      "Training iteration: 3598\n",
      "Improved validation loss from: 0.033483749628067015  to: 0.033404237031936644\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.033414289355278015\n",
      "Training iteration: 3600\n",
      "Improved validation loss from: 0.033404237031936644  to: 0.033389776945114136\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.03347702920436859\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.033594390749931334\n",
      "Training iteration: 3603\n",
      "Validation loss (no improvement): 0.033596330881118776\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.03351163566112518\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.033409392833709715\n",
      "Training iteration: 3606\n",
      "Improved validation loss from: 0.033389776945114136  to: 0.033334726095199586\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.03335839211940765\n",
      "Training iteration: 3608\n",
      "Improved validation loss from: 0.033334726095199586  to: 0.03331920504570007\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.033371728658676145\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.03347296416759491\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.033515697717666625\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.03343241810798645\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.033327871561050416\n",
      "Training iteration: 3614\n",
      "Improved validation loss from: 0.03331920504570007  to: 0.03327752649784088\n",
      "Training iteration: 3615\n",
      "Improved validation loss from: 0.03327752649784088  to: 0.033259603381156924\n",
      "Training iteration: 3616\n",
      "Improved validation loss from: 0.033259603381156924  to: 0.033257827162742615\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.0333312064409256\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.03341017663478851\n",
      "Training iteration: 3619\n",
      "Validation loss (no improvement): 0.03341427743434906\n",
      "Training iteration: 3620\n",
      "Validation loss (no improvement): 0.033369007706642154\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.03328291773796081\n",
      "Training iteration: 3622\n",
      "Improved validation loss from: 0.033257827162742615  to: 0.033213558793067935\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.0332834392786026\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.03324329555034637\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.03323185741901398\n",
      "Training iteration: 3626\n",
      "Validation loss (no improvement): 0.03325014114379883\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.033242902159690856\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.033250609040260316\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.033275851607322694\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.03332708477973938\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.03337710499763489\n",
      "Training iteration: 3632\n",
      "Validation loss (no improvement): 0.03337329924106598\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.03327484726905823\n",
      "Training iteration: 3634\n",
      "Improved validation loss from: 0.033213558793067935  to: 0.033199805021286014\n",
      "Training iteration: 3635\n",
      "Improved validation loss from: 0.033199805021286014  to: 0.033139610290527345\n",
      "Training iteration: 3636\n",
      "Validation loss (no improvement): 0.033190011978149414\n",
      "Training iteration: 3637\n",
      "Improved validation loss from: 0.033139610290527345  to: 0.033113375306129456\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.033118733763694765\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.033125919103622434\n",
      "Training iteration: 3640\n",
      "Improved validation loss from: 0.033113375306129456  to: 0.033082318305969236\n",
      "Training iteration: 3641\n",
      "Improved validation loss from: 0.033082318305969236  to: 0.03307548761367798\n",
      "Training iteration: 3642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03308577537536621\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.03309071958065033\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.033115547895431516\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.0331104576587677\n",
      "Training iteration: 3646\n",
      "Validation loss (no improvement): 0.033116430044174194\n",
      "Training iteration: 3647\n",
      "Improved validation loss from: 0.03307548761367798  to: 0.03307000696659088\n",
      "Training iteration: 3648\n",
      "Improved validation loss from: 0.03307000696659088  to: 0.03300931453704834\n",
      "Training iteration: 3649\n",
      "Validation loss (no improvement): 0.03307119011878967\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.033117732405662535\n",
      "Training iteration: 3651\n",
      "Validation loss (no improvement): 0.03310568630695343\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.0331562489271164\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.03319886326789856\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.033168426156044005\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.03319126963615417\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.03315266668796539\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.03312692046165466\n",
      "Training iteration: 3658\n",
      "Validation loss (no improvement): 0.033121833205223085\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.03305332064628601\n",
      "Training iteration: 3660\n",
      "Validation loss (no improvement): 0.03304788768291474\n",
      "Training iteration: 3661\n",
      "Improved validation loss from: 0.03300931453704834  to: 0.032944709062576294\n",
      "Training iteration: 3662\n",
      "Improved validation loss from: 0.032944709062576294  to: 0.03282262682914734\n",
      "Training iteration: 3663\n",
      "Improved validation loss from: 0.03282262682914734  to: 0.03282178044319153\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.03283596932888031\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.032855677604675296\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.032859203219413755\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.03285635709762573\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.032869619131088254\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.03285776674747467\n",
      "Training iteration: 3670\n",
      "Validation loss (no improvement): 0.0328917533159256\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.032922875881195066\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.032853975892066956\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.032882985472679135\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.03302678465843201\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.03299263119697571\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.032981911301612855\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.03294493854045868\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.03300561308860779\n",
      "Training iteration: 3679\n",
      "Validation loss (no improvement): 0.032939720153808597\n",
      "Training iteration: 3680\n",
      "Validation loss (no improvement): 0.03282569944858551\n",
      "Training iteration: 3681\n",
      "Improved validation loss from: 0.03282178044319153  to: 0.03276060819625855\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.03284479975700379\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.03289934694766998\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.032830077409744265\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.03282198905944824\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.03288308978080749\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.0329799234867096\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.03294396996498108\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.03294300138950348\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.03300140500068664\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.03315084874629974\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.033153265714645386\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.03312801420688629\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.033079391717910765\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.03303243517875672\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.03301098942756653\n",
      "Training iteration: 3697\n",
      "Validation loss (no improvement): 0.03288808465003967\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.032931408286094664\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.03305641114711762\n",
      "Training iteration: 3700\n",
      "Validation loss (no improvement): 0.033166900277137756\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.03312215507030487\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.03302937150001526\n",
      "Training iteration: 3703\n",
      "Validation loss (no improvement): 0.032979929447174074\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.032953318953514096\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.032909950613975524\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.03288204371929169\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.03292799592018127\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.03307066559791565\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.03308518528938294\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.03305564522743225\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.03295179605484009\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.032904961705207826\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.03285927176475525\n",
      "Training iteration: 3714\n",
      "Improved validation loss from: 0.03276060819625855  to: 0.032752588391304016\n",
      "Training iteration: 3715\n",
      "Improved validation loss from: 0.032752588391304016  to: 0.03270960450172424\n",
      "Training iteration: 3716\n",
      "Improved validation loss from: 0.03270960450172424  to: 0.0327066570520401\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.032752835750579835\n",
      "Training iteration: 3718\n",
      "Improved validation loss from: 0.0327066570520401  to: 0.03260896801948547\n",
      "Training iteration: 3719\n",
      "Improved validation loss from: 0.03260896801948547  to: 0.032506197690963745\n",
      "Training iteration: 3720\n",
      "Improved validation loss from: 0.032506197690963745  to: 0.032493630051612855\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.032612985372543334\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.03270826041698456\n",
      "Training iteration: 3723\n",
      "Validation loss (no improvement): 0.032676109671592714\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.03266233801841736\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.03271636962890625\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.03283337950706482\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.03289320468902588\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.03301898539066315\n",
      "Training iteration: 3729\n",
      "Validation loss (no improvement): 0.03303160965442657\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.032996845245361325\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.032817572355270386\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.032607823610305786\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.03254512846469879\n",
      "Training iteration: 3734\n",
      "Validation loss (no improvement): 0.03263236582279205\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.03272204399108887\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.03275925517082214\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.03283289074897766\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.03280946612358093\n",
      "Training iteration: 3739\n",
      "Validation loss (no improvement): 0.03279598355293274\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.03277758955955505\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.03268933892250061\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.03271708190441132\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.03290795683860779\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.03302266299724579\n",
      "Training iteration: 3745\n",
      "Validation loss (no improvement): 0.03300226628780365\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.032871407270431516\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.03275856375694275\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.032691121101379395\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.03254642486572266\n",
      "Training iteration: 3750\n",
      "Improved validation loss from: 0.032493630051612855  to: 0.03247953951358795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3751\n",
      "Validation loss (no improvement): 0.03248501718044281\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.032536131143569944\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.03255210518836975\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.032518982887268066\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.032524770498275755\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.03258729875087738\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.032488659024238586\n",
      "Training iteration: 3758\n",
      "Validation loss (no improvement): 0.03252238631248474\n",
      "Training iteration: 3759\n",
      "Validation loss (no improvement): 0.032591056823730466\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.032519346475601195\n",
      "Training iteration: 3761\n",
      "Improved validation loss from: 0.03247953951358795  to: 0.03237685561180115\n",
      "Training iteration: 3762\n",
      "Improved validation loss from: 0.03237685561180115  to: 0.03237210214138031\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.032427817583084106\n",
      "Training iteration: 3764\n",
      "Improved validation loss from: 0.03237210214138031  to: 0.032356935739517215\n",
      "Training iteration: 3765\n",
      "Improved validation loss from: 0.032356935739517215  to: 0.03227376341819763\n",
      "Training iteration: 3766\n",
      "Validation loss (no improvement): 0.03228309154510498\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.03233066499233246\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.03237332701683045\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.032315999269485474\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.032281717658042906\n",
      "Training iteration: 3771\n",
      "Validation loss (no improvement): 0.03237863481044769\n",
      "Training iteration: 3772\n",
      "Validation loss (no improvement): 0.03249402344226837\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.0325161874294281\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.03238126039505005\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.03229115009307861\n",
      "Training iteration: 3776\n",
      "Validation loss (no improvement): 0.03234566450119018\n",
      "Training iteration: 3777\n",
      "Validation loss (no improvement): 0.03232932090759277\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.03228700459003449\n",
      "Training iteration: 3779\n",
      "Improved validation loss from: 0.03227376341819763  to: 0.03224724233150482\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.032285171747207644\n",
      "Training iteration: 3781\n",
      "Improved validation loss from: 0.03224724233150482  to: 0.03224678635597229\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.03233771920204163\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.03243217170238495\n",
      "Training iteration: 3784\n",
      "Validation loss (no improvement): 0.03242870271205902\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.032350650429725646\n",
      "Training iteration: 3786\n",
      "Improved validation loss from: 0.03224678635597229  to: 0.032191887497901917\n",
      "Training iteration: 3787\n",
      "Improved validation loss from: 0.032191887497901917  to: 0.03216130137443542\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.032264339923858645\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.0323386549949646\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.03235713541507721\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.03253030776977539\n",
      "Training iteration: 3792\n",
      "Validation loss (no improvement): 0.03266384601593018\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.032631087303161624\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.03247591853141785\n",
      "Training iteration: 3795\n",
      "Validation loss (no improvement): 0.03231264054775238\n",
      "Training iteration: 3796\n",
      "Validation loss (no improvement): 0.03225178718566894\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.03228263854980469\n",
      "Training iteration: 3798\n",
      "Validation loss (no improvement): 0.03222919702529907\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.03225970864295959\n",
      "Training iteration: 3800\n",
      "Validation loss (no improvement): 0.03222743570804596\n",
      "Training iteration: 3801\n",
      "Improved validation loss from: 0.03216130137443542  to: 0.03211866021156311\n",
      "Training iteration: 3802\n",
      "Improved validation loss from: 0.03211866021156311  to: 0.03202221691608429\n",
      "Training iteration: 3803\n",
      "Improved validation loss from: 0.03202221691608429  to: 0.03198945820331574\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.032003411650657655\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.03204922676086426\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.03216755986213684\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.03222920894622803\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.032214117050170896\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.0321276843547821\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.032033282518386844\n",
      "Training iteration: 3811\n",
      "Improved validation loss from: 0.03198945820331574  to: 0.0319604903459549\n",
      "Training iteration: 3812\n",
      "Improved validation loss from: 0.0319604903459549  to: 0.03191096186637878\n",
      "Training iteration: 3813\n",
      "Validation loss (no improvement): 0.03192998766899109\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.0320805698633194\n",
      "Training iteration: 3815\n",
      "Validation loss (no improvement): 0.03217320740222931\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.03213303983211517\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.032020360231399536\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.031913813948631284\n",
      "Training iteration: 3819\n",
      "Improved validation loss from: 0.03191096186637878  to: 0.03176925182342529\n",
      "Training iteration: 3820\n",
      "Improved validation loss from: 0.03176925182342529  to: 0.031766071915626526\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.0319403737783432\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.032046443223953246\n",
      "Training iteration: 3823\n",
      "Validation loss (no improvement): 0.03209443986415863\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.032066091895103455\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.03183497488498688\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.03201825320720673\n",
      "Training iteration: 3827\n",
      "Validation loss (no improvement): 0.032087701559066775\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.03192325532436371\n",
      "Training iteration: 3829\n",
      "Validation loss (no improvement): 0.03202143013477325\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.032105821371078494\n",
      "Training iteration: 3831\n",
      "Validation loss (no improvement): 0.03226005434989929\n",
      "Training iteration: 3832\n",
      "Validation loss (no improvement): 0.032371881604194644\n",
      "Training iteration: 3833\n",
      "Validation loss (no improvement): 0.03229486048221588\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.032295072078704835\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.0323617160320282\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.032613709568977356\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.03269706964492798\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.032690155506134036\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.03269812166690826\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.03262072205543518\n",
      "Training iteration: 3841\n",
      "Validation loss (no improvement): 0.03255446553230286\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.03240155577659607\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.032249423861503604\n",
      "Training iteration: 3844\n",
      "Validation loss (no improvement): 0.03214605152606964\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.03210062980651855\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.0320685863494873\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.031982171535491946\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.03192747533321381\n",
      "Training iteration: 3849\n",
      "Validation loss (no improvement): 0.03184544742107391\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.031796124577522275\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.031830978393554685\n",
      "Training iteration: 3852\n",
      "Improved validation loss from: 0.031766071915626526  to: 0.031720906496047974\n",
      "Training iteration: 3853\n",
      "Improved validation loss from: 0.031720906496047974  to: 0.03170634210109711\n",
      "Training iteration: 3854\n",
      "Validation loss (no improvement): 0.03172373473644256\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.03187274634838104\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.03181337118148804\n",
      "Training iteration: 3857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03170634210109711  to: 0.03164374828338623\n",
      "Training iteration: 3858\n",
      "Improved validation loss from: 0.03164374828338623  to: 0.031524038314819335\n",
      "Training iteration: 3859\n",
      "Improved validation loss from: 0.031524038314819335  to: 0.03148913979530334\n",
      "Training iteration: 3860\n",
      "Validation loss (no improvement): 0.031537145376205444\n",
      "Training iteration: 3861\n",
      "Improved validation loss from: 0.03148913979530334  to: 0.031422263383865355\n",
      "Training iteration: 3862\n",
      "Improved validation loss from: 0.031422263383865355  to: 0.0313762366771698\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.03139764666557312\n",
      "Training iteration: 3864\n",
      "Validation loss (no improvement): 0.03144622445106506\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.031458505988121034\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.03143925666809082\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.031476455926895144\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.031624621152877806\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.031747615337371825\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.0317385733127594\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.03167020678520203\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.03156573176383972\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.031509536504745486\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.031512966752052306\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.03154991269111633\n",
      "Training iteration: 3876\n",
      "Validation loss (no improvement): 0.031547412276268005\n",
      "Training iteration: 3877\n",
      "Validation loss (no improvement): 0.03173493444919586\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.03171224296092987\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.031593573093414304\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.03137781620025635\n",
      "Training iteration: 3881\n",
      "Improved validation loss from: 0.0313762366771698  to: 0.031237363815307617\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.03126075863838196\n",
      "Training iteration: 3883\n",
      "Improved validation loss from: 0.031237363815307617  to: 0.03110676407814026\n",
      "Training iteration: 3884\n",
      "Improved validation loss from: 0.03110676407814026  to: 0.031050825119018556\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.03115091025829315\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.03134658932685852\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.031403249502182005\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.03135332465171814\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.0313540905714035\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.03131679892539978\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.03152249753475189\n",
      "Training iteration: 3892\n",
      "Validation loss (no improvement): 0.03156228959560394\n",
      "Training iteration: 3893\n",
      "Validation loss (no improvement): 0.03158134818077087\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.03172103464603424\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.03187102973461151\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.03189021050930023\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.031692090630531314\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.03156627118587494\n",
      "Training iteration: 3899\n",
      "Validation loss (no improvement): 0.03152513206005096\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.031532582640647885\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.03157615065574646\n",
      "Training iteration: 3902\n",
      "Validation loss (no improvement): 0.03165843188762665\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.03167748153209686\n",
      "Training iteration: 3904\n",
      "Validation loss (no improvement): 0.03152889609336853\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.03146970570087433\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.03134957253932953\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.031249648332595824\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.031227412819862365\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.03132727146148682\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.03147531151771545\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.03154030442237854\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.031516483426094054\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.03144540786743164\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.031453457474708554\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.03132358491420746\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.031120878458023072\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.031120184063911437\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.031173062324523926\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.03117348849773407\n",
      "Training iteration: 3920\n",
      "Validation loss (no improvement): 0.031066903471946718\n",
      "Training iteration: 3921\n",
      "Improved validation loss from: 0.031050825119018556  to: 0.030957889556884766\n",
      "Training iteration: 3922\n",
      "Improved validation loss from: 0.030957889556884766  to: 0.030949339270591736\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.031131568551063537\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.03111446797847748\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.031081539392471314\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.03124966025352478\n",
      "Training iteration: 3927\n",
      "Validation loss (no improvement): 0.031336274743080136\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.03135705590248108\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.031206566095352172\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.031118220090866087\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.031178849935531616\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.0313306987285614\n",
      "Training iteration: 3933\n",
      "Validation loss (no improvement): 0.031315818428993225\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.031380492448806765\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.03144504129886627\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.03140586018562317\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.03129851818084717\n",
      "Training iteration: 3938\n",
      "Validation loss (no improvement): 0.0312023788690567\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.03116185963153839\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.031155109405517578\n",
      "Training iteration: 3941\n",
      "Validation loss (no improvement): 0.03117862045764923\n",
      "Training iteration: 3942\n",
      "Validation loss (no improvement): 0.031232041120529175\n",
      "Training iteration: 3943\n",
      "Validation loss (no improvement): 0.031301409006118774\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.031221255660057068\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.031109225749969483\n",
      "Training iteration: 3946\n",
      "Validation loss (no improvement): 0.031023412942886353\n",
      "Training iteration: 3947\n",
      "Validation loss (no improvement): 0.03095220625400543\n",
      "Training iteration: 3948\n",
      "Improved validation loss from: 0.030949339270591736  to: 0.030867844820022583\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.030903860926628113\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.030893921852111816\n",
      "Training iteration: 3951\n",
      "Improved validation loss from: 0.030867844820022583  to: 0.030832093954086304\n",
      "Training iteration: 3952\n",
      "Improved validation loss from: 0.030832093954086304  to: 0.030732154846191406\n",
      "Training iteration: 3953\n",
      "Improved validation loss from: 0.030732154846191406  to: 0.03069118857383728\n",
      "Training iteration: 3954\n",
      "Improved validation loss from: 0.03069118857383728  to: 0.03060218393802643\n",
      "Training iteration: 3955\n",
      "Improved validation loss from: 0.03060218393802643  to: 0.03052746653556824\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.030567646026611328\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.030580398440361024\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.030611580610275267\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.03059999644756317\n",
      "Training iteration: 3960\n",
      "Validation loss (no improvement): 0.030588817596435548\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.03073757588863373\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.030825811624526977\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.030927753448486327\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.03099067509174347\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.03094523847103119\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.03079792559146881\n",
      "Training iteration: 3967\n",
      "Validation loss (no improvement): 0.030602550506591795\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.03057127892971039\n",
      "Training iteration: 3969\n",
      "Improved validation loss from: 0.03052746653556824  to: 0.03050283491611481\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.030517444014549255\n",
      "Training iteration: 3971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03065234124660492\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.030692028999328613\n",
      "Training iteration: 3973\n",
      "Validation loss (no improvement): 0.030644649267196657\n",
      "Training iteration: 3974\n",
      "Improved validation loss from: 0.03050283491611481  to: 0.0304996520280838\n",
      "Training iteration: 3975\n",
      "Improved validation loss from: 0.0304996520280838  to: 0.030403929948806762\n",
      "Training iteration: 3976\n",
      "Improved validation loss from: 0.030403929948806762  to: 0.03038954734802246\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.030413055419921876\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.0305098295211792\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.030622631311416626\n",
      "Training iteration: 3980\n",
      "Validation loss (no improvement): 0.030720922350883483\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.030801483988761903\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.030841550230979918\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.030722326040267943\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.030608749389648436\n",
      "Training iteration: 3985\n",
      "Validation loss (no improvement): 0.030591383576393127\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.03065383732318878\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.03061297833919525\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.030510663986206055\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.03050273358821869\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.030490580201148986\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.030521529912948608\n",
      "Training iteration: 3992\n",
      "Validation loss (no improvement): 0.03045131266117096\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.030414289236068724\n",
      "Training iteration: 3994\n",
      "Validation loss (no improvement): 0.03042847216129303\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.03041948676109314\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.030464476346969603\n",
      "Training iteration: 3997\n",
      "Improved validation loss from: 0.03038954734802246  to: 0.0303840309381485\n",
      "Training iteration: 3998\n",
      "Improved validation loss from: 0.0303840309381485  to: 0.030371332168579103\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.030474433302879335\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_3 = toy_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_3.train_model()\n",
    "ensemble_model_3.model_inference()\n",
    "\n",
    "ensemble_mean_3 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_3 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_3 = np.sqrt(np.exp(ensemble_logvar_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 24.81236877441406\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 24.81236877441406  to: 18.330003356933595\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 18.330003356933595  to: 13.779043579101563\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 13.779043579101563  to: 10.525178527832031\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 10.525178527832031  to: 8.154560852050782\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 8.154560852050782  to: 6.411561584472656\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 6.411561584472656  to: 5.113006973266602\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 5.113006973266602  to: 4.122084426879883\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 4.122084426879883  to: 3.358328628540039\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 3.358328628540039  to: 2.764166831970215\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 2.764166831970215  to: 2.2974979400634767\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 2.2974979400634767  to: 1.9277454376220704\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.9277454376220704  to: 1.6325038909912108\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 1.6325038909912108  to: 1.3950162887573243\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 1.3950162887573243  to: 1.2027012825012207\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 1.2027012825012207  to: 1.0458055496215821\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 1.0458055496215821  to: 0.917051124572754\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.917051124572754  to: 0.8107140541076661\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.8107140541076661  to: 0.7223901271820068\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.7223901271820068  to: 0.6486308097839355\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.6486308097839355  to: 0.5867024421691894\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.5867024421691894  to: 0.5344355583190918\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.5344355583190918  to: 0.49010653495788575\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.49010653495788575  to: 0.45232515335083007\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.45232515335083007  to: 0.41997675895690917\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.41997675895690917  to: 0.3921541690826416\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.3921541690826416  to: 0.36811950206756594\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.36811950206756594  to: 0.3472693920135498\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.3472693920135498  to: 0.32910847663879395\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.32910847663879395  to: 0.31322579383850097\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.31322579383850097  to: 0.29928109645843504\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.29928109645843504  to: 0.28699066638946535\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.28699066638946535  to: 0.27612009048461916\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.27612009048461916  to: 0.26647167205810546\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.26647167205810546  to: 0.25787835121154784\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.25787835121154784  to: 0.25019738674163816\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.25019738674163816  to: 0.24331169128417968\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.24331169128417968  to: 0.2371192216873169\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.2371192216873169  to: 0.23153293132781982\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.23153293132781982  to: 0.2264791250228882\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.2264791250228882  to: 0.2218928098678589\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.2218928098678589  to: 0.2177186965942383\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.2177186965942383  to: 0.21390922069549562\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.21390922069549562  to: 0.21042273044586182\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.21042273044586182  to: 0.20722317695617676\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.20722317695617676  to: 0.20427930355072021\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.20427930355072021  to: 0.20156378746032716\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.20156378746032716  to: 0.19905258417129518\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.19905258417129518  to: 0.19672473669052123\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.19672473669052123  to: 0.19456230401992797\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.19456230401992797  to: 0.19254850149154662\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.19254850149154662  to: 0.19066884517669677\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.19066884517669677  to: 0.18891063928604127\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.18891063928604127  to: 0.18726177215576173\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.18726177215576173  to: 0.18571245670318604\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.18571245670318604  to: 0.18425413370132446\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.18425413370132446  to: 0.1828786849975586\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1828786849975586  to: 0.18157904148101806\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.18157904148101806  to: 0.1803479790687561\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.1803479790687561  to: 0.17918016910552978\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.17918016910552978  to: 0.1780707359313965\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.1780707359313965  to: 0.1770149827003479\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.1770149827003479  to: 0.17600834369659424\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.17600834369659424  to: 0.17504749298095704\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.17504749298095704  to: 0.17412904500961304\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.17412904500961304  to: 0.1732499361038208\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.1732499361038208  to: 0.17240731716156005\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.17240731716156005  to: 0.1715986967086792\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.1715986967086792  to: 0.17082151174545288\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.17082151174545288  to: 0.17007381916046144\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.17007381916046144  to: 0.16935386657714843\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.16935386657714843  to: 0.16865991353988646\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.16865991353988646  to: 0.16799033880233766\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.16799033880233766  to: 0.16734374761581422\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.16734374761581422  to: 0.1667188286781311\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.1667188286781311  to: 0.16611430644989014\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.16611430644989014  to: 0.1655290961265564\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.1655290961265564  to: 0.16496217250823975\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.16496217250823975  to: 0.16441255807876587\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.16441255807876587  to: 0.16387935876846313\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.16387935876846313  to: 0.1633618116378784\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.1633618116378784  to: 0.1628590226173401\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.1628590226173401  to: 0.1623703956604004\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1623703956604004  to: 0.16189537048339844\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.16189537048339844  to: 0.1614332914352417\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.1614332914352417  to: 0.16098368167877197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 86\n",
      "Improved validation loss from: 0.16098368167877197  to: 0.16054595708847047\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.16054595708847047  to: 0.16011966466903688\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.16011966466903688  to: 0.15970431566238402\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.15970431566238402  to: 0.15929949283599854\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.15929949283599854  to: 0.1589048385620117\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.1589048385620117  to: 0.1585199475288391\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1585199475288391  to: 0.15814447402954102\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.15814447402954102  to: 0.15777807235717772\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15777807235717772  to: 0.15742045640945435\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.15742045640945435  to: 0.15707130432128907\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15707130432128907  to: 0.1567303419113159\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.1567303419113159  to: 0.15639731884002686\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.15639731884002686  to: 0.15607199668884278\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.15607199668884278  to: 0.15575408935546875\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.15575408935546875  to: 0.1554434061050415\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.1554434061050415  to: 0.155139696598053\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.155139696598053  to: 0.15484287738800048\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.15484287738800048  to: 0.15455271005630494\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.15455271005630494  to: 0.1542690396308899\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.1542690396308899  to: 0.15399162769317626\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.15399162769317626  to: 0.15372031927108765\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.15372031927108765  to: 0.1534549117088318\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1534549117088318  to: 0.15319528579711914\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.15319528579711914  to: 0.15294123888015748\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.15294123888015748  to: 0.15269261598587036\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.15269261598587036  to: 0.1524492859840393\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.1524492859840393  to: 0.15221109390258789\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.15221109390258789  to: 0.15197789669036865\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.15197789669036865  to: 0.15174974203109742\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.15174974203109742  to: 0.15152647495269775\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.15152647495269775  to: 0.15130794048309326\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.15130794048309326  to: 0.15109398365020751\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.15109398365020751  to: 0.1508846640586853\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.1508846640586853  to: 0.1506798028945923\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.1506798028945923  to: 0.15047943592071533\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.15047943592071533  to: 0.1502835273742676\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.1502835273742676  to: 0.1500919222831726\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.1500919222831726  to: 0.14990460872650146\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14990460872650146  to: 0.1497214674949646\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.1497214674949646  to: 0.14954246282577516\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14954246282577516  to: 0.14936741590499877\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14936741590499877  to: 0.1491962194442749\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.1491962194442749  to: 0.1490286707878113\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.1490286707878113  to: 0.1488646984100342\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.1488646984100342  to: 0.14870427846908568\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14870427846908568  to: 0.14854730367660524\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14854730367660524  to: 0.14839365482330322\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14839365482330322  to: 0.14824316501617432\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14824316501617432  to: 0.14809576272964478\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14809576272964478  to: 0.147951340675354\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.147951340675354  to: 0.14780980348587036\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14780980348587036  to: 0.14767104387283325\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14767104387283325  to: 0.14753495454788207\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14753495454788207  to: 0.14740147590637206\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14740147590637206  to: 0.14727054834365844\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14727054834365844  to: 0.14714218378067018\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14714218378067018  to: 0.1470163345336914\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.1470163345336914  to: 0.14689288139343262\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14689288139343262  to: 0.14677175283432006\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14677175283432006  to: 0.1466529130935669\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.1466529130935669  to: 0.1465362310409546\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1465362310409546  to: 0.14642181396484374\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14642181396484374  to: 0.14630969762802123\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.14630969762802123  to: 0.14619979858398438\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14619979858398438  to: 0.14609203338623047\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14609203338623047  to: 0.14598630666732787\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14598630666732787  to: 0.14588254690170288\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14588254690170288  to: 0.1457806944847107\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.1457806944847107  to: 0.1456805944442749\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.1456805944442749  to: 0.1455822229385376\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.1455822229385376  to: 0.14548553228378297\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14548553228378297  to: 0.14539049863815307\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14539049863815307  to: 0.14529707431793212\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14529707431793212  to: 0.14520516395568847\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14520516395568847  to: 0.1451147675514221\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.1451147675514221  to: 0.14502581357955932\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14502581357955932  to: 0.14493825435638427\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14493825435638427  to: 0.14485204219818115\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14485204219818115  to: 0.14476717710494996\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14476717710494996  to: 0.14468358755111693\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14468358755111693  to: 0.14460127353668212\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.14460127353668212  to: 0.14452016353607178\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.14452016353607178  to: 0.14444025754928588\n",
      "Training iteration: 169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14444025754928588  to: 0.1443614959716797\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.1443614959716797  to: 0.14428389072418213\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14428389072418213  to: 0.1442073702812195\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.1442073702812195  to: 0.1441319465637207\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.1441319465637207  to: 0.14405757188796997\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14405757188796997  to: 0.14398422241210937\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14398422241210937  to: 0.14391186237335205\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14391186237335205  to: 0.14384050369262696\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14384050369262696  to: 0.14377022981643678\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.14377022981643678  to: 0.14370089769363403\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14370089769363403  to: 0.14363254308700563\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14363254308700563  to: 0.14356508255004882\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14356508255004882  to: 0.14349853992462158\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14349853992462158  to: 0.1434329032897949\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.1434329032897949  to: 0.14336814880371093\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14336814880371093  to: 0.14330424070358277\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14330424070358277  to: 0.14324116706848145\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.14324116706848145  to: 0.14317891597747803\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.14317891597747803  to: 0.1431174397468567\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.1431174397468567  to: 0.14305672645568848\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.14305672645568848  to: 0.14299676418304444\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.14299676418304444  to: 0.14293750524520873\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.14293750524520873  to: 0.1428789734840393\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.1428789734840393  to: 0.1428211212158203\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.1428211212158203  to: 0.1427639365196228\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.1427639365196228  to: 0.1427075147628784\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.1427075147628784  to: 0.1426518440246582\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.1426518440246582  to: 0.14259687662124634\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14259687662124634  to: 0.14254257678985596\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.14254257678985596  to: 0.1424889326095581\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1424889326095581  to: 0.14243595600128173\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.14243595600128173  to: 0.1423835873603821\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.1423835873603821  to: 0.14233179092407228\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14233179092407228  to: 0.14228057861328125\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.14228057861328125  to: 0.14222991466522217\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.14222991466522217  to: 0.14217979907989503\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.14217979907989503  to: 0.1421302080154419\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.1421302080154419  to: 0.14208110570907592\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.14208110570907592  to: 0.14203250408172607\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.14203250408172607  to: 0.14198439121246337\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.14198439121246337  to: 0.141936731338501\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.141936731338501  to: 0.14188953638076782\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.14188953638076782  to: 0.14184277057647704\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.14184277057647704  to: 0.14179642200469972\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.14179642200469972  to: 0.14175050258636473\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.14175050258636473  to: 0.1417050004005432\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.1417050004005432  to: 0.14165998697280885\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.14165998697280885  to: 0.14161545038223267\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.14161545038223267  to: 0.14157135486602784\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.14157135486602784  to: 0.1415277361869812\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1415277361869812  to: 0.1414845108985901\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.1414845108985901  to: 0.1414417028427124\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.1414417028427124  to: 0.14139926433563232\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.14139926433563232  to: 0.14135721921920777\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.14135721921920777  to: 0.14131553173065187\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.14131553173065187  to: 0.1412742018699646\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.1412742018699646  to: 0.14123320579528809\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.14123320579528809  to: 0.14119253158569336\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.14119253158569336  to: 0.14115216732025146\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.14115216732025146  to: 0.1411121129989624\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.1411121129989624  to: 0.14107234477996827\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.14107234477996827  to: 0.14103286266326903\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.14103286266326903  to: 0.14099366664886476\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.14099366664886476  to: 0.14095473289489746\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.14095473289489746  to: 0.1409160614013672\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.1409160614013672  to: 0.14087765216827391\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.14087765216827391  to: 0.14083948135375976\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.14083948135375976  to: 0.14080156087875367\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.14080156087875367  to: 0.1407638669013977\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1407638669013977  to: 0.14072638750076294\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.14072638750076294  to: 0.1406891465187073\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.1406891465187073  to: 0.14065210819244384\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.14065210819244384  to: 0.1406152844429016\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.1406152844429016  to: 0.14057861566543578\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.14057861566543578  to: 0.14054208993911743\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.14054208993911743  to: 0.14050571918487548\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.14050571918487548  to: 0.14046947956085204\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.14046947956085204  to: 0.14043328762054444\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.14043328762054444  to: 0.14039719104766846\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.14039719104766846  to: 0.14036126136779786\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.14036126136779786  to: 0.1403254508972168\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.1403254508972168  to: 0.1402898073196411\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.1402898073196411  to: 0.14025428295135497\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.14025428295135497  to: 0.1402189016342163\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.1402189016342163  to: 0.1401836633682251\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.1401836633682251  to: 0.14014856815338134\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.14014856815338134  to: 0.14011358022689818\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.14011358022689818  to: 0.14007875919342042\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.14007875919342042  to: 0.14004404544830323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 258\n",
      "Improved validation loss from: 0.14004404544830323  to: 0.1400094747543335\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.1400094747543335  to: 0.13997501134872437\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13997501134872437  to: 0.13994070291519164\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13994070291519164  to: 0.13990650177001954\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.13990650177001954  to: 0.13987243175506592\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13987243175506592  to: 0.13983848094940185\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13983848094940185  to: 0.13980462551116943\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13980462551116943  to: 0.13977088928222656\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13977088928222656  to: 0.13973729610443114\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13973729610443114  to: 0.13970377445220947\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.13970377445220947  to: 0.13967028856277466\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13967028856277466  to: 0.13963690996170045\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.13963690996170045  to: 0.1396036386489868\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.1396036386489868  to: 0.1395704984664917\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1395704984664917  to: 0.13953764438629152\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13953764438629152  to: 0.139504873752594\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.139504873752594  to: 0.13947221040725707\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13947221040725707  to: 0.1394396424293518\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.1394396424293518  to: 0.13940718173980712\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13940718173980712  to: 0.13937480449676515\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13937480449676515  to: 0.1393425226211548\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1393425226211548  to: 0.13931033611297608\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13931033611297608  to: 0.13927824497222902\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13927824497222902  to: 0.13924623727798463\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13924623727798463  to: 0.13921431303024293\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13921431303024293  to: 0.13918248414993287\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13918248414993287  to: 0.1391507387161255\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1391507387161255  to: 0.1391190767288208\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.1391190767288208  to: 0.13908748626708983\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13908748626708983  to: 0.13905599117279052\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13905599117279052  to: 0.139024555683136\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.139024555683136  to: 0.13899320363998413\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13899320363998413  to: 0.13896192312240602\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13896192312240602  to: 0.13893072605133056\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13893072605133056  to: 0.1388996124267578\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.1388996124267578  to: 0.13886855840682982\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13886855840682982  to: 0.13883756399154662\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13883756399154662  to: 0.13880665302276612\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13880665302276612  to: 0.13877580165863038\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13877580165863038  to: 0.1387450337409973\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1387450337409973  to: 0.13871432542800904\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13871432542800904  to: 0.13868367671966553\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13868367671966553  to: 0.13865308761596679\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13865308761596679  to: 0.13862257003784179\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13862257003784179  to: 0.13859211206436156\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13859211206436156  to: 0.13856172561645508\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13856172561645508  to: 0.13853137493133544\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13853137493133544  to: 0.13850109577178954\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13850109577178954  to: 0.13847087621688842\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13847087621688842  to: 0.13844072818756104\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13844072818756104  to: 0.13841060400009156\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13841060400009156  to: 0.13838056325912476\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13838056325912476  to: 0.13835058212280274\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13835058212280274  to: 0.13832062482833862\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13832062482833862  to: 0.13829073905944825\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13829073905944825  to: 0.13826091289520265\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13826091289520265  to: 0.1382311224937439\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.1382311224937439  to: 0.13820140361785888\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13820140361785888  to: 0.13817174434661866\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13817174434661866  to: 0.1381421208381653\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.1381421208381653  to: 0.13811256885528564\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13811256885528564  to: 0.13808305263519288\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13808305263519288  to: 0.1380535840988159\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.1380535840988159  to: 0.1380241632461548\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1380241632461548  to: 0.13799481391906737\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13799481391906737  to: 0.13796548843383788\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13796548843383788  to: 0.13793621063232422\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13793621063232422  to: 0.1379069685935974\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.1379069685935974  to: 0.13787779808044434\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13787779808044434  to: 0.1378486394882202\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.1378486394882202  to: 0.13781955242156982\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13781955242156982  to: 0.13779048919677733\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13779048919677733  to: 0.13776147365570068\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13776147365570068  to: 0.13773249387741088\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.13773249387741088  to: 0.13770357370376587\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13770357370376587  to: 0.13767467737197875\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13767467737197875  to: 0.13764578104019165\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13764578104019165  to: 0.1376168966293335\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.1376168966293335  to: 0.13758808374404907\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13758808374404907  to: 0.13755929470062256\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13755929470062256  to: 0.13753054141998292\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13753054141998292  to: 0.13750183582305908\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13750183582305908  to: 0.13747315406799315\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13747315406799315  to: 0.13744451999664306\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13744451999664306  to: 0.13741590976715087\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13741590976715087  to: 0.13738735914230346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13738735914230346  to: 0.13735883235931395\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13735883235931395  to: 0.13733036518096925\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13733036518096925  to: 0.13730194568634033\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13730194568634033  to: 0.1372735619544983\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.1372735619544983  to: 0.13724523782730103\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13724523782730103  to: 0.1372169852256775\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.1372169852256775  to: 0.13718879222869873\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13718879222869873  to: 0.13716064691543578\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.13716064691543578  to: 0.13713253736495973\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13713253736495973  to: 0.13710447549819946\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13710447549819946  to: 0.13707643747329712\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13707643747329712  to: 0.13704845905303956\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.13704845905303956  to: 0.13702051639556884\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13702051639556884  to: 0.1369925856590271\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1369925856590271  to: 0.13696470260620117\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13696470260620117  to: 0.13693686723709106\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.13693686723709106  to: 0.13690905570983886\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13690905570983886  to: 0.13688126802444459\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13688126802444459  to: 0.13685352802276612\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13685352802276612  to: 0.1368258237838745\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.1368258237838745  to: 0.1367981553077698\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.1367981553077698  to: 0.13677051067352294\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13677051067352294  to: 0.136742901802063\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.136742901802063  to: 0.13671534061431884\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13671534061431884  to: 0.1366877794265747\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.1366877794265747  to: 0.13666026592254638\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13666026592254638  to: 0.13663278818130492\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.13663278818130492  to: 0.1366053342819214\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.1366053342819214  to: 0.13657791614532472\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13657791614532472  to: 0.13655052185058594\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13655052185058594  to: 0.13652317523956298\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.13652317523956298  to: 0.136495840549469\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.136495840549469  to: 0.1364685297012329\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1364685297012329  to: 0.13644126653671265\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13644126653671265  to: 0.1364140272140503\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.1364140272140503  to: 0.13638681173324585\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13638681173324585  to: 0.13635963201522827\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.13635963201522827  to: 0.13633248805999756\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13633248805999756  to: 0.13630536794662476\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13630536794662476  to: 0.13627828359603883\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13627828359603883  to: 0.1362512230873108\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.1362512230873108  to: 0.13622418642044068\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13622418642044068  to: 0.13619717359542846\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13619717359542846  to: 0.13617019653320311\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13617019653320311  to: 0.1361432433128357\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1361432433128357  to: 0.1361163377761841\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1361163377761841  to: 0.13608944416046143\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.13608944416046143  to: 0.1360625743865967\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.1360625743865967  to: 0.13603575229644777\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.13603575229644777  to: 0.13600895404815674\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13600895404815674  to: 0.13598215579986572\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13598215579986572  to: 0.13595541715621948\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13595541715621948  to: 0.13592870235443116\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13592870235443116  to: 0.1359019994735718\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1359019994735718  to: 0.13587534427642822\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.13587534427642822  to: 0.1358487129211426\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.1358487129211426  to: 0.1358220934867859\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.1358220934867859  to: 0.135795521736145\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.135795521736145  to: 0.13576896190643312\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.13576896190643312  to: 0.13574243783950807\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.13574243783950807  to: 0.13571593761444092\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13571593761444092  to: 0.13568947315216065\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13568947315216065  to: 0.13566303253173828\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13566303253173828  to: 0.13563660383224488\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13563660383224488  to: 0.13561023473739625\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.13561023473739625  to: 0.13558387756347656\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13558387756347656  to: 0.13555755615234374\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13555755615234374  to: 0.13553125858306886\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.13553125858306886  to: 0.13550498485565185\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13550498485565185  to: 0.13547875881195068\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.13547875881195068  to: 0.1354525566101074\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1354525566101074  to: 0.13542639017105101\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13542639017105101  to: 0.13540024757385255\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13540024757385255  to: 0.13537414073944093\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.13537414073944093  to: 0.13534809350967408\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.13534809350967408  to: 0.13532204627990724\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13532204627990724  to: 0.13529603481292723\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.13529603481292723  to: 0.13527008295059204\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.13527008295059204  to: 0.13524415493011474\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13524415493011474  to: 0.13521825075149535\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.13521825075149535  to: 0.13519238233566283\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13519238233566283  to: 0.13516656160354615\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13516656160354615  to: 0.13514076471328734\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13514076471328734  to: 0.13511501550674437\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.13511501550674437  to: 0.13508929014205934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13508929014205934  to: 0.13506361246109008\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.13506361246109008  to: 0.1350379467010498\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1350379467010498  to: 0.13501231670379638\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.13501231670379638  to: 0.1349867343902588\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1349867343902588  to: 0.1349611759185791\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.1349611759185791  to: 0.13493564128875732\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13493564128875732  to: 0.13491014242172242\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13491014242172242  to: 0.13488467931747436\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13488467931747436  to: 0.13485925197601317\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13485925197601317  to: 0.13483383655548095\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13483383655548095  to: 0.13480846881866454\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.13480846881866454  to: 0.1347831130027771\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1347831130027771  to: 0.13475781679153442\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.13475781679153442  to: 0.13473252058029175\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13473252058029175  to: 0.13470726013183593\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.13470726013183593  to: 0.13468226194381713\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.13468226194381713  to: 0.13465735912322999\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13465735912322999  to: 0.13463252782821655\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13463252782821655  to: 0.13460772037506102\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.13460772037506102  to: 0.1345829725265503\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.1345829725265503  to: 0.13455827236175538\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13455827236175538  to: 0.1345336079597473\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.1345336079597473  to: 0.13450899124145507\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13450899124145507  to: 0.1344844102859497\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.1344844102859497  to: 0.13445985317230225\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13445985317230225  to: 0.13443535566329956\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.13443535566329956  to: 0.13441089391708375\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13441089391708375  to: 0.13438646793365477\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13438646793365477  to: 0.13436208963394164\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.13436208963394164  to: 0.13433773517608644\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13433773517608644  to: 0.13431341648101808\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.13431341648101808  to: 0.13428912162780762\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.13428912162780762  to: 0.13426487445831298\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13426487445831298  to: 0.13424065113067626\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.13424065113067626  to: 0.13421646356582642\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13421646356582642  to: 0.13419231176376342\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13419231176376342  to: 0.13416818380355836\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13416818380355836  to: 0.13414409160614013\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13414409160614013  to: 0.13412003517150878\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.13412003517150878  to: 0.1340959906578064\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.1340959906578064  to: 0.13407201766967775\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13407201766967775  to: 0.13404804468154907\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13404804468154907  to: 0.13402410745620727\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.13402410745620727  to: 0.13400020599365234\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.13400020599365234  to: 0.1339763283729553\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1339763283729553  to: 0.13395248651504515\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.13395248651504515  to: 0.13392865657806396\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13392865657806396  to: 0.13390486240386962\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.13390486240386962  to: 0.1338811159133911\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1338811159133911  to: 0.13385738134384156\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13385738134384156  to: 0.13383368253707886\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13383368253707886  to: 0.13381001949310303\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.13381001949310303  to: 0.13378636837005614\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.13378636837005614  to: 0.1337627649307251\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.1337627649307251  to: 0.1337391972541809\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.1337391972541809  to: 0.13371564149856568\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13371564149856568  to: 0.13369210958480834\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.13369210958480834  to: 0.1336686134338379\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.1336686134338379  to: 0.13364516496658324\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13364516496658324  to: 0.13362172842025757\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.13362172842025757  to: 0.13359835147857665\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.13359835147857665  to: 0.13357502222061157\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.13357502222061157  to: 0.13355175256729127\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.13355175256729127  to: 0.1335285186767578\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.1335285186767578  to: 0.13350534439086914\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13350534439086914  to: 0.13348225355148316\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13348225355148316  to: 0.13345922231674195\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.13345922231674195  to: 0.1334362506866455\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.1334362506866455  to: 0.13341333866119384\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.13341333866119384  to: 0.13339048624038696\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.13339048624038696  to: 0.13336766958236695\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.13336766958236695  to: 0.1333449125289917\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1333449125289917  to: 0.13332217931747437\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13332217931747437  to: 0.13329951763153075\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.13329951763153075  to: 0.13327689170837403\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.13327689170837403  to: 0.1332543134689331\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.1332543134689331  to: 0.13323177099227906\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.13323177099227906  to: 0.13320926427841187\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.13320926427841187  to: 0.1331868052482605\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.1331868052482605  to: 0.13316441774368287\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.13316441774368287  to: 0.13314205408096313\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.13314205408096313  to: 0.1331197738647461\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.1331197738647461  to: 0.13309752941131592\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.13309752941131592  to: 0.1330753445625305\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.1330753445625305  to: 0.133053195476532\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.133053195476532  to: 0.13303108215332032\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.13303108215332032  to: 0.13300902843475343\n",
      "Training iteration: 516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13300902843475343  to: 0.13298707008361815\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.13298707008361815  to: 0.13296515941619874\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.13296515941619874  to: 0.1329432964324951\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.1329432964324951  to: 0.13292144536972045\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.13292144536972045  to: 0.13289966583251953\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.13289966583251953  to: 0.13287789821624757\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13287789821624757  to: 0.1328561782836914\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.1328561782836914  to: 0.13283450603485109\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.13283450603485109  to: 0.13281282186508178\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.13281282186508178  to: 0.1327911615371704\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.1327911615371704  to: 0.13276951313018798\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.13276951313018798  to: 0.13274792432785035\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.13274792432785035  to: 0.1327263593673706\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.1327263593673706  to: 0.13270483016967774\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.13270483016967774  to: 0.13268333673477173\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.13268333673477173  to: 0.13266185522079468\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13266185522079468  to: 0.13264042139053345\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.13264042139053345  to: 0.13261899948120118\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13261899948120118  to: 0.13259761333465575\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.13259761333465575  to: 0.13257642984390258\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.13257642984390258  to: 0.13255544900894164\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.13255544900894164  to: 0.13253459930419922\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.13253459930419922  to: 0.1325139045715332\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.1325139045715332  to: 0.1324933648109436\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.1324933648109436  to: 0.13247294425964357\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13247294425964357  to: 0.132452654838562\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.132452654838562  to: 0.13243247270584108\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13243247270584108  to: 0.1324123978614807\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.1324123978614807  to: 0.132392418384552\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.132392418384552  to: 0.13237251043319703\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13237251043319703  to: 0.13235270977020264\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.13235270977020264  to: 0.13233296871185302\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.13233296871185302  to: 0.1323133111000061\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.1323133111000061  to: 0.13229371309280397\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.13229371309280397  to: 0.13227417469024658\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13227417469024658  to: 0.13225470781326293\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.13225470781326293  to: 0.13223528861999512\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13223528861999512  to: 0.13221592903137208\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.13221592903137208  to: 0.13219661712646485\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.13219661712646485  to: 0.13217732906341553\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.13217732906341553  to: 0.13215811252593995\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.13215811252593995  to: 0.13213891983032228\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.13213891983032228  to: 0.1321197748184204\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.1321197748184204  to: 0.13210066556930541\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.13210066556930541  to: 0.13208158016204835\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.13208158016204835  to: 0.13206255435943604\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13206255435943604  to: 0.13204351663589478\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.13204351663589478  to: 0.1320245385169983\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.1320245385169983  to: 0.13200557231903076\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.13200557231903076  to: 0.1319866418838501\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.1319866418838501  to: 0.13196773529052735\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.13196773529052735  to: 0.1319488286972046\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.1319488286972046  to: 0.13192996978759766\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.13192996978759766  to: 0.13191114664077758\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.13191114664077758  to: 0.13189232349395752\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13189232349395752  to: 0.13187352418899537\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.13187352418899537  to: 0.13185474872589112\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.13185474872589112  to: 0.1318359851837158\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.1318359851837158  to: 0.13181726932525634\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.13181726932525634  to: 0.13179852962493896\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.13179852962493896  to: 0.1317798376083374\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.1317798376083374  to: 0.13176116943359376\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.13176116943359376  to: 0.13174251317977906\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.13174251317977906  to: 0.13172385692596436\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.13172385692596436  to: 0.13170523643493653\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.13170523643493653  to: 0.13168662786483765\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.13168662786483765  to: 0.13166803121566772\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.13166803121566772  to: 0.13164947032928467\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.13164947032928467  to: 0.13163092136383056\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.13163092136383056  to: 0.13161239624023438\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.13161239624023438  to: 0.13159393072128295\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13159393072128295  to: 0.13157551288604735\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.13157551288604735  to: 0.13155713081359863\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.13155713081359863  to: 0.131538724899292\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.131538724899292  to: 0.1315203070640564\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.1315203070640564  to: 0.13150193691253662\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.13150193691253662  to: 0.1314835786819458\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.1314835786819458  to: 0.13146529197692872\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.13146529197692872  to: 0.13144704103469848\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.13144704103469848  to: 0.13142882585525512\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.13142882585525512  to: 0.13141067028045655\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.13141067028045655  to: 0.13139253854751587\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.13139253854751587  to: 0.13137443065643312\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.13137443065643312  to: 0.1313563585281372\n",
      "Training iteration: 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1313563585281372  to: 0.13133833408355713\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.13133833408355713  to: 0.13132035732269287\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.13132035732269287  to: 0.13130242824554444\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.13130242824554444  to: 0.13128457069396973\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.13128457069396973  to: 0.1312667489051819\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.1312667489051819  to: 0.13124897480010986\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.13124897480010986  to: 0.1312312364578247\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.1312312364578247  to: 0.13121354579925537\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.13121354579925537  to: 0.1311958909034729\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.1311958909034729  to: 0.13117830753326415\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.13117830753326415  to: 0.13116075992584228\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.13116075992584228  to: 0.13114326000213622\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.13114326000213622  to: 0.1311257839202881\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.1311257839202881  to: 0.13110835552215577\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.13110835552215577  to: 0.13109095096588136\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.13109095096588136  to: 0.1310735821723938\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1310735821723938  to: 0.1310562252998352\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.1310562252998352  to: 0.13103891611099244\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.13103891611099244  to: 0.1310216188430786\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.1310216188430786  to: 0.13100435733795165\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.13100435733795165  to: 0.13098710775375366\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.13098710775375366  to: 0.13096990585327148\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.13096990585327148  to: 0.13095285892486572\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.13095285892486572  to: 0.1309359312057495\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.1309359312057495  to: 0.13091915845870972\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.13091915845870972  to: 0.1309024930000305\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.1309024930000305  to: 0.13088594675064086\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.13088594675064086  to: 0.1308694839477539\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1308694839477539  to: 0.13085311651229858\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.13085311651229858  to: 0.1308368444442749\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1308368444442749  to: 0.13082048892974854\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.13082048892974854  to: 0.13080408573150634\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.13080408573150634  to: 0.13078763484954833\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.13078763484954833  to: 0.13077110052108765\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.13077110052108765  to: 0.130754554271698\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.130754554271698  to: 0.13073800802230834\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.13073800802230834  to: 0.13072144985198975\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.13072144985198975  to: 0.130704927444458\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.130704927444458  to: 0.13068853616714476\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.13068853616714476  to: 0.13067225217819214\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.13067225217819214  to: 0.13065611124038695\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.13065611124038695  to: 0.1306399703025818\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.1306399703025818  to: 0.13062384128570556\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.13062384128570556  to: 0.1306077003479004\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.1306077003479004  to: 0.13059157133102417\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.13059157133102417  to: 0.13057546615600585\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.13057546615600585  to: 0.1305594801902771\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.1305594801902771  to: 0.1305435299873352\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.1305435299873352  to: 0.13052762746810914\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.13052762746810914  to: 0.13051176071166992\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.13051176071166992  to: 0.13049592971801757\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.13049592971801757  to: 0.13048012256622316\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.13048012256622316  to: 0.13046433925628662\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.13046433925628662  to: 0.13044859170913697\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.13044859170913697  to: 0.13043301105499266\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.13043301105499266  to: 0.13041757345199584\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.13041757345199584  to: 0.13040215969085694\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.13040215969085694  to: 0.13038672208786012\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.13038672208786012  to: 0.13037129640579223\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.13037129640579223  to: 0.1303558588027954\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.1303558588027954  to: 0.13034044504165648\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.13034044504165648  to: 0.13032501935958862\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.13032501935958862  to: 0.1303096055984497\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.1303096055984497  to: 0.13029419183731078\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.13029419183731078  to: 0.13027876615524292\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.13027876615524292  to: 0.13026336431503296\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.13026336431503296  to: 0.13024797439575195\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.13024797439575195  to: 0.13023271560668945\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.13023271560668945  to: 0.1302175998687744\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1302175998687744  to: 0.13020247220993042\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.13020247220993042  to: 0.1301873207092285\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.1301873207092285  to: 0.13017215728759765\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.13017215728759765  to: 0.1301569700241089\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1301569700241089  to: 0.13014179468154907\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.13014179468154907  to: 0.13012659549713135\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.13012659549713135  to: 0.13011153936386108\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.13011153936386108  to: 0.1300966262817383\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.1300966262817383  to: 0.1300816774368286\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.1300816774368286  to: 0.13006670475006105\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.13006670475006105  to: 0.13005170822143555\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.13005170822143555  to: 0.1300366759300232\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.1300366759300232  to: 0.1300216555595398\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.1300216555595398  to: 0.13000675439834594\n",
      "Training iteration: 683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13000675439834594  to: 0.12999199628829955\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12999199628829955  to: 0.1299771785736084\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.1299771785736084  to: 0.12996234893798828\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12996234893798828  to: 0.12994749546051027\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12994749546051027  to: 0.12993259429931642\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12993259429931642  to: 0.12991783618927003\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.12991783618927003  to: 0.12990319728851318\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12990319728851318  to: 0.12988849878311157\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12988849878311157  to: 0.12987375259399414\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12987375259399414  to: 0.12985897064208984\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12985897064208984  to: 0.12984418869018555\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.12984418869018555  to: 0.1298295259475708\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.1298295259475708  to: 0.1298149824142456\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1298149824142456  to: 0.12980053424835206\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12980053424835206  to: 0.12978605031967164\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12978605031967164  to: 0.12977149486541747\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12977149486541747  to: 0.12975698709487915\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12975698709487915  to: 0.12974246740341186\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.12974246740341186  to: 0.1297279953956604\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.1297279953956604  to: 0.12971352338790892\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12971352338790892  to: 0.12969907522201538\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12969907522201538  to: 0.1296846389770508\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.1296846389770508  to: 0.1296703815460205\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1296703815460205  to: 0.1296562910079956\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.1296562910079956  to: 0.12964217662811278\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12964217662811278  to: 0.12962806224822998\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12962806224822998  to: 0.12961392402648925\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12961392402648925  to: 0.12959979772567748\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.12959979772567748  to: 0.1295856714248657\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.1295856714248657  to: 0.1295715093612671\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.1295715093612671  to: 0.12955738306045533\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12955738306045533  to: 0.1295432448387146\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.1295432448387146  to: 0.12952911853790283\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12952911853790283  to: 0.1295149803161621\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.1295149803161621  to: 0.1295008659362793\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.1295008659362793  to: 0.1294869065284729\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.1294869065284729  to: 0.12947309017181396\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12947309017181396  to: 0.12945926189422607\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.12945926189422607  to: 0.12944542169570922\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12944542169570922  to: 0.12943153381347655\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12943153381347655  to: 0.12941765785217285\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12941765785217285  to: 0.12940375804901122\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12940375804901122  to: 0.12938988208770752\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12938988208770752  to: 0.12937595844268798\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12937595844268798  to: 0.12936221361160277\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.12936221361160277  to: 0.12934859991073608\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12934859991073608  to: 0.12933496236801148\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12933496236801148  to: 0.12932121753692627\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12932121753692627  to: 0.12930736541748047\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12930736541748047  to: 0.12929344177246094\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12929344177246094  to: 0.12927958965301514\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12927958965301514  to: 0.12926584482192993\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12926584482192993  to: 0.12925221920013427\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12925221920013427  to: 0.12923874855041503\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12923874855041503  to: 0.12922526597976686\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12922526597976686  to: 0.12921180725097656\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12921180725097656  to: 0.12919833660125732\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12919833660125732  to: 0.12918486595153808\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12918486595153808  to: 0.1291714072227478\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.1291714072227478  to: 0.12915796041488647\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12915796041488647  to: 0.12914443016052246\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12914443016052246  to: 0.12913092374801635\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12913092374801635  to: 0.1291174292564392\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.1291174292564392  to: 0.12910393476486207\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12910393476486207  to: 0.12909038066864015\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12909038066864015  to: 0.1290770173072815\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.1290770173072815  to: 0.12906382083892823\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12906382083892823  to: 0.129050612449646\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.129050612449646  to: 0.12903733253479005\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12903733253479005  to: 0.12902404069900514\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12902404069900514  to: 0.12901074886322023\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12901074886322023  to: 0.12899757623672486\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12899757623672486  to: 0.12898423671722412\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12898423671722412  to: 0.1289709448814392\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.1289709448814392  to: 0.1289576292037964\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1289576292037964  to: 0.12894448041915893\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12894448041915893  to: 0.12893131971359253\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12893131971359253  to: 0.12891814708709717\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12891814708709717  to: 0.12890485525131226\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12890485525131226  to: 0.12889162302017212\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12889162302017212  to: 0.12887856960296631\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12887856960296631  to: 0.1288654923439026\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.1288654923439026  to: 0.12885236740112305\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12885236740112305  to: 0.12883923053741456\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12883923053741456  to: 0.12882606983184813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12882606983184813  to: 0.12881290912628174\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12881290912628174  to: 0.1287997245788574\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.1287997245788574  to: 0.1287865400314331\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.1287865400314331  to: 0.12877352237701417\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12877352237701417  to: 0.12876039743423462\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12876039743423462  to: 0.1287473440170288\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.1287473440170288  to: 0.12873445749282836\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12873445749282836  to: 0.12872153520584106\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12872153520584106  to: 0.12870858907699584\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12870858907699584  to: 0.12869564294815064\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12869564294815064  to: 0.12868263721466064\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12868263721466064  to: 0.1286696195602417\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1286696195602417  to: 0.1286565065383911\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.1286565065383911  to: 0.12864353656768798\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12864353656768798  to: 0.12863075733184814\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12863075733184814  to: 0.12861793041229247\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12861793041229247  to: 0.12860496044158937\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12860496044158937  to: 0.12859196662902833\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12859196662902833  to: 0.12857894897460936\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12857894897460936  to: 0.1285660982131958\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.1285660982131958  to: 0.12855340242385865\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12855340242385865  to: 0.12854063510894775\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12854063510894775  to: 0.12852783203125\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12852783203125  to: 0.12851487398147582\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12851487398147582  to: 0.12850176095962523\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12850176095962523  to: 0.12848882675170897\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12848882675170897  to: 0.12847602367401123\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12847602367401123  to: 0.1284632682800293\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.1284632682800293  to: 0.1284505009651184\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.1284505009651184  to: 0.1284376859664917\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.1284376859664917  to: 0.1284250259399414\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.1284250259399414  to: 0.1284123182296753\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.1284123182296753  to: 0.1283995509147644\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.1283995509147644  to: 0.1283867597579956\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.1283867597579956  to: 0.1283740997314453\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.1283740997314453  to: 0.12836127281188964\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12836127281188964  to: 0.1283484697341919\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.1283484697341919  to: 0.12833579778671264\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12833579778671264  to: 0.12832307815551758\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12832307815551758  to: 0.12831032276153564\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12831032276153564  to: 0.12829749584197997\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.12829749584197997  to: 0.12828462123870848\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.12828462123870848  to: 0.1282719373703003\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.1282719373703003  to: 0.12825934886932372\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12825934886932372  to: 0.12824671268463134\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12824671268463134  to: 0.12823400497436524\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12823400497436524  to: 0.12822109460830688\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12822109460830688  to: 0.1282080054283142\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.1282080054283142  to: 0.1281950831413269\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.1281950831413269  to: 0.128182315826416\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.128182315826416  to: 0.12816967964172363\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12816967964172363  to: 0.1281571626663208\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.1281571626663208  to: 0.12814456224441528\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12814456224441528  to: 0.12813185453414916\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.12813185453414916  to: 0.12811909914016723\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12811909914016723  to: 0.12810628414154052\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12810628414154052  to: 0.12809340953826903\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12809340953826903  to: 0.12808047533035277\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12808047533035277  to: 0.12806733846664428\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12806733846664428  to: 0.12805436849594115\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12805436849594115  to: 0.12804155349731444\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12804155349731444  to: 0.12802886962890625\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12802886962890625  to: 0.12801616191864013\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.12801616191864013  to: 0.12800354957580568\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12800354957580568  to: 0.12799085378646852\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12799085378646852  to: 0.12797809839248658\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.12797809839248658  to: 0.12796523571014404\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12796523571014404  to: 0.12795234918594361\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12795234918594361  to: 0.12793939113616942\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12793939113616942  to: 0.12792621850967406\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12792621850967406  to: 0.1279132127761841\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.1279132127761841  to: 0.12790037393569947\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12790037393569947  to: 0.12788765430450438\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12788765430450438  to: 0.12787506580352784\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12787506580352784  to: 0.1278625726699829\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.1278625726699829  to: 0.12785003185272217\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12785003185272217  to: 0.12783724069595337\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12783724069595337  to: 0.12782422304153443\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12782422304153443  to: 0.12781113386154175\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.12781113386154175  to: 0.12779836654663085\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12779836654663085  to: 0.12778589725494385\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12778589725494385  to: 0.12777357101440429\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.12777357101440429  to: 0.1277613401412964\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.1277613401412964  to: 0.1277490019798279\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.1277490019798279  to: 0.12773659229278564\n",
      "Training iteration: 853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12773659229278564  to: 0.1277240753173828\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.1277240753173828  to: 0.12771146297454833\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.12771146297454833  to: 0.1276987910270691\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.1276987910270691  to: 0.1276862382888794\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.1276862382888794  to: 0.12767382860183715\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12767382860183715  to: 0.12766149044036865\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.12766149044036865  to: 0.1276490569114685\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.1276490569114685  to: 0.12763651609420776\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12763651609420776  to: 0.12762370109558105\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12762370109558105  to: 0.12761081457138063\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.12761081457138063  to: 0.12759807109832763\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12759807109832763  to: 0.12758547067642212\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.12758547067642212  to: 0.1275729537010193\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1275729537010193  to: 0.12756054401397704\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12756054401397704  to: 0.1275482416152954\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.1275482416152954  to: 0.12753578424453735\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12753578424453735  to: 0.1275231957435608\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.1275231957435608  to: 0.12751048803329468\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12751048803329468  to: 0.12749770879745484\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.12749770879745484  to: 0.12748481035232545\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12748481035232545  to: 0.12747204303741455\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.12747204303741455  to: 0.1274593710899353\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.1274593710899353  to: 0.12744659185409546\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12744659185409546  to: 0.1274339199066162\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.1274339199066162  to: 0.1274213433265686\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.1274213433265686  to: 0.12740862369537354\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12740862369537354  to: 0.12739579677581786\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.12739579677581786  to: 0.1273826241493225\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.1273826241493225  to: 0.12736959457397462\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.12736959457397462  to: 0.12735668420791627\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.12735668420791627  to: 0.12734386920928956\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12734386920928956  to: 0.1273311734199524\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.1273311734199524  to: 0.12731858491897582\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.12731858491897582  to: 0.12730607986450196\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12730607986450196  to: 0.12729346752166748\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12729346752166748  to: 0.12728071212768555\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12728071212768555  to: 0.12726781368255616\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.12726781368255616  to: 0.12725480794906616\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12725480794906616  to: 0.12724144458770753\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.12724144458770753  to: 0.12722822427749633\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.12722822427749633  to: 0.1272151231765747\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.1272151231765747  to: 0.12720212936401368\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.12720212936401368  to: 0.12718931436538697\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.12718931436538697  to: 0.12717655897140503\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12717655897140503  to: 0.12716389894485475\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.12716389894485475  to: 0.12715128660202027\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.12715128660202027  to: 0.12713873386383057\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12713873386383057  to: 0.1271257519721985\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.1271257519721985  to: 0.1271126389503479\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.1271126389503479  to: 0.1270995855331421\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.1270995855331421  to: 0.12708666324615478\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12708666324615478  to: 0.12707356214523316\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12707356214523316  to: 0.12706056833267212\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12706056833267212  to: 0.12704769372940064\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.12704769372940064  to: 0.12703473567962648\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.12703473567962648  to: 0.12702165842056273\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12702165842056273  to: 0.12700867652893066\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.12700867652893066  to: 0.12699551582336427\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12699551582336427  to: 0.12698246240615846\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12698246240615846  to: 0.12696948051452636\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.12696948051452636  to: 0.12695658206939697\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12695658206939697  to: 0.12694377899169923\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.12694377899169923  to: 0.1269307851791382\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.1269307851791382  to: 0.12691764831542968\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.12691764831542968  to: 0.12690436840057373\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.12690436840057373  to: 0.1268909454345703\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.1268909454345703  to: 0.12687762975692748\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12687762975692748  to: 0.12686444520950318\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12686444520950318  to: 0.1268513321876526\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.1268513321876526  to: 0.1268382787704468\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.1268382787704468  to: 0.12682502269744872\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12682502269744872  to: 0.12681185007095336\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.12681185007095336  to: 0.12679873704910277\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12679873704910277  to: 0.12678568363189696\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12678568363189696  to: 0.12677266597747802\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12677266597747802  to: 0.12675946950912476\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.12675946950912476  to: 0.1267460823059082\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.1267460823059082  to: 0.1267325162887573\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.1267325162887573  to: 0.12671880722045897\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.12671880722045897  to: 0.12670519351959228\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12670519351959228  to: 0.12669165134429933\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12669165134429933  to: 0.126678204536438\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.126678204536438  to: 0.12666479349136353\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12666479349136353  to: 0.12665145397186278\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.12665145397186278  to: 0.12663780450820922\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12663780450820922  to: 0.12662439346313475\n",
      "Training iteration: 939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12662439346313475  to: 0.12661103010177613\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.12661103010177613  to: 0.12659769058227538\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12659769058227538  to: 0.12658437490463256\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12658437490463256  to: 0.12657084465026855\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12657084465026855  to: 0.12655709981918334\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12655709981918334  to: 0.12654316425323486\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.12654316425323486  to: 0.12652931213378907\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12652931213378907  to: 0.12651550769805908\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12651550769805908  to: 0.12650177478790284\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12650177478790284  to: 0.12648807764053344\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.12648807764053344  to: 0.12647440433502197\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.12647440433502197  to: 0.1264607787132263\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.1264607787132263  to: 0.1264477014541626\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.1264477014541626  to: 0.1264347791671753\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.1264347791671753  to: 0.12642216682434082\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12642216682434082  to: 0.12640975713729857\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12640975713729857  to: 0.12639776468276978\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.12639776468276978  to: 0.1263861298561096\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.1263861298561096  to: 0.12637484073638916\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12637484073638916  to: 0.12636383771896362\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12636383771896362  to: 0.12635283470153807\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12635283470153807  to: 0.1263418197631836\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.1263418197631836  to: 0.12633103132247925\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.12633103132247925  to: 0.12632044553756713\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.12632044553756713  to: 0.12631003856658934\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.12631003856658934  to: 0.12629977464675904\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12629977464675904  to: 0.12628939151763915\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12628939151763915  to: 0.12627910375595092\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12627910375595092  to: 0.1262689709663391\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.1262689709663391  to: 0.12625895738601683\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12625895738601683  to: 0.12624852657318114\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12624852657318114  to: 0.12623765468597412\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12623765468597412  to: 0.12622640132904053\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.12622640132904053  to: 0.12621479034423827\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12621479034423827  to: 0.12620283365249635\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12620283365249635  to: 0.12619057893753052\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.12619057893753052  to: 0.12617777585983275\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12617777585983275  to: 0.12616456747055055\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12616456747055055  to: 0.12615123987197877\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12615123987197877  to: 0.12613779306411743\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12613779306411743  to: 0.1261242389678955\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.1261242389678955  to: 0.12611038684844972\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12611038684844972  to: 0.12609630823135376\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12609630823135376  to: 0.12608191967010499\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12608191967010499  to: 0.12606723308563234\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12606723308563234  to: 0.12605255842208862\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12605255842208862  to: 0.12603787183761597\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.12603787183761597  to: 0.12602317333221436\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12602317333221436  to: 0.12600820064544677\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12600820064544677  to: 0.125992751121521\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.125992751121521  to: 0.12597733736038208\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.12597733736038208  to: 0.12596200704574584\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12596200704574584  to: 0.12594645023345946\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.12594645023345946  to: 0.12593095302581786\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12593095302581786  to: 0.1259154796600342\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.1259154796600342  to: 0.12590035200119018\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12590035200119018  to: 0.125885534286499\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.125885534286499  to: 0.12587125301361085\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12587125301361085  to: 0.12585744857788086\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12585744857788086  to: 0.12584381103515624\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12584381103515624  to: 0.1258303165435791\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.1258303165435791  to: 0.1258172035217285\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.1258172035217285  to: 0.12580444812774658\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.12580444812774658  to: 0.12579114437103273\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12579114437103273  to: 0.12577760219573975\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12577760219573975  to: 0.12576385736465454\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12576385736465454  to: 0.1257496476173401\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.1257496476173401  to: 0.12573500871658325\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12573500871658325  to: 0.12572025060653685\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12572025060653685  to: 0.12570540904998778\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12570540904998778  to: 0.12569046020507812\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12569046020507812  to: 0.12567541599273682\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.12567541599273682  to: 0.12566030025482178\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.12566030025482178  to: 0.12564538717269896\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.12564538717269896  to: 0.12563066482543944\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.12563066482543944  to: 0.12561590671539308\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12561590671539308  to: 0.12560157775878905\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12560157775878905  to: 0.1255876302719116\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.1255876302719116  to: 0.125573468208313\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.125573468208313  to: 0.1255590558052063\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.1255590558052063  to: 0.12554442882537842\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12554442882537842  to: 0.12552931308746337\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.12552931308746337  to: 0.12551376819610596\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.12551376819610596  to: 0.12549813985824584\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.12549813985824584  to: 0.125482439994812\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.125482439994812  to: 0.12546722888946532\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.12546722888946532  to: 0.1254524827003479\n",
      "Training iteration: 1026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1254524827003479  to: 0.1254378080368042\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.1254378080368042  to: 0.12542331218719482\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.12542331218719482  to: 0.12540838718414307\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.12540838718414307  to: 0.12539336681365967\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.12539336681365967  to: 0.12537825107574463\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.12537825107574463  to: 0.12536303997039794\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.12536303997039794  to: 0.1253477454185486\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.1253477454185486  to: 0.12533266544342042\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.12533266544342042  to: 0.12531776428222657\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12531776428222657  to: 0.12530304193496705\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.12530304193496705  to: 0.12528846263885499\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.12528846263885499  to: 0.12527428865432738\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.12527428865432738  to: 0.12526050806045533\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12526050806045533  to: 0.12524675130844115\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.12524675130844115  to: 0.12523300647735597\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.12523300647735597  to: 0.12521957159042357\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12521957159042357  to: 0.12520641088485718\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.12520641088485718  to: 0.12519316673278807\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.12519316673278807  to: 0.12517985105514526\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.12517985105514526  to: 0.12516673803329467\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12516673803329467  to: 0.12515385150909425\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.12515385150909425  to: 0.12514078617095947\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.12514078617095947  to: 0.12512757778167724\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12512757778167724  to: 0.12511421442031861\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12511421442031861  to: 0.12510099411010742\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.12510099411010742  to: 0.12508792877197267\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.12508792877197267  to: 0.12507498264312744\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.12507498264312744  to: 0.12506178617477418\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.12506178617477418  to: 0.1250483751296997\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.1250483751296997  to: 0.1250347375869751\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.1250347375869751  to: 0.125021231174469\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.125021231174469  to: 0.12500782012939454\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12500782012939454  to: 0.12499449253082276\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.12499449253082276  to: 0.12498085498809815\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12498085498809815  to: 0.12496697902679443\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.12496697902679443  to: 0.12495286464691162\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12495286464691162  to: 0.12493884563446045\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.12493884563446045  to: 0.12492492198944091\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.12492492198944091  to: 0.12491105794906616\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.12491105794906616  to: 0.12489725351333618\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12489725351333618  to: 0.12488315105438233\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.12488315105438233  to: 0.1248687744140625\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.1248687744140625  to: 0.12485413551330567\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12485413551330567  to: 0.12483924627304077\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.12483924627304077  to: 0.12482446432113647\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.12482446432113647  to: 0.12480976581573486\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12480976581573486  to: 0.12479511499404908\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.12479511499404908  to: 0.12478049993515014\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.12478049993515014  to: 0.12476590871810914\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.12476590871810914  to: 0.12475097179412842\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.12475097179412842  to: 0.12473573684692382\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.12473573684692382  to: 0.12472019195556641\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.12472019195556641  to: 0.12470471858978271\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.12470471858978271  to: 0.12468943595886231\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.12468943595886231  to: 0.12467429637908936\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12467429637908936  to: 0.12465893030166626\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.12465893030166626  to: 0.12464334964752197\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12464334964752197  to: 0.12462756633758545\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12462756633758545  to: 0.12461159229278565\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.12461159229278565  to: 0.12459542751312255\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12459542751312255  to: 0.12457945346832275\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.12457945346832275  to: 0.12456361055374146\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.12456361055374146  to: 0.12454789876937866\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12454789876937866  to: 0.12453193664550781\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.12453193664550781  to: 0.12451574802398682\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12451574802398682  to: 0.12449934482574462\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12449934482574462  to: 0.12448276281356811\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12448276281356811  to: 0.12446637153625488\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.12446637153625488  to: 0.12445013523101807\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.12445013523101807  to: 0.12443366050720214\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12443366050720214  to: 0.124416983127594\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.124416983127594  to: 0.1244000792503357\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.1244000792503357  to: 0.12438335418701171\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.12438335418701171  to: 0.12436678409576415\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12436678409576415  to: 0.12434995174407959\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.12434995174407959  to: 0.12433292865753173\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12433292865753173  to: 0.12431565523147584\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12431565523147584  to: 0.12429821491241455\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.12429821491241455  to: 0.12428095340728759\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.12428095340728759  to: 0.12426387071609497\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.12426387071609497  to: 0.12424690723419189\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.12424690723419189  to: 0.12422969341278076\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12422969341278076  to: 0.12421224117279053\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.12421224117279053  to: 0.1241945505142212\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.1241945505142212  to: 0.12417665719985962\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.12417665719985962  to: 0.12415896654129029\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.12415896654129029  to: 0.12414140701293945\n",
      "Training iteration: 1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12414140701293945  to: 0.12412360906600953\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12412360906600953  to: 0.12410558462142944\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.12410558462142944  to: 0.12408770322799682\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.12408770322799682  to: 0.12406959533691406\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.12406959533691406  to: 0.12405164241790771\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12405164241790771  to: 0.12403347492218017\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.12403347492218017  to: 0.12401506900787354\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.12401506900787354  to: 0.1239968180656433\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.1239968180656433  to: 0.12397886514663696\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.12397886514663696  to: 0.1239607572555542\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.1239607572555542  to: 0.12394251823425292\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12394251823425292  to: 0.1239241600036621\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.1239241600036621  to: 0.12390568256378173\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.12390568256378173  to: 0.12388708591461181\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.12388708591461181  to: 0.12386837005615234\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.12386837005615234  to: 0.12384955883026123\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.12384955883026123  to: 0.12383065223693848\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.12383065223693848  to: 0.12381203174591064\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.12381203174591064  to: 0.123793625831604\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.123793625831604  to: 0.1237750768661499\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.1237750768661499  to: 0.12375633716583252\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.12375633716583252  to: 0.12373745441436768\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.12373745441436768  to: 0.12371842861175537\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.12371842861175537  to: 0.1236992597579956\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.1236992597579956  to: 0.12367995977401733\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.12367995977401733  to: 0.12366094589233398\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.12366094589233398  to: 0.1236422300338745\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.1236422300338745  to: 0.12362334728240967\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.12362334728240967  to: 0.12360432147979736\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.12360432147979736  to: 0.1235851526260376\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.1235851526260376  to: 0.12356586456298828\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.12356586456298828  to: 0.12354648113250732\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.12354648113250732  to: 0.1235269546508789\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.1235269546508789  to: 0.12350730895996094\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.12350730895996094  to: 0.12348754405975342\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.12348754405975342  to: 0.12346811294555664\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.12346811294555664  to: 0.12344894409179688\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.12344894409179688  to: 0.12342962026596069\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.12342962026596069  to: 0.12341012954711914\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.12341012954711914  to: 0.12339047193527222\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.12339047193527222  to: 0.12337064743041992\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.12337064743041992  to: 0.12335069179534912\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.12335069179534912  to: 0.12333058118820191\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.12333058118820191  to: 0.12331031560897827\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.12331031560897827  to: 0.12329038381576538\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.12329038381576538  to: 0.12327070236206054\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.12327070236206054  to: 0.12325084209442139\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.12325084209442139  to: 0.1232308030128479\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.1232308030128479  to: 0.1232105851173401\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.1232105851173401  to: 0.12319018840789794\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.12319018840789794  to: 0.1231696367263794\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.1231696367263794  to: 0.12314893007278442\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.12314893007278442  to: 0.12312848567962646\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.12312848567962646  to: 0.12310831546783448\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.12310831546783448  to: 0.12308789491653442\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.12308789491653442  to: 0.12306727170944214\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.12306727170944214  to: 0.12304646968841552\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.12304646968841552  to: 0.12302550077438354\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.12302550077438354  to: 0.12300437688827515\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.12300437688827515  to: 0.12298353910446166\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.12298353910446166  to: 0.12296251058578492\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.12296251058578492  to: 0.12294175624847412\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.12294175624847412  to: 0.12292081117630005\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.12292081117630005  to: 0.12289965152740479\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.12289965152740479  to: 0.12287828922271729\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.12287828922271729  to: 0.12285677194595337\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.12285677194595337  to: 0.12283552885055542\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.12283552885055542  to: 0.12281405925750732\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.12281405925750732  to: 0.12279242277145386\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.12279242277145386  to: 0.1227710485458374\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.1227710485458374  to: 0.122749924659729\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.122749924659729  to: 0.12272853851318359\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.12272853851318359  to: 0.12270691394805908\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.12270691394805908  to: 0.12268507480621338\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.12268507480621338  to: 0.12266302108764648\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.12266302108764648  to: 0.12264076471328736\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.12264076471328736  to: 0.1226183295249939\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.1226183295249939  to: 0.12259620428085327\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.12259620428085327  to: 0.1225743293762207\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.1225743293762207  to: 0.12255270481109619\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.12255270481109619  to: 0.12253086566925049\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.12253086566925049  to: 0.12250880002975464\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.12250880002975464  to: 0.12248653173446655\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.12248653173446655  to: 0.12246406078338623\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.12246406078338623  to: 0.12244141101837158\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.12244141101837158  to: 0.12241857051849366\n",
      "Training iteration: 1199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12241857051849366  to: 0.12239558696746826\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.12239558696746826  to: 0.12237246036529541\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.12237246036529541  to: 0.12234969139099121\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.12234969139099121  to: 0.12232723236083984\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.12232723236083984  to: 0.12230502367019654\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.12230502367019654  to: 0.12228257656097412\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.12228257656097412  to: 0.1222598671913147\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.1222598671913147  to: 0.12223691940307617\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.12223691940307617  to: 0.12221375703811646\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.12221375703811646  to: 0.12219038009643554\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.12219038009643554  to: 0.12216682434082031\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.12216682434082031  to: 0.1221431016921997\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.1221431016921997  to: 0.12211968898773193\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.12211968898773193  to: 0.12209653854370117\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.12209653854370117  to: 0.12207367420196533\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.12207367420196533  to: 0.12205052375793457\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.12205052375793457  to: 0.12202708721160889\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.12202708721160889  to: 0.12200340032577514\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.12200340032577514  to: 0.12197948694229126\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.12197948694229126  to: 0.12195532321929932\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.12195532321929932  to: 0.12193096876144409\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.12193096876144409  to: 0.12190641164779663\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.12190641164779663  to: 0.12188217639923096\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.12188217639923096  to: 0.12185822725296021\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.12185822725296021  to: 0.12183454036712646\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.12183454036712646  to: 0.1218111276626587\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.1218111276626587  to: 0.12178744077682495\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.12178744077682495  to: 0.12176344394683838\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.12176344394683838  to: 0.12173912525177003\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.12173912525177003  to: 0.12171454429626465\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.12171454429626465  to: 0.12168970108032226\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.12168970108032226  to: 0.12166461944580079\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.12166461944580079  to: 0.12163927555084228\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.12163927555084228  to: 0.1216137170791626\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.1216137170791626  to: 0.12158845663070679\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.12158845663070679  to: 0.1215634822845459\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.1215634822845459  to: 0.1215387225151062\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.1215387225151062  to: 0.1215142011642456\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.1215142011642456  to: 0.12148929834365844\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.12148929834365844  to: 0.1214640974998474\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.1214640974998474  to: 0.12143861055374146\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.12143861055374146  to: 0.12141282558441162\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.12141282558441162  to: 0.12138679027557372\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.12138679027557372  to: 0.12136049270629883\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.12136049270629883  to: 0.1213344931602478\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.1213344931602478  to: 0.1213087797164917\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.1213087797164917  to: 0.12128329277038574\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.12128329277038574  to: 0.1212574601173401\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.1212574601173401  to: 0.1212312936782837\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.1212312936782837  to: 0.12120485305786133\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.12120485305786133  to: 0.121178138256073\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.121178138256073  to: 0.12115175724029541\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.12115175724029541  to: 0.12112563848495483\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.12112563848495483  to: 0.12109918594360351\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.12109918594360351  to: 0.12107241153717041\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.12107241153717041  to: 0.12104533910751343\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.12104533910751343  to: 0.12101860046386718\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.12101860046386718  to: 0.12099208831787109\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.12099208831787109  to: 0.12096581459045411\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.12096581459045411  to: 0.12093915939331054\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.12093915939331054  to: 0.12091214656829834\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.12091214656829834  to: 0.1208848237991333\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.1208848237991333  to: 0.12085721492767335\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.12085721492767335  to: 0.12082928419113159\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.12082928419113159  to: 0.12080165147781372\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.12080165147781372  to: 0.120774245262146\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.120774245262146  to: 0.12074705362319946\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.12074705362319946  to: 0.12072004079818725\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.12072004079818725  to: 0.12069259881973267\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.12069259881973267  to: 0.12066471576690674\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.12066471576690674  to: 0.1206364631652832\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.1206364631652832  to: 0.12060785293579102\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.12060785293579102  to: 0.120578932762146\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.120578932762146  to: 0.12055031061172486\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.12055031061172486  to: 0.12052192687988281\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.12052192687988281  to: 0.12049378156661987\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.12049378156661987  to: 0.12046581506729126\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.12046581506729126  to: 0.1204373598098755\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.1204373598098755  to: 0.1204085111618042\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.1204085111618042  to: 0.12037923336029052\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.12037923336029052  to: 0.1203495740890503\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.1203495740890503  to: 0.1203201413154602\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.1203201413154602  to: 0.12029097080230713\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.12029097080230713  to: 0.12026201486587525\n",
      "Training iteration: 1283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12026201486587525  to: 0.12023322582244873\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.12023322582244873  to: 0.1202039122581482\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.1202039122581482  to: 0.12017415761947632\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.12017415761947632  to: 0.1201439619064331\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.1201439619064331  to: 0.12011336088180542\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.12011336088180542  to: 0.1200830340385437\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.1200830340385437  to: 0.12005290985107422\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.12005290985107422  to: 0.12002298831939698\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.12002298831939698  to: 0.11999336481094361\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.11999336481094361  to: 0.11996335983276367\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.11996335983276367  to: 0.11993286609649659\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.11993286609649659  to: 0.1199019193649292\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.1199019193649292  to: 0.11987123489379883\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.11987123489379883  to: 0.11984074115753174\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.11984074115753174  to: 0.11981040239334106\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.11981040239334106  to: 0.11978023052215576\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.11978023052215576  to: 0.11974952220916749\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.11974952220916749  to: 0.11971833705902099\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.11971833705902099  to: 0.11968666315078735\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.11968666315078735  to: 0.1196552038192749\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.1196552038192749  to: 0.11962392330169677\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.11962392330169677  to: 0.11959280967712402\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.11959280967712402  to: 0.11956117153167725\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.11956117153167725  to: 0.11952903270721435\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.11952903270721435  to: 0.11949713230133056\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.11949713230133056  to: 0.11946539878845215\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.11946539878845215  to: 0.1194338083267212\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.1194338083267212  to: 0.11940231323242187\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.11940231323242187  to: 0.11937024593353271\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.11937024593353271  to: 0.11933761835098267\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.11933761835098267  to: 0.1193041205406189\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.1193041205406189  to: 0.11927086114883423\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.11927086114883423  to: 0.11923780441284179\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.11923780441284179  to: 0.11920487880706787\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.11920487880706787  to: 0.11917208433151245\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.11917208433151245  to: 0.11913938522338867\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.11913938522338867  to: 0.11910604238510132\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.11910604238510132  to: 0.11907211542129517\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.11907211542129517  to: 0.1190376877784729\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.1190376877784729  to: 0.11900542974472046\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.11900542974472046  to: 0.11897351741790771\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.11897351741790771  to: 0.11894174814224243\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.11894174814224243  to: 0.11891006231307984\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.11891006231307984  to: 0.11887842416763306\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.11887842416763306  to: 0.11884682178497315\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.11884682178497315  to: 0.11881465911865234\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.11881465911865234  to: 0.1187819480895996\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.1187819480895996  to: 0.11874876022338868\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.11874876022338868  to: 0.11871575117111206\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.11871575117111206  to: 0.118682861328125\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.118682861328125  to: 0.11865006685256958\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.11865006685256958  to: 0.1186173439025879\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.1186173439025879  to: 0.11858466863632203\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.11858466863632203  to: 0.11855205297470092\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.11855205297470092  to: 0.1185194730758667\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.1185194730758667  to: 0.11848627328872681\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.11848627328872681  to: 0.11845251321792602\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.11845251321792602  to: 0.11841825246810914\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.11841825246810914  to: 0.11838414669036865\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.11838414669036865  to: 0.11835014820098877\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.11835014820098877  to: 0.11831624507904052\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.11831624507904052  to: 0.11828241348266602\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.11828241348266602  to: 0.11824867725372315\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.11824867725372315  to: 0.11821498870849609\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.11821498870849609  to: 0.11818132400512696\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.11818132400512696  to: 0.11814701557159424\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.11814701557159424  to: 0.11811226606369019\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.11811226606369019  to: 0.11807773113250733\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.11807773113250733  to: 0.11804326772689819\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.11804326772689819  to: 0.11800886392593384\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.11800886392593384  to: 0.11797448396682739\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.11797448396682739  to: 0.11793944835662842\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.11793944835662842  to: 0.11790450811386108\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.11790450811386108  to: 0.11786960363388062\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.11786960363388062  to: 0.11783407926559449\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.11783407926559449  to: 0.11779860258102418\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.11779860258102418  to: 0.11776316165924072\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.11776316165924072  to: 0.11772773265838624\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.11772773265838624  to: 0.11769232749938965\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.11769232749938965  to: 0.1176568865776062\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.1176568865776062  to: 0.11762075424194336\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.11762075424194336  to: 0.11758397817611695\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.11758397817611695  to: 0.11754729747772216\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.11754729747772216  to: 0.1175106406211853\n",
      "Training iteration: 1367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1175106406211853  to: 0.11747405529022217\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.11747405529022217  to: 0.11743749380111694\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.11743749380111694  to: 0.11740096807479858\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.11740096807479858  to: 0.11736444234848023\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.11736444234848023  to: 0.11732906103134155\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.11732906103134155  to: 0.11729484796524048\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.11729484796524048  to: 0.11726057529449463\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.11726057529449463  to: 0.11722564697265625\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.11722564697265625  to: 0.11719043254852295\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.11719043254852295  to: 0.11715543270111084\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.11715543270111084  to: 0.11712043285369873\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.11712043285369873  to: 0.11708542108535766\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.11708542108535766  to: 0.1170503854751587\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.1170503854751587  to: 0.1170153021812439\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.1170153021812439  to: 0.11698020696640014\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.11698020696640014  to: 0.11694505214691162\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.11694505214691162  to: 0.11690984964370728\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.11690984964370728  to: 0.1168745994567871\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.1168745994567871  to: 0.11683862209320069\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.11683862209320069  to: 0.11680206060409545\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.11680206060409545  to: 0.11676604747772217\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.11676604747772217  to: 0.116729998588562\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.116729998588562  to: 0.11669387817382812\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.11669387817382812  to: 0.11665769815444946\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.11665769815444946  to: 0.11662144660949707\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.11662144660949707  to: 0.11658512353897095\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.11658512353897095  to: 0.11654874086380004\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.11654874086380004  to: 0.1165122628211975\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.1165122628211975  to: 0.11647571325302124\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.11647571325302124  to: 0.11643908023834229\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.11643908023834229  to: 0.11640236377716065\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.11640236377716065  to: 0.11636555194854736\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.11636555194854736  to: 0.11632862091064453\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.11632862091064453  to: 0.1162915825843811\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.1162915825843811  to: 0.11625384092330933\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.11625384092330933  to: 0.11621546745300293\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.11621546745300293  to: 0.11617711782455445\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.11617711782455445  to: 0.116138756275177\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.116138756275177  to: 0.11610040664672852\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.11610040664672852  to: 0.11606204509735107\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.11606204509735107  to: 0.11602368354797363\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.11602368354797363  to: 0.11598527431488037\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.11598527431488037  to: 0.1159468412399292\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.1159468412399292  to: 0.11590837240219116\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.11590837240219116  to: 0.11586983203887939\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.11586983203887939  to: 0.11583125591278076\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.11583125591278076  to: 0.11579259634017944\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.11579259634017944  to: 0.11575387716293335\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.11575387716293335  to: 0.11571509838104248\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.11571509838104248  to: 0.11567624807357788\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.11567624807357788  to: 0.11563736200332642\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.11563736200332642  to: 0.11559839248657226\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.11559839248657226  to: 0.11555932760238648\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.11555932760238648  to: 0.11552034616470337\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.11552034616470337  to: 0.11548091173171997\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.11548091173171997  to: 0.1154414176940918\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.1154414176940918  to: 0.1154018759727478\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.1154018759727478  to: 0.11536229848861694\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.11536229848861694  to: 0.1153226613998413\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.1153226613998413  to: 0.11528297662734985\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.11528297662734985  to: 0.11524319648742676\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.11524319648742676  to: 0.11520334482192993\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.11520334482192993  to: 0.11516339778900146\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.11516339778900146  to: 0.11512335538864135\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.11512335538864135  to: 0.11508321762084961\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.11508321762084961  to: 0.11504348516464233\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.11504348516464233  to: 0.11500374078750611\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.11500374078750611  to: 0.11496388912200928\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.11496388912200928  to: 0.11492332220077514\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.11492332220077514  to: 0.11488285064697265\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.11488285064697265  to: 0.11484304666519166\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.11484304666519166  to: 0.11480319499969482\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.11480319499969482  to: 0.11476316452026367\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.11476316452026367  to: 0.11472299098968505\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.11472299098968505  to: 0.11468266248703003\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.11468266248703003  to: 0.1146422028541565\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.1146422028541565  to: 0.1146016240119934\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.1146016240119934  to: 0.11456091403961181\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.11456091403961181  to: 0.11452009677886962\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.11452009677886962  to: 0.1144791603088379\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.1144791603088379  to: 0.11443814039230346\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.11443814039230346  to: 0.11439700126647949\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.11439700126647949  to: 0.11435573101043701\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.11435573101043701  to: 0.11431369781494141\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.11431369781494141  to: 0.11427158117294312\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.11427158117294312  to: 0.11422941684722901\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.11422941684722901  to: 0.11418716907501221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.11418716907501221  to: 0.11414486169815063\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.11414486169815063  to: 0.11410247087478638\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.11410247087478638  to: 0.11406000852584838\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.11406000852584838  to: 0.11401746273040772\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.11401746273040772  to: 0.11397484540939332\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.11397484540939332  to: 0.11393219232559204\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.11393219232559204  to: 0.1138887882232666\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.1138887882232666  to: 0.1138453722000122\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.1138453722000122  to: 0.11380193233489991\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.11380193233489991  to: 0.11375844478607178\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.11375844478607178  to: 0.11371490955352784\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.11371490955352784  to: 0.11367130279541016\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.11367130279541016  to: 0.11362764835357667\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.11362764835357667  to: 0.11358394622802734\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.11358394622802734  to: 0.1135401964187622\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.1135401964187622  to: 0.11349639892578126\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.11349639892578126  to: 0.11345183849334717\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.11345183849334717  to: 0.11340734958648682\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.11340734958648682  to: 0.11336289644241333\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.11336289644241333  to: 0.11331870555877685\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.11331870555877685  to: 0.11327476501464843\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.11327476501464843  to: 0.11323082447052002\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.11323082447052002  to: 0.11318614482879638\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.11318614482879638  to: 0.11314152479171753\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.11314152479171753  to: 0.11309692859649659\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.11309692859649659  to: 0.1130523443222046\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.1130523443222046  to: 0.11300778388977051\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.11300778388977051  to: 0.11296247243881226\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.11296247243881226  to: 0.11291724443435669\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.11291724443435669  to: 0.11287204027175904\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.11287204027175904  to: 0.11282685995101929\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.11282685995101929  to: 0.11278153657913208\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.11278153657913208  to: 0.11273715496063233\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.11273715496063233  to: 0.11269471645355225\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.11269471645355225  to: 0.11265392303466797\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.11265392303466797  to: 0.11261482238769531\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.11261482238769531  to: 0.1125771164894104\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.1125771164894104  to: 0.11254050731658935\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.11254050731658935  to: 0.11250486373901367\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.11250486373901367  to: 0.11246988773345948\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.11246988773345948  to: 0.11243546009063721\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.11243546009063721  to: 0.11240142583847046\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.11240142583847046  to: 0.11236597299575805\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.11236597299575805  to: 0.11232906579971313\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.11232906579971313  to: 0.11229063272476196\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.11229063272476196  to: 0.11225075721740722\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.11225075721740722  to: 0.11220953464508057\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.11220953464508057  to: 0.11216723918914795\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.11216723918914795  to: 0.11212441921234131\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.11212441921234131  to: 0.11208099126815796\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.11208099126815796  to: 0.11203696727752685\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.11203696727752685  to: 0.11199241876602173\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.11199241876602173  to: 0.11194735765457153\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.11194735765457153  to: 0.11190180778503418\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.11190180778503418  to: 0.11185394525527954\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.11185394525527954  to: 0.11180517673492432\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.11180517673492432  to: 0.11175553798675537\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.11175553798675537  to: 0.1117051362991333\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.1117051362991333  to: 0.11165595054626465\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.11165595054626465  to: 0.11160786151885986\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.11160786151885986  to: 0.1115607500076294\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.1115607500076294  to: 0.11151453256607055\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.11151453256607055  to: 0.11146941184997558\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.11146941184997558  to: 0.11142499446868896\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.11142499446868896  to: 0.11137914657592773\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.11137914657592773  to: 0.1113319993019104\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.1113319993019104  to: 0.11128361225128174\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.11128361225128174  to: 0.11123607158660889\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.11123607158660889  to: 0.11118929386138916\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.11118929386138916  to: 0.11114314794540406\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.11114314794540406  to: 0.1110975980758667\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.1110975980758667  to: 0.111050546169281\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.111050546169281  to: 0.11100406646728515\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.11100406646728515  to: 0.11095603704452514\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.11095603704452514  to: 0.11090764999389649\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.11090764999389649  to: 0.11085889339447022\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.11085889339447022  to: 0.1108097791671753\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.1108097791671753  to: 0.11075823307037354\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.11075823307037354  to: 0.11070655584335327\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.11070655584335327  to: 0.11065589189529419\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.11065589189529419  to: 0.11060400009155273\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.11060400009155273  to: 0.11055101156234741\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.11055101156234741  to: 0.11049919128417969\n",
      "Training iteration: 1537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11049919128417969  to: 0.11044842004776001\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.11044842004776001  to: 0.11039636135101319\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.11039636135101319  to: 0.11034536361694336\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.11034536361694336  to: 0.11029214859008789\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.11029214859008789  to: 0.11023914813995361\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.11023914813995361  to: 0.11018626689910889\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.11018626689910889  to: 0.11013481616973878\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.11013481616973878  to: 0.11008460521697998\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.11008460521697998  to: 0.11003549098968506\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.11003549098968506  to: 0.10998525619506835\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.10998525619506835  to: 0.10993616580963135\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.10993616580963135  to: 0.10988800525665283\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.10988800525665283  to: 0.10984065532684326\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.10984065532684326  to: 0.10979272127151489\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.10979272127151489  to: 0.10974400043487549\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.10974400043487549  to: 0.10969456434249877\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.10969456434249877  to: 0.10964444875717164\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.10964444875717164  to: 0.10959131717681884\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.10959131717681884  to: 0.10953781604766846\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.10953781604766846  to: 0.10948396921157837\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.10948396921157837  to: 0.10942981243133545\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.10942981243133545  to: 0.10937538146972656\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.10937538146972656  to: 0.10932067632675171\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.10932067632675171  to: 0.1092670202255249\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.1092670202255249  to: 0.10921428203582764\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.10921428203582764  to: 0.10916234254837036\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.10916234254837036  to: 0.10910985469818116\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.10910985469818116  to: 0.10905684232711792\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.10905684232711792  to: 0.10900337696075439\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.10900337696075439  to: 0.10894945859909058\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.10894945859909058  to: 0.10889517068862915\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.10889517068862915  to: 0.1088404893875122\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.1088404893875122  to: 0.10878547430038452\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.10878547430038452  to: 0.10873013734817505\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.10873013734817505  to: 0.1086745023727417\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.1086745023727417  to: 0.10861859321594239\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.10861859321594239  to: 0.10856239795684815\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.10856239795684815  to: 0.1085059404373169\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.1085059404373169  to: 0.10845048427581787\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.10845048427581787  to: 0.10839470624923705\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.10839470624923705  to: 0.10833859443664551\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.10833859443664551  to: 0.10828219652175904\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.10828219652175904  to: 0.10822551250457764\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.10822551250457764  to: 0.10816870927810669\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.10816870927810669  to: 0.10811176300048828\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.10811176300048828  to: 0.10805461406707764\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.10805461406707764  to: 0.10799727439880372\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.10799727439880372  to: 0.10793979167938232\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.10793979167938232  to: 0.10788213014602661\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.10788213014602661  to: 0.10782436132431031\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.10782436132431031  to: 0.10776647329330444\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.10776647329330444  to: 0.10770847797393798\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.10770847797393798  to: 0.10765043497085572\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.10765043497085572  to: 0.10759227275848389\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.10759227275848389  to: 0.10753407478332519\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.10753407478332519  to: 0.10747579336166382\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.10747579336166382  to: 0.10741748809814453\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.10741748809814453  to: 0.10735911130905151\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.10735911130905151  to: 0.10730116367340088\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.10730116367340088  to: 0.10724350214004516\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.10724350214004516  to: 0.10718580484390258\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.10718580484390258  to: 0.10712807178497315\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.10712807178497315  to: 0.10707060098648072\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.10707060098648072  to: 0.10701316595077515\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.10701316595077515  to: 0.10695569515228272\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.10695569515228272  to: 0.10689818859100342\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.10689818859100342  to: 0.1068406343460083\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.1068406343460083  to: 0.10678300857543946\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.10678300857543946  to: 0.10672534704208374\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.10672534704208374  to: 0.10666763782501221\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.10666763782501221  to: 0.10660969018936158\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.10660969018936158  to: 0.10655202865600585\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.10655202865600585  to: 0.1064942717552185\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.1064942717552185  to: 0.10643632411956787\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.10643632411956787  to: 0.10637818574905396\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.10637818574905396  to: 0.10631989240646363\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.10631989240646363  to: 0.10626144409179687\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.10626144409179687  to: 0.10620285272598266\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.10620285272598266  to: 0.10614408254623413\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.10614408254623413  to: 0.10608519315719604\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.10608519315719604  to: 0.10602614879608155\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.10602614879608155  to: 0.10596697330474854\n",
      "Training iteration: 1619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10596697330474854  to: 0.10590765476226807\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.10590765476226807  to: 0.10584820508956909\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.10584820508956909  to: 0.10578863620758057\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.10578863620758057  to: 0.10572892427444458\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.10572892427444458  to: 0.10566909313201904\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.10566909313201904  to: 0.10560914278030395\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.10560914278030395  to: 0.10554907321929932\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.10554907321929932  to: 0.10548887252807618\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.10548887252807618  to: 0.10542854070663452\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.10542854070663452  to: 0.10536810159683227\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.10536810159683227  to: 0.10530754327774047\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.10530754327774047  to: 0.10524688959121704\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.10524688959121704  to: 0.10518617630004883\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.10518617630004883  to: 0.10512559413909912\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.10512559413909912  to: 0.10506503582000733\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.10506503582000733  to: 0.10500457286834716\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.10500457286834716  to: 0.10494415760040283\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.10494415760040283  to: 0.10488376617431641\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.10488376617431641  to: 0.1048233985900879\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.1048233985900879  to: 0.10476303100585938\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.10476303100585938  to: 0.104702627658844\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.104702627658844  to: 0.10464202165603638\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.10464202165603638  to: 0.10458091497421265\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.10458091497421265  to: 0.10451934337615967\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.10451934337615967  to: 0.1044573426246643\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.1044573426246643  to: 0.10439493656158447\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.10439493656158447  to: 0.10433217287063598\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.10433217287063598  to: 0.10426905155181884\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.10426905155181884  to: 0.10420557260513305\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.10420557260513305  to: 0.1041418194770813\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.1041418194770813  to: 0.10407778024673461\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.10407778024673461  to: 0.10401346683502197\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.10401346683502197  to: 0.10394891500473022\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.10394891500473022  to: 0.1038843035697937\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.1038843035697937  to: 0.10381965637207032\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.10381965637207032  to: 0.10375490188598632\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.10375490188598632  to: 0.10369012355804444\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.10369012355804444  to: 0.10362509489059449\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.10362509489059449  to: 0.10355985164642334\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.10355985164642334  to: 0.10349441766738891\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.10349441766738891  to: 0.10342878103256226\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.10342878103256226  to: 0.10336297750473022\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.10336297750473022  to: 0.10329704284667969\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.10329704284667969  to: 0.10323095321655273\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.10323095321655273  to: 0.10316473245620728\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.10316473245620728  to: 0.10309836864471436\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.10309836864471436  to: 0.1030319094657898\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.1030319094657898  to: 0.10296566486358642\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.10296566486358642  to: 0.1028997540473938\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.1028997540473938  to: 0.10283417701721191\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.10283417701721191  to: 0.10276861190795898\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.10276861190795898  to: 0.10270299911499023\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.10270299911499023  to: 0.1026379108428955\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.1026379108428955  to: 0.1025734782218933\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.1025734782218933  to: 0.1025086522102356\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.1025086522102356  to: 0.10244345664978027\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.10244345664978027  to: 0.10237810611724854\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.10237810611724854  to: 0.10231260061264039\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.10231260061264039  to: 0.10224673748016358\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.10224673748016358  to: 0.10218056440353393\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.10218056440353393  to: 0.10211405754089356\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.10211405754089356  to: 0.1020472526550293\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.1020472526550293  to: 0.10198028087615967\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.10198028087615967  to: 0.10191333293914795\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.10191333293914795  to: 0.10184614658355713\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.10184614658355713  to: 0.10177872180938721\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.10177872180938721  to: 0.101711106300354\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.101711106300354  to: 0.10164393186569214\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.10164393186569214  to: 0.10157709121704102\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.10157709121704102  to: 0.1015106201171875\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.1015106201171875  to: 0.10144388675689697\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.10144388675689697  to: 0.10137686729431153\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.10137686729431153  to: 0.10130956172943115\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.10130956172943115  to: 0.10124212503433228\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.10124212503433228  to: 0.1011745810508728\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.1011745810508728  to: 0.10110700130462646\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.10110700130462646  to: 0.10103943347930908\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.10103943347930908  to: 0.10097177028656006\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.10097177028656006  to: 0.10090402364730836\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.10090402364730836  to: 0.10083602666854859\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.10083602666854859  to: 0.10076833963394165\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.10076833963394165  to: 0.1007009744644165\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.1007009744644165  to: 0.1006333589553833\n",
      "Training iteration: 1702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1006333589553833  to: 0.10056564807891846\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.10056564807891846  to: 0.1004978895187378\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.1004978895187378  to: 0.1004298448562622\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.1004298448562622  to: 0.10036189556121826\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.10036189556121826  to: 0.10029413700103759\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.10029413700103759  to: 0.10022609233856201\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.10022609233856201  to: 0.10015780925750732\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.10015780925750732  to: 0.10008928775787354\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.10008928775787354  to: 0.10002052783966064\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.10002052783966064  to: 0.09995189905166627\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.09995189905166627  to: 0.09988301396369934\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.09988301396369934  to: 0.09981401562690735\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.09981401562690735  to: 0.09974491000175476\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.09974491000175476  to: 0.09967570304870606\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.09967570304870606  to: 0.09960659146308899\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.09960659146308899  to: 0.09953719973564149\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.09953719973564149  to: 0.09946754574775696\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.09946754574775696  to: 0.09939764142036438\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.09939764142036438  to: 0.09932748675346374\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.09932748675346374  to: 0.09925710558891296\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.09925710558891296  to: 0.0991865336894989\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.0991865336894989  to: 0.09911629557609558\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.09911629557609558  to: 0.09904583096504212\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.09904583096504212  to: 0.09897515177726746\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.09897515177726746  to: 0.09890484809875488\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.09890484809875488  to: 0.0988347887992859\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.0988347887992859  to: 0.09876449704170227\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.09876449704170227  to: 0.09869416356086731\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.09869416356086731  to: 0.09862359762191772\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.09862359762191772  to: 0.09855281710624694\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.09855281710624694  to: 0.09848182797431945\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.09848182797431945  to: 0.0984106183052063\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.0984106183052063  to: 0.09833918809890747\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.09833918809890747  to: 0.09826757311820984\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.09826757311820984  to: 0.09819574356079101\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.09819574356079101  to: 0.09812374114990234\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.09812374114990234  to: 0.09805192947387695\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.09805192947387695  to: 0.0979800820350647\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.0979800820350647  to: 0.0979080080986023\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.0979080080986023  to: 0.09783574938774109\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.09783574938774109  to: 0.0977632999420166\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.0977632999420166  to: 0.09769065976142884\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.09769065976142884  to: 0.0976178526878357\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.0976178526878357  to: 0.09754487872123718\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.09754487872123718  to: 0.09747190475463867\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.09747190475463867  to: 0.09739875793457031\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.09739875793457031  to: 0.09732545614242553\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.09732545614242553  to: 0.09725200533866882\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.09725200533866882  to: 0.09717836380004882\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.09717836380004882  to: 0.09710454940795898\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.09710454940795898  to: 0.09703058004379272\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.09703058004379272  to: 0.09695650339126587\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.09695650339126587  to: 0.09688228368759155\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.09688228368759155  to: 0.09680794477462769\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.09680794477462769  to: 0.09673351049423218\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.09673351049423218  to: 0.0966592013835907\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.0966592013835907  to: 0.09658501744270324\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.09658501744270324  to: 0.09651095271110535\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.09651095271110535  to: 0.09643697738647461\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.09643697738647461  to: 0.0963632583618164\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.0963632583618164  to: 0.09628959894180297\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.09628959894180297  to: 0.09621592760086059\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.09621592760086059  to: 0.09614225625991821\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.09614225625991821  to: 0.09606857299804687\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.09606857299804687  to: 0.09599482417106628\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.09599482417106628  to: 0.09592103958129883\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.09592103958129883  to: 0.0958471953868866\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.0958471953868866  to: 0.09577330350875854\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.09577330350875854  to: 0.09569943547248841\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.09569943547248841  to: 0.09562576413154603\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.09562576413154603  to: 0.09555205106735229\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.09555205106735229  to: 0.09547828435897827\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.09547828435897827  to: 0.09540449380874634\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.09540449380874634  to: 0.09533065557479858\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.09533065557479858  to: 0.09525679349899292\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.09525679349899292  to: 0.09518291354179383\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.09518291354179383  to: 0.09510900378227234\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.09510900378227234  to: 0.09503509402275086\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.09503509402275086  to: 0.09496135711669922\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.09496135711669922  to: 0.09488779902458191\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.09488779902458191  to: 0.09481437802314759\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.09481437802314759  to: 0.09474111795425415\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.09474111795425415  to: 0.09466797113418579\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.09466797113418579  to: 0.09459495544433594\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.09459495544433594  to: 0.09452205896377563\n",
      "Training iteration: 1787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.09452205896377563  to: 0.09444926977157593\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.09444926977157593  to: 0.0943765938282013\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.0943765938282013  to: 0.09430634379386901\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.09430634379386901  to: 0.09423846006393433\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.09423846006393433  to: 0.09417317509651184\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.09417317509651184  to: 0.09410701990127564\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.09410701990127564  to: 0.09403955340385436\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.09403955340385436  to: 0.09397065043449401\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.09397065043449401  to: 0.0938997745513916\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.0938997745513916  to: 0.09382685422897338\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.09382685422897338  to: 0.09375202059745788\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.09375202059745788  to: 0.09367993474006653\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.09367993474006653  to: 0.0936104953289032\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.0936104953289032  to: 0.09354370236396789\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.09354370236396789  to: 0.09347952604293823\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.09347952604293823  to: 0.09341166615486145\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.09341166615486145  to: 0.09333959817886353\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.09333959817886353  to: 0.09326329231262206\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.09326329231262206  to: 0.09318310022354126\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.09318310022354126  to: 0.09310609102249146\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.09310609102249146  to: 0.09303193092346192\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.09303193092346192  to: 0.0929604172706604\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.0929604172706604  to: 0.09289138913154601\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.09289138913154601  to: 0.09282468557357788\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.09282468557357788  to: 0.09276006817817688\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.09276006817817688  to: 0.09269723892211915\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.09269723892211915  to: 0.09262642860412598\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.09262642860412598  to: 0.09254749417304993\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.09254749417304993  to: 0.09246217608451843\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.09246217608451843  to: 0.09238053560256958\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.09238053560256958  to: 0.09230185747146606\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.09230185747146606  to: 0.09222596883773804\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.09222596883773804  to: 0.09215272665023803\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.09215272665023803  to: 0.09208192825317382\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.09208192825317382  to: 0.0920133888721466\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.0920133888721466  to: 0.09194685816764832\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.09194685816764832  to: 0.09188205599784852\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.09188205599784852  to: 0.0918186366558075\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.0918186366558075  to: 0.0917563796043396\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.0917563796043396  to: 0.09169643521308898\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.09169643521308898  to: 0.09163708686828613\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.09163708686828613  to: 0.09157834053039551\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.09157834053039551  to: 0.091519296169281\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.091519296169281  to: 0.09145956039428711\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.09145956039428711  to: 0.09139877557754517\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.09139877557754517  to: 0.09132105708122254\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.09132105708122254  to: 0.09124387502670288\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.09124387502670288  to: 0.09116756319999694\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.09116756319999694  to: 0.09109236001968384\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.09109236001968384  to: 0.09101883172988892\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.09101883172988892  to: 0.09094711542129516\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.09094711542129516  to: 0.09087724685668945\n",
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.09087724685668945  to: 0.09080927968025207\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.09080927968025207  to: 0.09074316024780274\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.09074316024780274  to: 0.0906788468360901\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.0906788468360901  to: 0.09061623811721801\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.09061623811721801  to: 0.09055496454238891\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.09055496454238891  to: 0.09049493074417114\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.09049493074417114  to: 0.09043598175048828\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.09043598175048828  to: 0.09037798047065734\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.09037798047065734  to: 0.09032074213027955\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.09032074213027955  to: 0.09026412963867188\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.09026412963867188  to: 0.09020791053771973\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.09020791053771973  to: 0.0901519775390625\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.0901519775390625  to: 0.0900961697101593\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.0900961697101593  to: 0.09004040956497192\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.09004040956497192  to: 0.08998519778251649\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.08998519778251649  to: 0.08993043899536132\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.08993043899536132  to: 0.08987601995468139\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.08987601995468139  to: 0.08981808423995971\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.08981808423995971  to: 0.08975703120231629\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.08975703120231629  to: 0.0896934688091278\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.0896934688091278  to: 0.08962802886962891\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.08962802886962891  to: 0.089561927318573\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.089561927318573  to: 0.08949577212333679\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.08949577212333679  to: 0.08943373560905457\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.08943373560905457  to: 0.08937588930130005\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.08937588930130005  to: 0.08932199478149414\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.08932199478149414  to: 0.08926793932914734\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.08926793932914734  to: 0.08921391367912293\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.08921391367912293  to: 0.08915937542915345\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.08915937542915345  to: 0.08910456895828248\n",
      "Training iteration: 1869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.08910456895828248  to: 0.08904969096183776\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.08904969096183776  to: 0.08898304104804992\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.08898304104804992  to: 0.08890832662582397\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.08890832662582397  to: 0.08883336782455445\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.08883336782455445  to: 0.08876199722290039\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.08876199722290039  to: 0.08869767189025879\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.08869767189025879  to: 0.08864309191703797\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.08864309191703797  to: 0.08854429125785827\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.08854429125785827  to: 0.08843590021133423\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.08843590021133423  to: 0.08833538889884948\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.08833538889884948  to: 0.08824344873428344\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.08824344873428344  to: 0.08817529678344727\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.08817529678344727  to: 0.08812708854675293\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.08812708854675293  to: 0.08809660077095031\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.08809660077095031  to: 0.08808284997940063\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.08808468580245972\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.08810148239135743\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.08813155293464661\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.08814247250556946\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.08812965154647827\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.0880911648273468\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.08808284997940063  to: 0.08802750706672668\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.08802750706672668  to: 0.08794134259223937\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.08794134259223937  to: 0.08783676028251648\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.08783676028251648  to: 0.08774343729019166\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.08774343729019166  to: 0.08766061663627625\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.08766061663627625  to: 0.0875880241394043\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.0875880241394043  to: 0.087525475025177\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.087525475025177  to: 0.08747203946113587\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.08747203946113587  to: 0.0874267876148224\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.0874267876148224  to: 0.08738870620727539\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.08738870620727539  to: 0.08735629916191101\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.08735629916191101  to: 0.08732845187187195\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.08732845187187195  to: 0.08730360269546508\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.08730360269546508  to: 0.08728009462356567\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.08728009462356567  to: 0.08725607991218567\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.08725607991218567  to: 0.0872297465801239\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.0872297465801239  to: 0.08716281652450561\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.08716281652450561  to: 0.08709438443183899\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.08709438443183899  to: 0.08702461123466491\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.08702461123466491  to: 0.0869537353515625\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.0869537353515625  to: 0.08688195943832397\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.08688195943832397  to: 0.0868095576763153\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.0868095576763153  to: 0.08673677444458008\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.08673677444458008  to: 0.0866639792919159\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.0866639792919159  to: 0.0865913987159729\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.0865913987159729  to: 0.08651927709579468\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.08651927709579468  to: 0.08644781112670899\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.08644781112670899  to: 0.0863770842552185\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.0863770842552185  to: 0.08630728721618652\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.08630728721618652  to: 0.0862385094165802\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.0862385094165802  to: 0.08617084622383117\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.08617084622383117  to: 0.08610433340072632\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.08610433340072632  to: 0.08603900074958801\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.08603900074958801  to: 0.08597472906112671\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.08597472906112671  to: 0.085911625623703\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.085911625623703  to: 0.08584966659545898\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.08584966659545898  to: 0.08578873872756958\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.08578873872756958  to: 0.08572878837585449\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.08572878837585449  to: 0.08566967248916627\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.08566967248916627  to: 0.08561149835586548\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.08561149835586548  to: 0.0855538547039032\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.0855538547039032  to: 0.08549662828445434\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.08549662828445434  to: 0.0854396641254425\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.0854396641254425  to: 0.08538285493850709\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.08538285493850709  to: 0.08532617688179016\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.08532617688179016  to: 0.08526957631111146\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.08526957631111146  to: 0.08521299362182617\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.08521299362182617  to: 0.08515653610229493\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.08515653610229493  to: 0.08510015606880188\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.08510015606880188  to: 0.08504378199577331\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.08504378199577331  to: 0.08498746752738953\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.08498746752738953  to: 0.08493123054504395\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.08493123054504395  to: 0.08487514257431031\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.08487514257431031  to: 0.0848192036151886\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.0848192036151886  to: 0.08476341366767884\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.08476341366767884  to: 0.08470794558525085\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.08470794558525085  to: 0.08465272784233094\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.08465272784233094  to: 0.08459774851799011\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.08459774851799011  to: 0.08454297184944153\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.08454297184944153  to: 0.08448798060417176\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.08448798060417176  to: 0.08443274497985839\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.08443274497985839  to: 0.08437727093696594\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.08437727093696594  to: 0.08432157635688782\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.08432157635688782  to: 0.08426567316055297\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.08426567316055297  to: 0.08420956730842591\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.08420956730842591  to: 0.08415324091911316\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.08415324091911316  to: 0.08409668207168579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.08409668207168579  to: 0.0840392291545868\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.0840392291545868  to: 0.08398081064224243\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.08398081064224243  to: 0.08392159342765808\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.08392159342765808  to: 0.08386203050613403\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.08386203050613403  to: 0.08380203247070313\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.08380203247070313  to: 0.08374158143997193\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.08374158143997193  to: 0.08368072509765626\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.08368072509765626  to: 0.08361912965774536\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.08361912965774536  to: 0.08355684280395508\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.08355684280395508  to: 0.08349395990371704\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.08349395990371704  to: 0.08343062400817872\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.08343062400817872  to: 0.08336690664291382\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.08336690664291382  to: 0.08330297470092773\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.08330297470092773  to: 0.08323888778686524\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.08323888778686524  to: 0.08317471742630005\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.08317471742630005  to: 0.08311055302619934\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.08311055302619934  to: 0.0830464243888855\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.0830464243888855  to: 0.08298239707946778\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.08298239707946778  to: 0.08291851282119751\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.08291851282119751  to: 0.08285505175590516\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.08285505175590516  to: 0.0827919602394104\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.0827919602394104  to: 0.08272902369499206\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.08272902369499206  to: 0.0826662838459015\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.0826662838459015  to: 0.08260374069213867\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.08260374069213867  to: 0.08254110217094421\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.08254110217094421  to: 0.08247841000556946\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.08247841000556946  to: 0.08241568803787232\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.08241568803787232  to: 0.08235295414924622\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.08235295414924622  to: 0.08229023814201356\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.08229023814201356  to: 0.08222756385803223\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.08222756385803223  to: 0.08216493725776672\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.08216493725776672  to: 0.08210237622261048\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.08210237622261048  to: 0.08203986883163453\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.08203986883163453  to: 0.08198056221008301\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.08198056221008301  to: 0.08192169070243835\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.08192169070243835  to: 0.0818629264831543\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.0818629264831543  to: 0.08180562853813171\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.08180562853813171  to: 0.08174994587898254\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.08174994587898254  to: 0.08169438242912293\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.08169438242912293  to: 0.08163896799087525\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.08163896799087525  to: 0.08158367872238159\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.08158367872238159  to: 0.08152852058410645\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.08152852058410645  to: 0.08147347569465638\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.08147347569465638  to: 0.08141854405403137\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.08141854405403137  to: 0.08136369585990906\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.08136369585990906  to: 0.08130895495414733\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.08130895495414733  to: 0.08125429153442383\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.08125429153442383  to: 0.08119968175888062\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.08119968175888062  to: 0.08114513158798217\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.08114513158798217  to: 0.08109064102172851\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.08109064102172851  to: 0.08103616833686829\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.08103616833686829  to: 0.08098171353340149\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.08098171353340149  to: 0.0809272587299347\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.0809272587299347  to: 0.0808728039264679\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.0808728039264679  to: 0.08081830739974975\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.08081830739974975  to: 0.08076379895210266\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.08076379895210266  to: 0.08070923686027527\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.08070923686027527  to: 0.08065463304519653\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.08065463304519653  to: 0.08059993982315064\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.08059993982315064  to: 0.08054519891738891\n",
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.08054519891738891  to: 0.08049033880233765\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.08049033880233765  to: 0.08043540716171264\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.08043540716171264  to: 0.080380380153656\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.080380380153656  to: 0.08032523989677429\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.08032523989677429  to: 0.0802699863910675\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.0802699863910675  to: 0.08021462559700013\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.08021462559700013  to: 0.08015910983085632\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.08015910983085632  to: 0.08010348081588745\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.08010348081588745  to: 0.08004770278930665\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.08004770278930665  to: 0.07999178767204285\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.07999178767204285  to: 0.07993571758270264\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.07993571758270264  to: 0.07987948656082153\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.07987948656082153  to: 0.07982310056686401\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.07982310056686401  to: 0.07976651191711426\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.07976651191711426  to: 0.07970969080924988\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.07970969080924988  to: 0.07965269088745117\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.07965269088745117  to: 0.07959545850753784\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.07959545850753784  to: 0.07953517436981201\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.07953517436981201  to: 0.07947207093238831\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.07947207093238831  to: 0.07940649390220642\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.07940649390220642  to: 0.07933884859085083\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.07933884859085083  to: 0.07927239537239075\n",
      "Training iteration: 2039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.07927239537239075  to: 0.07920716404914856\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.07920716404914856  to: 0.07914316058158874\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.07914316058158874  to: 0.0790803611278534\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.0790803611278534  to: 0.0790186882019043\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.0790186882019043  to: 0.07895807027816773\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.07895807027816773  to: 0.07889857292175292\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.07889857292175292  to: 0.07884026765823364\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.07884026765823364  to: 0.07878266572952271\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.07878266572952271  to: 0.0787256121635437\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.0787256121635437  to: 0.07866592407226562\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.07866592407226562  to: 0.07860381603240967\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.07860381603240967  to: 0.07854056358337402\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.07854056358337402  to: 0.07847627401351928\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.07847627401351928  to: 0.0784110188484192\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.0784110188484192  to: 0.07834882140159607\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.07834882140159607  to: 0.07828928232192993\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.07828928232192993  to: 0.07823197841644287\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.07823197841644287  to: 0.0781764805316925\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.0781764805316925  to: 0.0781223475933075\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.0781223475933075  to: 0.07806922197341919\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.07806922197341919  to: 0.07801674604415894\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.07801674604415894  to: 0.07796136736869812\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.07796136736869812  to: 0.07790322303771972\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.07790322303771972  to: 0.0778425693511963\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.0778425693511963  to: 0.07777970433235168\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.07777970433235168  to: 0.07771501541137696\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.07771501541137696  to: 0.0776488721370697\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.0776488721370697  to: 0.07758494615554809\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.07758494615554809  to: 0.0775232195854187\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.0775232195854187  to: 0.07746367454528809\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.07746367454528809  to: 0.07740615010261535\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.07740615010261535  to: 0.07735046744346619\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.07735046744346619  to: 0.07729305624961853\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.07729305624961853  to: 0.07723402976989746\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.07723402976989746  to: 0.07717350721359253\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.07717350721359253  to: 0.0771116554737091\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.0771116554737091  to: 0.07704855799674988\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.07704855799674988  to: 0.07698771357536316\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.07698771357536316  to: 0.07692877054214478\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.07692877054214478  to: 0.07687137722969055\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.07687137722969055  to: 0.076811683177948\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.076811683177948  to: 0.07674980163574219\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.07674980163574219  to: 0.07668569684028625\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.07668569684028625  to: 0.07662304043769837\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.07662304043769837  to: 0.0765579879283905\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.0765579879283905  to: 0.07649436593055725\n",
      "Training iteration: 2085\n",
      "Improved validation loss from: 0.07649436593055725  to: 0.07643197774887085\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.07643197774887085  to: 0.07636713981628418\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.07636713981628418  to: 0.076300048828125\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.076300048828125  to: 0.07623099088668824\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.07623099088668824  to: 0.07616392374038697\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.07616392374038697  to: 0.07609876990318298\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.07609876990318298  to: 0.07603539228439331\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.07603539228439331  to: 0.07596997022628785\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.07596997022628785  to: 0.07590268850326538\n",
      "Training iteration: 2094\n",
      "Improved validation loss from: 0.07590268850326538  to: 0.0758337140083313\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.0758337140083313  to: 0.07576701641082764\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.07576701641082764  to: 0.07570232152938842\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.07570232152938842  to: 0.07563928365707398\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.07563928365707398  to: 0.07557376623153686\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.07557376623153686  to: 0.07550590634346008\n",
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.07550590634346008  to: 0.07543979287147522\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.07543979287147522  to: 0.07537481188774109\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.07537481188774109  to: 0.0753058671951294\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.0753058671951294  to: 0.0752374291419983\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.0752374291419983  to: 0.0751694917678833\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.0751694917678833  to: 0.07510202527046203\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.07510202527046203  to: 0.07503488063812255\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.07503488063812255  to: 0.07496799230575561\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.07496799230575561  to: 0.07490107417106628\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.07490107417106628  to: 0.07483409643173218\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.07483409643173218  to: 0.07476710081100464\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.07476710081100464  to: 0.07470002174377441\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.07470002174377441  to: 0.07463275194168091\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.07463275194168091  to: 0.07456526756286622\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.07456526756286622  to: 0.07449759244918823\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.07449759244918823  to: 0.07442976236343384\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.07442976236343384  to: 0.07435755729675293\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.07435755729675293  to: 0.07428571581840515\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.07428571581840515  to: 0.07421430945396423\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.07421430945396423  to: 0.07414340376853942\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.07414340376853942  to: 0.07407301664352417\n",
      "Training iteration: 2121\n",
      "Improved validation loss from: 0.07407301664352417  to: 0.07400321960449219\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.07400321960449219  to: 0.07393396496772767\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.07393396496772767  to: 0.07386515736579895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.07386515736579895  to: 0.07379664182662964\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.07379664182662964  to: 0.07372835874557496\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.07372835874557496  to: 0.07366017103195191\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.07366017103195191  to: 0.0735920786857605\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.0735920786857605  to: 0.0735240638256073\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.0735240638256073  to: 0.07345610857009888\n",
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.07345610857009888  to: 0.07338861227035523\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.07338861227035523  to: 0.0733214020729065\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.0733214020729065  to: 0.0732503056526184\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.0732503056526184  to: 0.07317582964897155\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.07317582964897155  to: 0.07308517098426819\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.07308517098426819  to: 0.07298136949539184\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.07298136949539184  to: 0.07287198305130005\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.07287198305130005  to: 0.07277299761772156\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.07277299761772156  to: 0.07268462777137756\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.07268462777137756  to: 0.0726064383983612\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.0726064383983612  to: 0.07253754734992982\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.07253754734992982  to: 0.0724632203578949\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.0724632203578949  to: 0.07238390445709228\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.07238390445709228  to: 0.07230017185211182\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: 0.07230017185211182  to: 0.07221275568008423\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.07221275568008423  to: 0.07212241291999817\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.07212241291999817  to: 0.07203001976013183\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.07203001976013183  to: 0.07193639874458313\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.07193639874458313  to: 0.07184235453605652\n",
      "Training iteration: 2149\n",
      "Improved validation loss from: 0.07184235453605652  to: 0.07174864411354065\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.07174864411354065  to: 0.07165591716766358\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.07165591716766358  to: 0.07156469821929931\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.07156469821929931  to: 0.07147539854049682\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.07147539854049682  to: 0.07138826251029969\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.07138826251029969  to: 0.07130345106124877\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.07130345106124877  to: 0.07122091054916382\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.07122091054916382  to: 0.0711364507675171\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.0711364507675171  to: 0.07105048298835755\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.07105048298835755  to: 0.0709635853767395\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.0709635853767395  to: 0.07087783813476563\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.07087783813476563  to: 0.07079349756240845\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.07079349756240845  to: 0.07071120142936707\n",
      "Training iteration: 2162\n",
      "Improved validation loss from: 0.07071120142936707  to: 0.07063106298446656\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.07063106298446656  to: 0.07055295705795288\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.07055295705795288  to: 0.07047690153121948\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.07047690153121948  to: 0.07040282487869262\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.07040282487869262  to: 0.07033060193061828\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.07033060193061828  to: 0.07026002407073975\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.07026002407073975  to: 0.07019085884094238\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.07019085884094238  to: 0.07012293338775635\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.07012293338775635  to: 0.07005596160888672\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: 0.07005596160888672  to: 0.06998960375785827\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.06998960375785827  to: 0.06992363333702087\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.06992363333702087  to: 0.06985790729522705\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.06985790729522705  to: 0.06979232430458068\n",
      "Training iteration: 2175\n",
      "Improved validation loss from: 0.06979232430458068  to: 0.06972690820693969\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.06972690820693969  to: 0.06966164112091064\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.06966164112091064  to: 0.06959657669067383\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.06959657669067383  to: 0.0695318341255188\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.0695318341255188  to: 0.06946741342544556\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.06946741342544556  to: 0.06940330862998963\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.06940330862998963  to: 0.0693393349647522\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.0693393349647522  to: 0.06927546858787537\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.06927546858787537  to: 0.06921172142028809\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.06921172142028809  to: 0.069148188829422\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.069148188829422  to: 0.06908489465713501\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.06908489465713501  to: 0.06902204751968384\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.06902204751968384  to: 0.06895974278450012\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.06895974278450012  to: 0.06889765858650207\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.06889765858650207  to: 0.06883565783500671\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.06883565783500671  to: 0.06877344846725464\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.06877344846725464  to: 0.06871086359024048\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.06871086359024048  to: 0.0686478853225708\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.0686478853225708  to: 0.0685845136642456\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.0685845136642456  to: 0.0685208797454834\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.0685208797454834  to: 0.06845706701278687\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.06845706701278687  to: 0.0683933138847351\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.0683933138847351  to: 0.06832965016365052\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.06832965016365052  to: 0.06826597452163696\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.06826597452163696  to: 0.06819618940353393\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.06819618940353393  to: 0.06812143325805664\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.06812143325805664  to: 0.06804906129837036\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.06804906129837036  to: 0.06797946691513061\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.06797946691513061  to: 0.06791282892227173\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.06791282892227173  to: 0.06784897446632385\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.06784897446632385  to: 0.0677878201007843\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.0677878201007843  to: 0.0677287757396698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.0677287757396698  to: 0.0676646888256073\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.0676646888256073  to: 0.06759567856788636\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.06759567856788636  to: 0.067522531747818\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.067522531747818  to: 0.06745299100875854\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.06745299100875854  to: 0.06738730669021606\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.06738730669021606  to: 0.06732528805732726\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.06732528805732726  to: 0.06725939512252807\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.06725939512252807  to: 0.06718974709510803\n",
      "Training iteration: 2215\n",
      "Improved validation loss from: 0.06718974709510803  to: 0.06711708903312683\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.06711708903312683  to: 0.06704937219619751\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.06704937219619751  to: 0.0669887125492096\n",
      "Training iteration: 2218\n",
      "Improved validation loss from: 0.0669887125492096  to: 0.06692691445350647\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.06692691445350647  to: 0.06686367988586425\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.06686367988586425  to: 0.06679853200912475\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.06679853200912475  to: 0.06673141717910766\n",
      "Training iteration: 2222\n",
      "Improved validation loss from: 0.06673141717910766  to: 0.06666260957717896\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.06666260957717896  to: 0.06659238934516906\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.06659238934516906  to: 0.06652086973190308\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.06652086973190308  to: 0.06644877195358276\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.06644877195358276  to: 0.06637663245201111\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.06637663245201111  to: 0.0663068175315857\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.0663068175315857  to: 0.0662392795085907\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.0662392795085907  to: 0.06617390513420104\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.06617390513420104  to: 0.0661100447177887\n",
      "Training iteration: 2231\n",
      "Improved validation loss from: 0.0661100447177887  to: 0.06604430079460144\n",
      "Training iteration: 2232\n",
      "Improved validation loss from: 0.06604430079460144  to: 0.06597592234611512\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.06597592234611512  to: 0.06590509414672852\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.06590509414672852  to: 0.06583213806152344\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.06583213806152344  to: 0.06575707197189332\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.06575707197189332  to: 0.06568366289138794\n",
      "Training iteration: 2237\n",
      "Improved validation loss from: 0.06568366289138794  to: 0.06561198234558105\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.06561198234558105  to: 0.0655418574810028\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.0655418574810028  to: 0.06547293663024903\n",
      "Training iteration: 2240\n",
      "Improved validation loss from: 0.06547293663024903  to: 0.06540151834487914\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.06540151834487914  to: 0.06532758474349976\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.06532758474349976  to: 0.06525189280509949\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.06525189280509949  to: 0.06517756581306458\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.06517756581306458  to: 0.06510462760925292\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.06510462760925292  to: 0.06503286361694335\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.06503286361694335  to: 0.06491150856018066\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.06491150856018066  to: 0.06474946737289429\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.06474946737289429  to: 0.0646167278289795\n",
      "Training iteration: 2249\n",
      "Improved validation loss from: 0.0646167278289795  to: 0.06451244354248047\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.06451244354248047  to: 0.06443406343460083\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.06443406343460083  to: 0.06437767744064331\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.06437767744064331  to: 0.06428544521331787\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.06428544521331787  to: 0.06414451599121093\n",
      "Training iteration: 2254\n",
      "Improved validation loss from: 0.06414451599121093  to: 0.06396330595016479\n",
      "Training iteration: 2255\n",
      "Improved validation loss from: 0.06396330595016479  to: 0.0638163924217224\n",
      "Training iteration: 2256\n",
      "Improved validation loss from: 0.0638163924217224  to: 0.0637024998664856\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.0637024998664856  to: 0.06361907124519348\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.06361907124519348  to: 0.06354676485061646\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.06354676485061646  to: 0.0634781301021576\n",
      "Training iteration: 2260\n",
      "Improved validation loss from: 0.0634781301021576  to: 0.06335852146148682\n",
      "Training iteration: 2261\n",
      "Improved validation loss from: 0.06335852146148682  to: 0.06319682598114014\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.06319682598114014  to: 0.0630600929260254\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.0630600929260254  to: 0.06296522617340088\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.06296522617340088  to: 0.06290931701660156\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.06290931701660156  to: 0.0628352403640747\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.0628352403640747  to: 0.06273099184036254\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.06273099184036254  to: 0.06266511678695678\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: 0.06266511678695678  to: 0.06256651878356934\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.06256651878356934  to: 0.06243923306465149\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.06243923306465149  to: 0.06235443949699402\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: 0.06235443949699402  to: 0.06230736970901489\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.06230736970901489  to: 0.06222383379936218\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.06222383379936218  to: 0.062106192111968994\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.062106192111968994  to: 0.06196142435073852\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.06196142435073852  to: 0.06186501979827881\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.06186501979827881  to: 0.061813390254974364\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.061813390254974364  to: 0.06173129677772522\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.06173129677772522  to: 0.06162077188491821\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.06162077188491821  to: 0.061487716436386106\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.061487716436386106  to: 0.06140813231468201\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: 0.06140813231468201  to: 0.061377257108688354\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.061377257108688354  to: 0.061314857006073\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.061314857006073  to: 0.061216104030609134\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.061216104030609134  to: 0.061091256141662595\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.061091256141662595  to: 0.06100567579269409\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.06100567579269409  to: 0.06096971035003662\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.06096971035003662  to: 0.06090155839920044\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.06090155839920044  to: 0.06080508828163147\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.06080508828163147  to: 0.060686147212982176\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.060686147212982176  to: 0.06062597036361694\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.06062597036361694  to: 0.06056227684020996\n",
      "Training iteration: 2292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.06056227684020996  to: 0.0604936957359314\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.0604936957359314  to: 0.060444051027297975\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.060444051027297975  to: 0.060366314649581906\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.060366314649581906  to: 0.06027886867523193\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.06027886867523193  to: 0.06024033427238464\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.06024033427238464  to: 0.060167205333709714\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.060167205333709714  to: 0.060062086582183837\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.060062086582183837  to: 0.05995672345161438\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.05995672345161438  to: 0.059871631860733035\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.059871631860733035  to: 0.05986526608467102\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.05986526608467102  to: 0.059839832782745364\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.059839832782745364  to: 0.05977169275283813\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.05977169275283813  to: 0.05966628789901733\n",
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.05966628789901733  to: 0.05957778096199036\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.05957778096199036  to: 0.05950685739517212\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.05950685739517212  to: 0.05945224165916443\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.05945224165916443  to: 0.05940845608711243\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.05940845608711243  to: 0.059343773126602176\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.059343773126602176  to: 0.05925916433334351\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.05925916433334351  to: 0.059159839153289796\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.059159839153289796  to: 0.059077388048171996\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.059077388048171996  to: 0.05901175141334534\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.0590204656124115\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.05901175141334534  to: 0.058965539932250975\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.058965539932250975  to: 0.05887646675109863\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.05887646675109863  to: 0.05882914662361145\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.05882914662361145  to: 0.05882116556167603\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.05882116556167603  to: 0.05878594517707825\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.05878594517707825  to: 0.05873335599899292\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.05875421166419983\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.05873335599899292  to: 0.05872014760971069\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.05872014760971069  to: 0.058663904666900635\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.058663904666900635  to: 0.05861550569534302\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.05861550569534302  to: 0.05857474207878113\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.05857474207878113  to: 0.058535677194595334\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.058560353517532346\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.058535677194595334  to: 0.05851297378540039\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.05851297378540039  to: 0.058416610956192015\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.058416610956192015  to: 0.058287590742111206\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.058287590742111206  to: 0.05820717811584473\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.05820717811584473  to: 0.05817212462425232\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 0.05817660689353943\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.05817212462425232  to: 0.058136528730392455\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.058136528730392455  to: 0.05805305242538452\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.05805305242538452  to: 0.0579382061958313\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.0579382061958313  to: 0.05790399312973023\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.05790399312973023  to: 0.057864326238632205\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.057864326238632205  to: 0.057818096876144406\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.057818096876144406  to: 0.05773028135299683\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.05773028135299683  to: 0.05770999193191528\n",
      "Training iteration: 2342\n",
      "Improved validation loss from: 0.05770999193191528  to: 0.0576241672039032\n",
      "Training iteration: 2343\n",
      "Improved validation loss from: 0.0576241672039032  to: 0.05747691988945007\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.05747691988945007  to: 0.05737208127975464\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.05737208127975464  to: 0.05734155774116516\n",
      "Training iteration: 2346\n",
      "Improved validation loss from: 0.05734155774116516  to: 0.05733173489570618\n",
      "Training iteration: 2347\n",
      "Improved validation loss from: 0.05733173489570618  to: 0.05720881223678589\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.05720881223678589  to: 0.05703603029251099\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.05703603029251099  to: 0.0568810760974884\n",
      "Training iteration: 2350\n",
      "Improved validation loss from: 0.0568810760974884  to: 0.05682653188705444\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.05686030387878418\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.05684555172920227\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.05682653188705444  to: 0.05676177740097046\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.05676177740097046  to: 0.05661600232124329\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.05661600232124329  to: 0.056438112258911134\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.056438112258911134  to: 0.05628455281257629\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.05628455281257629  to: 0.05623738169670105\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.05628178715705871\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.056269919872283934\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.05623738169670105  to: 0.05609889030456543\n",
      "Training iteration: 2361\n",
      "Improved validation loss from: 0.05609889030456543  to: 0.05588582754135132\n",
      "Training iteration: 2362\n",
      "Improved validation loss from: 0.05588582754135132  to: 0.05570654273033142\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.05570654273033142  to: 0.05565181970596313\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.05570741891860962\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.055761796236038205\n",
      "Training iteration: 2366\n",
      "Improved validation loss from: 0.05565181970596313  to: 0.05564766526222229\n",
      "Training iteration: 2367\n",
      "Improved validation loss from: 0.05564766526222229  to: 0.05543498992919922\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.05543498992919922  to: 0.05525888204574585\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.05525888204574585  to: 0.05513356328010559\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.05516895055770874\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.05522357821464539\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.05526682138442993\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.05513356328010559  to: 0.055129450559616086\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.055129450559616086  to: 0.054894101619720456\n",
      "Training iteration: 2375\n",
      "Improved validation loss from: 0.054894101619720456  to: 0.05470342636108398\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.05470342636108398  to: 0.05468462705612183\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.054694521427154544\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.054714351892471313\n",
      "Training iteration: 2379\n",
      "Improved validation loss from: 0.05468462705612183  to: 0.05461928844451904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.05461928844451904  to: 0.054434961080551146\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.054434961080551146  to: 0.05428682565689087\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.05428682565689087  to: 0.05427290797233582\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.05437875390052795\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.05436984300613403\n",
      "Training iteration: 2385\n",
      "Improved validation loss from: 0.05427290797233582  to: 0.05419785380363464\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.05419785380363464  to: 0.05406187772750855\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.05406187772750855  to: 0.05396844148635864\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.05396844148635864  to: 0.05395600199699402\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.053975141048431395\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.05400663614273071\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.05395600199699402  to: 0.05393803119659424\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.05393803119659424  to: 0.05376332998275757\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.05376332998275757  to: 0.053615760803222653\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.053615760803222653  to: 0.053609061241149905\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): 0.05372883081436157\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): 0.053627967834472656\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.053609061241149905  to: 0.05343145132064819\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.05343145132064819  to: 0.05328910350799561\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.05330122709274292\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.05334743857383728\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.05340886116027832\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.05336872339248657\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.05328910350799561  to: 0.053233349323272706\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: 0.053233349323272706  to: 0.05312097668647766\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: 0.05312097668647766  to: 0.053041934967041016\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.053041934967041016  to: 0.052995824813842775\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): 0.053085577487945554\n",
      "Training iteration: 2408\n",
      "Validation loss (no improvement): 0.0530110239982605\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.052995824813842775  to: 0.05279348492622375\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.05279348492622375  to: 0.052735263109207155\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.05282508730888367\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.052825373411178586\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.052735263109207155  to: 0.052696478366851804\n",
      "Training iteration: 2414\n",
      "Improved validation loss from: 0.052696478366851804  to: 0.052618551254272464\n",
      "Training iteration: 2415\n",
      "Improved validation loss from: 0.052618551254272464  to: 0.05259850025177002\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.05267714262008667\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.052778178453445436\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.052831077575683595\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.05259850025177002  to: 0.05217049121856689\n",
      "Training iteration: 2420\n",
      "Improved validation loss from: 0.05217049121856689  to: 0.05176286101341247\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.05176286101341247  to: 0.05162134170532227\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.05169872641563415\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.05189456939697266\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.05198485255241394\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.05209304690361023\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.0522748589515686\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.05194702744483948\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.051774442195892334\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.05163726806640625\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: 0.05162134170532227  to: 0.05151255130767822\n",
      "Training iteration: 2431\n",
      "Improved validation loss from: 0.05151255130767822  to: 0.05140646100044251\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: 0.05140646100044251  to: 0.051329421997070315\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.051329421997070315  to: 0.051318156719207766\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.05146675705909729\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.05161882638931274\n",
      "Training iteration: 2436\n",
      "Improved validation loss from: 0.051318156719207766  to: 0.05105692148208618\n",
      "Training iteration: 2437\n",
      "Improved validation loss from: 0.05105692148208618  to: 0.05067525506019592\n",
      "Training iteration: 2438\n",
      "Improved validation loss from: 0.05067525506019592  to: 0.05047549605369568\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.05052211284637451\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.05066213607788086\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.05061439275741577\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.05063343048095703\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.050920283794403075\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.05057072639465332\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.05047549605369568  to: 0.050298655033111574\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.050298655033111574  to: 0.050038158893585205\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.050038158893585205  to: 0.05001567602157593\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.05001567602157593  to: 0.04989313185214996\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.050039881467819215\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.0502511203289032\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.050195395946502686\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.04989313185214996  to: 0.04973078370094299\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.04973078370094299  to: 0.049619370698928834\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.049619370698928834  to: 0.0493731826543808\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.0493731826543808  to: 0.049108535051345825\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.049108535051345825  to: 0.049027013778686526\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.04928530752658844\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.04966264367103577\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.04903613030910492\n",
      "Training iteration: 2460\n",
      "Improved validation loss from: 0.049027013778686526  to: 0.048533111810684204\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.048533111810684204  to: 0.04833743572235107\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.04841663241386414\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.048555055260658266\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.048539590835571286\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.04853506684303284\n",
      "Training iteration: 2466\n",
      "Improved validation loss from: 0.04833743572235107  to: 0.04805750846862793\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.04805750846862793  to: 0.04798189699649811\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.04799486100673676\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.04798189699649811  to: 0.047892028093338014\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.047892028093338014  to: 0.047857698798179624\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.04802211821079254\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.04842077791690826\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.04805395007133484\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.047857698798179624  to: 0.04763454794883728\n",
      "Training iteration: 2475\n",
      "Improved validation loss from: 0.04763454794883728  to: 0.047356462478637694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.04739364683628082\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.04763835966587067\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.0475306510925293\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.04761001467704773\n",
      "Training iteration: 2480\n",
      "Improved validation loss from: 0.047356462478637694  to: 0.047249984741210935\n",
      "Training iteration: 2481\n",
      "Improved validation loss from: 0.047249984741210935  to: 0.04704732894897461\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.04704732894897461  to: 0.046865415573120114\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.046865415573120114  to: 0.046637806296348575\n",
      "Training iteration: 2484\n",
      "Validation loss (no improvement): 0.04682818055152893\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.047013649344444276\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.04708948731422424\n",
      "Training iteration: 2487\n",
      "Improved validation loss from: 0.046637806296348575  to: 0.04654703140258789\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.04654703140258789  to: 0.046450677514076236\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.046482259035110475\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.046450677514076236  to: 0.04619161486625671\n",
      "Training iteration: 2491\n",
      "Improved validation loss from: 0.04619161486625671  to: 0.04606315493583679\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.04634929299354553\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.0467977374792099\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.046090316772460935\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.04606315493583679  to: 0.04560704827308655\n",
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.04560704827308655  to: 0.04559973180294037\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): 0.045834246277809146\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): 0.04564911723136902\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.045602554082870485\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.045965418219566345\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.04577253460884094\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.04559973180294037  to: 0.04527246952056885\n",
      "Training iteration: 2503\n",
      "Improved validation loss from: 0.04527246952056885  to: 0.04504519402980804\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.045253410935401917\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.045348954200744626\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.0451272577047348\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.04532210230827331\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.04519534111022949\n",
      "Training iteration: 2509\n",
      "Improved validation loss from: 0.04504519402980804  to: 0.04490521848201752\n",
      "Training iteration: 2510\n",
      "Improved validation loss from: 0.04490521848201752  to: 0.04449354112148285\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.04449354112148285  to: 0.044439688324928284\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.044714197516441345\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.04476325511932373\n",
      "Training iteration: 2514\n",
      "Improved validation loss from: 0.044439688324928284  to: 0.044160643219947816\n",
      "Training iteration: 2515\n",
      "Improved validation loss from: 0.044160643219947816  to: 0.04396385252475739\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): 0.044116029143333436\n",
      "Training iteration: 2517\n",
      "Validation loss (no improvement): 0.044105783104896545\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.04415807723999023\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.04433833956718445\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.04411358833312988\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.04396385252475739  to: 0.04383575916290283\n",
      "Training iteration: 2522\n",
      "Improved validation loss from: 0.04383575916290283  to: 0.04363713264465332\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.04378764629364014\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.04406291842460632\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.04394365251064301\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.04363713264465332  to: 0.04337493777275085\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.04337493777275085  to: 0.04331544935703278\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.04342460036277771\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.04331544935703278  to: 0.043161243200302124\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.043161243200302124  to: 0.04310876727104187\n",
      "Training iteration: 2531\n",
      "Validation loss (no improvement): 0.0435093492269516\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.043501314520835874\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.04310876727104187  to: 0.04293913245201111\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.04293913245201111  to: 0.042764633893966675\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.043020567297935484\n",
      "Training iteration: 2536\n",
      "Validation loss (no improvement): 0.04304246008396149\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.043122658133506776\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.042764633893966675  to: 0.042611080408096316\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: 0.042611080408096316  to: 0.04247986674308777\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.04252559542655945\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.04247986674308777  to: 0.04232783913612366\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.04248065948486328\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): 0.043038949370384216\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.042555809020996094\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: 0.04232783913612366  to: 0.04173884391784668\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.04173884391784668  to: 0.041482549905776975\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.041482549905776975  to: 0.041474375128746035\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.04165737628936768\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.041798552870750426\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.042462104558944704\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.042616653442382815\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.04210216104984284\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.041790738701820374\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.0414995014667511\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.04162023961544037\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.04172317385673523\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.04218899309635162\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.04180609285831451\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.04167273044586182\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.041768819093704224\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.04162023067474365\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.0417057603597641\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.041474375128746035  to: 0.04144565463066101\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.04144565463066101  to: 0.0411407858133316\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.0411407858133316  to: 0.041046780347824094\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.04114566743373871\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.04137844443321228\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.041695204377174375\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.041280508041381836\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.041046780347824094  to: 0.04099934697151184\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.04099934697151184  to: 0.04076161980628967\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.04092839658260346\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.04097545146942139\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.0411034494638443\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.04076161980628967  to: 0.04059584736824036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.04059584736824036  to: 0.04049737453460693\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: 0.04049737453460693  to: 0.04037196040153503\n",
      "Training iteration: 2578\n",
      "Improved validation loss from: 0.04037196040153503  to: 0.04022576212882996\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.04061000943183899\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.04030148983001709\n",
      "Training iteration: 2581\n",
      "Improved validation loss from: 0.04022576212882996  to: 0.03999972939491272\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.04022220969200134\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.040472564101219174\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.03999972939491272  to: 0.0398230642080307\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.0398230642080307  to: 0.03967909514904022\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.03995450437068939\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.039947277307510375\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.040110301971435544\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.0400122344493866\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.04003732204437256\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.03981748521327973\n",
      "Training iteration: 2592\n",
      "Improved validation loss from: 0.03967909514904022  to: 0.03965577185153961\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.03995600342750549\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.03965577185153961  to: 0.03947838246822357\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.03947838246822357  to: 0.03918317258358002\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.03942709863185882\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.039412251114845274\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.039587026834487914\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.03949277997016907\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.03938312828540802\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.03918317258358002  to: 0.038841113448143005\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): 0.038922810554504396\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.03951703608036041\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): 0.03941843509674072\n",
      "Training iteration: 2605\n",
      "Improved validation loss from: 0.038841113448143005  to: 0.03876396119594574\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.03876396119594574  to: 0.03862002789974213\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.0389945775270462\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.039567941427230836\n",
      "Training iteration: 2609\n",
      "Validation loss (no improvement): 0.0387699156999588\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.03862002789974213  to: 0.038506776094436646\n",
      "Training iteration: 2611\n",
      "Validation loss (no improvement): 0.03875816464424133\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): 0.039319977164268494\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.03856044411659241\n",
      "Training iteration: 2614\n",
      "Improved validation loss from: 0.038506776094436646  to: 0.03815225660800934\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.038276657462120056\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.038944351673126223\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.039183932542800906\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.03839080333709717\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.03816826641559601\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.038436171412467954\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.0388912558555603\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.03815225660800934  to: 0.03814803063869476\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.03814803063869476  to: 0.03783407211303711\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.03799891173839569\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.03827580809593201\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.03853008151054382\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.038132548332214355\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.03816175758838654\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.038334259390830995\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.03818958401679993\n",
      "Training iteration: 2631\n",
      "Improved validation loss from: 0.03783407211303711  to: 0.03762671053409576\n",
      "Training iteration: 2632\n",
      "Improved validation loss from: 0.03762671053409576  to: 0.03760325312614441\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.037992507219314575\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.038017645478248596\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.03804369270801544\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.03784540295600891\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.03806784451007843\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.03809020519256592\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.03783645033836365\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.03799182176589966\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.03786289691925049\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.037797316908836365\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.03760325312614441  to: 0.037376871705055235\n",
      "Training iteration: 2644\n",
      "Improved validation loss from: 0.037376871705055235  to: 0.03737244606018066\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.037816566228866574\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.037703010439872744\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.03737244606018066  to: 0.03726049959659576\n",
      "Training iteration: 2648\n",
      "Improved validation loss from: 0.03726049959659576  to: 0.03721898198127747\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.037611952424049376\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.03799212276935578\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.03724063336849213\n",
      "Training iteration: 2652\n",
      "Improved validation loss from: 0.03721898198127747  to: 0.03703537583351135\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.037272784113883975\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.0375650942325592\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.037381744384765624\n",
      "Training iteration: 2656\n",
      "Improved validation loss from: 0.03703537583351135  to: 0.03697754442691803\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.037029415369033813\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.03724927604198456\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.03703763484954834\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.037116020917892456\n",
      "Training iteration: 2661\n",
      "Improved validation loss from: 0.03697754442691803  to: 0.036944398283958436\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.03723161816596985\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.037120431661605835\n",
      "Training iteration: 2664\n",
      "Improved validation loss from: 0.036944398283958436  to: 0.03675659000873566\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.036911758780479434\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.03684898316860199\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.03728730678558349\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.037347611784934995\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.03690800964832306\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.036948204040527344\n",
      "Training iteration: 2671\n",
      "Improved validation loss from: 0.03675659000873566  to: 0.03673138320446014\n",
      "Training iteration: 2672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03696366250514984\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): 0.036833333969116214\n",
      "Training iteration: 2674\n",
      "Improved validation loss from: 0.03673138320446014  to: 0.03659586012363434\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.036786222457885744\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.03671792149543762\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.036723166704177856\n",
      "Training iteration: 2678\n",
      "Improved validation loss from: 0.03659586012363434  to: 0.0364407867193222\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.036508983373641966\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.03689142763614654\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): 0.03679245710372925\n",
      "Training iteration: 2682\n",
      "Improved validation loss from: 0.0364407867193222  to: 0.0362290769815445\n",
      "Training iteration: 2683\n",
      "Improved validation loss from: 0.0362290769815445  to: 0.035692927241325376\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.035692927241325376  to: 0.03562202155590057\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.03590996861457825\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.036478108167648314\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.03678855001926422\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.03656046986579895\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.0361257940530777\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.03618136942386627\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.036039745807647704\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.035888591408729555\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.03600766658782959\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.03655877709388733\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.036521580815315244\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.03568876683712006\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.03562202155590057  to: 0.03533574044704437\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.035390990972518924\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.03561261594295502\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.03560504913330078\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.03591607809066773\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.03597169816493988\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.03565849661827088\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.035411393642425536\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.03533574044704437  to: 0.03513174057006836\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.03518269956111908\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.035648387670516965\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.03601728081703186\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.03555366992950439\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.03545812368392944\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.0356860876083374\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.035199758410453794\n",
      "Training iteration: 2713\n",
      "Improved validation loss from: 0.03513174057006836  to: 0.03470936417579651\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.03470936417579651  to: 0.03462144732475281\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.034919065237045285\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.03555243909358978\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.03584814965724945\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.0353152334690094\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.03465395867824554\n",
      "Training iteration: 2720\n",
      "Improved validation loss from: 0.03462144732475281  to: 0.034560152888298036\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.0345898449420929\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): 0.034604203701019284\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): 0.0346608430147171\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.03517472743988037\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.035268718004226686\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.03468096852302551\n",
      "Training iteration: 2727\n",
      "Improved validation loss from: 0.034560152888298036  to: 0.034325987100601196\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.034325987100601196  to: 0.0342662900686264\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.034414428472518924\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.03460308015346527\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.035075679421424866\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.03509760499000549\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.034530410170555116\n",
      "Training iteration: 2734\n",
      "Improved validation loss from: 0.0342662900686264  to: 0.03416786789894104\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.03416786789894104  to: 0.03408302664756775\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.034138885140419004\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.034346452355384825\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.03466222286224365\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.03457615077495575\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.03461636006832123\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.03444355428218841\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.03449461162090302\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.034103798866271975\n",
      "Training iteration: 2744\n",
      "Improved validation loss from: 0.03408302664756775  to: 0.03385707139968872\n",
      "Training iteration: 2745\n",
      "Improved validation loss from: 0.03385707139968872  to: 0.03377867043018341\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.03398179411888123\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.03417789340019226\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.03444581627845764\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.0341533362865448\n",
      "Training iteration: 2750\n",
      "Improved validation loss from: 0.03377867043018341  to: 0.033585387468338015\n",
      "Training iteration: 2751\n",
      "Improved validation loss from: 0.033585387468338015  to: 0.03338377773761749\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.03338377773761749  to: 0.03325117826461792\n",
      "Training iteration: 2753\n",
      "Validation loss (no improvement): 0.03338279724121094\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): 0.03365221619606018\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.03404906988143921\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.03396643996238709\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.03349210321903229\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.03341527581214905\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.03327871859073639\n",
      "Training iteration: 2760\n",
      "Improved validation loss from: 0.03325117826461792  to: 0.03322242796421051\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.03355422019958496\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.03407578468322754\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.033692938089370725\n",
      "Training iteration: 2764\n",
      "Improved validation loss from: 0.03322242796421051  to: 0.033163100481033325\n",
      "Training iteration: 2765\n",
      "Improved validation loss from: 0.033163100481033325  to: 0.03311347961425781\n",
      "Training iteration: 2766\n",
      "Improved validation loss from: 0.03311347961425781  to: 0.0330723911523819\n",
      "Training iteration: 2767\n",
      "Improved validation loss from: 0.0330723911523819  to: 0.032849863171577454\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.033013802766799924\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.033614975214004514\n",
      "Training iteration: 2770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03380703330039978\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.03306677341461182\n",
      "Training iteration: 2772\n",
      "Improved validation loss from: 0.032849863171577454  to: 0.03276240825653076\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.03276887536048889\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.033060452342033385\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.033440059423446654\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.033458462357521056\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.033253470063209535\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.033365511894226076\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.03380455374717713\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.03323681354522705\n",
      "Training iteration: 2781\n",
      "Improved validation loss from: 0.03276240825653076  to: 0.03271457552909851\n",
      "Training iteration: 2782\n",
      "Improved validation loss from: 0.03271457552909851  to: 0.032638901472091676\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.03287580609321594\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.03351003527641296\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.033644679188728335\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.0330506831407547\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.03288366198539734\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.03299819529056549\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.033473032712936404\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.033062615990638734\n",
      "Training iteration: 2791\n",
      "Improved validation loss from: 0.032638901472091676  to: 0.03257257342338562\n",
      "Training iteration: 2792\n",
      "Improved validation loss from: 0.03257257342338562  to: 0.03251031339168549\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.03273440599441528\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.03339723944664001\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.03388669490814209\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.03320756852626801\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): 0.032545676827430724\n",
      "Training iteration: 2798\n",
      "Improved validation loss from: 0.03251031339168549  to: 0.03241600394248963\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.03255057036876678\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.0328094333410263\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.03274964988231659\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.03281478881835938\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.033329179883003233\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.03415306508541107\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.03476091921329498\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.03458174169063568\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): 0.03387829661369324\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): 0.033767780661582945\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.03392339050769806\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.03422925174236298\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.03427386283874512\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): 0.03410454392433167\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.0337538480758667\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): 0.03331490457057953\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): 0.03286700248718262\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.03291400372982025\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.032999074459075926\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.03289625346660614\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.03255512118339539\n",
      "Training iteration: 2820\n",
      "Improved validation loss from: 0.03241600394248963  to: 0.03222033977508545\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.03237243592739105\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): 0.032592982053756714\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.032297718524932864\n",
      "Training iteration: 2824\n",
      "Improved validation loss from: 0.03222033977508545  to: 0.031986108422279357\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.03213018774986267\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.032269296050071714\n",
      "Training iteration: 2827\n",
      "Improved validation loss from: 0.031986108422279357  to: 0.031910264492034913\n",
      "Training iteration: 2828\n",
      "Improved validation loss from: 0.031910264492034913  to: 0.03163942396640777\n",
      "Training iteration: 2829\n",
      "Validation loss (no improvement): 0.03171105682849884\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.031781232357025145\n",
      "Training iteration: 2831\n",
      "Improved validation loss from: 0.03163942396640777  to: 0.031442826986312865\n",
      "Training iteration: 2832\n",
      "Improved validation loss from: 0.031442826986312865  to: 0.031120139360427856\n",
      "Training iteration: 2833\n",
      "Improved validation loss from: 0.031120139360427856  to: 0.03108101487159729\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.031413310766220094\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.031421166658401486\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.031184405088424683\n",
      "Training iteration: 2837\n",
      "Improved validation loss from: 0.03108101487159729  to: 0.03078717291355133\n",
      "Training iteration: 2838\n",
      "Improved validation loss from: 0.03078717291355133  to: 0.030646881461143492\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.030720847845077514\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.030942991375923157\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.031355655193328856\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.03188888430595398\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.032108479738235475\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.032224157452583314\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.03243706226348877\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.03225720822811127\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.03176926076412201\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.031606587767601016\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.03153007030487061\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.030969423055648804\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.030652254819869995\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.03069041669368744\n",
      "Training iteration: 2853\n",
      "Improved validation loss from: 0.030646881461143492  to: 0.030481767654418946\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.030481767654418946  to: 0.030281123518943787\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.030281123518943787  to: 0.03025471568107605\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): 0.030383265018463133\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.03036455512046814\n",
      "Training iteration: 2858\n",
      "Improved validation loss from: 0.03025471568107605  to: 0.029965332150459288\n",
      "Training iteration: 2859\n",
      "Improved validation loss from: 0.029965332150459288  to: 0.02983294427394867\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.029894322156906128\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): 0.02991066873073578\n",
      "Training iteration: 2862\n",
      "Improved validation loss from: 0.02983294427394867  to: 0.029661291837692262\n",
      "Training iteration: 2863\n",
      "Improved validation loss from: 0.029661291837692262  to: 0.029494643211364746\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.029545050859451295\n",
      "Training iteration: 2865\n",
      "Improved validation loss from: 0.029494643211364746  to: 0.02944691777229309\n",
      "Training iteration: 2866\n",
      "Improved validation loss from: 0.02944691777229309  to: 0.029112830758094788\n",
      "Training iteration: 2867\n",
      "Improved validation loss from: 0.029112830758094788  to: 0.028977546095848083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2868\n",
      "Improved validation loss from: 0.028977546095848083  to: 0.02886582911014557\n",
      "Training iteration: 2869\n",
      "Improved validation loss from: 0.02886582911014557  to: 0.02876388728618622\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): 0.028763896226882933\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): 0.028806698322296143\n",
      "Training iteration: 2872\n",
      "Improved validation loss from: 0.02876388728618622  to: 0.0287207692861557\n",
      "Training iteration: 2873\n",
      "Improved validation loss from: 0.0287207692861557  to: 0.02837853729724884\n",
      "Training iteration: 2874\n",
      "Improved validation loss from: 0.02837853729724884  to: 0.028239181637763976\n",
      "Training iteration: 2875\n",
      "Improved validation loss from: 0.028239181637763976  to: 0.028225645422935486\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.028225645422935486  to: 0.028122514486312866\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.028122514486312866  to: 0.02788318395614624\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.02788318395614624  to: 0.027717667818069457\n",
      "Training iteration: 2879\n",
      "Improved validation loss from: 0.027717667818069457  to: 0.027716007828712464\n",
      "Training iteration: 2880\n",
      "Improved validation loss from: 0.027716007828712464  to: 0.02770744264125824\n",
      "Training iteration: 2881\n",
      "Improved validation loss from: 0.02770744264125824  to: 0.02746324837207794\n",
      "Training iteration: 2882\n",
      "Improved validation loss from: 0.02746324837207794  to: 0.027272811532020567\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.02728508710861206\n",
      "Training iteration: 2884\n",
      "Improved validation loss from: 0.027272811532020567  to: 0.027039462327957155\n",
      "Training iteration: 2885\n",
      "Improved validation loss from: 0.027039462327957155  to: 0.02669885754585266\n",
      "Training iteration: 2886\n",
      "Improved validation loss from: 0.02669885754585266  to: 0.026472967863082886\n",
      "Training iteration: 2887\n",
      "Improved validation loss from: 0.026472967863082886  to: 0.02623998522758484\n",
      "Training iteration: 2888\n",
      "Improved validation loss from: 0.02623998522758484  to: 0.025912895798683167\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.025953072309494018\n",
      "Training iteration: 2890\n",
      "Improved validation loss from: 0.025912895798683167  to: 0.02583855390548706\n",
      "Training iteration: 2891\n",
      "Improved validation loss from: 0.02583855390548706  to: 0.025698980689048766\n",
      "Training iteration: 2892\n",
      "Improved validation loss from: 0.025698980689048766  to: 0.025435787439346314\n",
      "Training iteration: 2893\n",
      "Improved validation loss from: 0.025435787439346314  to: 0.025062954425811766\n",
      "Training iteration: 2894\n",
      "Improved validation loss from: 0.025062954425811766  to: 0.024926424026489258\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.025034064054489137\n",
      "Training iteration: 2896\n",
      "Improved validation loss from: 0.024926424026489258  to: 0.02488688975572586\n",
      "Training iteration: 2897\n",
      "Improved validation loss from: 0.02488688975572586  to: 0.024681968986988066\n",
      "Training iteration: 2898\n",
      "Improved validation loss from: 0.024681968986988066  to: 0.024681711196899415\n",
      "Training iteration: 2899\n",
      "Improved validation loss from: 0.024681711196899415  to: 0.024555301666259764\n",
      "Training iteration: 2900\n",
      "Improved validation loss from: 0.024555301666259764  to: 0.024190247058868408\n",
      "Training iteration: 2901\n",
      "Improved validation loss from: 0.024190247058868408  to: 0.02408480942249298\n",
      "Training iteration: 2902\n",
      "Improved validation loss from: 0.02408480942249298  to: 0.024079327285289765\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.024079327285289765  to: 0.024026045203208925\n",
      "Training iteration: 2904\n",
      "Improved validation loss from: 0.024026045203208925  to: 0.02388371229171753\n",
      "Training iteration: 2905\n",
      "Improved validation loss from: 0.02388371229171753  to: 0.023752689361572266\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: 0.023752689361572266  to: 0.023590922355651855\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.023590922355651855  to: 0.023427903652191162\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.023427903652191162  to: 0.023406139016151427\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): 0.023417417705059052\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.023410284519195558\n",
      "Training iteration: 2911\n",
      "Improved validation loss from: 0.023406139016151427  to: 0.023395547270774843\n",
      "Training iteration: 2912\n",
      "Improved validation loss from: 0.023395547270774843  to: 0.023359742760658265\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): 0.023394909501075745\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.023359742760658265  to: 0.023256048560142517\n",
      "Training iteration: 2915\n",
      "Improved validation loss from: 0.023256048560142517  to: 0.023076538741588593\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.023218762874603272\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.023139612376689912\n",
      "Training iteration: 2918\n",
      "Improved validation loss from: 0.023076538741588593  to: 0.022901996970176697\n",
      "Training iteration: 2919\n",
      "Improved validation loss from: 0.022901996970176697  to: 0.022860217094421386\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): 0.022986528277397156\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.022860217094421386  to: 0.022766557335853577\n",
      "Training iteration: 2922\n",
      "Improved validation loss from: 0.022766557335853577  to: 0.02260397970676422\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.022646331787109376\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.022734978795051576\n",
      "Training iteration: 2925\n",
      "Improved validation loss from: 0.02260397970676422  to: 0.022598747909069062\n",
      "Training iteration: 2926\n",
      "Improved validation loss from: 0.022598747909069062  to: 0.022498516738414763\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.022674372792243956\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.02292589396238327\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.022617988288402557\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.02256506234407425\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.02256264239549637\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.022560377418994904\n",
      "Training iteration: 2933\n",
      "Improved validation loss from: 0.022498516738414763  to: 0.022475025057792662\n",
      "Training iteration: 2934\n",
      "Improved validation loss from: 0.022475025057792662  to: 0.022406895458698273\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): 0.02246430218219757\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): 0.022561657428741454\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.022494201362133027\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.022470033168792723\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.022537116706371308\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.022509098052978516\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.022486491501331328\n",
      "Training iteration: 2942\n",
      "Improved validation loss from: 0.022406895458698273  to: 0.02237836867570877\n",
      "Training iteration: 2943\n",
      "Improved validation loss from: 0.02237836867570877  to: 0.022308215498924255\n",
      "Training iteration: 2944\n",
      "Improved validation loss from: 0.022308215498924255  to: 0.02206481248140335\n",
      "Training iteration: 2945\n",
      "Improved validation loss from: 0.02206481248140335  to: 0.021974071860313416\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.022013139724731446\n",
      "Training iteration: 2947\n",
      "Improved validation loss from: 0.021974071860313416  to: 0.02193327397108078\n",
      "Training iteration: 2948\n",
      "Improved validation loss from: 0.02193327397108078  to: 0.02182898074388504\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.021871764957904816\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.022006425261497497\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.0220597580075264\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.021904918551445007\n",
      "Training iteration: 2953\n",
      "Improved validation loss from: 0.02182898074388504  to: 0.021663841605186463\n",
      "Training iteration: 2954\n",
      "Improved validation loss from: 0.021663841605186463  to: 0.02162596881389618\n",
      "Training iteration: 2955\n",
      "Improved validation loss from: 0.02162596881389618  to: 0.021544304490089417\n",
      "Training iteration: 2956\n",
      "Improved validation loss from: 0.021544304490089417  to: 0.02142171859741211\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.02144764959812164\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.02142171859741211  to: 0.02138688862323761\n",
      "Training iteration: 2959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.02138688862323761  to: 0.02137136459350586\n",
      "Training iteration: 2960\n",
      "Improved validation loss from: 0.02137136459350586  to: 0.021368113160133363\n",
      "Training iteration: 2961\n",
      "Improved validation loss from: 0.021368113160133363  to: 0.021278175711631774\n",
      "Training iteration: 2962\n",
      "Improved validation loss from: 0.021278175711631774  to: 0.021188244223594666\n",
      "Training iteration: 2963\n",
      "Improved validation loss from: 0.021188244223594666  to: 0.021164365112781525\n",
      "Training iteration: 2964\n",
      "Improved validation loss from: 0.021164365112781525  to: 0.021050310134887694\n",
      "Training iteration: 2965\n",
      "Improved validation loss from: 0.021050310134887694  to: 0.020964136719703673\n",
      "Training iteration: 2966\n",
      "Improved validation loss from: 0.020964136719703673  to: 0.020802068710327148\n",
      "Training iteration: 2967\n",
      "Improved validation loss from: 0.020802068710327148  to: 0.02071071118116379\n",
      "Training iteration: 2968\n",
      "Improved validation loss from: 0.02071071118116379  to: 0.020668569207191467\n",
      "Training iteration: 2969\n",
      "Improved validation loss from: 0.020668569207191467  to: 0.02065419703722\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.020680682361125947\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.02071942538022995\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.02073083370923996\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.020684997737407684\n",
      "Training iteration: 2974\n",
      "Improved validation loss from: 0.02065419703722  to: 0.020462813973426818\n",
      "Training iteration: 2975\n",
      "Improved validation loss from: 0.020462813973426818  to: 0.02037205696105957\n",
      "Training iteration: 2976\n",
      "Improved validation loss from: 0.02037205696105957  to: 0.02025865614414215\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.02030421793460846\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.020428280532360076\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.020377278327941895\n",
      "Training iteration: 2980\n",
      "Improved validation loss from: 0.02025865614414215  to: 0.02022317349910736\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.02031427323818207\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.020413318276405336\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.0202266663312912\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.020275673270225524\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.02022363245487213\n",
      "Training iteration: 2986\n",
      "Validation loss (no improvement): 0.020263774693012236\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.02024533748626709\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.02024696171283722\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.02026899755001068\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.020419976115226744\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.02041906863451004\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.020407631993293762\n",
      "Training iteration: 2993\n",
      "Improved validation loss from: 0.02022317349910736  to: 0.02016734629869461\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.02019294798374176\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.02029401808977127\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.02043568193912506\n",
      "Training iteration: 2997\n",
      "Improved validation loss from: 0.02016734629869461  to: 0.02008447200059891\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.02009202688932419\n",
      "Training iteration: 2999\n",
      "Improved validation loss from: 0.02008447200059891  to: 0.019989074766635896\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.020407864451408388\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.020236456394195558\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.020079152286052705\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.020042309165000917\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.0202872633934021\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.020108406245708466\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.020091724395751954\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.020006808638572692\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.020072503387928008\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.020206475257873537\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.020012454688549043\n",
      "Training iteration: 3011\n",
      "Improved validation loss from: 0.019989074766635896  to: 0.019907672703266144\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): 0.020062430202960967\n",
      "Training iteration: 3013\n",
      "Improved validation loss from: 0.019907672703266144  to: 0.019799631834030152\n",
      "Training iteration: 3014\n",
      "Improved validation loss from: 0.019799631834030152  to: 0.01968473494052887\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): 0.019724714756011962\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.019964011013507844\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.019858038425445555\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.01977495104074478\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.020138387382030488\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.02006872445344925\n",
      "Training iteration: 3021\n",
      "Improved validation loss from: 0.01968473494052887  to: 0.019571389257907867\n",
      "Training iteration: 3022\n",
      "Improved validation loss from: 0.019571389257907867  to: 0.01938398778438568\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.019385504722595214\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.0195592001080513\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.019838643074035645\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.01971183717250824\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.019966818392276764\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.019871291518211365\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.01995241194963455\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.019699187576770784\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.019626958668231963\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.019717155396938323\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.02009221613407135\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.019898319244384767\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.019492706656455992\n",
      "Training iteration: 3036\n",
      "Improved validation loss from: 0.01938398778438568  to: 0.01924325227737427\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.019365064799785614\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.020170970261096953\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.019880422949790956\n",
      "Training iteration: 3040\n",
      "Improved validation loss from: 0.01924325227737427  to: 0.019028134644031525\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.019127492606639863\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.019222673773765565\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.019100119173526765\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.020052817463874818\n",
      "Training iteration: 3045\n",
      "Validation loss (no improvement): 0.020123882591724394\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.019470110535621643\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.019201061129570006\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.019240067899227144\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.019804199039936066\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.019830504059791566\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.019233362376689912\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.019101786613464355\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.019209128618240357\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.0196374848484993\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.019862623512744905\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.019519390165805818\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.01957956552505493\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.019995835423469544\n",
      "Training iteration: 3059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.019484154880046844\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.01932276487350464\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.019337797164916994\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.019748905301094057\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.019600489735603334\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.01961623877286911\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.019684013724327088\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): 0.0194888636469841\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): 0.01949433535337448\n",
      "Training iteration: 3068\n",
      "Validation loss (no improvement): 0.019304823875427247\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.01926232874393463\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.019408003985881807\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.019437322020530702\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.019161544740200043\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.019246232509613038\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.01964094191789627\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.019389602541923522\n",
      "Training iteration: 3076\n",
      "Improved validation loss from: 0.019028134644031525  to: 0.01870879381895065\n",
      "Training iteration: 3077\n",
      "Improved validation loss from: 0.01870879381895065  to: 0.018560072779655455\n",
      "Training iteration: 3078\n",
      "Improved validation loss from: 0.018560072779655455  to: 0.018453916907310484\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.018664816021919252\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.019153329730033874\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.01896432340145111\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.018871223926544188\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.01913498342037201\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.01949412524700165\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.019159626960754395\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.01854352205991745\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.01850547343492508\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.01866045743227005\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.01879459172487259\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.01922820061445236\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.019073106348514557\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.01907399445772171\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.018915118277072908\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.018923477828502656\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.019744960963726042\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.019417285919189453\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.018722794950008392\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.018635472655296324\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.01855415999889374\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.018884976208209992\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.019884590804576874\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.01979234367609024\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.018952903151512147\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.019033430516719817\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.01908612698316574\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.019404926896095277\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.01987350732088089\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.019333383440971373\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.01856474131345749\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.018481376767158508\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.01847102344036102\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.018755260109901428\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.01929919421672821\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.018960316479206086\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.018890005350112916\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.01890706121921539\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.018789893388748168\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.019293975830078126\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.019495709240436553\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.018758068978786468\n",
      "Training iteration: 3121\n",
      "Improved validation loss from: 0.018453916907310484  to: 0.01839199513196945\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.018436133861541748\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.018687792122364044\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.019022688269615173\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.018774017691612244\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.018786640465259553\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.019117715954780578\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.019211393594741822\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.019382186233997345\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.019154778122901915\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.018871182203292848\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.01848074346780777\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.01858113557100296\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.019183118641376496\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.018867965042591094\n",
      "Training iteration: 3136\n",
      "Improved validation loss from: 0.01839199513196945  to: 0.018194897472858428\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.018317797780036928\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.018283927440643312\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.018357884883880616\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.019094988703727722\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.018975646793842317\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.018209829926490784\n",
      "Training iteration: 3143\n",
      "Improved validation loss from: 0.018194897472858428  to: 0.018074679374694824\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.018105322122573854\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.018074803054332733\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.018702152371406554\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.018387424945831298\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.018141728639602662\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.018230921030044554\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.018395280838012694\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.019054654240608215\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.018875624239444732\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.01821940243244171\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.01821560114622116\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.018148285150527955\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.018396537005901336\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.018130521476268768\n",
      "Training iteration: 3158\n",
      "Improved validation loss from: 0.018074679374694824  to: 0.017883585393428804\n",
      "Training iteration: 3159\n",
      "Improved validation loss from: 0.017883585393428804  to: 0.017808300256729127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.01783958375453949\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.018171551823616027\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.018176928162574768\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.018040183186531066\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.018017369508743285\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.018139225244522095\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.018293648958206177\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.01818675696849823\n",
      "Training iteration: 3168\n",
      "Improved validation loss from: 0.017808300256729127  to: 0.01773426979780197\n",
      "Training iteration: 3169\n",
      "Improved validation loss from: 0.01773426979780197  to: 0.01766302138566971\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.01782372295856476\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.01829586923122406\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.017847427725791933\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.017863729596138002\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.01792728006839752\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.01769733726978302\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.018050284683704378\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.018473033607006074\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.017735385894775392\n",
      "Training iteration: 3179\n",
      "Improved validation loss from: 0.01766302138566971  to: 0.01743532419204712\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.017609317600727082\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.017477653920650482\n",
      "Training iteration: 3182\n",
      "Improved validation loss from: 0.01743532419204712  to: 0.01742136925458908\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.018481218814849855\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.018363805115222932\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.017527756094932557\n",
      "Training iteration: 3186\n",
      "Improved validation loss from: 0.01742136925458908  to: 0.017393457889556884\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.017535917460918427\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.01782675236463547\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.018392397463321684\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.018345320224761964\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.01768297851085663\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.017794311046600342\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.017503151297569276\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.01764523535966873\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.018342196941375732\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.018528810143470763\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.018146349489688872\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.017826454341411592\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.01790761947631836\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.01803114414215088\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.018319229781627654\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.01821995824575424\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.018250100314617157\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.0182932585477829\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.0181558758020401\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.018828563392162323\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.018812096118927\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.018103089928627015\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.017941007018089296\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.017939172685146332\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.01827583760023117\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.01838306188583374\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.017993643879890442\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.01773121953010559\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.017771801352500914\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.017729267477989197\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.01797970086336136\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.018155840039253236\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.01767694652080536\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.017484399676322936\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.01753733158111572\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.017880049347877503\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.017883504927158355\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.017603656649589537\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.01766457110643387\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.017530123889446258\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.01739812195301056\n",
      "Training iteration: 3228\n",
      "Improved validation loss from: 0.017393457889556884  to: 0.017333506047725676\n",
      "Training iteration: 3229\n",
      "Improved validation loss from: 0.017333506047725676  to: 0.017098884284496307\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.017102354764938356\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.01746581941843033\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.017332060635089873\n",
      "Training iteration: 3233\n",
      "Improved validation loss from: 0.017098884284496307  to: 0.017037925124168397\n",
      "Training iteration: 3234\n",
      "Improved validation loss from: 0.017037925124168397  to: 0.017031244933605194\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.01731911152601242\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.01766304224729538\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.01714193820953369\n",
      "Training iteration: 3238\n",
      "Improved validation loss from: 0.017031244933605194  to: 0.016810759902000427\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.016842710971832275\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.017107100784778596\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.017109158635139465\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.01683623194694519\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.017044369876384736\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.0172915056347847\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.017127740383148193\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.01684750020503998\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.016930973529815672\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.017009779810905457\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.017094436287879943\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.017157396674156188\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.017000988125801086\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.0169371634721756\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.016810837388038635\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.016985420882701874\n",
      "Training iteration: 3255\n",
      "Improved validation loss from: 0.016810759902000427  to: 0.016757695376873015\n",
      "Training iteration: 3256\n",
      "Improved validation loss from: 0.016757695376873015  to: 0.016715419292449952\n",
      "Training iteration: 3257\n",
      "Improved validation loss from: 0.016715419292449952  to: 0.01664866954088211\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.01702096164226532\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.017079055309295654\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.01672254353761673\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.01684829145669937\n",
      "Training iteration: 3262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.017274847626686095\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.017092613875865935\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.016844718158245085\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.016843998432159425\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.016867898404598236\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.01673692315816879\n",
      "Training iteration: 3268\n",
      "Improved validation loss from: 0.01664866954088211  to: 0.01658492088317871\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.016834078729152678\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.017206096649169923\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.017149177193641663\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.01665380448102951\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.016598482429981232\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.016714784502983093\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.01688188761472702\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.016695071756839753\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.016748085618019104\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.016605478525161744\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.0167997807264328\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.01683136969804764\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.016736683249473572\n",
      "Training iteration: 3282\n",
      "Improved validation loss from: 0.01658492088317871  to: 0.016529425978660583\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.016535912454128266\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.016771116852760316\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.016933992505073547\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.016861395537853242\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.01670694202184677\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.016781474649906158\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.01673385202884674\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.016930460929870605\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.017021453380584715\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.016749194264411925\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.01663956940174103\n",
      "Training iteration: 3294\n",
      "Improved validation loss from: 0.016529425978660583  to: 0.01649857759475708\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.016600537300109863\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.016803851723670958\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.016615355014801027\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.016680148243904114\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.016541187465190888\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.016754159331321718\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.017061741650104524\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.017026597261428834\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.016572172939777374\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.016603545844554903\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.016769449412822723\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.01698618084192276\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.016950801014900208\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.016695454716682434\n",
      "Training iteration: 3309\n",
      "Improved validation loss from: 0.01649857759475708  to: 0.016488301753997802\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.016809526085853576\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.01695598065853119\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.016538158059120178\n",
      "Training iteration: 3313\n",
      "Improved validation loss from: 0.016488301753997802  to: 0.016380167007446288\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.016688136756420134\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.016577044129371644\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.017059502005577088\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.017026169598102568\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.01650751382112503\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.016582810878753663\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.016585800051689147\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.016987256705760956\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.01684148609638214\n",
      "Training iteration: 3323\n",
      "Improved validation loss from: 0.016380167007446288  to: 0.016338109970092773\n",
      "Training iteration: 3324\n",
      "Improved validation loss from: 0.016338109970092773  to: 0.01612255871295929\n",
      "Training iteration: 3325\n",
      "Improved validation loss from: 0.01612255871295929  to: 0.016110184788703918\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.016523821651935576\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.01650335341691971\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.01640227437019348\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.016702742874622346\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.01679927557706833\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.01688891798257828\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.016449645161628723\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.016157448291778564\n",
      "Training iteration: 3334\n",
      "Improved validation loss from: 0.016110184788703918  to: 0.01601884067058563\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.016211578249931337\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.016375529766082763\n",
      "Training iteration: 3337\n",
      "Improved validation loss from: 0.01601884067058563  to: 0.015927709639072418\n",
      "Training iteration: 3338\n",
      "Improved validation loss from: 0.015927709639072418  to: 0.015761344134807585\n",
      "Training iteration: 3339\n",
      "Improved validation loss from: 0.015761344134807585  to: 0.015747781097888946\n",
      "Training iteration: 3340\n",
      "Improved validation loss from: 0.015747781097888946  to: 0.015628448128700255\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.016109491884708404\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.01636536866426468\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.01579667180776596\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.015761235356330873\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.0158792182803154\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.016195045411586763\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.015979620814323425\n",
      "Training iteration: 3348\n",
      "Improved validation loss from: 0.015628448128700255  to: 0.015529060363769531\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.01572023630142212\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.015592008829116821\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.01599373072385788\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.016050057113170625\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.015801006555557252\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.015594187378883361\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.015552285313606262\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.016096463799476622\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.01602303683757782\n",
      "Training iteration: 3358\n",
      "Improved validation loss from: 0.015529060363769531  to: 0.015510697662830353\n",
      "Training iteration: 3359\n",
      "Improved validation loss from: 0.015510697662830353  to: 0.01549164652824402\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.015668360888957976\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.016023513674736024\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.016052143275737764\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.015585896372795106\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.015800543129444122\n",
      "Training iteration: 3365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015903255343437193\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.016169476509094238\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.016159522533416747\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.015893523395061494\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.015544454753398895\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.015570171177387238\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.015860335528850557\n",
      "Training iteration: 3372\n",
      "Validation loss (no improvement): 0.015799400210380555\n",
      "Training iteration: 3373\n",
      "Improved validation loss from: 0.01549164652824402  to: 0.015459243953227998\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.01565301716327667\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.01597178876399994\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.01585897207260132\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.015594756603240967\n",
      "Training iteration: 3378\n",
      "Improved validation loss from: 0.015459243953227998  to: 0.01533973217010498\n",
      "Training iteration: 3379\n",
      "Improved validation loss from: 0.01533973217010498  to: 0.015259632468223571\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.015507782995700835\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.015583708882331848\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.015588365495204926\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.0155874103307724\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.015830221772193908\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): 0.015716405212879182\n",
      "Training iteration: 3386\n",
      "Improved validation loss from: 0.015259632468223571  to: 0.01525115966796875\n",
      "Training iteration: 3387\n",
      "Improved validation loss from: 0.01525115966796875  to: 0.015082341432571412\n",
      "Training iteration: 3388\n",
      "Improved validation loss from: 0.015082341432571412  to: 0.01495358645915985\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.015073299407958984\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.015151324868202209\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.015270833671092988\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.015313203632831573\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.01518886536359787\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.01503073275089264\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.015032514929771423\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.01508413553237915\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.015120486915111541\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.015104857087135316\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.014979219436645508\n",
      "Training iteration: 3400\n",
      "Improved validation loss from: 0.01495358645915985  to: 0.014739644527435303\n",
      "Training iteration: 3401\n",
      "Improved validation loss from: 0.014739644527435303  to: 0.014572279155254364\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.014815299212932587\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.014682090282440186\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.014579513669013977\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.014623489975929261\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.014818832278251648\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): 0.015580594539642334\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.015447863936424255\n",
      "Training iteration: 3409\n",
      "Validation loss (no improvement): 0.014894907176494599\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): 0.014906282722949981\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.014859792590141297\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.015107481181621552\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.01511949896812439\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.014750167727470398\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.014748519659042359\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.014954131841659547\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): 0.015282467007637024\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.015257009863853454\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.014831802248954773\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.014768674969673157\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): 0.014845648407936096\n",
      "Training iteration: 3422\n",
      "Improved validation loss from: 0.014572279155254364  to: 0.014513757824897767\n",
      "Training iteration: 3423\n",
      "Improved validation loss from: 0.014513757824897767  to: 0.014489284157752991\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): 0.01490311324596405\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.015187166631221771\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.014787039160728455\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.014687475562095643\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.014925852417945862\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): 0.01482970267534256\n",
      "Training iteration: 3430\n",
      "Improved validation loss from: 0.014489284157752991  to: 0.014282122254371643\n",
      "Training iteration: 3431\n",
      "Improved validation loss from: 0.014282122254371643  to: 0.014143922924995422\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.014362116158008576\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.014465634524822236\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.014401793479919434\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.014226503670215607\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.014512772858142852\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.015039482712745666\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.014900633692741394\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.014460888504981995\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.014487557113170624\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.014813187718391418\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.01472133696079254\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.014377552270889282\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.01446869820356369\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.014953014254570008\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.015066218376159669\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.014751729369163514\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.01458546370267868\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): 0.014705738425254822\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.014523915946483612\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.014226926863193512\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.014297838509082793\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.014744699001312256\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.014625921845436096\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.01439952552318573\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.014528132975101471\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.014666172862052917\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.014594385027885437\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.014552247524261475\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.014450478553771972\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.014582614600658416\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.014571776986122132\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.014283275604248047\n",
      "Training iteration: 3464\n",
      "Improved validation loss from: 0.014143922924995422  to: 0.014016285538673401\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.014303770661354066\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.01416490375995636\n",
      "Training iteration: 3467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.014016285538673401  to: 0.014005661010742188\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.014095492660999298\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.014477667212486268\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.014110934734344483\n",
      "Training iteration: 3471\n",
      "Improved validation loss from: 0.014005661010742188  to: 0.013805024325847626\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.014023122191429139\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.013888041675090789\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.013872568309307099\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.01393565684556961\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.01386624127626419\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.013821093738079071\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.013968726992607117\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.013916432857513428\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.013996140658855438\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.013956120610237122\n",
      "Training iteration: 3482\n",
      "Improved validation loss from: 0.013805024325847626  to: 0.013706013560295105\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.013741025328636169\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.014082190394401551\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.013943831622600555\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.013940289616584778\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.014004500210285186\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.014242683351039887\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): 0.013968643546104432\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.013810281455516816\n",
      "Training iteration: 3491\n",
      "Improved validation loss from: 0.013706013560295105  to: 0.01368246078491211\n",
      "Training iteration: 3492\n",
      "Improved validation loss from: 0.01368246078491211  to: 0.013348810374736786\n",
      "Training iteration: 3493\n",
      "Improved validation loss from: 0.013348810374736786  to: 0.01330496370792389\n",
      "Training iteration: 3494\n",
      "Validation loss (no improvement): 0.014012296497821809\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.013999775052070618\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.013618838787078858\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): 0.013980905711650848\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.014084884524345398\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): 0.014172233641147614\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): 0.01439884454011917\n",
      "Training iteration: 3501\n",
      "Validation loss (no improvement): 0.014074631035327911\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): 0.01369851380586624\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.01363358348608017\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.013760185241699219\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.013750259578227998\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): 0.013678619265556335\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): 0.013876557350158691\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.013967259228229523\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.013925263285636902\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.01413680613040924\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.014078359305858611\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.013692429661750794\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.013499732315540313\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.013645787537097932\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.013575030863285065\n",
      "Training iteration: 3516\n",
      "Improved validation loss from: 0.01330496370792389  to: 0.01311343014240265\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.013139578700065612\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.013566681742668152\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.013737364113330841\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.013609953224658966\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.01342523843050003\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.013511183857917785\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): 0.013494904339313506\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): 0.013513407111167908\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.013555121421813966\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.013701489567756653\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.013490192592144012\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.013555853068828583\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.01337566077709198\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.013398191332817078\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.013131561875343322\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.01340218484401703\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.0135239839553833\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.013374349474906922\n",
      "Training iteration: 3535\n",
      "Improved validation loss from: 0.01311343014240265  to: 0.013064113259315491\n",
      "Training iteration: 3536\n",
      "Validation loss (no improvement): 0.013250167667865752\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.013615985214710236\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): 0.013593071699142456\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.013308064639568329\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): 0.013369542360305787\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.013579325377941131\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.013710446655750275\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.013307411968708039\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.013164730370044708\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.013217706978321076\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.013681444525718688\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): 0.013618193566799164\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): 0.013393349945545197\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.013213834166526795\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.013308976590633393\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.013209483027458191\n",
      "Training iteration: 3552\n",
      "Improved validation loss from: 0.013064113259315491  to: 0.013026118278503418\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.013243567943572999\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.01351052224636078\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.013230575621128083\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.013153177499771119\n",
      "Training iteration: 3557\n",
      "Improved validation loss from: 0.013026118278503418  to: 0.012926085293293\n",
      "Training iteration: 3558\n",
      "Improved validation loss from: 0.012926085293293  to: 0.012882271409034729\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): 0.01305648684501648\n",
      "Training iteration: 3560\n",
      "Improved validation loss from: 0.012882271409034729  to: 0.012780316174030304\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.012842407822608948\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.012952522933483123\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.012903708219528198\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): 0.012791481614112855\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): 0.012810571491718293\n",
      "Training iteration: 3566\n",
      "Improved validation loss from: 0.012780316174030304  to: 0.012764637172222138\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): 0.012789702415466309\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.01281147003173828\n",
      "Training iteration: 3569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.013046325743198394\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.012886197865009308\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): 0.012844128906726838\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.012851616740226746\n",
      "Training iteration: 3573\n",
      "Improved validation loss from: 0.012764637172222138  to: 0.012604042887687683\n",
      "Training iteration: 3574\n",
      "Improved validation loss from: 0.012604042887687683  to: 0.012594321370124817\n",
      "Training iteration: 3575\n",
      "Validation loss (no improvement): 0.012810395658016204\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): 0.013049837946891785\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.012895132601261138\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.012853720784187317\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.012861117720603943\n",
      "Training iteration: 3580\n",
      "Improved validation loss from: 0.012594321370124817  to: 0.012588027119636535\n",
      "Training iteration: 3581\n",
      "Improved validation loss from: 0.012588027119636535  to: 0.012428645789623261\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.012631285190582275\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.012775304913520812\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.012811201810836791\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.012902936339378357\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.012749707698822022\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.012808321416378022\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.012534067034721375\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.012660190463066101\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.012995703518390656\n",
      "Training iteration: 3591\n",
      "Validation loss (no improvement): 0.012796321511268615\n",
      "Training iteration: 3592\n",
      "Validation loss (no improvement): 0.012687866389751435\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.012745881080627441\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.012831023335456848\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.012527374923229218\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.012512101233005524\n",
      "Training iteration: 3597\n",
      "Validation loss (no improvement): 0.012554004788398743\n",
      "Training iteration: 3598\n",
      "Improved validation loss from: 0.012428645789623261  to: 0.012399034202098846\n",
      "Training iteration: 3599\n",
      "Improved validation loss from: 0.012399034202098846  to: 0.012167439609766007\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.012196465581655502\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.01256464421749115\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.012727849185466766\n",
      "Training iteration: 3603\n",
      "Validation loss (no improvement): 0.012762506306171418\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.012867705523967743\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.012615850567817688\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.012542718648910522\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.01268482357263565\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.012514063715934753\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.01265704184770584\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.012680332362651824\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.012376749515533447\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.012322807312011718\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.012331122159957885\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.012408693879842758\n",
      "Training iteration: 3615\n",
      "Validation loss (no improvement): 0.012604570388793946\n",
      "Training iteration: 3616\n",
      "Validation loss (no improvement): 0.012561583518981933\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.012654560804367065\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.0125042125582695\n",
      "Training iteration: 3619\n",
      "Improved validation loss from: 0.012167439609766007  to: 0.012161916494369507\n",
      "Training iteration: 3620\n",
      "Improved validation loss from: 0.012161916494369507  to: 0.012095501273870468\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.012135414779186249\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.012243635952472687\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.012582695484161377\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.012555937469005584\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.012226929515600204\n",
      "Training iteration: 3626\n",
      "Improved validation loss from: 0.012095501273870468  to: 0.01203903928399086\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.012218210846185684\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.012040112167596817\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.012054302543401719\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.012250152975320816\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.012353134155273438\n",
      "Training iteration: 3632\n",
      "Validation loss (no improvement): 0.01226903647184372\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.01228533536195755\n",
      "Training iteration: 3634\n",
      "Validation loss (no improvement): 0.012406893074512482\n",
      "Training iteration: 3635\n",
      "Validation loss (no improvement): 0.01205519214272499\n",
      "Training iteration: 3636\n",
      "Improved validation loss from: 0.01203903928399086  to: 0.011845703423023223\n",
      "Training iteration: 3637\n",
      "Validation loss (no improvement): 0.012075138092041016\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.012660197913646698\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.012788419425487519\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.012454526126384735\n",
      "Training iteration: 3641\n",
      "Validation loss (no improvement): 0.012262833118438721\n",
      "Training iteration: 3642\n",
      "Validation loss (no improvement): 0.012352164089679717\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.012044326961040496\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.011991071701049804\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.012046130001544952\n",
      "Training iteration: 3646\n",
      "Validation loss (no improvement): 0.012355214357376099\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.012756237387657165\n",
      "Training iteration: 3648\n",
      "Validation loss (no improvement): 0.012574023008346558\n",
      "Training iteration: 3649\n",
      "Validation loss (no improvement): 0.01252526044845581\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.012567657232284545\n",
      "Training iteration: 3651\n",
      "Validation loss (no improvement): 0.012460096180438996\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.012029421329498292\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.01201193779706955\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.012503978610038758\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.012291822582483292\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.012387974560260773\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.012648531794548034\n",
      "Training iteration: 3658\n",
      "Validation loss (no improvement): 0.01270839273929596\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.01267649233341217\n",
      "Training iteration: 3660\n",
      "Validation loss (no improvement): 0.012541143596172333\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.012469053268432617\n",
      "Training iteration: 3662\n",
      "Validation loss (no improvement): 0.012342815101146699\n",
      "Training iteration: 3663\n",
      "Validation loss (no improvement): 0.012319193035364152\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.012509700655937196\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.012339100986719132\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.01225949078798294\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.012475121021270751\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.012479814141988755\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.012176354229450227\n",
      "Training iteration: 3670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.012049944698810577\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.012371467053890228\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.01229817420244217\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.012311270087957382\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.012546977400779724\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.012598657608032226\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.012279192358255387\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.01203794926404953\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.012093826383352279\n",
      "Training iteration: 3679\n",
      "Improved validation loss from: 0.011845703423023223  to: 0.011799411475658416\n",
      "Training iteration: 3680\n",
      "Improved validation loss from: 0.011799411475658416  to: 0.011734452098608017\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.012160758674144744\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.01211686134338379\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.0122461199760437\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.012463729083538055\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.012064634263515473\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.01191989928483963\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.011956314742565154\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.012035828828811646\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.012141348421573639\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.012125551700592041\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.01214456781744957\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.012180812656879425\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.011946377903223037\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.011952967941761016\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.012091884762048722\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.011872382462024688\n",
      "Training iteration: 3697\n",
      "Improved validation loss from: 0.011734452098608017  to: 0.011652912944555283\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.01185852512717247\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.01224379539489746\n",
      "Training iteration: 3700\n",
      "Validation loss (no improvement): 0.012095718085765839\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.01185203790664673\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.011754421889781952\n",
      "Training iteration: 3703\n",
      "Validation loss (no improvement): 0.012050764262676239\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.01187012568116188\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.011772213131189346\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.011867962032556533\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.012084861099720002\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.012020021677017212\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.011888811737298966\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.012021927535533905\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.0120997354388237\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.011858439445495606\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.011859599500894547\n",
      "Training iteration: 3714\n",
      "Validation loss (no improvement): 0.012074725329875946\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.012016894668340683\n",
      "Training iteration: 3716\n",
      "Validation loss (no improvement): 0.011927156150341034\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.011938295513391494\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.01190042495727539\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.011978300660848618\n",
      "Training iteration: 3720\n",
      "Validation loss (no improvement): 0.012030906975269318\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.012098157405853271\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.011763493716716766\n",
      "Training iteration: 3723\n",
      "Improved validation loss from: 0.011652912944555283  to: 0.011594555526971816\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.011876770108938218\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.011772651970386506\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.011704238504171372\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.011695575714111329\n",
      "Training iteration: 3728\n",
      "Improved validation loss from: 0.011594555526971816  to: 0.01142479181289673\n",
      "Training iteration: 3729\n",
      "Improved validation loss from: 0.01142479181289673  to: 0.011368100345134736\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.011409447342157365\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.011369051039218902\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.01141217201948166\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.011404523998498917\n",
      "Training iteration: 3734\n",
      "Improved validation loss from: 0.011368100345134736  to: 0.011324651539325714\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.011436355113983155\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.011535991728305817\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.011462507396936416\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.011634044349193573\n",
      "Training iteration: 3739\n",
      "Validation loss (no improvement): 0.011386766284704208\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.01141820102930069\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.011596423387527467\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.011411907523870468\n",
      "Training iteration: 3743\n",
      "Improved validation loss from: 0.011324651539325714  to: 0.01105501651763916\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.011207232624292374\n",
      "Training iteration: 3745\n",
      "Validation loss (no improvement): 0.011697757244110107\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.011584677547216416\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.011309397220611573\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.011350949108600617\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.011718127876520157\n",
      "Training iteration: 3750\n",
      "Validation loss (no improvement): 0.011609619855880738\n",
      "Training iteration: 3751\n",
      "Validation loss (no improvement): 0.011456327140331268\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.011552281677722931\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.011846955120563506\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.011750026792287826\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.011897945404052734\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.012022240459918976\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.011629040539264678\n",
      "Training iteration: 3758\n",
      "Validation loss (no improvement): 0.01141841858625412\n",
      "Training iteration: 3759\n",
      "Validation loss (no improvement): 0.011548229306936265\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.01151365488767624\n",
      "Training iteration: 3761\n",
      "Validation loss (no improvement): 0.011291327327489853\n",
      "Training iteration: 3762\n",
      "Validation loss (no improvement): 0.011431419849395752\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.011666744947433472\n",
      "Training iteration: 3764\n",
      "Validation loss (no improvement): 0.011730895191431046\n",
      "Training iteration: 3765\n",
      "Validation loss (no improvement): 0.011619285494089127\n",
      "Training iteration: 3766\n",
      "Validation loss (no improvement): 0.011553537845611573\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.011357605457305908\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.011136956512928009\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.011134252697229386\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.01125762015581131\n",
      "Training iteration: 3771\n",
      "Validation loss (no improvement): 0.011412835121154786\n",
      "Training iteration: 3772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.011558550596237182\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.011715759336948395\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.0114124596118927\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.01122594326734543\n",
      "Training iteration: 3776\n",
      "Validation loss (no improvement): 0.011195886135101318\n",
      "Training iteration: 3777\n",
      "Improved validation loss from: 0.01105501651763916  to: 0.010997778177261353\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.011207078397274018\n",
      "Training iteration: 3779\n",
      "Validation loss (no improvement): 0.011605157703161239\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.011437080055475234\n",
      "Training iteration: 3781\n",
      "Validation loss (no improvement): 0.011385247856378556\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.011423473060131074\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.011242900788784028\n",
      "Training iteration: 3784\n",
      "Improved validation loss from: 0.010997778177261353  to: 0.010858859121799468\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.010898850113153457\n",
      "Training iteration: 3786\n",
      "Validation loss (no improvement): 0.011170893907546997\n",
      "Training iteration: 3787\n",
      "Validation loss (no improvement): 0.011340349912643433\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.011611220985651016\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.011446776241064072\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.011203531920909882\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.010937802493572235\n",
      "Training iteration: 3792\n",
      "Improved validation loss from: 0.010858859121799468  to: 0.010812880843877793\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.01090877503156662\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.011313509941101075\n",
      "Training iteration: 3795\n",
      "Validation loss (no improvement): 0.011342383921146393\n",
      "Training iteration: 3796\n",
      "Validation loss (no improvement): 0.011262104660272599\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.010873131453990936\n",
      "Training iteration: 3798\n",
      "Improved validation loss from: 0.010812880843877793  to: 0.010714630782604217\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.010716617107391357\n",
      "Training iteration: 3800\n",
      "Validation loss (no improvement): 0.010757219791412354\n",
      "Training iteration: 3801\n",
      "Validation loss (no improvement): 0.01101001501083374\n",
      "Training iteration: 3802\n",
      "Validation loss (no improvement): 0.011022238433361054\n",
      "Training iteration: 3803\n",
      "Validation loss (no improvement): 0.01108439415693283\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.010927870124578475\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.011012277752161025\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.010841464996337891\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.010820136219263077\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.010875003039836883\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.010821463912725449\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.010882899910211564\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.011092430353164673\n",
      "Training iteration: 3812\n",
      "Validation loss (no improvement): 0.010932954400777817\n",
      "Training iteration: 3813\n",
      "Validation loss (no improvement): 0.010806889832019806\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.01096980944275856\n",
      "Training iteration: 3815\n",
      "Improved validation loss from: 0.010714630782604217  to: 0.010594625771045686\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.010846118628978729\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.011063315719366074\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.011003997176885605\n",
      "Training iteration: 3819\n",
      "Validation loss (no improvement): 0.011034126579761504\n",
      "Training iteration: 3820\n",
      "Validation loss (no improvement): 0.011203666031360627\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.010952335596084595\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.01107574924826622\n",
      "Training iteration: 3823\n",
      "Validation loss (no improvement): 0.011232976615428925\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.011131608486175537\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.011089000850915909\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.011479678004980088\n",
      "Training iteration: 3827\n",
      "Validation loss (no improvement): 0.011209724098443985\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.011146359145641327\n",
      "Training iteration: 3829\n",
      "Validation loss (no improvement): 0.011097799241542815\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.01087266206741333\n",
      "Training iteration: 3831\n",
      "Validation loss (no improvement): 0.010903353989124297\n",
      "Training iteration: 3832\n",
      "Validation loss (no improvement): 0.01112520843744278\n",
      "Training iteration: 3833\n",
      "Validation loss (no improvement): 0.010735861957073212\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.010613691806793214\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.010808879137039184\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.010677530616521835\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.010913683474063874\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.011047329753637314\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.010861103236675263\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.010801732540130615\n",
      "Training iteration: 3841\n",
      "Validation loss (no improvement): 0.010844752937555314\n",
      "Training iteration: 3842\n",
      "Improved validation loss from: 0.010594625771045686  to: 0.010592148452997208\n",
      "Training iteration: 3843\n",
      "Improved validation loss from: 0.010592148452997208  to: 0.010543648153543472\n",
      "Training iteration: 3844\n",
      "Validation loss (no improvement): 0.010970232635736465\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.010735406726598739\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.010748358815908432\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.010889385640621186\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.010559768974781036\n",
      "Training iteration: 3849\n",
      "Improved validation loss from: 0.010543648153543472  to: 0.010455141216516495\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.01052650660276413\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.010600030422210693\n",
      "Training iteration: 3852\n",
      "Validation loss (no improvement): 0.010536811500787734\n",
      "Training iteration: 3853\n",
      "Improved validation loss from: 0.010455141216516495  to: 0.010366322845220566\n",
      "Training iteration: 3854\n",
      "Validation loss (no improvement): 0.010477225482463836\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.010514563322067261\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.010388723760843276\n",
      "Training iteration: 3857\n",
      "Validation loss (no improvement): 0.010485390573740006\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.010585764795541764\n",
      "Training iteration: 3859\n",
      "Improved validation loss from: 0.010366322845220566  to: 0.010316983610391618\n",
      "Training iteration: 3860\n",
      "Validation loss (no improvement): 0.010453034937381745\n",
      "Training iteration: 3861\n",
      "Validation loss (no improvement): 0.010951034724712372\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.010630714893341064\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.010408437252044678\n",
      "Training iteration: 3864\n",
      "Improved validation loss from: 0.010316983610391618  to: 0.010302655398845673\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.010572650283575059\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.010337038338184357\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.010532458871603012\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.010689391940832137\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.010885729640722274\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.01079169288277626\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.010663177818059921\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.010800357908010483\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.01066741943359375\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.01041620746254921\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.010318448394536972\n",
      "Training iteration: 3876\n",
      "Improved validation loss from: 0.010302655398845673  to: 0.010188882052898408\n",
      "Training iteration: 3877\n",
      "Validation loss (no improvement): 0.010440953820943833\n",
      "Training iteration: 3878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.010761447250843048\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.01102469116449356\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.010790382325649262\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.010788612067699432\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.010679223388433457\n",
      "Training iteration: 3883\n",
      "Validation loss (no improvement): 0.010566753149032593\n",
      "Training iteration: 3884\n",
      "Validation loss (no improvement): 0.010384551435708999\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.010389463603496551\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.010540678352117538\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.010889096558094025\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.01100517064332962\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.010898493230342865\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.010778174549341202\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.010196216404438019\n",
      "Training iteration: 3892\n",
      "Improved validation loss from: 0.010188882052898408  to: 0.009883645921945572\n",
      "Training iteration: 3893\n",
      "Validation loss (no improvement): 0.009981387853622436\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.010231099277734756\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.010338220745325089\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.010894779860973359\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.010986921936273574\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.010891126096248626\n",
      "Training iteration: 3899\n",
      "Validation loss (no improvement): 0.011159209907054901\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.010804390907287598\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.010552410036325455\n",
      "Training iteration: 3902\n",
      "Validation loss (no improvement): 0.010594288259744645\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.010263018310070038\n",
      "Training iteration: 3904\n",
      "Validation loss (no improvement): 0.010375132411718368\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.010558594763278962\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.01057998314499855\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.010910701751708985\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.011091991513967513\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.011146561801433563\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.010712575912475587\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.01058787852525711\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.010976797342300415\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.01059909611940384\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.010598798096179963\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.010794379562139512\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.010622155666351319\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.011009180545806884\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.011287667602300645\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.010733135044574738\n",
      "Training iteration: 3920\n",
      "Validation loss (no improvement): 0.010823836177587509\n",
      "Training iteration: 3921\n",
      "Validation loss (no improvement): 0.01085844486951828\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.010680127143859863\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.010827799141407014\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.0106825090944767\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.010448677837848664\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.010631201416254043\n",
      "Training iteration: 3927\n",
      "Validation loss (no improvement): 0.010528037697076798\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.010718182474374772\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.010941805690526963\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.010485951602458955\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.010482631623744965\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.010712225735187531\n",
      "Training iteration: 3933\n",
      "Validation loss (no improvement): 0.010355402529239655\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.01018138900399208\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.010312505066394806\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.010195384919643401\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.010443978011608124\n",
      "Training iteration: 3938\n",
      "Validation loss (no improvement): 0.010654468834400178\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.010467946529388428\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.010230760276317596\n",
      "Training iteration: 3941\n",
      "Validation loss (no improvement): 0.010092724859714509\n",
      "Training iteration: 3942\n",
      "Validation loss (no improvement): 0.01004236787557602\n",
      "Training iteration: 3943\n",
      "Validation loss (no improvement): 0.009937340766191483\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.009958268702030182\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.009936226904392243\n",
      "Training iteration: 3946\n",
      "Improved validation loss from: 0.009883645921945572  to: 0.009729914367198944\n",
      "Training iteration: 3947\n",
      "Improved validation loss from: 0.009729914367198944  to: 0.00931084007024765\n",
      "Training iteration: 3948\n",
      "Validation loss (no improvement): 0.00947093740105629\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.009878648817539215\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.010081037133932113\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.0103285051882267\n",
      "Training iteration: 3952\n",
      "Validation loss (no improvement): 0.01052442416548729\n",
      "Training iteration: 3953\n",
      "Validation loss (no improvement): 0.010052001476287842\n",
      "Training iteration: 3954\n",
      "Validation loss (no improvement): 0.009643206745386124\n",
      "Training iteration: 3955\n",
      "Validation loss (no improvement): 0.009667095541954041\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.00932118147611618\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.00954642817378044\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.009809665381908417\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.010351469367742538\n",
      "Training iteration: 3960\n",
      "Validation loss (no improvement): 0.010080938041210175\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.009923143684864045\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.009924370795488358\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.009594117105007172\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.009451693296432495\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.009360630810260773\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.009331414848566056\n",
      "Training iteration: 3967\n",
      "Validation loss (no improvement): 0.009680649638175965\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.010417555272579194\n",
      "Training iteration: 3969\n",
      "Validation loss (no improvement): 0.010630502551794051\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.010315488278865814\n",
      "Training iteration: 3971\n",
      "Validation loss (no improvement): 0.010319557040929794\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.010461942851543426\n",
      "Training iteration: 3973\n",
      "Validation loss (no improvement): 0.010012376308441161\n",
      "Training iteration: 3974\n",
      "Validation loss (no improvement): 0.00979388877749443\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.009871281683444977\n",
      "Training iteration: 3976\n",
      "Validation loss (no improvement): 0.009695709496736527\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.009790132194757462\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.010192753374576568\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.010053811222314834\n",
      "Training iteration: 3980\n",
      "Validation loss (no improvement): 0.009853268414735794\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.010029951483011246\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.009906820952892303\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.01019798293709755\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.010598579794168473\n",
      "Training iteration: 3985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.010164929926395417\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.009899195283651352\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.010041098296642303\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.009807553142309189\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.01015922799706459\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.010341602563858032\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.009844084084033967\n",
      "Training iteration: 3992\n",
      "Validation loss (no improvement): 0.009769307076931\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.009808732569217682\n",
      "Training iteration: 3994\n",
      "Validation loss (no improvement): 0.010113110393285751\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.010433753579854965\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.009972763806581497\n",
      "Training iteration: 3997\n",
      "Validation loss (no improvement): 0.00973069965839386\n",
      "Training iteration: 3998\n",
      "Validation loss (no improvement): 0.009848269075155259\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.009671159088611603\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_4 = toy_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_4.train_model()\n",
    "ensemble_model_4.model_inference()\n",
    "\n",
    "ensemble_mean_4 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_4 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_4 = np.sqrt(np.exp(ensemble_logvar_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 10.002531433105469\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 10.002531433105469  to: 7.288521575927734\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 7.288521575927734  to: 5.35277214050293\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 5.35277214050293  to: 3.975823974609375\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 3.975823974609375  to: 2.9960927963256836\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 2.9960927963256836  to: 2.2863468170166015\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 2.2863468170166015  to: 1.76827392578125\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 1.76827392578125  to: 1.3877294540405274\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.3877294540405274  to: 1.105201530456543\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.105201530456543  to: 0.893736743927002\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 0.893736743927002  to: 0.7340273380279541\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.7340273380279541  to: 0.6124832153320312\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.6124832153320312  to: 0.5192154884338379\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.5192154884338379  to: 0.447081470489502\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.447081470489502  to: 0.39091904163360597\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.39091904163360597  to: 0.3468113660812378\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.3468113660812378  to: 0.31193997859954836\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.31193997859954836  to: 0.28401751518249513\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.28401751518249513  to: 0.2614830255508423\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.2614830255508423  to: 0.24317288398742676\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.24317288398742676  to: 0.22821547985076904\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.22821547985076904  to: 0.2159367561340332\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.2159367561340332  to: 0.20579383373260499\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.20579383373260499  to: 0.19737304449081422\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.19737304449081422  to: 0.19034525156021118\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.19034525156021118  to: 0.18444942235946654\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.18444942235946654  to: 0.17948360443115235\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.17948360443115235  to: 0.17528291940689086\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.17528291940689086  to: 0.17171380519866944\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.17171380519866944  to: 0.16866976022720337\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.16866976022720337  to: 0.16606252193450927\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.16606252193450927  to: 0.16382074356079102\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.16382074356079102  to: 0.16188619136810303\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.16188619136810303  to: 0.16021101474761962\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.16021101474761962  to: 0.15875483751296998\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.15875483751296998  to: 0.15748437643051147\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.15748437643051147  to: 0.1563718557357788\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.1563718557357788  to: 0.15539408922195436\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.15539408922195436  to: 0.15453157424926758\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.15453157424926758  to: 0.1537676453590393\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.1537676453590393  to: 0.15308868885040283\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.15308868885040283  to: 0.1524832844734192\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.1524832844734192  to: 0.1519413709640503\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1519413709640503  to: 0.15145425796508788\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.15145425796508788  to: 0.15101487636566163\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.15101487636566163  to: 0.15061724185943604\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.15061724185943604  to: 0.15025604963302613\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.15025604963302613  to: 0.14992674589157104\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.14992674589157104  to: 0.14962544441223144\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.14962544441223144  to: 0.14934866428375243\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.14934866428375243  to: 0.14909353256225585\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.14909353256225585  to: 0.1488574981689453\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.1488574981689453  to: 0.14863860607147217\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.14863860607147217  to: 0.14843491315841675\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.14843491315841675  to: 0.14824482202529907\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.14824482202529907  to: 0.14806692600250243\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.14806692600250243  to: 0.14790000915527343\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.14790000915527343  to: 0.1477428674697876\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.1477428674697876  to: 0.14759451150894165\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.14759451150894165  to: 0.14745407104492186\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.14745407104492186  to: 0.14732038974761963\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.14732038974761963  to: 0.14719293117523194\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.14719293117523194  to: 0.14707140922546386\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.14707140922546386  to: 0.14695526361465455\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.14695526361465455  to: 0.14684393405914306\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.14684393405914306  to: 0.1467369794845581\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.1467369794845581  to: 0.14663417339324952\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.14663417339324952  to: 0.14653512239456176\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.14653512239456176  to: 0.14643945693969726\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.14643945693969726  to: 0.14634691476821898\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.14634691476821898  to: 0.14625734090805054\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14625734090805054  to: 0.1461704969406128\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.1461704969406128  to: 0.14608612060546874\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.14608612060546874  to: 0.14600398540496826\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.14600398540496826  to: 0.14592394828796387\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.14592394828796387  to: 0.14584579467773437\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.14584579467773437  to: 0.14576938152313232\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.14576938152313232  to: 0.1456945776939392\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.1456945776939392  to: 0.14562124013900757\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.14562124013900757  to: 0.1455492615699768\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.1455492615699768  to: 0.14547837972640992\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14547837972640992  to: 0.14540876150131227\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.14540876150131227  to: 0.14534028768539428\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.14534028768539428  to: 0.14527292251586915\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.14527292251586915  to: 0.14520652294158937\n",
      "Training iteration: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14520652294158937  to: 0.14514108896255493\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.14514108896255493  to: 0.1450765013694763\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1450765013694763  to: 0.14501274824142457\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14501274824142457  to: 0.14494974613189698\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.14494974613189698  to: 0.14488747119903564\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.14488747119903564  to: 0.14482585191726685\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.14482585191726685  to: 0.14476487636566163\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.14476487636566163  to: 0.14470455646514893\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14470455646514893  to: 0.14464484453201293\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.14464484453201293  to: 0.14458569288253784\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14458569288253784  to: 0.1445271134376526\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.1445271134376526  to: 0.14446903467178346\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14446903467178346  to: 0.1444114327430725\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.1444114327430725  to: 0.14435428380966187\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14435428380966187  to: 0.14429757595062256\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14429757595062256  to: 0.14424129724502563\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14424129724502563  to: 0.1441854476928711\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.1441854476928711  to: 0.14413001537322997\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14413001537322997  to: 0.14407496452331542\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.14407496452331542  to: 0.14402027130126954\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14402027130126954  to: 0.14396591186523439\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14396591186523439  to: 0.14391186237335205\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14391186237335205  to: 0.1438581943511963\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1438581943511963  to: 0.14380484819412231\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14380484819412231  to: 0.14375182390213012\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14375182390213012  to: 0.1436990976333618\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.1436990976333618  to: 0.14364664554595946\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14364664554595946  to: 0.1435944676399231\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.1435944676399231  to: 0.14354251623153685\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14354251623153685  to: 0.14349082708358765\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14349082708358765  to: 0.14343935251235962\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14343935251235962  to: 0.14338806867599488\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14338806867599488  to: 0.14333701133728027\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14333701133728027  to: 0.1432861089706421\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.1432861089706421  to: 0.14323532581329346\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14323532581329346  to: 0.14318472146987915\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14318472146987915  to: 0.14313427209854127\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14313427209854127  to: 0.14308397769927977\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14308397769927977  to: 0.14303381443023683\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14303381443023683  to: 0.1429837942123413\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.1429837942123413  to: 0.14293384552001953\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14293384552001953  to: 0.14288330078125\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14288330078125  to: 0.14283287525177002\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14283287525177002  to: 0.1427825689315796\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.1427825689315796  to: 0.14273239374160768\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14273239374160768  to: 0.14268232583999635\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14268232583999635  to: 0.14263235330581664\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14263235330581664  to: 0.1425824761390686\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.1425824761390686  to: 0.14253270626068115\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14253270626068115  to: 0.14248301982879638\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14248301982879638  to: 0.14243342876434326\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14243342876434326  to: 0.1423839211463928\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.1423839211463928  to: 0.1423344850540161\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1423344850540161  to: 0.14228513240814208\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14228513240814208  to: 0.1422358512878418\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.1422358512878418  to: 0.14218664169311523\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14218664169311523  to: 0.1421375036239624\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1421375036239624  to: 0.14208842515945436\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14208842515945436  to: 0.14203939437866211\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14203939437866211  to: 0.14199043512344361\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14199043512344361  to: 0.14194152355194092\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14194152355194092  to: 0.14189268350601197\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14189268350601197  to: 0.14184387922286987\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14184387922286987  to: 0.1417951226234436\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1417951226234436  to: 0.1417463541030884\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.1417463541030884  to: 0.14169714450836182\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14169714450836182  to: 0.14164787530899048\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14164787530899048  to: 0.14159858226776123\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14159858226776123  to: 0.14154927730560302\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14154927730560302  to: 0.14149997234344483\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14149997234344483  to: 0.14145067930221558\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14145067930221558  to: 0.1414014220237732\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.1414014220237732  to: 0.14135217666625977\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14135217666625977  to: 0.14130290746688842\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14130290746688842  to: 0.14125367403030395\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14125367403030395  to: 0.14120442867279054\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14120442867279054  to: 0.1411551833152771\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.1411551833152771  to: 0.1411059617996216\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.1411059617996216  to: 0.14105675220489503\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14105675220489503  to: 0.14100754261016846\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14100754261016846  to: 0.1409583330154419\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.1409583330154419  to: 0.1409091591835022\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.1409091591835022  to: 0.1408599615097046\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1408599615097046  to: 0.1408108115196228\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1408108115196228  to: 0.14076164960861207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14076164960861207  to: 0.14071247577667237\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14071247577667237  to: 0.14066311120986938\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14066311120986938  to: 0.14061377048492432\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14061377048492432  to: 0.1405644178390503\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.1405644178390503  to: 0.14051510095596315\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14051510095596315  to: 0.1404658079147339\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.1404658079147339  to: 0.14041659832000733\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14041659832000733  to: 0.14036751985549928\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.14036751985549928  to: 0.14031846523284913\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14031846523284913  to: 0.14026944637298583\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14026944637298583  to: 0.14022042751312255\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14022042751312255  to: 0.1401714563369751\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.1401714563369751  to: 0.14012250900268555\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14012250900268555  to: 0.140073561668396\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.140073561668396  to: 0.14002467393875123\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14002467393875123  to: 0.13997578620910645\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.13997578620910645  to: 0.13992700576782227\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.13992700576782227  to: 0.1398782730102539\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.1398782730102539  to: 0.13982954025268554\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13982954025268554  to: 0.1397808313369751\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.1397808313369751  to: 0.13973214626312255\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13973214626312255  to: 0.13968350887298583\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13968350887298583  to: 0.13963491916656495\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13963491916656495  to: 0.13958632946014404\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13958632946014404  to: 0.13953776359558107\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13953776359558107  to: 0.13948923349380493\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13948923349380493  to: 0.13944072723388673\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13944072723388673  to: 0.13939225673675537\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.13939225673675537  to: 0.13934382200241088\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.13934382200241088  to: 0.13929541110992433\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13929541110992433  to: 0.13924703598022461\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13924703598022461  to: 0.13919867277145387\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13919867277145387  to: 0.13915035724639893\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13915035724639893  to: 0.1391020894050598\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.1391020894050598  to: 0.1390538454055786\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.1390538454055786  to: 0.13900563716888428\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13900563716888428  to: 0.1389574885368347\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.1389574885368347  to: 0.13890936374664306\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13890936374664306  to: 0.13886125087738038\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13886125087738038  to: 0.13881318569183348\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.13881318569183348  to: 0.1387651801109314\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.1387651801109314  to: 0.13871721029281617\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.13871721029281617  to: 0.1386693000793457\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.1386693000793457  to: 0.13862142562866211\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13862142562866211  to: 0.13857353925704957\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.13857353925704957  to: 0.13852571249008178\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13852571249008178  to: 0.13847789764404297\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13847789764404297  to: 0.13843010663986205\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.13843010663986205  to: 0.13838236331939696\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.13838236331939696  to: 0.13833461999893187\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.13833461999893187  to: 0.13828694820404053\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13828694820404053  to: 0.13823927640914918\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13823927640914918  to: 0.1381915330886841\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.1381915330886841  to: 0.1381437063217163\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1381437063217163  to: 0.13809583187103272\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13809583187103272  to: 0.1380478858947754\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.1380478858947754  to: 0.13799988031387328\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13799988031387328  to: 0.1379518151283264\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.1379518151283264  to: 0.13790361881256102\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13790361881256102  to: 0.1378553628921509\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.1378553628921509  to: 0.13780711889266967\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13780711889266967  to: 0.13775885105133057\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.13775885105133057  to: 0.1377105712890625\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.1377105712890625  to: 0.13766229152679443\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13766229152679443  to: 0.13761401176452637\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13761401176452637  to: 0.1375657558441162\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.1375657558441162  to: 0.13751754760742188\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13751754760742188  to: 0.1374693751335144\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1374693751335144  to: 0.13742125034332275\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13742125034332275  to: 0.137373149394989\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.137373149394989  to: 0.13732510805130005\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13732510805130005  to: 0.13727710247039795\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.13727710247039795  to: 0.1372291326522827\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.1372291326522827  to: 0.13718123435974122\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13718123435974122  to: 0.13713337182998658\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13713337182998658  to: 0.13708555698394775\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.13708555698394775  to: 0.13703778982162476\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13703778982162476  to: 0.13699007034301758\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.13699007034301758  to: 0.13694227933883668\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13694227933883668  to: 0.13689464330673218\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13689464330673218  to: 0.13684707880020142\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13684707880020142  to: 0.1367995858192444\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.1367995858192444  to: 0.13675212860107422\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13675212860107422  to: 0.13670474290847778\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13670474290847778  to: 0.13665741682052612\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13665741682052612  to: 0.13661013841629027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13661013841629027  to: 0.13656293153762816\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.13656293153762816  to: 0.13651578426361083\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13651578426361083  to: 0.13646869659423827\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13646869659423827  to: 0.1364216685295105\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.1364216685295105  to: 0.13637470006942748\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13637470006942748  to: 0.1363278031349182\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1363278031349182  to: 0.13628096580505372\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13628096580505372  to: 0.13623418807983398\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13623418807983398  to: 0.13618749380111694\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13618749380111694  to: 0.13614085912704468\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13614085912704468  to: 0.13609429597854614\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13609429597854614  to: 0.13604780435562133\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.13604780435562133  to: 0.1360013723373413\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.1360013723373413  to: 0.13595502376556395\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.13595502376556395  to: 0.13590874671936035\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13590874671936035  to: 0.13586254119873048\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13586254119873048  to: 0.13581640720367433\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13581640720367433  to: 0.1357703447341919\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.1357703447341919  to: 0.13572434186935425\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13572434186935425  to: 0.13567843437194824\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13567843437194824  to: 0.13563258647918702\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13563258647918702  to: 0.13558682203292846\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13558682203292846  to: 0.13554112911224364\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.13554112911224364  to: 0.13549550771713256\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13549550771713256  to: 0.13544998168945313\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13544998168945313  to: 0.1354045271873474\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.1354045271873474  to: 0.13535913228988647\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13535913228988647  to: 0.13531383275985717\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13531383275985717  to: 0.13526861667633056\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13526861667633056  to: 0.13522348403930665\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13522348403930665  to: 0.13517839908599855\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13517839908599855  to: 0.13513342142105103\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13513342142105103  to: 0.13508851528167726\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13508851528167726  to: 0.1350436806678772\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.1350436806678772  to: 0.13499895334243775\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13499895334243775  to: 0.13495428562164308\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13495428562164308  to: 0.13490970134735109\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13490970134735109  to: 0.1348652124404907\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.1348652124404907  to: 0.13482080698013305\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13482080698013305  to: 0.1347764849662781\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.1347764849662781  to: 0.13473231792449952\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13473231792449952  to: 0.13468825817108154\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13468825817108154  to: 0.13464435338973998\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13464435338973998  to: 0.13460056781768798\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13460056781768798  to: 0.13455690145492555\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13455690145492555  to: 0.13451335430145264\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13451335430145264  to: 0.13446972370147706\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13446972370147706  to: 0.13442595005035402\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13442595005035402  to: 0.13438212871551514\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13438212871551514  to: 0.13433821201324464\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13433821201324464  to: 0.13429425954818724\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13429425954818724  to: 0.134250271320343\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.134250271320343  to: 0.13420627117156983\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13420627117156983  to: 0.13416224718093872\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13416224718093872  to: 0.13411823511123658\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13411823511123658  to: 0.13407421112060547\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13407421112060547  to: 0.13403019905090333\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13403019905090333  to: 0.13398624658584596\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13398624658584596  to: 0.13394230604171753\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13394230604171753  to: 0.13389843702316284\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13389843702316284  to: 0.13385461568832396\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13385461568832396  to: 0.13381086587905883\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13381086587905883  to: 0.13376725912094117\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13376725912094117  to: 0.13372377157211304\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13372377157211304  to: 0.13368051052093505\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13368051052093505  to: 0.13363741636276244\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13363741636276244  to: 0.13359453678131103\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13359453678131103  to: 0.13355182409286498\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13355182409286498  to: 0.1335093140602112\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.1335093140602112  to: 0.1334669828414917\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.1334669828414917  to: 0.1334248185157776\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.1334248185157776  to: 0.1333828568458557\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.1333828568458557  to: 0.1333410620689392\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1333410620689392  to: 0.13329944610595704\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13329944610595704  to: 0.13325799703598024\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13325799703598024  to: 0.1332167387008667\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.1332167387008667  to: 0.13317563533782958\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13317563533782958  to: 0.13313472270965576\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13313472270965576  to: 0.13309396505355836\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13309396505355836  to: 0.13305338621139526\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13305338621139526  to: 0.13301297426223754\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13301297426223754  to: 0.13297274112701415\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13297274112701415  to: 0.1329328179359436\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.1329328179359436  to: 0.13289320468902588\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13289320468902588  to: 0.1328538775444031\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.1328538775444031  to: 0.13281482458114624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13281482458114624  to: 0.13277604579925537\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13277604579925537  to: 0.13273752927780152\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13273752927780152  to: 0.13269926309585572\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13269926309585572  to: 0.13266122341156006\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13266122341156006  to: 0.13262341022491456\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13262341022491456  to: 0.13258581161499022\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13258581161499022  to: 0.13254845142364502\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13254845142364502  to: 0.13251129388809205\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13251129388809205  to: 0.13247437477111818\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13247437477111818  to: 0.13243764638900757\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.13243764638900757  to: 0.13240112066268922\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13240112066268922  to: 0.13236478567123414\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13236478567123414  to: 0.13232862949371338\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13232862949371338  to: 0.1322929620742798\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1322929620742798  to: 0.1322577714920044\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.1322577714920044  to: 0.13222299814224242\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13222299814224242  to: 0.1321886420249939\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.1321886420249939  to: 0.132154643535614\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.132154643535614  to: 0.1321210265159607\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1321210265159607  to: 0.13208773136138915\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13208773136138915  to: 0.13205475807189943\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13205475807189943  to: 0.13202207088470458\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13202207088470458  to: 0.1319896697998047\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.1319896697998047  to: 0.1319575309753418\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.1319575309753418  to: 0.13192565441131593\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13192565441131593  to: 0.13189396858215333\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13189396858215333  to: 0.1318623423576355\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.1318623423576355  to: 0.13183085918426513\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13183085918426513  to: 0.1317995548248291\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1317995548248291  to: 0.1317683696746826\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.1317683696746826  to: 0.1317373514175415\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.1317373514175415  to: 0.1317066431045532\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.1317066431045532  to: 0.13167622089385986\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.13167622089385986  to: 0.13164608478546141\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13164608478546141  to: 0.1316161870956421\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1316161870956421  to: 0.13158655166625977\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13158655166625977  to: 0.13155696392059327\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13155696392059327  to: 0.13152744770050048\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13152744770050048  to: 0.1314980149269104\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.1314980149269104  to: 0.13146880865097046\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13146880865097046  to: 0.13143984079360962\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13143984079360962  to: 0.13141109943389892\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13141109943389892  to: 0.1313825726509094\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.1313825726509094  to: 0.13135405778884887\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13135405778884887  to: 0.13132559061050414\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13132559061050414  to: 0.13129732608795167\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13129732608795167  to: 0.1312692642211914\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1312692642211914  to: 0.13124139308929444\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.13124139308929444  to: 0.1312137246131897\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.1312137246131897  to: 0.131186044216156\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.131186044216156  to: 0.13115837574005126\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.13115837574005126  to: 0.13113090991973878\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13113090991973878  to: 0.13110363483428955\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13110363483428955  to: 0.13107652664184571\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13107652664184571  to: 0.13104959726333618\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13104959726333618  to: 0.13102264404296876\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.13102264404296876  to: 0.1309958815574646\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1309958815574646  to: 0.13096911907196046\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.13096911907196046  to: 0.13094252347946167\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13094252347946167  to: 0.13091609477996827\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13091609477996827  to: 0.13088985681533813\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.13088985681533813  to: 0.1308637499809265\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.1308637499809265  to: 0.1308378219604492\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.1308378219604492  to: 0.1308118462562561\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.1308118462562561  to: 0.13078584671020507\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13078584671020507  to: 0.1307600259780884\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.1307600259780884  to: 0.130734384059906\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.130734384059906  to: 0.130708909034729\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.130708909034729  to: 0.13068355321884156\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13068355321884156  to: 0.13065838813781738\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.13065838813781738  to: 0.13063335418701172\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13063335418701172  to: 0.13060827255249025\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.13060827255249025  to: 0.13058316707611084\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.13058316707611084  to: 0.13055820465087892\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13055820465087892  to: 0.1305334210395813\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.1305334210395813  to: 0.13050879240036012\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.13050879240036012  to: 0.13048431873321534\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.13048431873321534  to: 0.13045997619628907\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13045997619628907  to: 0.1304357886314392\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.1304357886314392  to: 0.1304117202758789\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1304117202758789  to: 0.1303877830505371\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.1303877830505371  to: 0.13036378622055053\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.13036378622055053  to: 0.13033974170684814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13033974170684814  to: 0.13031585216522218\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13031585216522218  to: 0.13029210567474364\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13029210567474364  to: 0.1302685022354126\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1302685022354126  to: 0.13024504184722902\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13024504184722902  to: 0.13022173643112184\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.13022173643112184  to: 0.13019853830337524\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.13019853830337524  to: 0.1301753878593445\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.1301753878593445  to: 0.13015233278274535\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.13015233278274535  to: 0.1301293969154358\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.1301293969154358  to: 0.13010656833648682\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13010656833648682  to: 0.13008368015289307\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13008368015289307  to: 0.13006069660186767\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13006069660186767  to: 0.13003787994384766\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13003787994384766  to: 0.13001518249511718\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13001518249511718  to: 0.12999264001846314\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.12999264001846314  to: 0.1299702286720276\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1299702286720276  to: 0.12994792461395263\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12994792461395263  to: 0.12992575168609619\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.12992575168609619  to: 0.1299036741256714\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.1299036741256714  to: 0.12988173961639404\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.12988173961639404  to: 0.12985987663269044\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12985987663269044  to: 0.12983810901641846\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.12983810901641846  to: 0.12981646060943602\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.12981646060943602  to: 0.1297948956489563\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.1297948956489563  to: 0.1297734260559082\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.1297734260559082  to: 0.12975183725357056\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.12975183725357056  to: 0.12973015308380126\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.12973015308380126  to: 0.12970857620239257\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.12970857620239257  to: 0.12968711853027343\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.12968711853027343  to: 0.12966578006744384\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.12966578006744384  to: 0.12964454889297486\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.12964454889297486  to: 0.12962342500686647\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.12962342500686647  to: 0.12960238456726075\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.12960238456726075  to: 0.12958142757415772\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.12958142757415772  to: 0.12956050634384156\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.12956050634384156  to: 0.12953966856002808\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12953966856002808  to: 0.12951881885528566\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.12951881885528566  to: 0.12949802875518798\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.12949802875518798  to: 0.12947732210159302\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.12947732210159302  to: 0.12945668697357177\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12945668697357177  to: 0.12943613529205322\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12943613529205322  to: 0.1294156312942505\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.1294156312942505  to: 0.1293951988220215\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.1293951988220215  to: 0.1293748140335083\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.1293748140335083  to: 0.12935433387756348\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.12935433387756348  to: 0.12933390140533446\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.12933390140533446  to: 0.1293135404586792\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.1293135404586792  to: 0.1292932152748108\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.1292932152748108  to: 0.1292729377746582\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1292729377746582  to: 0.12925249338150024\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.12925249338150024  to: 0.1292320966720581\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.1292320966720581  to: 0.1292117714881897\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.1292117714881897  to: 0.12919150590896605\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.12919150590896605  to: 0.1291712760925293\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.1291712760925293  to: 0.1291511058807373\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.1291511058807373  to: 0.1291309952735901\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.1291309952735901  to: 0.1291109085083008\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.1291109085083008  to: 0.1290908694267273\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.1290908694267273  to: 0.12907087802886963\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12907087802886963  to: 0.1290508985519409\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.1290508985519409  to: 0.12903096675872802\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12903096675872802  to: 0.129011070728302\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.129011070728302  to: 0.12899118661880493\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.12899118661880493  to: 0.12897132635116576\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.12897132635116576  to: 0.12895147800445556\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.12895147800445556  to: 0.12893216609954833\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12893216609954833  to: 0.12891334295272827\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12891334295272827  to: 0.12889455556869506\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.12889455556869506  to: 0.12887580394744874\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12887580394744874  to: 0.12885706424713134\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.12885706424713134  to: 0.12883834838867186\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.12883834838867186  to: 0.12881966829299926\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.12881966829299926  to: 0.12880100011825563\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.12880100011825563  to: 0.12878236770629883\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.12878236770629883  to: 0.128763747215271\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.128763747215271  to: 0.12874512672424315\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.12874512672424315  to: 0.1287265181541443\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.1287265181541443  to: 0.12870795726776124\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.12870795726776124  to: 0.12868943214416503\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.12868943214416503  to: 0.12867093086242676\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12867093086242676  to: 0.12865241765975952\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12865241765975952  to: 0.12863391637802124\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12863391637802124  to: 0.128615403175354\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.128615403175354  to: 0.1285969138145447\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.1285969138145447  to: 0.12857841253280639\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12857841253280639  to: 0.1285599112510681\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.1285599112510681  to: 0.12854139804840087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12854139804840087  to: 0.1285228729248047\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.1285228729248047  to: 0.1285043716430664\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1285043716430664  to: 0.12848583459854127\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12848583459854127  to: 0.12846717834472657\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.12846717834472657  to: 0.1284485101699829\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.1284485101699829  to: 0.1284298300743103\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.1284298300743103  to: 0.1284111261367798\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.1284111261367798  to: 0.12839239835739136\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.12839239835739136  to: 0.12837368249893188\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.12837368249893188  to: 0.1283549189567566\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.1283549189567566  to: 0.1283361792564392\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.1283361792564392  to: 0.12831741571426392\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.12831741571426392  to: 0.1282986044883728\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1282986044883728  to: 0.1282796859741211\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.1282796859741211  to: 0.12826066017150878\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12826066017150878  to: 0.12824137210845948\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.12824137210845948  to: 0.1282220244407654\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.1282220244407654  to: 0.12820265293121338\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12820265293121338  to: 0.1281832456588745\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.1281832456588745  to: 0.12816379070281983\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12816379070281983  to: 0.12814431190490722\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.12814431190490722  to: 0.12812478542327882\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.12812478542327882  to: 0.12810522317886353\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.12810522317886353  to: 0.12808644771575928\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.12808644771575928  to: 0.12806832790374756\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.12806832790374756  to: 0.128050696849823\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.128050696849823  to: 0.12803364992141725\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.12803364992141725  to: 0.12801706790924072\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.12801706790924072  to: 0.12800092697143556\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.12800092697143556  to: 0.12798513174057008\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.12798513174057008  to: 0.12796964645385742\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.12796964645385742  to: 0.1279544234275818\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.1279544234275818  to: 0.12793941497802735\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.12793941497802735  to: 0.12792456150054932\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.12792456150054932  to: 0.1279098153114319\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.1279098153114319  to: 0.12789514064788818\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.12789514064788818  to: 0.1278805136680603\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.1278805136680603  to: 0.12786500453948973\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.12786500453948973  to: 0.12784870862960815\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.12784870862960815  to: 0.12783164978027345\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12783164978027345  to: 0.12781386375427245\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12781386375427245  to: 0.12779541015625\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12779541015625  to: 0.12777633666992189\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.12777633666992189  to: 0.12775667905807495\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12775667905807495  to: 0.12773683071136474\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.12773683071136474  to: 0.12771685123443605\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.12771685123443605  to: 0.12769670486450196\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12769670486450196  to: 0.127677321434021\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.127677321434021  to: 0.12765862941741943\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12765862941741943  to: 0.12764055728912355\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12764055728912355  to: 0.12762292623519897\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12762292623519897  to: 0.12760570049285888\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12760570049285888  to: 0.12758893966674806\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12758893966674806  to: 0.12757256031036376\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.12757256031036376  to: 0.12755645513534547\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.12755645513534547  to: 0.12754062414169312\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12754062414169312  to: 0.12752405405044556\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.12752405405044556  to: 0.1275067925453186\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.1275067925453186  to: 0.12748886346817018\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.12748886346817018  to: 0.12747035026550294\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.12747035026550294  to: 0.1274522662162781\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.1274522662162781  to: 0.12743451595306396\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12743451595306396  to: 0.12741706371307374\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.12741706371307374  to: 0.12739989757537842\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12739989757537842  to: 0.12738292217254638\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12738292217254638  to: 0.12736518383026124\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.12736518383026124  to: 0.1273467183113098\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1273467183113098  to: 0.12732826471328734\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.12732826471328734  to: 0.12730979919433594\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12730979919433594  to: 0.1272913098335266\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.1272913098335266  to: 0.12727277278900145\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12727277278900145  to: 0.12725369930267333\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.12725369930267333  to: 0.1272340774536133\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1272340774536133  to: 0.12721498012542726\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12721498012542726  to: 0.1271963119506836\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.1271963119506836  to: 0.12717806100845336\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.12717806100845336  to: 0.12716020345687867\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12716020345687867  to: 0.1271427869796753\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.1271427869796753  to: 0.12712564468383789\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.12712564468383789  to: 0.12710874080657958\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.12710874080657958  to: 0.12709105014801025\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12709105014801025  to: 0.1270726203918457\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.1270726203918457  to: 0.12705450057983397\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12705450057983397  to: 0.1270366907119751\n",
      "Training iteration: 596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1270366907119751  to: 0.12701911926269532\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12701911926269532  to: 0.12700176239013672\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12700176239013672  to: 0.1269845724105835\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.1269845724105835  to: 0.12696750164031984\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.12696750164031984  to: 0.1269505500793457\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.1269505500793457  to: 0.12693264484405517\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12693264484405517  to: 0.12691380977630615\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12691380977630615  to: 0.12689526081085206\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.12689526081085206  to: 0.1268768548965454\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.1268768548965454  to: 0.1268586277961731\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.1268586277961731  to: 0.12684051990509032\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.12684051990509032  to: 0.12682251930236815\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12682251930236815  to: 0.12680461406707763\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12680461406707763  to: 0.1267867922782898\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.1267867922782898  to: 0.12676900625228882\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12676900625228882  to: 0.12675126791000366\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12675126791000366  to: 0.12673354148864746\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12673354148864746  to: 0.1267158269882202\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.1267158269882202  to: 0.12669696807861328\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12669696807861328  to: 0.12667710781097413\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12667710781097413  to: 0.12665740251541138\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12665740251541138  to: 0.1266378402709961\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.1266378402709961  to: 0.1266184091567993\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.1266184091567993  to: 0.1265990614891052\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.1265990614891052  to: 0.12657978534698486\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12657978534698486  to: 0.12656060457229615\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12656060457229615  to: 0.12654143571853638\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12654143571853638  to: 0.12652231454849244\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12652231454849244  to: 0.12650320529937745\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12650320529937745  to: 0.12648409605026245\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12648409605026245  to: 0.12646498680114746\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12646498680114746  to: 0.12644584178924562\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.12644584178924562  to: 0.12642669677734375\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12642669677734375  to: 0.1264074921607971\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1264074921607971  to: 0.12638825178146362\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12638825178146362  to: 0.1263689398765564\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.1263689398765564  to: 0.1263495922088623\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.1263495922088623  to: 0.12633016109466552\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12633016109466552  to: 0.12631065845489503\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12631065845489503  to: 0.12629108428955077\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12629108428955077  to: 0.1262714147567749\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.1262714147567749  to: 0.1262515068054199\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.1262515068054199  to: 0.12623144388198854\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12623144388198854  to: 0.12621123790740968\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12621123790740968  to: 0.1261909246444702\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.1261909246444702  to: 0.1261705279350281\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.1261705279350281  to: 0.12615000009536742\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12615000009536742  to: 0.12612937688827514\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12612937688827514  to: 0.12610864639282227\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12610864639282227  to: 0.1260878324508667\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.1260878324508667  to: 0.1260668992996216\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.1260668992996216  to: 0.1260458707809448\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.1260458707809448  to: 0.12602474689483642\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12602474689483642  to: 0.1260034918785095\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.1260034918785095  to: 0.1259821653366089\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.1259821653366089  to: 0.12596070766448975\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12596070766448975  to: 0.12593917846679686\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12593917846679686  to: 0.12591755390167236\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12591755390167236  to: 0.1258958101272583\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.1258958101272583  to: 0.12587398290634155\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12587398290634155  to: 0.1258520483970642\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.1258520483970642  to: 0.12583003044128419\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12583003044128419  to: 0.12580792903900145\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12580792903900145  to: 0.12578574419021607\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12578574419021607  to: 0.1257634401321411\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.1257634401321411  to: 0.12574106454849243\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12574106454849243  to: 0.1257185697555542\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.1257185697555542  to: 0.12569600343704224\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12569600343704224  to: 0.12567331790924072\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12567331790924072  to: 0.12565057277679442\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12565057277679442  to: 0.12562769651412964\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12562769651412964  to: 0.12560473680496215\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12560473680496215  to: 0.12558166980743407\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.12558166980743407  to: 0.12555850744247438\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.12555850744247438  to: 0.1255352735519409\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.1255352735519409  to: 0.1255119562149048\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.1255119562149048  to: 0.12548853158950807\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.12548853158950807  to: 0.1254650115966797\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.1254650115966797  to: 0.12544139623641967\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12544139623641967  to: 0.12541770935058594\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12541770935058594  to: 0.12539392709732056\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12539392709732056  to: 0.1253700613975525\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.1253700613975525  to: 0.12534611225128173\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.12534611225128173  to: 0.12532206773757934\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.12532206773757934  to: 0.1252979516983032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 681\n",
      "Improved validation loss from: 0.1252979516983032  to: 0.1252737283706665\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.1252737283706665  to: 0.125249445438385\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.125249445438385  to: 0.1252250552177429\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.1252250552177429  to: 0.12520079612731932\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12520079612731932  to: 0.12517651319503784\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12517651319503784  to: 0.1251521348953247\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.1251521348953247  to: 0.12512768507003785\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12512768507003785  to: 0.12510311603546143\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.12510311603546143  to: 0.12507842779159545\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12507842779159545  to: 0.12505363225936889\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12505363225936889  to: 0.12502872943878174\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12502872943878174  to: 0.12500371932983398\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12500371932983398  to: 0.12497861385345459\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.12497861385345459  to: 0.1249534010887146\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.1249534010887146  to: 0.12492811679840088\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.12492811679840088  to: 0.12490272521972656\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12490272521972656  to: 0.12487722635269165\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12487722635269165  to: 0.124851655960083\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.124851655960083  to: 0.12482597827911376\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12482597827911376  to: 0.1248002290725708\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.1248002290725708  to: 0.12477439641952515\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.12477439641952515  to: 0.12474849224090576\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12474849224090576  to: 0.12472251653671265\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12472251653671265  to: 0.1246964693069458\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.1246964693069458  to: 0.12467033863067627\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12467033863067627  to: 0.12464414834976197\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12464414834976197  to: 0.12461786270141602\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12461786270141602  to: 0.12459150552749634\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12459150552749634  to: 0.12456357479095459\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12456357479095459  to: 0.12453423738479615\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.12453423738479615  to: 0.1245036005973816\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.1245036005973816  to: 0.12447183132171631\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12447183132171631  to: 0.1244390606880188\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.1244390606880188  to: 0.12440540790557861\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12440540790557861  to: 0.12437098026275635\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12437098026275635  to: 0.1243358850479126\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.1243358850479126  to: 0.12430000305175781\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12430000305175781  to: 0.12426365613937378\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.12426365613937378  to: 0.12422695159912109\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12422695159912109  to: 0.12419159412384033\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.12419159412384033  to: 0.12415746450424195\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12415746450424195  to: 0.12412441968917846\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12412441968917846  to: 0.12409236431121826\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12409236431121826  to: 0.12406086921691895\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12406086921691895  to: 0.12403008937835694\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12403008937835694  to: 0.12400007247924805\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12400007247924805  to: 0.12397079467773438\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.12397079467773438  to: 0.12394213676452637\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12394213676452637  to: 0.12391402721405029\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12391402721405029  to: 0.12388674020767212\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12388674020767212  to: 0.1238599419593811\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.1238599419593811  to: 0.12383352518081665\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12383352518081665  to: 0.12380740642547608\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12380740642547608  to: 0.12378154993057251\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12378154993057251  to: 0.12375588417053222\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12375588417053222  to: 0.12373039722442628\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12373039722442628  to: 0.12370502948760986\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12370502948760986  to: 0.12367974519729615\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12367974519729615  to: 0.12365455627441406\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12365455627441406  to: 0.12362937927246094\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12362937927246094  to: 0.12360423803329468\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12360423803329468  to: 0.12357906103134156\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12357906103134156  to: 0.12355380058288574\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12355380058288574  to: 0.12352845668792725\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12352845668792725  to: 0.1235029935836792\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.1235029935836792  to: 0.12347757816314697\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12347757816314697  to: 0.12345212697982788\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12345212697982788  to: 0.12342649698257446\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12342649698257446  to: 0.12340067625045777\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12340067625045777  to: 0.12337462902069092\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12337462902069092  to: 0.12334840297698975\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12334840297698975  to: 0.12332189083099365\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12332189083099365  to: 0.1232951521873474\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.1232951521873474  to: 0.12326815128326415\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12326815128326415  to: 0.12324087619781494\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12324087619781494  to: 0.1232133150100708\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.1232133150100708  to: 0.12318547964096069\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12318547964096069  to: 0.1231573462486267\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.1231573462486267  to: 0.12312890291213989\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12312890291213989  to: 0.12310014963150025\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12310014963150025  to: 0.12307109832763671\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12307109832763671  to: 0.12304174900054932\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12304174900054932  to: 0.12301206588745117\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12301206588745117  to: 0.12298204898834228\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12298204898834228  to: 0.12295172214508057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12295172214508057  to: 0.1229210376739502\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.1229210376739502  to: 0.1228900671005249\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.1228900671005249  to: 0.12285877466201782\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12285877466201782  to: 0.12282725572586059\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12282725572586059  to: 0.12279548645019531\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12279548645019531  to: 0.12276351451873779\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12276351451873779  to: 0.12273132801055908\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12273132801055908  to: 0.12269895076751709\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12269895076751709  to: 0.12266638278961181\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12266638278961181  to: 0.12263361215591431\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12263361215591431  to: 0.12260067462921143\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12260067462921143  to: 0.12256749868392944\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12256749868392944  to: 0.12253410816192627\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12253410816192627  to: 0.12250058650970459\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.12250058650970459  to: 0.12246698141098022\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.12246698141098022  to: 0.1224332332611084\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1224332332611084  to: 0.12239929437637329\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12239929437637329  to: 0.1223651647567749\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.1223651647567749  to: 0.1223308801651001\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.1223308801651001  to: 0.12229641675949096\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12229641675949096  to: 0.12226181030273438\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12226181030273438  to: 0.12222708463668823\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12222708463668823  to: 0.12219222784042358\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12219222784042358  to: 0.12215722799301147\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12215722799301147  to: 0.12212210893630981\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12212210893630981  to: 0.12208684682846069\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12208684682846069  to: 0.12205140590667725\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12205140590667725  to: 0.1220158576965332\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.1220158576965332  to: 0.12198013067245483\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12198013067245483  to: 0.1219448447227478\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.1219448447227478  to: 0.12190994024276733\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12190994024276733  to: 0.12187477350234985\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12187477350234985  to: 0.12183930873870849\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12183930873870849  to: 0.12180358171463013\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12180358171463013  to: 0.12176755666732789\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.12176755666732789  to: 0.12173124551773071\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12173124551773071  to: 0.12169466018676758\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12169466018676758  to: 0.12165782451629639\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12165782451629639  to: 0.121620774269104\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.121620774269104  to: 0.12158349752426148\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12158349752426148  to: 0.12154628038406372\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12154628038406372  to: 0.12150886058807372\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12150886058807372  to: 0.12147127389907837\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.12147127389907837  to: 0.121433424949646\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.121433424949646  to: 0.12139555215835571\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12139555215835571  to: 0.12135814428329468\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12135814428329468  to: 0.12132043838500976\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12132043838500976  to: 0.12128245830535889\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12128245830535889  to: 0.12124422788619996\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12124422788619996  to: 0.121205735206604\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.121205735206604  to: 0.12116698026657105\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.12116698026657105  to: 0.12112795114517212\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12112795114517212  to: 0.12108867168426514\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12108867168426514  to: 0.12104983329772949\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12104983329772949  to: 0.12101075649261475\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12101075649261475  to: 0.120971417427063\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.120971417427063  to: 0.12093188762664794\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12093188762664794  to: 0.12089216709136963\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12089216709136963  to: 0.12085225582122802\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12085225582122802  to: 0.12081209421157837\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12081209421157837  to: 0.120772385597229\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.120772385597229  to: 0.12073190212249756\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12073190212249756  to: 0.12069023847579956\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12069023847579956  to: 0.12064837217330933\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12064837217330933  to: 0.1206062912940979\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.1206062912940979  to: 0.12056403160095215\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12056403160095215  to: 0.12052226066589355\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12052226066589355  to: 0.1204802393913269\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1204802393913269  to: 0.1204379916191101\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.1204379916191101  to: 0.12039546966552735\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12039546966552735  to: 0.12035266160964966\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12035266160964966  to: 0.12030959129333496\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12030959129333496  to: 0.12026703357696533\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12026703357696533  to: 0.12022418975830078\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12022418975830078  to: 0.12018109560012817\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12018109560012817  to: 0.12013766765594483\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12013766765594483  to: 0.12009468078613281\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12009468078613281  to: 0.12005138397216797\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12005138397216797  to: 0.12000777721405029\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12000777721405029  to: 0.11996383666992187\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.11996383666992187  to: 0.11991962194442748\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.11991962194442748  to: 0.11987589597702027\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.11987589597702027  to: 0.11983108520507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 849\n",
      "Improved validation loss from: 0.11983108520507812  to: 0.11978594064712525\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11978594064712525  to: 0.11974050998687744\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11974050998687744  to: 0.11969478130340576\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11969478130340576  to: 0.11964908838272095\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.11964908838272095  to: 0.11960382461547851\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.11960382461547851  to: 0.11955894231796264\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.11955894231796264  to: 0.1195136308670044\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.1195136308670044  to: 0.11946799755096435\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11946799755096435  to: 0.11942199468612671\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.11942199468612671  to: 0.11937592029571534\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.11937592029571534  to: 0.11932958364486694\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.11932958364486694  to: 0.11928281784057618\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.11928281784057618  to: 0.11923564672470092\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.11923564672470092  to: 0.11918808221817016\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.11918808221817016  to: 0.11914098262786865\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.11914098262786865  to: 0.11909428834915162\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.11909428834915162  to: 0.11904714107513428\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.11904714107513428  to: 0.11899973154067993\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.11899973154067993  to: 0.11895202398300171\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.11895202398300171  to: 0.11890391111373902\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.11890391111373902  to: 0.11885541677474976\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.11885541677474976  to: 0.11880733966827392\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.11880733966827392  to: 0.11875890493392945\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.11875890493392945  to: 0.1187101125717163\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.1187101125717163  to: 0.11866099834442138\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.11866099834442138  to: 0.11861263513565064\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.11861263513565064  to: 0.11856385469436645\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.11856385469436645  to: 0.1185154676437378\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.1185154676437378  to: 0.11846742630004883\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.11846742630004883  to: 0.11841965913772583\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.11841965913772583  to: 0.11837209463119507\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.11837209463119507  to: 0.11832478046417236\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.11832478046417236  to: 0.11827763319015502\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.11827763319015502  to: 0.11823058128356934\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.11823058128356934  to: 0.11818355321884155\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.11818355321884155  to: 0.11813650131225586\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.11813650131225586  to: 0.11808944940567016\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.11808944940567016  to: 0.11804234981536865\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.11804234981536865  to: 0.11799509525299072\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.11799509525299072  to: 0.11794767379760743\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.11794767379760743  to: 0.11790008544921875\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.11790008544921875  to: 0.11785233020782471\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.11785233020782471  to: 0.1178043007850647\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.1178043007850647  to: 0.11775596141815185\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.11775596141815185  to: 0.11770732402801513\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.11770732402801513  to: 0.11765842437744141\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.11765842437744141  to: 0.1176092267036438\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.1176092267036438  to: 0.11755971908569336\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.11755971908569336  to: 0.11750990152359009\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.11750990152359009  to: 0.11746008396148681\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.11746008396148681  to: 0.11740987300872803\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.11740987300872803  to: 0.11735930442810058\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.11735930442810058  to: 0.11730835437774659\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.11730835437774659  to: 0.11725702285766601\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.11725702285766601  to: 0.1172053337097168\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.1172053337097168  to: 0.1171532392501831\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.1171532392501831  to: 0.1171007752418518\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.1171007752418518  to: 0.11704791784286499\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.11704791784286499  to: 0.11699373722076416\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.11699373722076416  to: 0.11693834066390991\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.11693834066390991  to: 0.1168828010559082\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.1168828010559082  to: 0.11682704687118531\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.11682704687118531  to: 0.11677110195159912\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.11677110195159912  to: 0.11671526432037353\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.11671526432037353  to: 0.11665922403335571\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.11665922403335571  to: 0.11660292148590087\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.11660292148590087  to: 0.11654634475708008\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.11654634475708008  to: 0.11648952960968018\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.11648952960968018  to: 0.11643249988555908\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.11643249988555908  to: 0.11637529134750366\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.11637529134750366  to: 0.11631793975830078\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.11631793975830078  to: 0.11626042127609253\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.11626042127609253  to: 0.11620277166366577\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.11620277166366577  to: 0.11614491939544677\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.11614491939544677  to: 0.11608703136444092\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.11608703136444092  to: 0.11602897644042968\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.11602897644042968  to: 0.1159706950187683\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.1159706950187683  to: 0.11591230630874634\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.11591230630874634  to: 0.11585384607315063\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.11585384607315063  to: 0.11579532623291015\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.11579532623291015  to: 0.11573672294616699\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.11573672294616699  to: 0.11567792892456055\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.11567792892456055  to: 0.11561894416809082\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.11561894416809082  to: 0.1155597448348999\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.1155597448348999  to: 0.11550018787384034\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.11550018787384034  to: 0.11544036865234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 935\n",
      "Improved validation loss from: 0.11544036865234375  to: 0.11538025140762329\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.11538025140762329  to: 0.11531991958618164\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.11531991958618164  to: 0.11525926589965821\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.11525926589965821  to: 0.11519839763641357\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.11519839763641357  to: 0.11513727903366089\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.11513727903366089  to: 0.11507610082626343\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.11507610082626343  to: 0.11501495838165283\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.11501495838165283  to: 0.11495381593704224\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.11495381593704224  to: 0.11489255428314209\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.11489255428314209  to: 0.11483101844787598\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.11483101844787598  to: 0.11476929187774658\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.11476929187774658  to: 0.114707350730896\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.114707350730896  to: 0.11464513540267944\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.11464513540267944  to: 0.11458277702331543\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.11458277702331543  to: 0.1145203709602356\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.1145203709602356  to: 0.11445785760879516\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.11445785760879516  to: 0.11439520120620728\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.11439520120620728  to: 0.11433224678039551\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.11433224678039551  to: 0.1142690896987915\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.1142690896987915  to: 0.11420568227767944\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.11420568227767944  to: 0.11414210796356201\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.11414210796356201  to: 0.1140782356262207\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.1140782356262207  to: 0.11401422023773193\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.11401422023773193  to: 0.11394977569580078\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.11394977569580078  to: 0.11388490200042725\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.11388490200042725  to: 0.11381977796554565\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.11381977796554565  to: 0.1137542486190796\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.1137542486190796  to: 0.11368852853775024\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.11368852853775024  to: 0.11362241506576538\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.11362241506576538  to: 0.11355608701705933\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.11355608701705933  to: 0.11348937749862671\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.11348937749862671  to: 0.11342239379882812\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.11342239379882812  to: 0.11335527896881104\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.11335527896881104  to: 0.11328797340393067\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.11328797340393067  to: 0.1132203221321106\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.1132203221321106  to: 0.11315215826034546\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.11315215826034546  to: 0.11308368444442748\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.11308368444442748  to: 0.11301469802856445\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.11301469802856445  to: 0.11294538974761963\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.11294538974761963  to: 0.11287559270858764\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.11287559270858764  to: 0.11280556917190551\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.11280556917190551  to: 0.1127353549003601\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.1127353549003601  to: 0.11266491413116456\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.11266491413116456  to: 0.11259398460388184\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.11259398460388184  to: 0.112522554397583\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.112522554397583  to: 0.11245077848434448\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.11245077848434448  to: 0.11237891912460327\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.11237891912460327  to: 0.11230909824371338\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.11230909824371338  to: 0.11223887205123902\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.11223887205123902  to: 0.11216814517974853\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.11216814517974853  to: 0.11209702491760254\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.11209702491760254  to: 0.11202553510665894\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.11202553510665894  to: 0.1119537115097046\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.1119537115097046  to: 0.11188143491744995\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.11188143491744995  to: 0.11180875301361085\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.11180875301361085  to: 0.1117356538772583\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.1117356538772583  to: 0.11166213750839234\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.11166213750839234  to: 0.1115883469581604\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.1115883469581604  to: 0.11151425838470459\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.11151425838470459  to: 0.1114398717880249\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.1114398717880249  to: 0.1113652229309082\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.1113652229309082  to: 0.1112901210784912\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.1112901210784912  to: 0.11121464967727661\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.11121464967727661  to: 0.11113880872726441\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.11113880872726441  to: 0.11106264591217041\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.11106264591217041  to: 0.11098630428314209\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.11098630428314209  to: 0.1109097957611084\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.1109097957611084  to: 0.1108330488204956\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.1108330488204956  to: 0.11075584888458252\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.11075584888458252  to: 0.11067816019058227\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.11067816019058227  to: 0.1106001853942871\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.1106001853942871  to: 0.11052172183990479\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.11052172183990479  to: 0.11044305562973022\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.11044305562973022  to: 0.11036418676376343\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.11036418676376343  to: 0.11028487682342529\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.11028487682342529  to: 0.11020514965057374\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.11020514965057374  to: 0.1101249098777771\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.1101249098777771  to: 0.11004433631896973\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.11004433631896973  to: 0.10996382236480713\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.10996382236480713  to: 0.10988423824310303\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.10988423824310303  to: 0.10980424880981446\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.10980424880981446  to: 0.10972380638122559\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.10972380638122559  to: 0.10964313745498658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.10964313745498658  to: 0.1095621943473816\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.1095621943473816  to: 0.10948077440261841\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.10948077440261841  to: 0.10939878225326538\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.10939878225326538  to: 0.10931642055511474\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.10931642055511474  to: 0.10923343896865845\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.10923343896865845  to: 0.10914965867996215\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.10914965867996215  to: 0.10906516313552857\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.10906516313552857  to: 0.10898030996322632\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.10898030996322632  to: 0.10889512300491333\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.10889512300491333  to: 0.10880937576293945\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.10880937576293945  to: 0.1087233304977417\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.1087233304977417  to: 0.10863672494888306\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.10863672494888306  to: 0.10854982137680054\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.10854982137680054  to: 0.10846246480941772\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.10846246480941772  to: 0.10837472677230835\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.10837472677230835  to: 0.10828638076782227\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.10828638076782227  to: 0.10819880962371826\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.10819880962371826  to: 0.10811349153518676\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.10811349153518676  to: 0.10802762508392334\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.10802762508392334  to: 0.10794122219085693\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.10794122219085693  to: 0.10785436630249023\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.10785436630249023  to: 0.10776708126068116\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.10776708126068116  to: 0.10767915248870849\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.10767915248870849  to: 0.10759072303771973\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.10759072303771973  to: 0.10750203132629395\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.10750203132629395  to: 0.10741304159164429\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.10741304159164429  to: 0.10732365846633911\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.10732365846633911  to: 0.10723364353179932\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.10723364353179932  to: 0.10714313983917237\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.10714313983917237  to: 0.10705217123031616\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.10705217123031616  to: 0.1069608449935913\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.1069608449935913  to: 0.10686914920806885\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.10686914920806885  to: 0.10677720308303833\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.10677720308303833  to: 0.1066847562789917\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.1066847562789917  to: 0.10659167766571045\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.10659167766571045  to: 0.10649826526641845\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.10649826526641845  to: 0.10640460252761841\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.10640460252761841  to: 0.10631043910980224\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.10631043910980224  to: 0.10621578693389892\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.10621578693389892  to: 0.10612075328826905\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.10612075328826905  to: 0.1060253381729126\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.1060253381729126  to: 0.10592944622039795\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.10592944622039795  to: 0.10583302974700928\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.10583302974700928  to: 0.10573605298995972\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.10573605298995972  to: 0.10563855171203614\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.10563855171203614  to: 0.10554077625274658\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.10554077625274658  to: 0.10544254779815673\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.10544254779815673  to: 0.10534385442733765\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.10534385442733765  to: 0.1052444338798523\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.1052444338798523  to: 0.1051443099975586\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.1051443099975586  to: 0.10504387617111206\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.10504387617111206  to: 0.10494314432144165\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.10494314432144165  to: 0.1048420786857605\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.1048420786857605  to: 0.10474035739898682\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.10474035739898682  to: 0.10463802814483643\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.10463802814483643  to: 0.10453498363494873\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.10453498363494873  to: 0.10443143844604492\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.10443143844604492  to: 0.10432642698287964\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.10432642698287964  to: 0.10421938896179199\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.10421938896179199  to: 0.10411064624786377\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.10411064624786377  to: 0.10400004386901855\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.10400004386901855  to: 0.10388810634613037\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.10388810634613037  to: 0.10377498865127563\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.10377498865127563  to: 0.1036608338356018\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.1036608338356018  to: 0.10354557037353515\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.10354557037353515  to: 0.10342960357666016\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.10342960357666016  to: 0.10331311225891113\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.10331311225891113  to: 0.10319620370864868\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.10319620370864868  to: 0.10307853221893311\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.10307853221893311  to: 0.10296013355255126\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.10296013355255126  to: 0.10284136533737183\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.10284136533737183  to: 0.10272234678268433\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.10272234678268433  to: 0.10260331630706787\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.10260331630706787  to: 0.10248428583145142\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.10248428583145142  to: 0.10236525535583496\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.10236525535583496  to: 0.10224626064300538\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.10224626064300538  to: 0.10212697982788085\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.10212697982788085  to: 0.10200729370117187\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.10200729370117187  to: 0.10188720226287842\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.10188720226287842  to: 0.10176717042922974\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.10176717042922974  to: 0.10164533853530884\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.10164533853530884  to: 0.10152194499969483\n",
      "Training iteration: 1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10152194499969483  to: 0.10139706134796142\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.10139706134796142  to: 0.10127111673355102\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.10127111673355102  to: 0.10114418268203736\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.10114418268203736  to: 0.10101637840270997\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.10101637840270997  to: 0.10088796615600586\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.10088796615600586  to: 0.10075899362564086\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.10075899362564086  to: 0.10062987804412842\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.10062987804412842  to: 0.10050097703933716\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.10050097703933716  to: 0.10037260055541992\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.10037260055541992  to: 0.10024484395980834\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.10024484395980834  to: 0.10011758804321289\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.10011758804321289  to: 0.09999089241027832\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.09999089241027832  to: 0.09986494183540344\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.09986494183540344  to: 0.09973942041397095\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.09973942041397095  to: 0.09961429834365845\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.09961429834365845  to: 0.09948935508728027\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.09948935508728027  to: 0.09936453700065613\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.09936453700065613  to: 0.09923995137214661\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.09923995137214661  to: 0.0991158664226532\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.0991158664226532  to: 0.09899194836616516\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.09899194836616516  to: 0.09886825680732728\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.09886825680732728  to: 0.09874445796012879\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.09874445796012879  to: 0.09862058758735656\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.09862058758735656  to: 0.09849651455879212\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.09849651455879212  to: 0.0983715832233429\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.0983715832233429  to: 0.09824571609497071\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.09824571609497071  to: 0.09811906814575196\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.09811906814575196  to: 0.097991943359375\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.097991943359375  to: 0.09786421060562134\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.09786421060562134  to: 0.0977357029914856\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.0977357029914856  to: 0.09760613441467285\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.09760613441467285  to: 0.09747594594955444\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.09747594594955444  to: 0.09734477996826171\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.09734477996826171  to: 0.09721304774284363\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.09721304774284363  to: 0.09708078503608704\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.09708078503608704  to: 0.09694778323173522\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.09694778323173522  to: 0.09681426286697388\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.09681426286697388  to: 0.09668018221855164\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.09668018221855164  to: 0.09654361605644227\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.09654361605644227  to: 0.09640356302261352\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.09640356302261352  to: 0.09626027941703796\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.09626027941703796  to: 0.09611414074897766\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.09611414074897766  to: 0.09596529006958007\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.09596529006958007  to: 0.0958143413066864\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.0958143413066864  to: 0.09566171765327454\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.09566171765327454  to: 0.09550777673721314\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.09550777673721314  to: 0.09535287618637085\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.09535287618637085  to: 0.09519739151000976\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.09519739151000976  to: 0.09504123926162719\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.09504123926162719  to: 0.09488471150398255\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.09488471150398255  to: 0.09472864270210266\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.09472864270210266  to: 0.09457321166992187\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.09457321166992187  to: 0.0944182276725769\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.0944182276725769  to: 0.09426393508911132\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.09426393508911132  to: 0.09410972595214843\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.09410972595214843  to: 0.0939558207988739\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.0939558207988739  to: 0.0938023567199707\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.0938023567199707  to: 0.09364951848983764\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.09364951848983764  to: 0.09349743723869323\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.09349743723869323  to: 0.09334623217582702\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.09334623217582702  to: 0.09319599270820618\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.09319599270820618  to: 0.09304637908935547\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.09304637908935547  to: 0.09289749264717102\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.09289749264717102  to: 0.09274924397468567\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.09274924397468567  to: 0.0926021933555603\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.0926021933555603  to: 0.09245669245719909\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.09245669245719909  to: 0.09231284260749817\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.09231284260749817  to: 0.09217063784599304\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.09217063784599304  to: 0.09203017354011536\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.09203017354011536  to: 0.09189114570617676\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.09189114570617676  to: 0.0917535662651062\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.0917535662651062  to: 0.09161788821220399\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.09161788821220399  to: 0.0914838194847107\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.0914838194847107  to: 0.09135160446166993\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.09135160446166993  to: 0.0912214457988739\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.0912214457988739  to: 0.09109414219856263\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.09109414219856263  to: 0.09096992611885071\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.09096992611885071  to: 0.09082515835762024\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.09082515835762024  to: 0.09066416621208191\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.09066416621208191  to: 0.09050325155258179\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.09050325155258179  to: 0.09034184217453003\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.09034184217453003  to: 0.0901803195476532\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.0901803195476532  to: 0.09001873135566711\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.09001873135566711  to: 0.08985757827758789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.08985757827758789  to: 0.08969683647155761\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.08969683647155761  to: 0.08953644037246704\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.08953644037246704  to: 0.08937635421752929\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.08937635421752929  to: 0.08921570777893066\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.08921570777893066  to: 0.08905455470085144\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.08905455470085144  to: 0.08889304995536804\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.08889304995536804  to: 0.08873176574707031\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.08873176574707031  to: 0.08857091665267944\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.08857091665267944  to: 0.08841031789779663\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.08841031789779663  to: 0.08824890851974487\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.08824890851974487  to: 0.08808627128601074\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.08808627128601074  to: 0.08792219161987305\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.08792219161987305  to: 0.0877569079399109\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.0877569079399109  to: 0.08759063482284546\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.08759063482284546  to: 0.08742443919181823\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.08742443919181823  to: 0.08725849390029908\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.08725849390029908  to: 0.08708707690238952\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.08708707690238952  to: 0.08690980672836304\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.08690980672836304  to: 0.0867319405078888\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.0867319405078888  to: 0.08655206561088562\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.08655206561088562  to: 0.08636946678161621\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.08636946678161621  to: 0.08618504405021668\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.08618504405021668  to: 0.08599952459335328\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.08599952459335328  to: 0.08581496477127075\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.08581496477127075  to: 0.0856289267539978\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.0856289267539978  to: 0.0854400336742401\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.0854400336742401  to: 0.08525371551513672\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.08525371551513672  to: 0.0850713610649109\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.0850713610649109  to: 0.08489451408386231\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.08489451408386231  to: 0.08471208810806274\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.08471208810806274  to: 0.08452242016792297\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.08452242016792297  to: 0.0843370258808136\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.0843370258808136  to: 0.08415605425834656\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.08415605425834656  to: 0.08398057818412781\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.08398057818412781  to: 0.08379359245300293\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.08379359245300293  to: 0.08361226916313172\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.08361226916313172  to: 0.0834376335144043\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.0834376335144043  to: 0.08327022790908814\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.08327022790908814  to: 0.08308130502700806\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.08308130502700806  to: 0.08290045857429504\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.08290045857429504  to: 0.08272711038589478\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.08272711038589478  to: 0.08256136178970337\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.08256136178970337  to: 0.08239935040473938\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.08239935040473938  to: 0.08224096298217773\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.08224096298217773  to: 0.08208622932434081\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.08208622932434081  to: 0.08193463087081909\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.08193463087081909  to: 0.08178514242172241\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.08178514242172241  to: 0.08163787722587586\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.08163787722587586  to: 0.08149164915084839\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.08149164915084839  to: 0.08134621381759644\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.08134621381759644  to: 0.0812012493610382\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.0812012493610382  to: 0.08105686902999878\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.08105686902999878  to: 0.08091341257095337\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.08091341257095337  to: 0.08077062368392944\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.08077062368392944  to: 0.08062894940376282\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.08062894940376282  to: 0.08048892021179199\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.08048892021179199  to: 0.08035098910331726\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.08035098910331726  to: 0.08021554946899415\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.08021554946899415  to: 0.08008297681808471\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.08008297681808471  to: 0.07995363473892211\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.07995363473892211  to: 0.07982627749443054\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.07982627749443054  to: 0.0797012209892273\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.0797012209892273  to: 0.07957864999771118\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.07957864999771118  to: 0.07945868372917175\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.07945868372917175  to: 0.07934131026268006\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.07934131026268006  to: 0.07922647595405578\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.07922647595405578  to: 0.07911397218704223\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.07911397218704223  to: 0.07900358438491821\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.07900358438491821  to: 0.07889504432678222\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.07889504432678222  to: 0.07878823280334472\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.07878823280334472  to: 0.07868286371231079\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.07868286371231079  to: 0.07857853174209595\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.07857853174209595  to: 0.07847484350204467\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.07847484350204467  to: 0.07837202548980712\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.07837202548980712  to: 0.0782701015472412\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.0782701015472412  to: 0.07816923260688782\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.07816923260688782  to: 0.0780695378780365\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.0780695378780365  to: 0.07797098159790039\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.07797098159790039  to: 0.07787328362464904\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.07787328362464904  to: 0.07777668833732605\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.07777668833732605  to: 0.07768135666847228\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.07768135666847228  to: 0.07758731842041015\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.07758731842041015  to: 0.07749456763267518\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.07749456763267518  to: 0.0774030089378357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.0774030089378357  to: 0.07731250524520875\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.07731250524520875  to: 0.07722291350364685\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.07722291350364685  to: 0.07713406682014465\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.07713406682014465  to: 0.07704581022262573\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.07704581022262573  to: 0.07695798873901367\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.07695798873901367  to: 0.07687047719955445\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.07687047719955445  to: 0.0767831027507782\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.0767831027507782  to: 0.07669575214385986\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.07669575214385986  to: 0.07660801410675049\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.07660801410675049  to: 0.07651857137680054\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.07651857137680054  to: 0.07642780542373658\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.07642780542373658  to: 0.07633576393127442\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.07633576393127442  to: 0.07624261379241944\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.07624261379241944  to: 0.076148521900177\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.076148521900177  to: 0.07605371475219727\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.07605371475219727  to: 0.07595518231391907\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.07595518231391907  to: 0.07585387825965881\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.07585387825965881  to: 0.07575082778930664\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.07575082778930664  to: 0.07564663887023926\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.07564663887023926  to: 0.07554152011871337\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.07554152011871337  to: 0.07543641328811646\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.07543641328811646  to: 0.07533150911331177\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.07533150911331177  to: 0.07522681951522828\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.07522681951522828  to: 0.07512215971946716\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.07512215971946716  to: 0.07501680254936219\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.07501680254936219  to: 0.07491050362586975\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.07491050362586975  to: 0.07480334639549255\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.07480334639549255  to: 0.07469537854194641\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.07469537854194641  to: 0.07458688616752625\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.07458688616752625  to: 0.07447816729545594\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.07447816729545594  to: 0.07436806559562684\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.07436806559562684  to: 0.0742560863494873\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.0742560863494873  to: 0.07414350509643555\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.07414350509643555  to: 0.07402956485748291\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.07402956485748291  to: 0.07391438484191895\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.07391438484191895  to: 0.07379803657531739\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.07379803657531739  to: 0.073680579662323\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.073680579662323  to: 0.07356215119361878\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.07356215119361878  to: 0.07344304323196411\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.07344304323196411  to: 0.07332361936569214\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.07332361936569214  to: 0.07320435047149658\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.07320435047149658  to: 0.07308567166328431\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.07308567166328431  to: 0.0729677677154541\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.0729677677154541  to: 0.07285071611404419\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.07285071611404419  to: 0.07273437976837158\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.07273437976837158  to: 0.07261860966682435\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.07261860966682435  to: 0.0725007951259613\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.0725007951259613  to: 0.07238361239433289\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.07238361239433289  to: 0.07225432395935058\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.07225432395935058  to: 0.07210569381713867\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.07210569381713867  to: 0.07194966673851014\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.07194966673851014  to: 0.07178858518600464\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.07178858518600464  to: 0.07162288427352906\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.07162288427352906  to: 0.07145223617553711\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.07145223617553711  to: 0.0712757408618927\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.0712757408618927  to: 0.07109230160713195\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.07109230160713195  to: 0.0709015667438507\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.0709015667438507  to: 0.07070594429969787\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.07070594429969787  to: 0.07050977945327759\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.07050977945327759  to: 0.07031760215759278\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.07031760215759278  to: 0.07013879418373108\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.07013879418373108  to: 0.06997525095939636\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.06997525095939636  to: 0.06982730627059937\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.06982730627059937  to: 0.06969455480575562\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.06969455480575562  to: 0.06957616209983826\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.06957616209983826  to: 0.0694758415222168\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.0694758415222168  to: 0.06938849687576294\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.06938849687576294  to: 0.06931269764900208\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.06931269764900208  to: 0.06923137307167053\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.06923137307167053  to: 0.06914165616035461\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.06914165616035461  to: 0.06903762221336365\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.06903762221336365  to: 0.06892093420028686\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.06892093420028686  to: 0.06879237890243531\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.06879237890243531  to: 0.06865606904029846\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.06865606904029846  to: 0.06852110028266907\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.06852110028266907  to: 0.06838740110397339\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.06838740110397339  to: 0.06825944185256957\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.06825944185256957  to: 0.06813587546348572\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.06813587546348572  to: 0.06801260113716126\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.06801260113716126  to: 0.06789116859436035\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.06789116859436035  to: 0.0677761197090149\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.0677761197090149  to: 0.06767369508743286\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.06767369508743286  to: 0.06757232546806335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.06757232546806335  to: 0.0674612820148468\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.0674612820148468  to: 0.06736577153205872\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.06736577153205872  to: 0.06728073358535766\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.06728073358535766  to: 0.06718769073486328\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.06718769073486328  to: 0.06708360910415649\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.06708360910415649  to: 0.06697360277175904\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.06697360277175904  to: 0.06687675714492798\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.06687675714492798  to: 0.06680079698562622\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.06680079698562622  to: 0.06670843958854675\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.06670843958854675  to: 0.06660021543502807\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.06660021543502807  to: 0.06650644540786743\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.06650644540786743  to: 0.0664193570613861\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.0664193570613861  to: 0.06630975008010864\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.06630975008010864  to: 0.06617949604988098\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.06617949604988098  to: 0.06603078842163086\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.06603078842163086  to: 0.06590771079063415\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.06590771079063415  to: 0.06574852466583252\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.06574852466583252  to: 0.06562069058418274\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.06562069058418274  to: 0.06546579003334045\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.06546579003334045  to: 0.06534271240234375\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.06534271240234375  to: 0.06526725292205811\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.06526725292205811  to: 0.06513935923576356\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.06513935923576356  to: 0.06500476598739624\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.06500476598739624  to: 0.06490198373794556\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.06490198373794556  to: 0.06484207510948181\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.06484207510948181  to: 0.06475964188575745\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.06475964188575745  to: 0.06462809443473816\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.06462809443473816  to: 0.06451491713523864\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.06451491713523864  to: 0.06443291902542114\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.06443291902542114  to: 0.06436316967010498\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.06436316967010498  to: 0.06425304412841797\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.06425304412841797  to: 0.06414116621017456\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.06414116621017456  to: 0.06403326988220215\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.06403326988220215  to: 0.06397844552993774\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.06397844552993774  to: 0.06388388872146607\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.06388388872146607  to: 0.06379078030586242\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.06379078030586242  to: 0.063754141330719\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.063754141330719  to: 0.06367340087890624\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.06367340087890624  to: 0.06364808678627014\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.06364808678627014  to: 0.06358414888381958\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.06358414888381958  to: 0.06356905698776245\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.06356905698776245  to: 0.0635193407535553\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.0635193407535553  to: 0.06351144313812256\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.06351144313812256  to: 0.06346063017845154\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.06346063017845154  to: 0.06339276432991028\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.06339276432991028  to: 0.06333751678466797\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.06333751678466797  to: 0.06331721544265748\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.06331721544265748  to: 0.06319000124931336\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.06319000124931336  to: 0.06313376426696778\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.06316950917243958\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.06313376426696778  to: 0.06297436952590943\n",
      "Training iteration: 1402\n",
      "Validation loss (no improvement): 0.06303802728652955\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.06297436952590943  to: 0.06296411752700806\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.06296411752700806  to: 0.06285380125045777\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.06285380125045777  to: 0.0626954436302185\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.0626954436302185  to: 0.062463265657424924\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.062463265657424924  to: 0.062436676025390624\n",
      "Training iteration: 1408\n",
      "Validation loss (no improvement): 0.06251894831657409\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.062436676025390624  to: 0.06211355924606323\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.06211355924606323  to: 0.06200955510139465\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.06200955510139465  to: 0.061899995803833006\n",
      "Training iteration: 1412\n",
      "Validation loss (no improvement): 0.062078696489334104\n",
      "Training iteration: 1413\n",
      "Validation loss (no improvement): 0.06191893219947815\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.061899995803833006  to: 0.06172432899475098\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.061729496717453\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.06172432899475098  to: 0.06160533428192139\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.06178239583969116\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.06160533428192139  to: 0.0615044891834259\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.0615044891834259  to: 0.06130757331848145\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.06130757331848145  to: 0.06119362711906433\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.06119362711906433  to: 0.061012250185012815\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.061012250185012815  to: 0.06099504232406616\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.06099504232406616  to: 0.060705310106277464\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.060705310106277464  to: 0.060496962070465087\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.060496962070465087  to: 0.060418474674224856\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.060418474674224856  to: 0.06033504009246826\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.060354626178741454\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.06033504009246826  to: 0.06021966934204102\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.06021966934204102  to: 0.06016632914543152\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.06016632914543152  to: 0.060085374116897586\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.060085374116897586  to: 0.06001778841018677\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.060095787048339844\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.06001778841018677  to: 0.059846621751785276\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.059846621751785276  to: 0.05968687534332275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.05968687534332275  to: 0.05959780812263489\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.05959780812263489  to: 0.059584587812423706\n",
      "Training iteration: 1437\n",
      "Validation loss (no improvement): 0.05959054231643677\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.059584587812423706  to: 0.05937570333480835\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.05937570333480835  to: 0.059225207567214964\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.059225207567214964  to: 0.059220516681671144\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.059220516681671144  to: 0.059214997291564944\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.05923845171928406\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.059214997291564944  to: 0.059108227491378784\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.059108227491378784  to: 0.059028524160385135\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.059028524160385135  to: 0.0589738130569458\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.0589738130569458  to: 0.05892577171325684\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.05892577171325684  to: 0.05887316465377808\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.05887316465377808  to: 0.05867096185684204\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.05867096185684204  to: 0.058587771654129026\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.058587771654129026  to: 0.058537262678146365\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.058537262678146365  to: 0.05845405459403992\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.05845405459403992  to: 0.058356177806854245\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.058356177806854245  to: 0.058245861530303956\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.058245861530303956  to: 0.058043545484542845\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.058043545484542845  to: 0.05796777606010437\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.05796777606010437  to: 0.05795036554336548\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.05795036554336548  to: 0.05791882276535034\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.05791882276535034  to: 0.057790344953536986\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.057790344953536986  to: 0.05766572952270508\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.05766572952270508  to: 0.05758894681930542\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.05758894681930542  to: 0.05749285817146301\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.05751606822013855\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.05753635168075562\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.05749285817146301  to: 0.057278645038604734\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.057278645038604734  to: 0.05713297128677368\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.05713297128677368  to: 0.057050657272338864\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.057210081815719606\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.057050657272338864  to: 0.05699654817581177\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.05699654817581177  to: 0.05693155527114868\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.05694805383682251\n",
      "Training iteration: 1471\n",
      "Validation loss (no improvement): 0.056989777088165286\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.05693155527114868  to: 0.056916600465774535\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.056916600465774535  to: 0.05662325620651245\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.05662325620651245  to: 0.05661832094192505\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.0566524863243103\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.056705957651138304\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.05661832094192505  to: 0.05647168755531311\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.05647168755531311  to: 0.05643869042396545\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.0565661370754242\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.05652562379837036\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.05643869042396545  to: 0.05629154443740845\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.05629154443740845  to: 0.056266188621520996\n",
      "Training iteration: 1483\n",
      "Validation loss (no improvement): 0.05640096664428711\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.056266188621520996  to: 0.05619767904281616\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.05619767904281616  to: 0.0559409499168396\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.0559409499168396  to: 0.055827361345291135\n",
      "Training iteration: 1487\n",
      "Validation loss (no improvement): 0.055960196256637576\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.055827361345291135  to: 0.055568534135818484\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.055568534135818484  to: 0.055496466159820554\n",
      "Training iteration: 1490\n",
      "Validation loss (no improvement): 0.055613505840301516\n",
      "Training iteration: 1491\n",
      "Validation loss (no improvement): 0.05582558512687683\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.055496466159820554  to: 0.0554848313331604\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.0554848313331604  to: 0.055404484272003174\n",
      "Training iteration: 1494\n",
      "Validation loss (no improvement): 0.055438220500946045\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.055589473247528075\n",
      "Training iteration: 1496\n",
      "Validation loss (no improvement): 0.055515193939208986\n",
      "Training iteration: 1497\n",
      "Validation loss (no improvement): 0.05565499067306519\n",
      "Training iteration: 1498\n",
      "Validation loss (no improvement): 0.0557026743888855\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.05554080605506897\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.055404484272003174  to: 0.05531716346740723\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.05531716346740723  to: 0.05487847328186035\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.05487847328186035  to: 0.05466696619987488\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.05466696619987488  to: 0.0545824408531189\n",
      "Training iteration: 1504\n",
      "Validation loss (no improvement): 0.05464354157447815\n",
      "Training iteration: 1505\n",
      "Validation loss (no improvement): 0.05482131242752075\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.0545824408531189  to: 0.05450739860534668\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.05450739860534668  to: 0.054468536376953126\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.054468536376953126  to: 0.05436699986457825\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.05453435182571411\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.05447729229927063\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.05436699986457825  to: 0.05421568155288696\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.05421568155288696  to: 0.054208993911743164\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.05430036783218384\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.05454915165901184\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.054362326860427856\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.05435056090354919\n",
      "Training iteration: 1517\n",
      "Validation loss (no improvement): 0.05433274507522583\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.05432158708572388\n",
      "Training iteration: 1519\n",
      "Validation loss (no improvement): 0.05440365076065064\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.054208993911743164  to: 0.05397981405258179\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.05397981405258179  to: 0.053849059343338015\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.053849059343338015  to: 0.05384572744369507\n",
      "Training iteration: 1523\n",
      "Validation loss (no improvement): 0.054048281908035276\n",
      "Training iteration: 1524\n",
      "Validation loss (no improvement): 0.05441937446594238\n",
      "Training iteration: 1525\n",
      "Validation loss (no improvement): 0.05420241355895996\n",
      "Training iteration: 1526\n",
      "Validation loss (no improvement): 0.0541098415851593\n",
      "Training iteration: 1527\n",
      "Validation loss (no improvement): 0.05387327671051025\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.05384572744369507  to: 0.05383098721504211\n",
      "Training iteration: 1529\n",
      "Validation loss (no improvement): 0.05417683720588684\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.05383098721504211  to: 0.053630053997039795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.053630053997039795  to: 0.053577864170074464\n",
      "Training iteration: 1532\n",
      "Validation loss (no improvement): 0.05371483564376831\n",
      "Training iteration: 1533\n",
      "Validation loss (no improvement): 0.053840601444244386\n",
      "Training iteration: 1534\n",
      "Validation loss (no improvement): 0.05418440103530884\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.05380087494850159\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.053577864170074464  to: 0.0534684956073761\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.0534684956073761  to: 0.05333033800125122\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.0533544659614563\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.05361989736557007\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.05363839268684387\n",
      "Training iteration: 1541\n",
      "Validation loss (no improvement): 0.053476732969284055\n",
      "Training iteration: 1542\n",
      "Validation loss (no improvement): 0.05348416566848755\n",
      "Training iteration: 1543\n",
      "Validation loss (no improvement): 0.05347481369972229\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.053539609909057616\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.05333033800125122  to: 0.053320133686065675\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.053320133686065675  to: 0.05319204330444336\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.05319204330444336  to: 0.05312363505363464\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.05318683385848999\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.05331031084060669\n",
      "Training iteration: 1550\n",
      "Validation loss (no improvement): 0.05328046083450318\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.05312363505363464  to: 0.053075194358825684\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.053075194358825684  to: 0.05287551283836365\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.05287551283836365  to: 0.05277809500694275\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.05277809500694275  to: 0.052612298727035524\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.052612298727035524  to: 0.05245052576065064\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.05245052576065064  to: 0.05244150161743164\n",
      "Training iteration: 1557\n",
      "Validation loss (no improvement): 0.05265483260154724\n",
      "Training iteration: 1558\n",
      "Validation loss (no improvement): 0.05286353826522827\n",
      "Training iteration: 1559\n",
      "Validation loss (no improvement): 0.05258526206016541\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.05244150161743164  to: 0.05232689380645752\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.05232689380645752  to: 0.05221356749534607\n",
      "Training iteration: 1562\n",
      "Validation loss (no improvement): 0.05237727165222168\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.05221356749534607  to: 0.0521088182926178\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.0521088182926178  to: 0.05197870135307312\n",
      "Training iteration: 1565\n",
      "Validation loss (no improvement): 0.05210222601890564\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.05227452516555786\n",
      "Training iteration: 1567\n",
      "Validation loss (no improvement): 0.052375507354736325\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.05197870135307312  to: 0.05176547169685364\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.05176547169685364  to: 0.05154794454574585\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.051652705669403075\n",
      "Training iteration: 1571\n",
      "Validation loss (no improvement): 0.05208715796470642\n",
      "Training iteration: 1572\n",
      "Validation loss (no improvement): 0.05190346837043762\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.051878297328948976\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.05203709602355957\n",
      "Training iteration: 1575\n",
      "Validation loss (no improvement): 0.052137362957000735\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.05196889638900757\n",
      "Training iteration: 1577\n",
      "Validation loss (no improvement): 0.05170045495033264\n",
      "Training iteration: 1578\n",
      "Validation loss (no improvement): 0.05160079002380371\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.05161622166633606\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.051763951778411865\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.05199750661849976\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.051933574676513675\n",
      "Training iteration: 1583\n",
      "Validation loss (no improvement): 0.051727741956710815\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.051557689905166626\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.05154794454574585  to: 0.05151476263999939\n",
      "Training iteration: 1586\n",
      "Validation loss (no improvement): 0.0516321063041687\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.05151476263999939  to: 0.051190918684005736\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.051190918684005736  to: 0.05118765234947205\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.05136911273002624\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.051591187715530396\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.05126520395278931\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.05118765234947205  to: 0.05109742283821106\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.05109742283821106  to: 0.05100868940353394\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.05110719203948975\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.051035749912261966\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.05100868940353394  to: 0.05070363283157349\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.05070363283157349  to: 0.05059009790420532\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.05079307556152344\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.0511684238910675\n",
      "Training iteration: 1600\n",
      "Validation loss (no improvement): 0.05087279081344605\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.050672852993011476\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.05082179307937622\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.051054513454437254\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.051025784015655516\n",
      "Training iteration: 1605\n",
      "Validation loss (no improvement): 0.05078116655349731\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.05066324472427368\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.050677573680877684\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.05079544186592102\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.05087370872497558\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.05080256462097168\n",
      "Training iteration: 1611\n",
      "Validation loss (no improvement): 0.05083625912666321\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.05082193613052368\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.050802648067474365\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.05059009790420532  to: 0.050577294826507566\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.050577294826507566  to: 0.05047503709793091\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.05053597688674927\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.05048573017120361\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.05047503709793091  to: 0.050288552045822145\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.05037993788719177\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.05058477520942688\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.05069156289100647\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.050478196144104\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.050288552045822145  to: 0.0502521812915802\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.05029991865158081\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.05047341585159302\n",
      "Training iteration: 1626\n",
      "Validation loss (no improvement): 0.05054846405982971\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.0502521812915802  to: 0.050105273723602295\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.050105273723602295  to: 0.049999538064002993\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.05017675161361694\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.05054344534873963\n",
      "Training iteration: 1631\n",
      "Validation loss (no improvement): 0.050272810459136966\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.05040380358695984\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.05039810538291931\n",
      "Training iteration: 1634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.05053762793540954\n",
      "Training iteration: 1635\n",
      "Validation loss (no improvement): 0.05044511556625366\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.049999538064002993  to: 0.049808812141418454\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.049808812141418454  to: 0.04959257245063782\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.04978521764278412\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.05023516416549682\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.05076689124107361\n",
      "Training iteration: 1641\n",
      "Validation loss (no improvement): 0.050279653072357176\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.05035020709037781\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.05047043561935425\n",
      "Training iteration: 1644\n",
      "Validation loss (no improvement): 0.05049799084663391\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.050304877758026126\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.050026053190231325\n",
      "Training iteration: 1647\n",
      "Validation loss (no improvement): 0.0499302864074707\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.05007107257843017\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.0502644956111908\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.05014188885688782\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.050057399272918704\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.04989666938781738\n",
      "Training iteration: 1653\n",
      "Validation loss (no improvement): 0.049874264001846316\n",
      "Training iteration: 1654\n",
      "Validation loss (no improvement): 0.049961519241333005\n",
      "Training iteration: 1655\n",
      "Validation loss (no improvement): 0.04989460110664368\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.04991037845611572\n",
      "Training iteration: 1657\n",
      "Validation loss (no improvement): 0.04983453750610352\n",
      "Training iteration: 1658\n",
      "Validation loss (no improvement): 0.04980268478393555\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.04959257245063782  to: 0.04937624931335449\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.04937624931335449  to: 0.04912556707859039\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.04918851852416992\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.04930965006351471\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.04928330481052399\n",
      "Training iteration: 1664\n",
      "Validation loss (no improvement): 0.04946710467338562\n",
      "Training iteration: 1665\n",
      "Validation loss (no improvement): 0.04941178262233734\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.04933258891105652\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.049294394254684445\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.04912556707859039  to: 0.04895828366279602\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.04895828366279602  to: 0.048867183923721316\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.04891413748264313\n",
      "Training iteration: 1671\n",
      "Validation loss (no improvement): 0.049000731110572814\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.048872271180152894\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.04893056750297546\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.04904868602752686\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.048968353867530824\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.048867183923721316  to: 0.04857802391052246\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.04857802391052246  to: 0.0484424889087677\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.04859839379787445\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.04883367121219635\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.04879172444343567\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.04875744879245758\n",
      "Training iteration: 1682\n",
      "Validation loss (no improvement): 0.04878380298614502\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.048839721083641055\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.048652762174606325\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.04847412705421448\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.04848140180110931\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.048597827553749084\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.04880216717720032\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.04867528975009918\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.048681384325027464\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.04864211678504944\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.04858565330505371\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.0484424889087677  to: 0.048252859711647035\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.048252859711647035  to: 0.04815670847892761\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.04835154414176941\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.04864187240600586\n",
      "Training iteration: 1697\n",
      "Validation loss (no improvement): 0.04842202663421631\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.048263350129127504\n",
      "Training iteration: 1699\n",
      "Validation loss (no improvement): 0.0483487606048584\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.04858659207820892\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.04846433699131012\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.04815670847892761  to: 0.04800915122032166\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.04800915122032166  to: 0.047891122102737424\n",
      "Training iteration: 1704\n",
      "Validation loss (no improvement): 0.04808887839317322\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.048574605584144594\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.04852223992347717\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.048410439491271974\n",
      "Training iteration: 1708\n",
      "Validation loss (no improvement): 0.048377180099487306\n",
      "Training iteration: 1709\n",
      "Validation loss (no improvement): 0.04830280840396881\n",
      "Training iteration: 1710\n",
      "Validation loss (no improvement): 0.04821266531944275\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.04811610281467438\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.04815074801445007\n",
      "Training iteration: 1713\n",
      "Validation loss (no improvement): 0.04805043637752533\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.04803667962551117\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.048078566789627075\n",
      "Training iteration: 1716\n",
      "Validation loss (no improvement): 0.047938570380210876\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.047891122102737424  to: 0.04772134423255921\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.04772134423255921  to: 0.0476907342672348\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.04773341715335846\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.04780611097812652\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.0477724939584732\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.04772849082946777\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.04779765009880066\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.0476907342672348  to: 0.047586259245872495\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.047586259245872495  to: 0.047518402338027954\n",
      "Training iteration: 1726\n",
      "Validation loss (no improvement): 0.04756136536598206\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.04776236414909363\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.047518402338027954  to: 0.047332143783569335\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.047332143783569335  to: 0.047160768508911134\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.047424903512001036\n",
      "Training iteration: 1731\n",
      "Validation loss (no improvement): 0.0477732241153717\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.04741771817207337\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.04723533689975738\n",
      "Training iteration: 1734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.04729516506195068\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.04754715859889984\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.04731091856956482\n",
      "Training iteration: 1737\n",
      "Validation loss (no improvement): 0.04719964861869812\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.0472717821598053\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.047361868619918826\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.047165554761886594\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.047160768508911134  to: 0.047054442763328555\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.0471294105052948\n",
      "Training iteration: 1743\n",
      "Validation loss (no improvement): 0.0474086195230484\n",
      "Training iteration: 1744\n",
      "Validation loss (no improvement): 0.04757189154624939\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.04755702018737793\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.047559338808059695\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.047474521398544314\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.04736664891242981\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.04708296358585358\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.047054442763328555  to: 0.047032952308654785\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.04710647165775299\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.04714051187038422\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.04720620512962341\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.04724463522434234\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.04704229831695557\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.047032952308654785  to: 0.04695541262626648\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.04698739945888519\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.04715721011161804\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.04695541262626648  to: 0.04674180150032044\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.04674180150032044  to: 0.046608129143714906\n",
      "Training iteration: 1761\n",
      "Validation loss (no improvement): 0.04671169221401215\n",
      "Training iteration: 1762\n",
      "Validation loss (no improvement): 0.046922311186790466\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.04679512977600098\n",
      "Training iteration: 1764\n",
      "Validation loss (no improvement): 0.04668794572353363\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.046708449721336365\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.046830636262893674\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.046608129143714906  to: 0.0464948832988739\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.0464948832988739  to: 0.046376338601112364\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.04646379053592682\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.04669691026210785\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.046638432145118716\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.04660195410251618\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.046680203080177306\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.046839022636413576\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.04670441746711731\n",
      "Training iteration: 1776\n",
      "Validation loss (no improvement): 0.04642543792724609\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.046376338601112364  to: 0.04633835852146149\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.04647294580936432\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.046689414978027345\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.046685370802879336\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.04647659659385681\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.046406134963035583\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.0465050220489502\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.04678898751735687\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.04661841988563538\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.04633835852146149  to: 0.04620635509490967\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.04620635509490967  to: 0.046102112531661986\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.0462524026632309\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.04652144014835358\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.04646410346031189\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.046317297220230105\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.04629868566989899\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.04637979567050934\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.046490725874900815\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.046102112531661986  to: 0.045999056100845336\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.045999056100845336  to: 0.04582513272762299\n",
      "Training iteration: 1797\n",
      "Validation loss (no improvement): 0.0459840714931488\n",
      "Training iteration: 1798\n",
      "Validation loss (no improvement): 0.046356573700904846\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.04624512195587158\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.04605671763420105\n",
      "Training iteration: 1801\n",
      "Validation loss (no improvement): 0.0460437536239624\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.04618821144104004\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.04643244743347168\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.046188491582870486\n",
      "Training iteration: 1805\n",
      "Validation loss (no improvement): 0.045992416143417356\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.045963454246521\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.046017351746559146\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.04607093930244446\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.046114498376846315\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.04599877893924713\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.04599964022636414\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.046058553457260135\n",
      "Training iteration: 1813\n",
      "Validation loss (no improvement): 0.04618632793426514\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.04582513272762299  to: 0.04571896493434906\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.04571896493434906  to: 0.0454937607049942\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.04552551209926605\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.0456394761800766\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.04567394256591797\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.04576912522315979\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.04585038721561432\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.045648375153541566\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.04558398723602295\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.04564317762851715\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.04581933617591858\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.04552125036716461\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.0454937607049942  to: 0.04546167850494385\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.04550004005432129\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.0455158531665802\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.04553723931312561\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.045603436231613156\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.04577955305576324\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.04555257260799408\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.04546167850494385  to: 0.04545876383781433\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.045494288206100464\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.04554431438446045\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.04545876383781433  to: 0.04528169631958008\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.04528169631958008  to: 0.04522981643676758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.04527353346347809\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.04541007876396179\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.04559925496578217\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.045643216371536253\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.045267677307128905\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.04522981643676758  to: 0.045166808366775515\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.04521665573120117\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.0454507440328598\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.04573614001274109\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.0452188104391098\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.045166808366775515  to: 0.04499613344669342\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.04508566856384277\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.04536670744419098\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.04572581648826599\n",
      "Training iteration: 1852\n",
      "Validation loss (no improvement): 0.045528832077980044\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.04532984793186188\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.045292052626609805\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.045426291227340695\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.04557391107082367\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.04559477865695953\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.04551082253456116\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.0453262984752655\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.04519316554069519\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.04520583748817444\n",
      "Training iteration: 1862\n",
      "Validation loss (no improvement): 0.04521152377128601\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.04504682123661041\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.045065665245056154\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.04519051611423493\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.045500296354293826\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.04514096677303314\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.04499613344669342  to: 0.04482724666595459\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.04482724666595459  to: 0.044754895567893985\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.04488487839698792\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.04507741034030914\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.04513140320777893\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.044992169737815856\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.04486469328403473\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.04486282467842102\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.04500690996646881\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.04521355628967285\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.04487122595310211\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.044754895567893985  to: 0.04474032819271088\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.04474570155143738\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.04488268792629242\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.04474032819271088  to: 0.044634073972702026\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.044634073972702026  to: 0.04458041787147522\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.044744402170181274\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.045076784491539\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.04527732729911804\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.044638365507125854\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.04458041787147522  to: 0.04436568319797516\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.044552117586135864\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.04506894052028656\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.04529412388801575\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.04468721449375153\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.044604793190956116\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.04476568102836609\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.045098152756690976\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.045536893606185916\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.044930630922317506\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.04458624720573425\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.04450700283050537\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.04460737109184265\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.04477898180484772\n",
      "Training iteration: 1902\n",
      "Validation loss (no improvement): 0.044649696350097655\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.044595494866371155\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.044705352187156676\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.04497202038764954\n",
      "Training iteration: 1906\n",
      "Validation loss (no improvement): 0.04510877728462219\n",
      "Training iteration: 1907\n",
      "Validation loss (no improvement): 0.04448509216308594\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.04436568319797516  to: 0.04420996606349945\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.04426259994506836\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.04453640580177307\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.04478479325771332\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.04451305270195007\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.04430355131626129\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.044284087419509885\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.044385313987731934\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.044605812430381774\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.04456433653831482\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.044230327010154724\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.04420996606349945  to: 0.04411196708679199\n",
      "Training iteration: 1920\n",
      "Validation loss (no improvement): 0.04415073394775391\n",
      "Training iteration: 1921\n",
      "Validation loss (no improvement): 0.04427784383296966\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.04411196708679199  to: 0.04398814141750336\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.04398814141750336  to: 0.043845510482788085\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.04393598437309265\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.04422124326229095\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.04459543228149414\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.04413058161735535\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.04387076497077942\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.043845510482788085  to: 0.04382852613925934\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.04391023516654968\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.04411281645298004\n",
      "Training iteration: 1932\n",
      "Validation loss (no improvement): 0.04412528574466705\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.043980914354324344\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.04395180344581604\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.044064635038375856\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.04423263669013977\n",
      "Training iteration: 1937\n",
      "Validation loss (no improvement): 0.04398164749145508\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.04386151432991028\n",
      "Training iteration: 1939\n",
      "Validation loss (no improvement): 0.04387679994106293\n",
      "Training iteration: 1940\n",
      "Validation loss (no improvement): 0.04408727586269379\n",
      "Training iteration: 1941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0438467413187027\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.04382852613925934  to: 0.043710464239120485\n",
      "Training iteration: 1943\n",
      "Validation loss (no improvement): 0.04376712739467621\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.04388956129550934\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.04406288266181946\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.044154304265975955\n",
      "Training iteration: 1947\n",
      "Validation loss (no improvement): 0.04390807151794433\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.04371647834777832\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.043710464239120485  to: 0.0436610996723175\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.0436610996723175  to: 0.04360365867614746\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.04360365867614746  to: 0.043491512537002563\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.043491512537002563  to: 0.04343542456626892\n",
      "Training iteration: 1953\n",
      "Validation loss (no improvement): 0.04352328181266785\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.04343542456626892  to: 0.04335526823997497\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.04335526823997497  to: 0.04332935214042664\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.043458718061447146\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.04359234869480133\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.04332935214042664  to: 0.043311303853988646\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.043311303853988646  to: 0.04321723878383636\n",
      "Training iteration: 1960\n",
      "Validation loss (no improvement): 0.043311518430709836\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.043532782793045045\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.04372921586036682\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.043543973565101625\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.04329846799373627\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.04321723878383636  to: 0.04316154420375824\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.0432243287563324\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.04348972737789154\n",
      "Training iteration: 1968\n",
      "Validation loss (no improvement): 0.04371747374534607\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.04336767196655274\n",
      "Training iteration: 1970\n",
      "Validation loss (no improvement): 0.043426379561424255\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.043561536073684695\n",
      "Training iteration: 1972\n",
      "Validation loss (no improvement): 0.04362744688987732\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.043637537956237794\n",
      "Training iteration: 1974\n",
      "Validation loss (no improvement): 0.043454509973526\n",
      "Training iteration: 1975\n",
      "Validation loss (no improvement): 0.04342024326324463\n",
      "Training iteration: 1976\n",
      "Validation loss (no improvement): 0.0433638870716095\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.04331721365451813\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.04341277182102203\n",
      "Training iteration: 1979\n",
      "Validation loss (no improvement): 0.043235141038894656\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.04316154420375824  to: 0.04312556684017181\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.04312556684017181  to: 0.043124324083328246\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.04323823451995849\n",
      "Training iteration: 1983\n",
      "Validation loss (no improvement): 0.04344702661037445\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.043124324083328246  to: 0.04307892918586731\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.04307892918586731  to: 0.04284406304359436\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.04284406304359436  to: 0.04282536506652832\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.043054229021072386\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.04344979822635651\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.043447738885879515\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.043164783716201784\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.042953959107398985\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.042918306589126584\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.043123874068260196\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.043304461240768435\n",
      "Training iteration: 1995\n",
      "Validation loss (no improvement): 0.043159016966819765\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.043048128485679626\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.042970743775367734\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.04297400414943695\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.043054240942001346\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.04282536506652832  to: 0.04279168248176575\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.04279168248176575  to: 0.04268437922000885\n",
      "Training iteration: 2002\n",
      "Validation loss (no improvement): 0.04276305735111237\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.042784538865089414\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.04274140894412994\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.04268437922000885  to: 0.04262566566467285\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.04262566566467285  to: 0.042563825845718384\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.042563825845718384  to: 0.0425299882888794\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.04263072907924652\n",
      "Training iteration: 2009\n",
      "Validation loss (no improvement): 0.04283599853515625\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.0425299882888794  to: 0.04238653779029846\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.04238653779029846  to: 0.04220515191555023\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.04227283596992493\n",
      "Training iteration: 2013\n",
      "Validation loss (no improvement): 0.04248653948307037\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.042535224556922914\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.0424822986125946\n",
      "Training iteration: 2016\n",
      "Validation loss (no improvement): 0.04247569441795349\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.042560082674026486\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.042672911286354066\n",
      "Training iteration: 2019\n",
      "Validation loss (no improvement): 0.04232939779758453\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.04220515191555023  to: 0.042178550362586976\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.04221064448356628\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.04246667325496674\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.042289289832115176\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.04219021797180176\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.042223206162452696\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.042354089021682736\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.042665344476699826\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.04286998808383942\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.042319083213806154\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.042178550362586976  to: 0.04209675788879395\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.04216582179069519\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.04247420430183411\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.04268786311149597\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.04233408570289612\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.042238831520080566\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.04227396845817566\n",
      "Training iteration: 2037\n",
      "Validation loss (no improvement): 0.04242278039455414\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.04275485873222351\n",
      "Training iteration: 2039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.04245328903198242\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.042235589027404784\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.042191901803016664\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.04217272400856018\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.04214841425418854\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.042109861969947815\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.04211136400699615\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.04209675788879395  to: 0.04205638468265534\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.04208648204803467\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.04205638468265534  to: 0.04180518090724945\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.04180518090724945  to: 0.041693711280822755\n",
      "Training iteration: 2050\n",
      "Validation loss (no improvement): 0.041717308759689334\n",
      "Training iteration: 2051\n",
      "Validation loss (no improvement): 0.041900402307510375\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.041770124435424806\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.041693711280822755  to: 0.041678118705749514\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.041678118705749514  to: 0.04158174991607666\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.041653615236282346\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.04181776940822601\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.041908422112464906\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.04192047119140625\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.04169944822788239\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.04158174991607666  to: 0.04157625138759613\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.04157625138759613  to: 0.04157423973083496\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): 0.04169647097587585\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.04157423973083496  to: 0.04147709906101227\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.04147709906101227  to: 0.041398486495018004\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.04140985608100891\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.04151619076728821\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.041667380928993226\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.041686910390853885\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.04165791869163513\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.04165501594543457\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.04165518283843994\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.041434329748153684\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.041398486495018004  to: 0.04137702584266663\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.04137702584266663  to: 0.04132640361785889\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.0413308709859848\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.04136093258857727\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.04132640361785889  to: 0.04116632044315338\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.04116632044315338  to: 0.0411547601222992\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): 0.0412724107503891\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): 0.04153905808925629\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.04136557579040527\n",
      "Training iteration: 2082\n",
      "Validation loss (no improvement): 0.041224971413612366\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): 0.041242951154708864\n",
      "Training iteration: 2084\n",
      "Validation loss (no improvement): 0.04142128527164459\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.04160591959953308\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): 0.04143158495426178\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): 0.04128542840480805\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): 0.04118963778018951\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): 0.04121158123016357\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.041334515810012816\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.0411547601222992  to: 0.04112992286682129\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.04112992286682129  to: 0.04106793999671936\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.0410946935415268\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.04114223420619965\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.041176286339759824\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.04106793999671936  to: 0.04093120694160461\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.04093120694160461  to: 0.04087108671665192\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.040950292348861696\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.04109465479850769\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.041224044561386106\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.041202545166015625\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.040951895713806155\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.04087108671665192  to: 0.04077048301696777\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.04077048301696777  to: 0.04075819551944733\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.040920719504356384\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.04102175235748291\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.04077385365962982\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.04075819551944733  to: 0.040707483887672424\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.04078435003757477\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.04090384840965271\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.04083716869354248\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.040774470567703246\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.04072030484676361\n",
      "Training iteration: 2114\n",
      "Validation loss (no improvement): 0.040776300430297854\n",
      "Training iteration: 2115\n",
      "Validation loss (no improvement): 0.04079417586326599\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.04072272777557373\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.040707483887672424  to: 0.04050510823726654\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.04050510823726654  to: 0.0404660165309906\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.040548700094223025\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.040655198693275454\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.040530234575271606\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.0404660165309906  to: 0.04045088887214661\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.04058764576911926\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.04077211022377014\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.04084393382072449\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.04075474739074707\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.04062136113643646\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.040603598952293395\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.040668338537216187\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.040491312742233276\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.04045088887214661  to: 0.04038386344909668\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.04044083058834076\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.04054365754127502\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.04064987599849701\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.040529781579971315\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.04045239388942719\n",
      "Training iteration: 2137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.04043187499046326\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.04038386344909668  to: 0.040368327498435976\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.040368327498435976  to: 0.04027983546257019\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.04027983546257019  to: 0.040002697706222536\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.040002697706222536  to: 0.04000137448310852\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.04016366004943848\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.040418806672096255\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.040244683623313904\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.040245255827903746\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.040322312712669374\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.04051479697227478\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.04067643284797669\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.04038024544715881\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): 0.040153568983078\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): 0.04012698233127594\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.04023967683315277\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.04035848081111908\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): 0.040153303742408754\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.0400383859872818\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): 0.04007957577705383\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.04032435417175293\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.04034946858882904\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.04000137448310852  to: 0.03993342518806457\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.03993971943855286\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.040082302689552304\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.04033117890357971\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.04005747437477112\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.03997840285301209\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): 0.0401024729013443\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): 0.04035023152828217\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.04040655195713043\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.040013113617897035\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.03994584083557129\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.03998680710792542\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.040178313851356506\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.04017529487609863\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.03993342518806457  to: 0.039842909574508666\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.039842909574508666  to: 0.03980270028114319\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.03992016911506653\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.04011513292789459\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.03991245329380035\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.03983061611652374\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.03980270028114319  to: 0.03975560069084168\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.03975560069084168  to: 0.03967588543891907\n",
      "Training iteration: 2181\n",
      "Validation loss (no improvement): 0.039677613973617555\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.03967588543891907  to: 0.03952818512916565\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.03952818512916565  to: 0.039333558082580565\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.039344263076782224\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): 0.039437538385391234\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): 0.039337530732154846\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.039333558082580565  to: 0.03921312689781189\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.03921312689781189  to: 0.03918020129203796\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.039284294843673705\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.03918020129203796  to: 0.03905081152915955\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.03905081152915955  to: 0.038978511095047\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): 0.03914341330528259\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.039380258321762084\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.03939096331596374\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.03926706910133362\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.03913678228855133\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.03906466364860535\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): 0.039080098271369934\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): 0.03901887834072113\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.038978511095047  to: 0.03890213370323181\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.03890213370323181  to: 0.03889437615871429\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.03889437615871429  to: 0.0388855516910553\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.0388855516910553  to: 0.038807344436645505\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): 0.03884977996349335\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.038807344436645505  to: 0.038690385222434995\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.038690385222434995  to: 0.03868319392204285\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.03882567882537842\n",
      "Training iteration: 2208\n",
      "Validation loss (no improvement): 0.03905225396156311\n",
      "Training iteration: 2209\n",
      "Validation loss (no improvement): 0.03896391987800598\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.03879905641078949\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.03872690498828888\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.03880264163017273\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.03891254961490631\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.03895113468170166\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.03891646265983582\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.03898903727531433\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): 0.0389259546995163\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.03880528509616852\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.03868319392204285  to: 0.03850728571414948\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.03850728571414948  to: 0.03845134377479553\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): 0.03849509656429291\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.038621658086776735\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.038673803210258484\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.03855842053890228\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): 0.03851887881755829\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.03848972320556641\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.038714200258255005\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.03886023163795471\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.03852318823337555\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.03856194019317627\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.03865397572517395\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.038872340321540834\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.03873043656349182\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.03852662146091461\n",
      "Training iteration: 2235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03845307230949402\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): 0.038504427671432494\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.03850938379764557\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.03845134377479553  to: 0.038247022032737735\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.038247022032737735  to: 0.03816859126091003\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.03823506236076355\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.03846522271633148\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.038275536894798276\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.0381714403629303\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.03816859126091003  to: 0.03813797831535339\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.03829579651355743\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.03847717642784119\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): 0.03822356462478638\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.03813797831535339  to: 0.038083013892173764\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.038120871782302855\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): 0.0382255494594574\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): 0.03838798105716705\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.038083013892173764  to: 0.03806212842464447\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.03806212842464447  to: 0.03795769214630127\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.038008618354797366\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.03819901943206787\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.038331881165504456\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.03839632868766785\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.03841065764427185\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.03828983306884766\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.038231420516967776\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.03810333609580994\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.03795769214630127  to: 0.03792363107204437\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.03792363107204437  to: 0.03783485591411591\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.03783485591411591  to: 0.03782461285591125\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.03782461285591125  to: 0.037789613008499146\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.037789613008499146  to: 0.037756696343421936\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.037756696343421936  to: 0.03775627017021179\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.03779768049716949\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.03775627017021179  to: 0.037657815217971805\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.037657815217971805  to: 0.03762193322181702\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.03769833445549011\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): 0.03785799145698547\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): 0.03786671757698059\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): 0.03765174746513367\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.03762193322181702  to: 0.03760311007499695\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): 0.03760835230350494\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): 0.037892335653305055\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.03777970373630524\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.037659460306167604\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.03769156336784363\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.037840145826339724\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.03784549832344055\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): 0.03771766722202301\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.03760311007499695  to: 0.03758324682712555\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): 0.03759035170078277\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): 0.03768327534198761\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): 0.03760293126106262\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.03758324682712555  to: 0.03739359676837921\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.03739359676837921  to: 0.03737802505493164\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.03737802505493164  to: 0.03736152350902557\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.037441062927246097\n",
      "Training iteration: 2292\n",
      "Improved validation loss from: 0.03736152350902557  to: 0.03717953562736511\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.03717953562736511  to: 0.03709989488124847\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): 0.037192076444625854\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): 0.037202635407447816\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): 0.03721594214439392\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.03730908632278442\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.03735761344432831\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): 0.037276247143745424\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.03721303939819336\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): 0.037206584215164186\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.037118536233901975\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.03709989488124847  to: 0.03700322806835175\n",
      "Training iteration: 2304\n",
      "Validation loss (no improvement): 0.03709891438484192\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.03716092705726624\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): 0.03721391558647156\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): 0.03724842667579651\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): 0.03737860321998596\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): 0.0374934583902359\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): 0.037273901700973514\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): 0.037154984474182126\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.0371847003698349\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.0374083936214447\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.03728445172309876\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.0370985746383667\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.03709120750427246\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): 0.03715339601039887\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.03722715973854065\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.03700432777404785\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.03700322806835175  to: 0.036859676241874695\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.0368798166513443\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.036859676241874695  to: 0.036842772364616395\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.036842772364616395  to: 0.036757934093475345\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.036757934093475345  to: 0.03669476509094238\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.03669476509094238  to: 0.03668237626552582\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): 0.03681055009365082\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.03685793876647949\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.036731451749801636\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.03674142956733704\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): 0.036732268333435056\n",
      "Training iteration: 2331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03668237626552582  to: 0.03662944436073303\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.03662944436073303  to: 0.03661537170410156\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 0.036729741096496585\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): 0.0366422712802887\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.03661537170410156  to: 0.036470192670822146\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.036470192670822146  to: 0.03641971349716187\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.03641971349716187  to: 0.03636184334754944\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.03636184334754944  to: 0.03631209433078766\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.03631209433078766  to: 0.0362664133310318\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): 0.036363035440444946\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.0362664133310318  to: 0.036213383078575134\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.0363018125295639\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.03650155365467071\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): 0.03653988540172577\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.03643878698348999\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.03651051223278046\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.03669423460960388\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.03683193922042847\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): 0.03687107264995575\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.036751431226730344\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.036591508984565736\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.03656952083110809\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): 0.036612921953201295\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.03635659217834473\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.03625492751598358\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.036213383078575134  to: 0.03618496060371399\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.03618496060371399  to: 0.03613062798976898\n",
      "Training iteration: 2358\n",
      "Improved validation loss from: 0.03613062798976898  to: 0.03602849841117859\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.03602849841117859  to: 0.036008843779563905\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.036008843779563905  to: 0.03598238527774811\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.03599126935005188\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.03607035577297211\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.03599096238613129\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.03598238527774811  to: 0.035917085409164426\n",
      "Training iteration: 2365\n",
      "Improved validation loss from: 0.035917085409164426  to: 0.03589627146720886\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.0360520601272583\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.036090216040611266\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.036098480224609375\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): 0.036029666662216187\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.03601809442043304\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.03603549897670746\n",
      "Training iteration: 2372\n",
      "Improved validation loss from: 0.03589627146720886  to: 0.03582644462585449\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.03582644462585449  to: 0.035788041353225705\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.03594657778739929\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.03618730902671814\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.03619051277637482\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.0360748678445816\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.035948926210403444\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.035998374223709106\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): 0.03610060513019562\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.03597915768623352\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.0359057366847992\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.03590289652347565\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.03597251772880554\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.03598052859306335\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.03595028519630432\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.03587444424629212\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): 0.035906273126602176\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.035788041353225705  to: 0.035775893926620485\n",
      "Training iteration: 2390\n",
      "Improved validation loss from: 0.035775893926620485  to: 0.0357088565826416\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.0357088565826416  to: 0.0356656551361084\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): 0.03570453226566315\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.0356656551361084  to: 0.03566531240940094\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.03566531240940094  to: 0.03564302325248718\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.03564302325248718  to: 0.03554630875587463\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.03554630875587463  to: 0.03549168407917023\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.03549168407917023  to: 0.03546956181526184\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.03546956181526184  to: 0.03543329834938049\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.03555914163589478\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.03548470139503479\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.03551673889160156\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.035711854696273804\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): 0.035722023248672484\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.03561099171638489\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.035626888275146484\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.03566901385784149\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): 0.035725101828575134\n",
      "Training iteration: 2408\n",
      "Validation loss (no improvement): 0.03572743237018585\n",
      "Training iteration: 2409\n",
      "Validation loss (no improvement): 0.035875290632247925\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): 0.03577205538749695\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.035753652453422546\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.03582603633403778\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.03580893576145172\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.03558974862098694\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.03551150858402252\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.0354895681142807\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.03553479015827179\n",
      "Training iteration: 2418\n",
      "Improved validation loss from: 0.03543329834938049  to: 0.035422173142433164\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.035422173142433164  to: 0.035348552465438846\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.035416021943092346\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.03541659712791443\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.035348552465438846  to: 0.035206574201583865\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.035206574201583865  to: 0.035094627737998964\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.035094627737998964  to: 0.03508118689060211\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.03508118689060211  to: 0.03496145009994507\n",
      "Training iteration: 2426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03504287600517273\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.03514989018440247\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.03513813018798828\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.03512409627437592\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.035252544283866885\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.03527132570743561\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.03518431186676026\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.035208120942115784\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.03506892919540405\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.03504579067230225\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.035123443603515624\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.035127490758895874\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.03505232334136963\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.03497596085071564\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.035025101900100705\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.03500625789165497\n",
      "Training iteration: 2442\n",
      "Improved validation loss from: 0.03496145009994507  to: 0.03488851189613342\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.034947183728218076\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.03502206802368164\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.03495535552501679\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): 0.03500761091709137\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.03503184616565704\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.03495742678642273\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.034911471605300906\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.03488851189613342  to: 0.034747928380966187\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.034747928380966187  to: 0.034646323323249816\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): 0.034700730443000795\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.034694337844848634\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.034646323323249816  to: 0.03459576964378357\n",
      "Training iteration: 2455\n",
      "Validation loss (no improvement): 0.03465673327445984\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): 0.03473303914070129\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.034619078040122986\n",
      "Training iteration: 2458\n",
      "Improved validation loss from: 0.03459576964378357  to: 0.03457103967666626\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.034636884927749634\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.03474007248878479\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.03457103967666626  to: 0.034531000256538394\n",
      "Training iteration: 2462\n",
      "Improved validation loss from: 0.034531000256538394  to: 0.03451118469238281\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.03469117283821106\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.03472164273262024\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.0346983015537262\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.034651336073875424\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.03458406925201416\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.03467234969139099\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): 0.034545129537582396\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.03451118469238281  to: 0.03438261151313782\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.034414041042327884\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.03448607921600342\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.034421682357788086\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.03438261151313782  to: 0.034316655993461606\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.03440837860107422\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.03462321162223816\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.034603160619735715\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.03454651236534119\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.03456067442893982\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.03451722264289856\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.034417441487312316\n",
      "Training iteration: 2482\n",
      "Validation loss (no improvement): 0.0344143807888031\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.034316655993461606  to: 0.034236523509025577\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.034236523509025577  to: 0.03416761457920074\n",
      "Training iteration: 2485\n",
      "Improved validation loss from: 0.03416761457920074  to: 0.034079113602638246\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.03413029313087464\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.03413832485675812\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): 0.03409613966941834\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.03412284255027771\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): 0.03414748013019562\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.03435056209564209\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.03433683514595032\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.0341610312461853\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.03414289355278015\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): 0.03412800431251526\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.034084805846214296\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.034079113602638246  to: 0.03382232189178467\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.03382232189178467  to: 0.03376325368881226\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.03391907215118408\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.03431500792503357\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.033956438302993774\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.03386038243770599\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.03390355408191681\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.0344634473323822\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.03458957672119141\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.03409191370010376\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.03411203324794769\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.034237608313560486\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.03432784974575043\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.03416333794593811\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.03396584987640381\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.03389202952384949\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.03397414982318878\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.03412428200244903\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.03385914266109467\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.03376325368881226  to: 0.03370969295501709\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.03370969295501709  to: 0.033693131804466245\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.034071537852287295\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.03391490578651428\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.033859390020370486\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): 0.03396243751049042\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.034177303314208984\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.034275537729263304\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.03385556936264038\n",
      "Training iteration: 2525\n",
      "Improved validation loss from: 0.033693131804466245  to: 0.03367648720741272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.033699828386306765\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): 0.033783060312271115\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: 0.03367648720741272  to: 0.03365454971790314\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.03365454971790314  to: 0.03360034823417664\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.0337249606847763\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.03360034823417664  to: 0.033583018183708194\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: 0.033583018183708194  to: 0.033442872762680056\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.0335788905620575\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.033442872762680056  to: 0.03329828679561615\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.03329828679561615  to: 0.03326235115528107\n",
      "Training iteration: 2536\n",
      "Validation loss (no improvement): 0.03333696722984314\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.033449241518974306\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): 0.03357582688331604\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.03366225361824036\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.03378889560699463\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.03375429213047028\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.033266210556030275\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.03326235115528107  to: 0.033064594864845274\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.03309442102909088\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.03322201371192932\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.033064594864845274  to: 0.033009305596351624\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.03307988047599793\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.03322383165359497\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.03336306512355804\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.033260154724121097\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.033140912652015686\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.03304176926612854\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: 0.033009305596351624  to: 0.032996976375579835\n",
      "Training iteration: 2554\n",
      "Improved validation loss from: 0.032996976375579835  to: 0.032871085405349734\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: 0.032871085405349734  to: 0.032812243700027464\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: 0.032812243700027464  to: 0.032777810096740724\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.032983437180519104\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.0328427255153656\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.032902884483337405\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.03307158350944519\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.03319254517555237\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.03282374143600464\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.032777810096740724  to: 0.03263650834560394\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.032684943079948424\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.033247795701026914\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.03332003653049469\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.032842966914176944\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.03296571373939514\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.03315490782260895\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.033341234922409056\n",
      "Training iteration: 2571\n",
      "Validation loss (no improvement): 0.03298104405403137\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.03265504240989685\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.032643461227416994\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.03293534219264984\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.03277705311775207\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.03263650834560394  to: 0.03257736265659332\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.032668524980545045\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.032709404826164246\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.032664552330970764\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.032590088248252866\n",
      "Training iteration: 2581\n",
      "Improved validation loss from: 0.03257736265659332  to: 0.032372626662254336\n",
      "Training iteration: 2582\n",
      "Improved validation loss from: 0.032372626662254336  to: 0.03220836520195007\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.03223505616188049\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.03220836520195007  to: 0.03208329677581787\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.03208329677581787  to: 0.032030874490737916\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.03225522637367249\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.03266294002532959\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.032322609424591066\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.032249432802200315\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.03242434859275818\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.03275380730628967\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.03252644538879394\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.032293477654457094\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): 0.03219760656356811\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): 0.032177197933197024\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.03228186368942261\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.032554760575294495\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.03228035867214203\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.032142838835716246\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.03212443292140961\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): 0.032121682167053224\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.032030874490737916  to: 0.031910866498947144\n",
      "Training iteration: 2603\n",
      "Improved validation loss from: 0.031910866498947144  to: 0.03181042075157166\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.03181042075157166  to: 0.03179385960102081\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.031990638375282286\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): 0.03203558921813965\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.031867372989654544\n",
      "Training iteration: 2608\n",
      "Improved validation loss from: 0.03179385960102081  to: 0.03173408508300781\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.03173408508300781  to: 0.03169000744819641\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): 0.031913644075393675\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.03169000744819641  to: 0.031461086869239804\n",
      "Training iteration: 2612\n",
      "Improved validation loss from: 0.031461086869239804  to: 0.03137151300907135\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.031483468413352964\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): 0.03171908259391785\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.03169841170310974\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.031601744890213015\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.03156732618808746\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.031644657254219055\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.031735047698020935\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.03153570294380188\n",
      "Training iteration: 2621\n",
      "Improved validation loss from: 0.03137151300907135  to: 0.03133266568183899\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.03133266568183899  to: 0.03132835924625397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.031468158960342406\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.031548982858657836\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.03145175576210022\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.0314106285572052\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.03159001767635346\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.031861263513565066\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.03149127960205078\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.03132835924625397  to: 0.03117983937263489\n",
      "Training iteration: 2631\n",
      "Improved validation loss from: 0.03117983937263489  to: 0.031099849939346315\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.031161585450172426\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.03131852447986603\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.031226176023483276\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.03126338422298432\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.031302076578140256\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.03125070631504059\n",
      "Training iteration: 2638\n",
      "Improved validation loss from: 0.031099849939346315  to: 0.030847841501235963\n",
      "Training iteration: 2639\n",
      "Improved validation loss from: 0.030847841501235963  to: 0.030679306387901305\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.03077074885368347\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.030800989270210265\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.030679306387901305  to: 0.030568242073059082\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.030568242073059082  to: 0.03053986132144928\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.03088516891002655\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.03142159879207611\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.03105800747871399\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): 0.03078504204750061\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.030735620856285097\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.030968159437179565\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.030917954444885255\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.030578118562698365\n",
      "Training iteration: 2652\n",
      "Improved validation loss from: 0.03053986132144928  to: 0.03050982058048248\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.030549782514572143\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.030793818831443786\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.03051319718360901\n",
      "Training iteration: 2656\n",
      "Improved validation loss from: 0.03050982058048248  to: 0.030425339937210083\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.03045450448989868\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.03065716028213501\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.030439358949661256\n",
      "Training iteration: 2660\n",
      "Improved validation loss from: 0.030425339937210083  to: 0.03031010031700134\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.030369502305984498\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.030509933829307556\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.030404478311538696\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.030331915616989134\n",
      "Training iteration: 2665\n",
      "Improved validation loss from: 0.03031010031700134  to: 0.030302399396896364\n",
      "Training iteration: 2666\n",
      "Improved validation loss from: 0.030302399396896364  to: 0.030234992504119873\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.030295020341873168\n",
      "Training iteration: 2668\n",
      "Improved validation loss from: 0.030234992504119873  to: 0.029801321029663087\n",
      "Training iteration: 2669\n",
      "Improved validation loss from: 0.029801321029663087  to: 0.029711619019508362\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.029794752597808838\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.030166703462600707\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): 0.03018655478954315\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): 0.030147379636764525\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.030120545625686647\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.030225518345832824\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.0302335262298584\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.030001649260520936\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.029811948537826538\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.029738220572471618\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.029769986867904663\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): 0.029797512292861938\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.02992669343948364\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.029756054282188416\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.029711619019508362  to: 0.02966983914375305\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.029692548513412475\n",
      "Training iteration: 2686\n",
      "Improved validation loss from: 0.02966983914375305  to: 0.02958267629146576\n",
      "Training iteration: 2687\n",
      "Improved validation loss from: 0.02958267629146576  to: 0.029383856058120727\n",
      "Training iteration: 2688\n",
      "Improved validation loss from: 0.029383856058120727  to: 0.029264026880264284\n",
      "Training iteration: 2689\n",
      "Improved validation loss from: 0.029264026880264284  to: 0.02922406792640686\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.029303798079490663\n",
      "Training iteration: 2691\n",
      "Improved validation loss from: 0.02922406792640686  to: 0.02911403775215149\n",
      "Training iteration: 2692\n",
      "Improved validation loss from: 0.02911403775215149  to: 0.02902844548225403\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.029102903604507447\n",
      "Training iteration: 2694\n",
      "Improved validation loss from: 0.02902844548225403  to: 0.02887771129608154\n",
      "Training iteration: 2695\n",
      "Improved validation loss from: 0.02887771129608154  to: 0.028862982988357544\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.029003629088401796\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.028862982988357544  to: 0.028782612085342406\n",
      "Training iteration: 2698\n",
      "Improved validation loss from: 0.028782612085342406  to: 0.02874998450279236\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.028844141960144044\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.02902023196220398\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.029089909791946412\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.028910022974014283\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.02886771261692047\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.028952980041503908\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.028875640034675597\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.02891032099723816\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.028977081179618835\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.028771981596946716\n",
      "Training iteration: 2709\n",
      "Improved validation loss from: 0.02874998450279236  to: 0.028718408942222596\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.02878797650337219\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.028895860910415648\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.02883053421974182\n",
      "Training iteration: 2713\n",
      "Improved validation loss from: 0.028718408942222596  to: 0.02860843241214752\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.02860843241214752  to: 0.028553277254104614\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.028632760047912598\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.028709572553634644\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.02887229323387146\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.02880428433418274\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.02869613766670227\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.02870965301990509\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.028934094309806823\n",
      "Training iteration: 2722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.028553277254104614  to: 0.02833806872367859\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.02833806872367859  to: 0.028236865997314453\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.02839536964893341\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.028668469190597533\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.028533726930618286\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.028453612327575685\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): 0.02846343517303467\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.02853383719921112\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.02860807776451111\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.028337138891220092\n",
      "Training iteration: 2732\n",
      "Improved validation loss from: 0.028236865997314453  to: 0.028133609890937807\n",
      "Training iteration: 2733\n",
      "Improved validation loss from: 0.028133609890937807  to: 0.027957883477211\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.02838086187839508\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): 0.028805503249168397\n",
      "Training iteration: 2736\n",
      "Improved validation loss from: 0.027957883477211  to: 0.02790023684501648\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.028052321076393126\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.02827439606189728\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.028916317224502563\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.02844310402870178\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.028181809186935424\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.02816661596298218\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.028610271215438843\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.02885546088218689\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.028207477927207947\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.028170278668403624\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.028173360228538512\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.028439536690711975\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.02816706895828247\n",
      "Training iteration: 2750\n",
      "Improved validation loss from: 0.02790023684501648  to: 0.027896770834922792\n",
      "Training iteration: 2751\n",
      "Improved validation loss from: 0.027896770834922792  to: 0.027749547362327577\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.027749547362327577  to: 0.027724307775497437\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.027724307775497437  to: 0.02744612693786621\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.02744612693786621  to: 0.027217552065849304\n",
      "Training iteration: 2755\n",
      "Improved validation loss from: 0.027217552065849304  to: 0.027187055349349974\n",
      "Training iteration: 2756\n",
      "Improved validation loss from: 0.027187055349349974  to: 0.027122318744659424\n",
      "Training iteration: 2757\n",
      "Improved validation loss from: 0.027122318744659424  to: 0.02678232789039612\n",
      "Training iteration: 2758\n",
      "Improved validation loss from: 0.02678232789039612  to: 0.026655450463294983\n",
      "Training iteration: 2759\n",
      "Improved validation loss from: 0.026655450463294983  to: 0.02663554549217224\n",
      "Training iteration: 2760\n",
      "Improved validation loss from: 0.02663554549217224  to: 0.026514869928359986\n",
      "Training iteration: 2761\n",
      "Improved validation loss from: 0.026514869928359986  to: 0.026436635851860048\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.026524311304092406\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.026510980725288392\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.02655557692050934\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.026444339752197267\n",
      "Training iteration: 2766\n",
      "Improved validation loss from: 0.026436635851860048  to: 0.026362240314483643\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.02638101875782013\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.026392561197280884\n",
      "Training iteration: 2769\n",
      "Improved validation loss from: 0.026362240314483643  to: 0.026353687047958374\n",
      "Training iteration: 2770\n",
      "Improved validation loss from: 0.026353687047958374  to: 0.02625795304775238\n",
      "Training iteration: 2771\n",
      "Improved validation loss from: 0.02625795304775238  to: 0.026255294680595398\n",
      "Training iteration: 2772\n",
      "Improved validation loss from: 0.026255294680595398  to: 0.026199916005134584\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.026276573538780212\n",
      "Training iteration: 2774\n",
      "Improved validation loss from: 0.026199916005134584  to: 0.026157766580581665\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.026313132047653197\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.026468023657798767\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.026259982585906984\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.02629680633544922\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.026586127281188966\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.026734715700149535\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.026436415314674378\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): 0.02653917372226715\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.026528450846672057\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.026611453294754027\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.026637205481529237\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.02643168568611145\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.02629680037498474\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.026337453722953798\n",
      "Training iteration: 2789\n",
      "Improved validation loss from: 0.026157766580581665  to: 0.026150208711624146\n",
      "Training iteration: 2790\n",
      "Improved validation loss from: 0.026150208711624146  to: 0.02607777714729309\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.02619212567806244\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.026136136054992674\n",
      "Training iteration: 2793\n",
      "Improved validation loss from: 0.02607777714729309  to: 0.026050040125846864\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.02606569230556488\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.026074913144111634\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.02605539858341217\n",
      "Training iteration: 2797\n",
      "Improved validation loss from: 0.026050040125846864  to: 0.02584535479545593\n",
      "Training iteration: 2798\n",
      "Improved validation loss from: 0.02584535479545593  to: 0.02542285919189453\n",
      "Training iteration: 2799\n",
      "Improved validation loss from: 0.02542285919189453  to: 0.025293806195259096\n",
      "Training iteration: 2800\n",
      "Improved validation loss from: 0.025293806195259096  to: 0.02520896792411804\n",
      "Training iteration: 2801\n",
      "Improved validation loss from: 0.02520896792411804  to: 0.02515753507614136\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.02537333369255066\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.02538518011569977\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.025307759642601013\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.025194230675697326\n",
      "Training iteration: 2806\n",
      "Improved validation loss from: 0.02515753507614136  to: 0.025147071480751036\n",
      "Training iteration: 2807\n",
      "Improved validation loss from: 0.025147071480751036  to: 0.025066855549812316\n",
      "Training iteration: 2808\n",
      "Improved validation loss from: 0.025066855549812316  to: 0.024819818139076234\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.024843916296958923\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.02517281174659729\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.025395074486732484\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): 0.0250063955783844\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.024915952980518342\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): 0.02520730495452881\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): 0.02539803981781006\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.025012058019638062\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.025088346004486083\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.02502451539039612\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.025172239542007445\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.025402966141700744\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.024949660897254942\n",
      "Training iteration: 2822\n",
      "Improved validation loss from: 0.024819818139076234  to: 0.024774834513664246\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.024842670559883116\n",
      "Training iteration: 2824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.024774834513664246  to: 0.02473578751087189\n",
      "Training iteration: 2825\n",
      "Improved validation loss from: 0.02473578751087189  to: 0.02459755390882492\n",
      "Training iteration: 2826\n",
      "Improved validation loss from: 0.02459755390882492  to: 0.024593672156333922\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.024660997092723846\n",
      "Training iteration: 2828\n",
      "Improved validation loss from: 0.024593672156333922  to: 0.0245180606842041\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.0245180606842041  to: 0.02430112361907959\n",
      "Training iteration: 2830\n",
      "Improved validation loss from: 0.02430112361907959  to: 0.02411523312330246\n",
      "Training iteration: 2831\n",
      "Improved validation loss from: 0.02411523312330246  to: 0.024030394852161407\n",
      "Training iteration: 2832\n",
      "Improved validation loss from: 0.024030394852161407  to: 0.023930275440216066\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.023933443427085876\n",
      "Training iteration: 2834\n",
      "Improved validation loss from: 0.023930275440216066  to: 0.023833337426185607\n",
      "Training iteration: 2835\n",
      "Improved validation loss from: 0.023833337426185607  to: 0.02379479855298996\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.023973879218101502\n",
      "Training iteration: 2837\n",
      "Improved validation loss from: 0.02379479855298996  to: 0.02360655814409256\n",
      "Training iteration: 2838\n",
      "Improved validation loss from: 0.02360655814409256  to: 0.02355435788631439\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.023848557472229005\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.024177710711956023\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.02430276423692703\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.024012823402881623\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.023868098855018616\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.024925041198730468\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.025204163789749146\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.024016037583351135\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.02422771155834198\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.024483633041381837\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.02542991042137146\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.02439333200454712\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.024083587527275085\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.02405896931886673\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.02446487843990326\n",
      "Training iteration: 2854\n",
      "Validation loss (no improvement): 0.024345819652080537\n",
      "Training iteration: 2855\n",
      "Validation loss (no improvement): 0.024299828708171843\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): 0.024318937957286835\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.02439276725053787\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): 0.024126796424388884\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.023659563064575194\n",
      "Training iteration: 2860\n",
      "Improved validation loss from: 0.02355435788631439  to: 0.023484568297863006\n",
      "Training iteration: 2861\n",
      "Improved validation loss from: 0.023484568297863006  to: 0.023442760109901428\n",
      "Training iteration: 2862\n",
      "Improved validation loss from: 0.023442760109901428  to: 0.023238353431224823\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.023287317156791686\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.02360435724258423\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.023685117065906525\n",
      "Training iteration: 2866\n",
      "Improved validation loss from: 0.023238353431224823  to: 0.02271619290113449\n",
      "Training iteration: 2867\n",
      "Improved validation loss from: 0.02271619290113449  to: 0.02251691371202469\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.022612838447093962\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.022942247986793517\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): 0.023055438697338105\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): 0.023057779669761656\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.022904637455940246\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.02298598736524582\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.022612671554088592\n",
      "Training iteration: 2875\n",
      "Improved validation loss from: 0.02251691371202469  to: 0.022327713668346405\n",
      "Training iteration: 2876\n",
      "Validation loss (no improvement): 0.022335505485534667\n",
      "Training iteration: 2877\n",
      "Validation loss (no improvement): 0.02253727912902832\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.022327713668346405  to: 0.022258885204792023\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.022365756332874298\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.02278454601764679\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.02289564162492752\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.02228488028049469\n",
      "Training iteration: 2883\n",
      "Improved validation loss from: 0.022258885204792023  to: 0.02198517769575119\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.02201276123523712\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): 0.022287440299987794\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.02232186049222946\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.022519095242023467\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): 0.022722086310386656\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.022675831615924836\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): 0.022404687106609346\n",
      "Training iteration: 2891\n",
      "Improved validation loss from: 0.02198517769575119  to: 0.02184264212846756\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.022098501026630402\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.02261575162410736\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.02218317538499832\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.02202618569135666\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.02197413444519043\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.02302916795015335\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.023368020355701447\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.02209087908267975\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.022115948796272277\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.022014980018138886\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): 0.02275313436985016\n",
      "Training iteration: 2903\n",
      "Validation loss (no improvement): 0.021904048323631287\n",
      "Training iteration: 2904\n",
      "Improved validation loss from: 0.02184264212846756  to: 0.02167000323534012\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.021888229250907897\n",
      "Training iteration: 2906\n",
      "Validation loss (no improvement): 0.022837014496326448\n",
      "Training iteration: 2907\n",
      "Validation loss (no improvement): 0.022450828552246095\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.02167000323534012  to: 0.021609362959861756\n",
      "Training iteration: 2909\n",
      "Improved validation loss from: 0.021609362959861756  to: 0.0214290589094162\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.02171582877635956\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.021832695603370665\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): 0.021563604474067688\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): 0.02144203186035156\n",
      "Training iteration: 2914\n",
      "Validation loss (no improvement): 0.02158696949481964\n",
      "Training iteration: 2915\n",
      "Improved validation loss from: 0.0214290589094162  to: 0.021319563686847686\n",
      "Training iteration: 2916\n",
      "Improved validation loss from: 0.021319563686847686  to: 0.02099866569042206\n",
      "Training iteration: 2917\n",
      "Improved validation loss from: 0.02099866569042206  to: 0.020669586956501007\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.020981483161449432\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.02137196362018585\n",
      "Training iteration: 2920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.020767004787921907\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.020669586956501007  to: 0.02064168006181717\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.020911240577697755\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.02091890573501587\n",
      "Training iteration: 2924\n",
      "Improved validation loss from: 0.02064168006181717  to: 0.020379121601581573\n",
      "Training iteration: 2925\n",
      "Improved validation loss from: 0.020379121601581573  to: 0.020366080105304718\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.020375673472881318\n",
      "Training iteration: 2927\n",
      "Improved validation loss from: 0.020366080105304718  to: 0.020165088772773742\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.020229999721050263\n",
      "Training iteration: 2929\n",
      "Improved validation loss from: 0.020165088772773742  to: 0.020016971230506896\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.02006284296512604\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.020365190505981446\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.020547278225421906\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.02009854018688202\n",
      "Training iteration: 2934\n",
      "Improved validation loss from: 0.020016971230506896  to: 0.019851122796535493\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): 0.019961130619049073\n",
      "Training iteration: 2936\n",
      "Improved validation loss from: 0.019851122796535493  to: 0.01982630044221878\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.019962820410728454\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.019864529371261597\n",
      "Training iteration: 2939\n",
      "Improved validation loss from: 0.01982630044221878  to: 0.019584348797798155\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.01983025372028351\n",
      "Training iteration: 2941\n",
      "Improved validation loss from: 0.019584348797798155  to: 0.019305804371833803\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.019374828040599822\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.02031257450580597\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.020037266612052917\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.019594025611877442\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.01958584040403366\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.02031777799129486\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.019722670316696167\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.019477152824401857\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.019527579843997955\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.02043815553188324\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.020269861817359923\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.019575759768486023\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): 0.020142841339111327\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.019965535402297972\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.020050349831581115\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.019604238867759704\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.019305804371833803  to: 0.01929907351732254\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.019667837023735046\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): 0.020504164695739745\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.020013639330863954\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.019364519417285918\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.019370976090431213\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): 0.020350074768066405\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.02050858289003372\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.019572193920612335\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.01938052475452423\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.020050033926963806\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.020463311672210695\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.02001611441373825\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.019807806611061095\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.01997377723455429\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.020923061668872832\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): 0.019975049793720244\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.019421610236167907\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.019997337460517885\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.01972576081752777\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.019648706912994383\n",
      "Training iteration: 2979\n",
      "Improved validation loss from: 0.01929907351732254  to: 0.018904466927051545\n",
      "Training iteration: 2980\n",
      "Improved validation loss from: 0.018904466927051545  to: 0.018715326488018037\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.01932257115840912\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.020255596935749055\n",
      "Training iteration: 2983\n",
      "Improved validation loss from: 0.018715326488018037  to: 0.018515816330909728\n",
      "Training iteration: 2984\n",
      "Improved validation loss from: 0.018515816330909728  to: 0.01813475489616394\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.01816955804824829\n",
      "Training iteration: 2986\n",
      "Validation loss (no improvement): 0.019055664539337158\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.018264767527580262\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.018608932197093964\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.0183122381567955\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.019224420189857483\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.019652622938156127\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.018797774612903596\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.018689517676830292\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.01842058151960373\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.01841011494398117\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.018187457323074342\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.018398728966712952\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.01860312670469284\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.018894118070602418\n",
      "Training iteration: 3000\n",
      "Improved validation loss from: 0.01813475489616394  to: 0.017808005213737488\n",
      "Training iteration: 3001\n",
      "Improved validation loss from: 0.017808005213737488  to: 0.017596693336963655\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.017700234055519105\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.018390846252441407\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.01794770061969757\n",
      "Training iteration: 3005\n",
      "Improved validation loss from: 0.017596693336963655  to: 0.017298954725265502\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.017471998929977417\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.017996318638324738\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.01875578463077545\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.01809113621711731\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.017588965594768524\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): 0.01731433570384979\n",
      "Training iteration: 3012\n",
      "Improved validation loss from: 0.017298954725265502  to: 0.017167551815509795\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.017189033329486847\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): 0.017404475808143617\n",
      "Training iteration: 3015\n",
      "Improved validation loss from: 0.017167551815509795  to: 0.017156444489955902\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.0174520343542099\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.017424723505973815\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.01755032241344452\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.017590621113777162\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.017339591681957246\n",
      "Training iteration: 3021\n",
      "Improved validation loss from: 0.017156444489955902  to: 0.017022338509559632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3022\n",
      "Improved validation loss from: 0.017022338509559632  to: 0.016637320816516876\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.0171035960316658\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.01761925369501114\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.017073772847652435\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.01689113974571228\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.017450268566608428\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.01760857701301575\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.017402364313602446\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.016716036200523376\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.01677444875240326\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.016819903254508974\n",
      "Training iteration: 3033\n",
      "Improved validation loss from: 0.016637320816516876  to: 0.016460466384887695\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.01664859354496002\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.01674463599920273\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.016798093914985657\n",
      "Training iteration: 3037\n",
      "Improved validation loss from: 0.016460466384887695  to: 0.016452763974666596\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.016764633357524872\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.016670313477516175\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.016751430928707123\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.01647748351097107\n",
      "Training iteration: 3042\n",
      "Improved validation loss from: 0.016452763974666596  to: 0.016205009818077088\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.01736089289188385\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.017623984813690187\n",
      "Training iteration: 3045\n",
      "Validation loss (no improvement): 0.0165763184428215\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.016421382129192353\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.016417470574378968\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.017155927419662476\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.018300600349903107\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.017452767491340636\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.016943897306919097\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.01688063144683838\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.01722249239683151\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.017992687225341798\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.018066391348838806\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.017130145430564882\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.016795900464057923\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.01674531102180481\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): 0.01717165410518646\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.01718563735485077\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.016819438338279723\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.016415107250213622\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.016356320679187776\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.01649462878704071\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.017144769430160522\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): 0.016523280739784242\n",
      "Training iteration: 3067\n",
      "Improved validation loss from: 0.016205009818077088  to: 0.01618601381778717\n",
      "Training iteration: 3068\n",
      "Validation loss (no improvement): 0.016268067061901093\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.016706982254981996\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.017301222681999205\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.016652899980545043\n",
      "Training iteration: 3072\n",
      "Improved validation loss from: 0.01618601381778717  to: 0.01617467254400253\n",
      "Training iteration: 3073\n",
      "Improved validation loss from: 0.01617467254400253  to: 0.015883100032806397\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.016289040446281433\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.016498997807502747\n",
      "Training iteration: 3076\n",
      "Improved validation loss from: 0.015883100032806397  to: 0.015844693779945372\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.015851306915283202\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.01642143279314041\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.017175504565238954\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.01743362843990326\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.01689784824848175\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.01706250160932541\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.016431200504302978\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.01680961549282074\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.016173635423183442\n",
      "Training iteration: 3086\n",
      "Improved validation loss from: 0.015844693779945372  to: 0.01576991528272629\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.016086164116859435\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.01665913760662079\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.017065715789794923\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.0171940416097641\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.016931669414043428\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.01646974980831146\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.016557994484901428\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.01687823235988617\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.01636488288640976\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.016126565635204315\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.016422614455223083\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.01651650220155716\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.016146089136600494\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.016887247562408447\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.016084811091423033\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.016318774223327635\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.01622132658958435\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.0164860337972641\n",
      "Training iteration: 3105\n",
      "Improved validation loss from: 0.01576991528272629  to: 0.015707893669605254\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.015928000211715698\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.01608489453792572\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.01609618216753006\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.015826314687728882\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.016176743805408476\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.016236123442649842\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.01623823195695877\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.01579359769821167\n",
      "Training iteration: 3114\n",
      "Improved validation loss from: 0.015707893669605254  to: 0.015707144141197206\n",
      "Training iteration: 3115\n",
      "Improved validation loss from: 0.015707144141197206  to: 0.014989158511161805\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.015016332268714905\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.01557929664850235\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.01646333038806915\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.016025443375110627\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.01574724018573761\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.015768375992774964\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.01600172221660614\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.015468473732471465\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.015465545654296874\n",
      "Training iteration: 3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015871858596801756\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.016418102383613586\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.01677016317844391\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.015934324264526366\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.015317144989967345\n",
      "Training iteration: 3130\n",
      "Improved validation loss from: 0.014989158511161805  to: 0.014980879426002503\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.015154419839382172\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.0157125860452652\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.01584162414073944\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.0158413827419281\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.015691295266151428\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.015346822142601014\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.015967957675457\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.015485121309757233\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.015683002769947052\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.015139961242675781\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.015144535899162292\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.015706117451190948\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.016486163437366485\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.015560141205787659\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.015183313190937043\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.015743462741374968\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.016524678468704222\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.01684720814228058\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.015321458876132964\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.015388771891593933\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.016295912861824035\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.018060027062892912\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.016325849294662475\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.016971488296985627\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.01659933179616928\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.017965593934059144\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.019053712487220764\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.016476112604141235\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.016942644119262697\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.016812916100025176\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.016993099451065065\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.017409908771514892\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.01709808111190796\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.017083695530891417\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.017113569378852844\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.017209565639495848\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.01665937006473541\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.015772294998168946\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.01532074362039566\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.01522204875946045\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.01554664820432663\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.01583305448293686\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.015279826521873475\n",
      "Training iteration: 3174\n",
      "Improved validation loss from: 0.014980879426002503  to: 0.014869026839733124\n",
      "Training iteration: 3175\n",
      "Improved validation loss from: 0.014869026839733124  to: 0.014666391909122467\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.014678238332271576\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.01496245115995407\n",
      "Training iteration: 3178\n",
      "Improved validation loss from: 0.014666391909122467  to: 0.014404077827930451\n",
      "Training iteration: 3179\n",
      "Improved validation loss from: 0.014404077827930451  to: 0.014264196157455444\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.014551623165607453\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.015115301311016082\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.01533111035823822\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.015576440095901489\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.015168876945972442\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.015299668908119202\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.014901983737945556\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.014359526336193085\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.014551842212677002\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.014720509946346282\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.015217193961143493\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.015392079949378967\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.015151861310005187\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.014763519167900085\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.01455552875995636\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.01465902030467987\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.014904338121414184\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.014655774831771851\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.01463085412979126\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.01476467251777649\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.01490996778011322\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.014664964377880096\n",
      "Training iteration: 3202\n",
      "Improved validation loss from: 0.014264196157455444  to: 0.01424168199300766\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.014281158149242402\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.01514824777841568\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.015322661399841309\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.015426573157310487\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.015046225488185882\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.01581510603427887\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.01594547778367996\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.014490465819835662\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.014849896728992461\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.014620158076286315\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.015163907408714294\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.01591492146253586\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.015676458179950715\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.01604834496974945\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.015893813967704774\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.01541612446308136\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.015079249441623688\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.01487801969051361\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.01477963626384735\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.01491309553384781\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.014829711616039276\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.014947240054607392\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.015173864364624024\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.015403611958026886\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.014605288207530976\n",
      "Training iteration: 3228\n",
      "Improved validation loss from: 0.01424168199300766  to: 0.014005650579929353\n",
      "Training iteration: 3229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.014005650579929353  to: 0.01397453099489212\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.014380982518196106\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.014562352001667023\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.01462445706129074\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.014554265141487121\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.014547213912010193\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.014330931007862091\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.014073920249938966\n",
      "Training iteration: 3237\n",
      "Improved validation loss from: 0.01397453099489212  to: 0.013957123458385467\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.014549502730369568\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.015265412628650665\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.014656396210193634\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.014391237497329712\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.014614339172840118\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.014926037192344666\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.015087062120437622\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.014968718588352203\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.015017467737197875\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.014821945130825043\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.014677433669567109\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.014421744644641877\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.014018426835536956\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.01443747878074646\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.01490330696105957\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.015110863745212555\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.015099138021469116\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.014942145347595215\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.014821934700012206\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.014449813961982727\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.014082589745521545\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.014288485050201416\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.014605119824409485\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.014550074934959412\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.014641612768173218\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.014847244322299957\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.014699247479438782\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.014411075413227082\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.01396983563899994\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.014214828610420227\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.014658530056476594\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.014855358004570007\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.014689874649047852\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.014554519951343537\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.014219936728477479\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.014107085764408112\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.014427880942821502\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.01418263465166092\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.014117525517940521\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.014641878008842469\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.015375575423240662\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.015129247307777404\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.013993307948112488\n",
      "Training iteration: 3281\n",
      "Improved validation loss from: 0.013957123458385467  to: 0.013806375861167907\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): 0.01409546434879303\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.014751127362251282\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.015144136548042298\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.014702798426151275\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.014431579411029816\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.01429038941860199\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.014023084938526154\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.014023858308792114\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.014192906022071839\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.014485464990139007\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.014525794982910156\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.014443382620811462\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.014323534071445465\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.014197099208831786\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.013886424899101257\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.013817521929740905\n",
      "Training iteration: 3298\n",
      "Improved validation loss from: 0.013806375861167907  to: 0.013404089212417602\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.013610617816448211\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.014079637825489044\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.014483241736888886\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.014081260561943055\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.01402575671672821\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.014403741061687469\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.014105850458145141\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.014081403613090515\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.014297983050346375\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.014345893263816833\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.014089205861091613\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.013910868763923645\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.01432303637266159\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.015042021870613098\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.015004083514213562\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.014825457334518432\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.014747574925422668\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.014708963036537171\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.01455165445804596\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.01419348418712616\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.01376628279685974\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.013883408904075623\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.014201685786247253\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.014293067157268524\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.013940715789794922\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.014100030064582825\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.014139066636562347\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.01380259096622467\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.013561920821666717\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.013522851467132568\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.014146462082862854\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.014295700192451476\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.014255061745643616\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.014466789364814759\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.014687283337116242\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.014114835858345031\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.013734373450279235\n",
      "Training iteration: 3336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01385301947593689\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.014320653676986695\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.014182844758033752\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.013414368033409119\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.01341073215007782\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.01394880712032318\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.014534012973308563\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.014686074852943421\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.01457797884941101\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.014506381750106812\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.015301133692264556\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.01458292156457901\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.013664743304252625\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.013592529296875\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.014177478849887848\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.014795464277267457\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.013780756294727326\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.013480807840824127\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.013889351487159729\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.014655517041683197\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.013577401638031006\n",
      "Training iteration: 3357\n",
      "Improved validation loss from: 0.013404089212417602  to: 0.013347506523132324\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.014017263054847717\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.015015581250190735\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.014306634664535522\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.014034250378608703\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.01414690911769867\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.014848661422729493\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.013660082221031189\n",
      "Training iteration: 3365\n",
      "Improved validation loss from: 0.013347506523132324  to: 0.013139274716377259\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.013427972793579102\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.014328479766845703\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.013966178894042969\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.01334938257932663\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.01350513994693756\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.014247286319732665\n",
      "Training iteration: 3372\n",
      "Validation loss (no improvement): 0.01433899849653244\n",
      "Training iteration: 3373\n",
      "Validation loss (no improvement): 0.013800591230392456\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.013659027218818665\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.013843517005443572\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.013809786736965179\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.013305750489234925\n",
      "Training iteration: 3378\n",
      "Validation loss (no improvement): 0.013236144185066223\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.013663752377033234\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.014630337059497834\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.014382417500019073\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.013735657930374146\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.013315729796886444\n",
      "Training iteration: 3384\n",
      "Improved validation loss from: 0.013139274716377259  to: 0.013038527965545655\n",
      "Training iteration: 3385\n",
      "Improved validation loss from: 0.013038527965545655  to: 0.01285412609577179\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.012869594991207123\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.012946994602680206\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): 0.013713973760604858\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.014339238405227661\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.013731880486011505\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.013462193310260773\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.013438846170902252\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.01318342685699463\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.01295408308506012\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.013205006718635559\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.013753291964530946\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.013932287693023682\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.01356009989976883\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.013208325207233428\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.01338052898645401\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): 0.012998920679092408\n",
      "Training iteration: 3402\n",
      "Improved validation loss from: 0.01285412609577179  to: 0.012744924426078797\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.013308921456336975\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.013787099719047546\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.01375957727432251\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.013720408082008362\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): 0.013608899712562562\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.013669316470623017\n",
      "Training iteration: 3409\n",
      "Validation loss (no improvement): 0.01410180628299713\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): 0.013418172299861909\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.013014379143714904\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.0131208136677742\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.013197751343250274\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.01341385841369629\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.013538436591625213\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.01313396692276001\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): 0.01327761709690094\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.014092805981636047\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.014325477182865143\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.013582047820091248\n",
      "Training iteration: 3421\n",
      "Improved validation loss from: 0.012744924426078797  to: 0.012645158171653747\n",
      "Training iteration: 3422\n",
      "Improved validation loss from: 0.012645158171653747  to: 0.012548026442527772\n",
      "Training iteration: 3423\n",
      "Validation loss (no improvement): 0.013664761185646057\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): 0.014688754081726074\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.013349649310112\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.01305844783782959\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.01374146193265915\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.014782246947288514\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): 0.013859760761260987\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.01356893926858902\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.01339564323425293\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.01434459090232849\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.014249256253242493\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.013201890885829926\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.013117508590221405\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.013370108604431153\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.013433286547660827\n",
      "Training iteration: 3438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.013995519280433655\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.01425410807132721\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.013647466897964478\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.013229840993881225\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.013513804972171783\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.01358669251203537\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.01294509619474411\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.01262730360031128\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.01278526484966278\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.012683756649494171\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.012676861882209779\n",
      "Training iteration: 3449\n",
      "Improved validation loss from: 0.012548026442527772  to: 0.012409423291683198\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.0125379741191864\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.013323746621608734\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.013244357705116273\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.013131311535835266\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.013380654156208038\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.013639208674430848\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.014008571207523347\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.013114121556282044\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.012632140517234802\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.01259830892086029\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.012888273596763611\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.013070589303970337\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.012955215573310853\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.013321121037006379\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.01363714188337326\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.012999334931373596\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.0128831684589386\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.012770263850688935\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.012962709367275237\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.012959548830986023\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.012894007563591003\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): 0.012965458631515502\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.013144277036190033\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.012949773669242859\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.012926504015922546\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.013045689463615418\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.013411608338356019\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.012851646542549134\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.012460877001285554\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.012564826011657714\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.012607282400131226\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.012452628463506699\n",
      "Training iteration: 3482\n",
      "Improved validation loss from: 0.012409423291683198  to: 0.012183652073144913\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.012607152760028838\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.01283276230096817\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.012623752653598785\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.0126274973154068\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.012888956069946288\n",
      "Training iteration: 3488\n",
      "Improved validation loss from: 0.012183652073144913  to: 0.012105269730091095\n",
      "Training iteration: 3489\n",
      "Improved validation loss from: 0.012105269730091095  to: 0.011622178554534911\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.01190616637468338\n",
      "Training iteration: 3491\n",
      "Validation loss (no improvement): 0.012361607700586318\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): 0.01252182424068451\n",
      "Training iteration: 3493\n",
      "Validation loss (no improvement): 0.013069503009319305\n",
      "Training iteration: 3494\n",
      "Validation loss (no improvement): 0.012761923670768737\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.012790454924106598\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.013163343071937561\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): 0.012491269409656525\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.012292828410863876\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): 0.013302037119865417\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): 0.013653050363063812\n",
      "Training iteration: 3501\n",
      "Validation loss (no improvement): 0.01263308823108673\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): 0.012381281703710556\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.013036313652992248\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.013171914219856262\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.012265505641698838\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): 0.012086190283298492\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): 0.01282263845205307\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.013434965908527375\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.012782888114452362\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.012113754451274873\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.011971300840377808\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.012954047322273255\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.013040260970592498\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.012617358565330505\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.012638881802558899\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.012589466571807862\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.012089798599481583\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.011682136356830597\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.01192345842719078\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.012300052493810654\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.012199406325817109\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.01214314103126526\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): 0.012875255942344666\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): 0.012752392888069152\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.011899411678314209\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.011826451122760772\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.012517818808555603\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.012956571578979493\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.012446673959493637\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.012197480350732804\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.012049200385808945\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.011980085074901581\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.011995117366313934\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.012285838276147843\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.012830337882041931\n",
      "Training iteration: 3536\n",
      "Validation loss (no improvement): 0.012586928904056549\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.012059672176837921\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): 0.01169099360704422\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.012263865768909454\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): 0.012832072377204896\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.01190459132194519\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.011625119298696519\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.012198634445667267\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.012291704118251801\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.011895604431629181\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.01192133203148842\n",
      "Training iteration: 3547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.012497980892658234\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): 0.013301996886730194\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.01257811039686203\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.01227680891752243\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.013386401534080505\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.013744720816612243\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.012486355006694793\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.012133872509002686\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.012149149179458618\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.013423185050487518\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.013038890063762664\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): 0.011833278089761734\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): 0.011870803683996201\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): 0.013081039488315582\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.012879887223243713\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.012474185228347779\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.012468353658914567\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): 0.012541571259498596\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): 0.012169761955738068\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): 0.01162264347076416\n",
      "Training iteration: 3567\n",
      "Improved validation loss from: 0.011622178554534911  to: 0.0115870401263237\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.01220378503203392\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.012421371042728424\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.012122835218906402\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): 0.011909545958042144\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.011898127943277359\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.01198933869600296\n",
      "Training iteration: 3574\n",
      "Improved validation loss from: 0.0115870401263237  to: 0.011165107786655425\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.011165107786655425  to: 0.01062273383140564\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): 0.011299689114093781\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.01113363727927208\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.010938924551010133\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.011599216610193253\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): 0.011860628426074982\n",
      "Training iteration: 3581\n",
      "Validation loss (no improvement): 0.011238284409046173\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.011219801753759385\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.012221847474575043\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.012076232582330704\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.01102461963891983\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.010900250822305679\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.011840415000915528\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.011878395080566406\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.011221162974834442\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.0112323597073555\n",
      "Training iteration: 3591\n",
      "Validation loss (no improvement): 0.01246977299451828\n",
      "Training iteration: 3592\n",
      "Validation loss (no improvement): 0.01238812953233719\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.011887681484222413\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.012063635885715485\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.012384674698114394\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.011999420076608657\n",
      "Training iteration: 3597\n",
      "Validation loss (no improvement): 0.01153162270784378\n",
      "Training iteration: 3598\n",
      "Validation loss (no improvement): 0.011996126174926758\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.0126418799161911\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.01183365359902382\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.010987631976604462\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.010910656303167343\n",
      "Training iteration: 3603\n",
      "Validation loss (no improvement): 0.01183830052614212\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.012120231240987777\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.011310014873743057\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.010874223709106446\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.01135747879743576\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.011367385089397431\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.011097029596567155\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.011265332996845245\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.01123826503753662\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.010636012256145477\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.010970932245254517\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.011884476989507675\n",
      "Training iteration: 3615\n",
      "Validation loss (no improvement): 0.01177162528038025\n",
      "Training iteration: 3616\n",
      "Validation loss (no improvement): 0.011211929470300674\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.010997281223535538\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.010813752561807633\n",
      "Training iteration: 3619\n",
      "Improved validation loss from: 0.01062273383140564  to: 0.010395358502864837\n",
      "Training iteration: 3620\n",
      "Improved validation loss from: 0.010395358502864837  to: 0.010310710966587066\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.010819629579782487\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.01078808680176735\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.010552072525024414\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.010668440163135529\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.010804548114538192\n",
      "Training iteration: 3626\n",
      "Validation loss (no improvement): 0.010759208351373672\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.010468219220638276\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.010712846368551254\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.010643488168716431\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.01053798794746399\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.010653135925531387\n",
      "Training iteration: 3632\n",
      "Validation loss (no improvement): 0.010745537281036378\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.010496240854263306\n",
      "Training iteration: 3634\n",
      "Validation loss (no improvement): 0.010397431999444961\n",
      "Training iteration: 3635\n",
      "Validation loss (no improvement): 0.010999237000942231\n",
      "Training iteration: 3636\n",
      "Improved validation loss from: 0.010310710966587066  to: 0.010300259292125701\n",
      "Training iteration: 3637\n",
      "Validation loss (no improvement): 0.010437335073947906\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.010648293793201447\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.010879655182361603\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.010300298035144807\n",
      "Training iteration: 3641\n",
      "Validation loss (no improvement): 0.010442837327718734\n",
      "Training iteration: 3642\n",
      "Validation loss (no improvement): 0.01032533198595047\n",
      "Training iteration: 3643\n",
      "Improved validation loss from: 0.010300259292125701  to: 0.010143645107746124\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.010210206359624862\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.010303940623998642\n",
      "Training iteration: 3646\n",
      "Validation loss (no improvement): 0.010490764677524567\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.010372233390808106\n",
      "Training iteration: 3648\n",
      "Improved validation loss from: 0.010143645107746124  to: 0.010138322412967683\n",
      "Training iteration: 3649\n",
      "Validation loss (no improvement): 0.010953976213932038\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.010398604720830918\n",
      "Training iteration: 3651\n",
      "Improved validation loss from: 0.010138322412967683  to: 0.010070111602544785\n",
      "Training iteration: 3652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.011393760144710541\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.01169942393898964\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.010486201196908951\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.010258668661117553\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.011292032152414321\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.0106698177754879\n",
      "Training iteration: 3658\n",
      "Improved validation loss from: 0.010070111602544785  to: 0.009538741409778595\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.009944039583206176\n",
      "Training iteration: 3660\n",
      "Validation loss (no improvement): 0.010409901291131974\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.00991186425089836\n",
      "Training iteration: 3662\n",
      "Improved validation loss from: 0.009538741409778595  to: 0.009265413135290146\n",
      "Training iteration: 3663\n",
      "Validation loss (no improvement): 0.010148799419403077\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.009888022392988204\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.00965319350361824\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.010658757388591766\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.010587390512228012\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.010018467903137207\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.00963868349790573\n",
      "Training iteration: 3670\n",
      "Validation loss (no improvement): 0.009951659291982651\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.009989768266677856\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.009747771918773651\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.009660938382148742\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.009330193698406219\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.00956592708826065\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.009850557148456573\n",
      "Training iteration: 3677\n",
      "Improved validation loss from: 0.009265413135290146  to: 0.009245924651622772\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.009339646995067596\n",
      "Training iteration: 3679\n",
      "Improved validation loss from: 0.009245924651622772  to: 0.009029832482337952\n",
      "Training iteration: 3680\n",
      "Validation loss (no improvement): 0.009605120122432708\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.00994935929775238\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.00940774530172348\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.010356831550598144\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.010578958690166474\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.009532371163368225\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.009217491000890731\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.010193412005901337\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.010172563791275024\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.009299830347299576\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.009071941673755645\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.009394442290067672\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.00913078561425209\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.009094145148992538\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.009525507688522339\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.009353293478488922\n",
      "Training iteration: 3696\n",
      "Improved validation loss from: 0.009029832482337952  to: 0.008995600044727325\n",
      "Training iteration: 3697\n",
      "Validation loss (no improvement): 0.009183763712644576\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.009161987155675889\n",
      "Training iteration: 3699\n",
      "Improved validation loss from: 0.008995600044727325  to: 0.008726377785205842\n",
      "Training iteration: 3700\n",
      "Improved validation loss from: 0.008726377785205842  to: 0.008598388731479644\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.010214036703109742\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.009318192303180695\n",
      "Training iteration: 3703\n",
      "Improved validation loss from: 0.008598388731479644  to: 0.008420227468013764\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.009349304437637328\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.010064812004566192\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.0088886559009552\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.008457125723361969\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.009483794122934342\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.00969221368432045\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.008422721177339554\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.008682650327682496\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.00940573364496231\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.009121789783239364\n",
      "Training iteration: 3714\n",
      "Improved validation loss from: 0.008420227468013764  to: 0.008157814294099808\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.008387291431427002\n",
      "Training iteration: 3716\n",
      "Validation loss (no improvement): 0.009134460985660554\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.009242018312215805\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.008894889056682587\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.009042732417583466\n",
      "Training iteration: 3720\n",
      "Validation loss (no improvement): 0.00904974713921547\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.00868569165468216\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.008507907390594482\n",
      "Training iteration: 3723\n",
      "Improved validation loss from: 0.008157814294099808  to: 0.00810125395655632\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.00833737626671791\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.00898391455411911\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.008604415506124497\n",
      "Training iteration: 3727\n",
      "Improved validation loss from: 0.00810125395655632  to: 0.007774724811315537\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.00804302766919136\n",
      "Training iteration: 3729\n",
      "Validation loss (no improvement): 0.007866261154413223\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.009038285911083221\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.009678579866886139\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.009465586394071579\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.008986090123653413\n",
      "Training iteration: 3734\n",
      "Validation loss (no improvement): 0.008756838738918304\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.008324922621250152\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.008405493199825287\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.00834239050745964\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.007968080043792725\n",
      "Training iteration: 3739\n",
      "Improved validation loss from: 0.007774724811315537  to: 0.007688906788825989\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.008047637343406678\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.007972117513418198\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.008240717649459838\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.008416296541690826\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.008024589717388153\n",
      "Training iteration: 3745\n",
      "Improved validation loss from: 0.007688906788825989  to: 0.007545509189367294\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.00860232561826706\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.008445321023464203\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.008149756491184235\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.007736186683177948\n",
      "Training iteration: 3750\n",
      "Validation loss (no improvement): 0.00852365642786026\n",
      "Training iteration: 3751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.008230682462453842\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.007687047868967056\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.008950816839933396\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.009422677755355834\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.008388470858335495\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.007848439365625381\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.008637397736310958\n",
      "Training iteration: 3758\n",
      "Validation loss (no improvement): 0.008653348684310913\n",
      "Training iteration: 3759\n",
      "Validation loss (no improvement): 0.007991626858711243\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.007620300352573395\n",
      "Training iteration: 3761\n",
      "Validation loss (no improvement): 0.008958785235881806\n",
      "Training iteration: 3762\n",
      "Validation loss (no improvement): 0.00929066389799118\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.008175143599510193\n",
      "Training iteration: 3764\n",
      "Validation loss (no improvement): 0.008292049914598466\n",
      "Training iteration: 3765\n",
      "Validation loss (no improvement): 0.009685517847537994\n",
      "Training iteration: 3766\n",
      "Validation loss (no improvement): 0.009530732035636902\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.008360277116298675\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.008095911145210266\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.009059353172779084\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.008844558894634248\n",
      "Training iteration: 3771\n",
      "Improved validation loss from: 0.007545509189367294  to: 0.006757231056690216\n",
      "Training iteration: 3772\n",
      "Validation loss (no improvement): 0.006776047497987747\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.009132613986730575\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.007982686161994934\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.007144208252429962\n",
      "Training iteration: 3776\n",
      "Validation loss (no improvement): 0.008493360131978989\n",
      "Training iteration: 3777\n",
      "Validation loss (no improvement): 0.009584681689739227\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.007980362325906754\n",
      "Training iteration: 3779\n",
      "Validation loss (no improvement): 0.00783369094133377\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.009528986364603042\n",
      "Training iteration: 3781\n",
      "Validation loss (no improvement): 0.010082993656396866\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.008303622901439666\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.007229314744472503\n",
      "Training iteration: 3784\n",
      "Validation loss (no improvement): 0.007853566110134125\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.008671475201845169\n",
      "Training iteration: 3786\n",
      "Validation loss (no improvement): 0.008580063283443452\n",
      "Training iteration: 3787\n",
      "Validation loss (no improvement): 0.007679548859596252\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.007509653270244598\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.007945632934570313\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.007674245536327362\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.0071788236498832704\n",
      "Training iteration: 3792\n",
      "Validation loss (no improvement): 0.007478745281696319\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.007814417779445647\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.007411831617355346\n",
      "Training iteration: 3795\n",
      "Validation loss (no improvement): 0.00797116681933403\n",
      "Training iteration: 3796\n",
      "Validation loss (no improvement): 0.007789338380098343\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.006926017999649048\n",
      "Training iteration: 3798\n",
      "Validation loss (no improvement): 0.007248487323522568\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.006808908283710479\n",
      "Training iteration: 3800\n",
      "Improved validation loss from: 0.006757231056690216  to: 0.006449700891971588\n",
      "Training iteration: 3801\n",
      "Validation loss (no improvement): 0.007086200267076492\n",
      "Training iteration: 3802\n",
      "Validation loss (no improvement): 0.007140567153692245\n",
      "Training iteration: 3803\n",
      "Validation loss (no improvement): 0.007015189528465271\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.00646560937166214\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.0067081846296787265\n",
      "Training iteration: 3806\n",
      "Improved validation loss from: 0.006449700891971588  to: 0.006128957867622376\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.0075284048914909365\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.007042299956083298\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.006617065519094467\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.008140628784894943\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.008304210007190704\n",
      "Training iteration: 3812\n",
      "Validation loss (no improvement): 0.006629812717437744\n",
      "Training iteration: 3813\n",
      "Validation loss (no improvement): 0.006167812272906303\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.007890961319208144\n",
      "Training iteration: 3815\n",
      "Validation loss (no improvement): 0.008192381262779236\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.006792350113391877\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.006415169686079025\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.007889648526906967\n",
      "Training iteration: 3819\n",
      "Validation loss (no improvement): 0.008362046629190444\n",
      "Training iteration: 3820\n",
      "Validation loss (no improvement): 0.006618036329746247\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.006198640540242195\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.007568643987178802\n",
      "Training iteration: 3823\n",
      "Validation loss (no improvement): 0.008040668070316314\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.006788074970245361\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.0066238269209861755\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.008346374332904815\n",
      "Training iteration: 3827\n",
      "Validation loss (no improvement): 0.008034038543701171\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.006195733696222306\n",
      "Training iteration: 3829\n",
      "Improved validation loss from: 0.006128957867622376  to: 0.005706899613142013\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.008024074137210846\n",
      "Training iteration: 3831\n",
      "Validation loss (no improvement): 0.008351520448923112\n",
      "Training iteration: 3832\n",
      "Improved validation loss from: 0.005706899613142013  to: 0.005461487919092178\n",
      "Training iteration: 3833\n",
      "Improved validation loss from: 0.005461487919092178  to: 0.005374445393681526\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.00780281201004982\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.007345551252365112\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.006609897315502167\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.007549052685499191\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.009020773321390152\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.006639105826616287\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.005492819473147392\n",
      "Training iteration: 3841\n",
      "Validation loss (no improvement): 0.007064888626337052\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.00871770828962326\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.0072278186678886415\n",
      "Training iteration: 3844\n",
      "Validation loss (no improvement): 0.006412648409605026\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.007159604132175446\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.00931609347462654\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.007414601743221283\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.006111905723810196\n",
      "Training iteration: 3849\n",
      "Validation loss (no improvement): 0.007131961733102798\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.00790092945098877\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.006492866575717926\n",
      "Training iteration: 3852\n",
      "Validation loss (no improvement): 0.006297092139720917\n",
      "Training iteration: 3853\n",
      "Validation loss (no improvement): 0.006445278227329254\n",
      "Training iteration: 3854\n",
      "Validation loss (no improvement): 0.006820784509181976\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.006424900889396667\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.005967118591070175\n",
      "Training iteration: 3857\n",
      "Validation loss (no improvement): 0.006403079628944397\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.006422287225723267\n",
      "Training iteration: 3859\n",
      "Validation loss (no improvement): 0.006077124923467636\n",
      "Training iteration: 3860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.005374445393681526  to: 0.005195967480540275\n",
      "Training iteration: 3861\n",
      "Validation loss (no improvement): 0.005740972235798836\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.006002060696482658\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.005270775035023689\n",
      "Training iteration: 3864\n",
      "Improved validation loss from: 0.005195967480540275  to: 0.004455864056944847\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.005367527157068253\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.006000811979174614\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.004951203987002373\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.0050124842673540115\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.006299902498722076\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.006243642419576645\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.004882962256669998\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.004611338302493095\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.005691880732774735\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.005371294170618057\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.00481952503323555\n",
      "Training iteration: 3876\n",
      "Validation loss (no improvement): 0.0059932351112365724\n",
      "Training iteration: 3877\n",
      "Validation loss (no improvement): 0.006890948861837387\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.005636321380734444\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.005231272429227829\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.007822661101818085\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.008226723968982696\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.005414522811770439\n",
      "Training iteration: 3883\n",
      "Validation loss (no improvement): 0.0046102374792099\n",
      "Training iteration: 3884\n",
      "Validation loss (no improvement): 0.005672439187765122\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.007554753124713898\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.006623679399490356\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.004954297468066216\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.005458713322877884\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.006278477609157562\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.006045414134860039\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.005491935461759567\n",
      "Training iteration: 3892\n",
      "Validation loss (no improvement): 0.0059597503393888475\n",
      "Training iteration: 3893\n",
      "Validation loss (no improvement): 0.006037142872810364\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.006291496753692627\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.004876230284571648\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.004855218529701233\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.0059259332716465\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.0059473920613527295\n",
      "Training iteration: 3899\n",
      "Validation loss (no improvement): 0.005057241767644882\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.00488150417804718\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.005343859642744064\n",
      "Training iteration: 3902\n",
      "Improved validation loss from: 0.004455864056944847  to: 0.004291336610913276\n",
      "Training iteration: 3903\n",
      "Improved validation loss from: 0.004291336610913276  to: 0.003751494362950325\n",
      "Training iteration: 3904\n",
      "Validation loss (no improvement): 0.005212988704442978\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.0050737161189317705\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.004655979201197624\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.004732754826545715\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.005408257991075516\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.005065887048840523\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.004326953738927841\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.005016456916928291\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.0055508680641651155\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.00464884452521801\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.00466647632420063\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.005516796186566353\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.004591311141848564\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.00430559329688549\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.0051307640969753265\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.005476327612996101\n",
      "Training iteration: 3920\n",
      "Validation loss (no improvement): 0.004191578552126885\n",
      "Training iteration: 3921\n",
      "Validation loss (no improvement): 0.004783964529633522\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.006042910739779472\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.005608127266168594\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.004727903008460999\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.0044457029551267626\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.005930940061807633\n",
      "Training iteration: 3927\n",
      "Validation loss (no improvement): 0.0069141343235969545\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.005133781954646111\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.00400722622871399\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.004921794682741165\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.006786420196294785\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.005744563415646553\n",
      "Training iteration: 3933\n",
      "Improved validation loss from: 0.003751494362950325  to: 0.0036948032677173613\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.00396709032356739\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.004215416312217712\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.004140952974557877\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.004672102257609367\n",
      "Training iteration: 3938\n",
      "Validation loss (no improvement): 0.005081641674041748\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.005520381405949593\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.004744275659322739\n",
      "Training iteration: 3941\n",
      "Improved validation loss from: 0.0036948032677173613  to: 0.0036856867372989655\n",
      "Training iteration: 3942\n",
      "Validation loss (no improvement): 0.004531668499112129\n",
      "Training iteration: 3943\n",
      "Validation loss (no improvement): 0.0053312331438064575\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.004057786613702774\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.0037803493440151216\n",
      "Training iteration: 3946\n",
      "Validation loss (no improvement): 0.005362697690725326\n",
      "Training iteration: 3947\n",
      "Validation loss (no improvement): 0.006035617738962174\n",
      "Training iteration: 3948\n",
      "Validation loss (no improvement): 0.004269523546099663\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.0037114083766937255\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.004420466348528862\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.004662847518920899\n",
      "Training iteration: 3952\n",
      "Improved validation loss from: 0.0036856867372989655  to: 0.0034674253314733506\n",
      "Training iteration: 3953\n",
      "Improved validation loss from: 0.0034674253314733506  to: 0.003116942569613457\n",
      "Training iteration: 3954\n",
      "Validation loss (no improvement): 0.003273191675543785\n",
      "Training iteration: 3955\n",
      "Validation loss (no improvement): 0.0035463210195302965\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.003237958997488022\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.0031583160161972046\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.004087695851922035\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.00400477759540081\n",
      "Training iteration: 3960\n",
      "Improved validation loss from: 0.003116942569613457  to: 0.00292833037674427\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.003909853100776672\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.0047939766198396684\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.005862929672002792\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.004451706632971763\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.004026298969984054\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.00496227853000164\n",
      "Training iteration: 3967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.004222728684544564\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.003571518883109093\n",
      "Training iteration: 3969\n",
      "Validation loss (no improvement): 0.003873341158032417\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.0040063560009002686\n",
      "Training iteration: 3971\n",
      "Validation loss (no improvement): 0.0037842761725187303\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.0033652935177087783\n",
      "Training iteration: 3973\n",
      "Improved validation loss from: 0.00292833037674427  to: 0.002880476973950863\n",
      "Training iteration: 3974\n",
      "Validation loss (no improvement): 0.003802146390080452\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.0045577619224786755\n",
      "Training iteration: 3976\n",
      "Validation loss (no improvement): 0.002893369272351265\n",
      "Training iteration: 3977\n",
      "Improved validation loss from: 0.002880476973950863  to: 0.0026205211877822878\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.005231760814785958\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.0045124940574169155\n",
      "Training iteration: 3980\n",
      "Validation loss (no improvement): 0.002877841517329216\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.003700864315032959\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.0048489771783351895\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.0036486104130744935\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.00413520336151123\n",
      "Training iteration: 3985\n",
      "Validation loss (no improvement): 0.004845722764730454\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.004521577805280686\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.004144369438290596\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.004173911735415458\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.003707067295908928\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.003043457865715027\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.003269079327583313\n",
      "Training iteration: 3992\n",
      "Validation loss (no improvement): 0.0031799085438251494\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.002879549190402031\n",
      "Training iteration: 3994\n",
      "Improved validation loss from: 0.0026205211877822878  to: 0.0025524547323584556\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.0029558217152953147\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.0032789498567581178\n",
      "Training iteration: 3997\n",
      "Validation loss (no improvement): 0.003072817251086235\n",
      "Training iteration: 3998\n",
      "Validation loss (no improvement): 0.002849398925900459\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.003335486724972725\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_5 = toy_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_5.train_model()\n",
    "ensemble_model_5.model_inference()\n",
    "\n",
    "ensemble_mean_5 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_5 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_5 = np.sqrt(np.exp(ensemble_logvar_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = (ensemble_mean_1+ensemble_mean_2+ensemble_mean_3+ensemble_mean_4+ensemble_mean_5)/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_var = ensemble_mean_1**2 + ensemble_std_1**2 + ensemble_mean_2**2 + ensemble_std_2**2 + ensemble_mean_3**2 + ensemble_std_3**2 + ensemble_mean_4**2 + ensemble_std_4**2 + ensemble_mean_5**2 + ensemble_std_5**2\n",
    "ensemble_var = ensemble_var/5.0 - ensemble_mean**2\n",
    "ensemble_std = np.sqrt(ensemble_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1inds = data_tuple[2][:,0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],baseline[arr1inds,0],label='Baseline')\n",
    "plt.plot(data_tuple[2][arr1inds,0],dropout_mean[arr1inds,0],label='Dropout mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Mixture mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],ensemble_mean[arr1inds,0],label='Ensemble mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],swa_mean[arr1inds,0],label='SWA mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],bayesian_mean[arr1inds,0],label='Bayesian mean')\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],bayesian_mean[arr1inds,0],label='Bayesian mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=bayesian_mean[arr1inds,0]-3*bayesian_std[arr1inds,0],y2=bayesian_mean[arr1inds,0]+3*bayesian_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],dropout_mean[arr1inds,0],label='Dropout mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=dropout_mean[arr1inds,0]-3*dropout_std[arr1inds,0],y2=dropout_mean[arr1inds,0]+3*dropout_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Mixture mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=mixture_mean[arr1inds,0]-3*mixture_std[arr1inds,0],y2=mixture_mean[arr1inds,0]+3*mixture_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Ensemble mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=ensemble_mean[arr1inds,0]-3*ensemble_std[arr1inds,0],y2=ensemble_mean[arr1inds,0]+3*ensemble_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],swa_mean[arr1inds,0],label='SWA mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=swa_mean[arr1inds,0]-3*swa_std[arr1inds,0],y2=swa_mean[arr1inds,0]+3*swa_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
