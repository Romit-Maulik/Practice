{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(10)\n",
    "tf.random.set_seed(10)\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras import optimizers, models, regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class krishnan_model(Model):\n",
    "    def __init__(self,data_tuple,arch_type='baseline',lrate=0.001,num_epochs=1000,eps_range=0.01):\n",
    "        super(krishnan_model, self).__init__()\n",
    "\n",
    "        train_data = data_tuple[0]\n",
    "        valid_data = data_tuple[1]\n",
    "        test_data = data_tuple[2]\n",
    "        \n",
    "        self.input_train_data = train_data[:,0].reshape(-1,1)\n",
    "        self.input_valid_data = valid_data[:,0].reshape(-1,1)\n",
    "        self.input_test_data = test_data[:,0].reshape(-1,1)\n",
    "        \n",
    "        self.output_train_data = train_data[:,1].reshape(-1,1)\n",
    "        self.output_valid_data = valid_data[:,1].reshape(-1,1)\n",
    "        self.output_test_data = train_data[:,1].reshape(-1,1)\n",
    "\n",
    "        self.ntrain = self.input_train_data.shape[0]\n",
    "        self.nvalid = self.input_valid_data.shape[0]\n",
    "        self.ntest = self.input_test_data.shape[0]\n",
    "        self.arch_type = arch_type\n",
    "        self.num_latent = 6\n",
    "\n",
    "        self.init_architecture_baseline()\n",
    "        self.train_op = tf.keras.optimizers.Adam(learning_rate=lrate)\n",
    "\n",
    "        # If adversarial training is used\n",
    "        self.eps_range = eps_range\n",
    "        \n",
    "        # num epochs\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    def init_architecture_baseline(self):\n",
    "\n",
    "        # Define model architecture\n",
    "        ## Encoder\n",
    "        self.l1 = Dense(50,activation='relu')\n",
    "\n",
    "        if self.arch_type == 'baseline':\n",
    "            self.out = Dense(1,activation='linear')\n",
    "        \n",
    "        elif self.arch_type =='mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa':\n",
    "            self.out_mean = Dense(1,activation='linear')\n",
    "            self.out_logvar = Dense(1,activation='linear') #sigma^2\n",
    "\n",
    "        if self.arch_type == 'dropout':\n",
    "            self.out = Dense(1,activation='linear')\n",
    "            self.dropout_layer = Dropout(0.1)\n",
    "\n",
    "    def call_baseline(self,X):\n",
    "\n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        out = self.out(hh)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def call_dropout(self,X):\n",
    "        \n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        hh = self.dropout_layer(hh,training=True)\n",
    "        out = self.out(hh)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def call_mixture(self,X):\n",
    "\n",
    "        # Encode\n",
    "        hh = self.l1(X)\n",
    "        mean = self.out_mean(hh)\n",
    "        logvar = self.out_logvar(hh)\n",
    "\n",
    "        return mean, logvar\n",
    "\n",
    "    # Running the model\n",
    "    def call(self,X):\n",
    "        if self.arch_type == 'baseline':\n",
    "            recon = self.call_baseline(X)\n",
    "            return recon\n",
    "        elif self.arch_type =='dropout':\n",
    "            recon = self.call_dropout(X)\n",
    "            return recon\n",
    "        elif self.arch_type =='mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa':\n",
    "            recon_mean, recon_logvar = self.call_mixture(X)\n",
    "            return recon_mean, recon_logvar\n",
    "    \n",
    "    # Regular MSE\n",
    "    def get_loss(self,X,Y):\n",
    "\n",
    "        if self.arch_type == 'mixture' or self.arch_type == 'ensemble' or self.arch_type == 'swa': # Log likelihood optimization\n",
    "            op_mean, op_logvar = self.call(X)\n",
    "            \n",
    "            op_var = tf.math.exp(op_logvar)\n",
    "            half_logvar = 0.5*op_logvar\n",
    "            \n",
    "            mse = (tf.math.square(op_mean-Y))*0.5/(op_var+K.epsilon())\n",
    "            loss = tf.reduce_mean(half_logvar+mse)\n",
    "\n",
    "        else: \n",
    "\n",
    "            op = self.call(X)\n",
    "            loss = tf.reduce_mean(tf.math.square(op-Y))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Regular MSE\n",
    "    def get_adversarial_loss(self,X,X_adv,Y):\n",
    "\n",
    "        op_mean, op_logvar = self.call(X_adv)\n",
    "        \n",
    "        op_var = tf.math.exp(op_logvar)\n",
    "        half_logvar = 0.5*op_logvar\n",
    "\n",
    "        mse = tf.math.square(op_mean-Y)*0.5/(op_var+K.epsilon())\n",
    "        loss = tf.reduce_mean(half_logvar+mse)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # get gradients - regular\n",
    "    def get_grad(self,X,Y):\n",
    "        if self.arch_type == 'ensemble':\n",
    "            # Adversarial training\n",
    "            X = tf.convert_to_tensor(X)\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(X)\n",
    "                L = self.get_loss(X,Y)\n",
    "                g_temp = tape.gradient(L,X)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(self.trainable_variables)\n",
    "                X_adv = X + self.eps_range*tf.math.sign(g_temp)\n",
    "                L_adv = self.get_adversarial_loss(X,X_adv,Y)\n",
    "                g = tape.gradient(L+L_adv, self.trainable_variables)\n",
    "        else:\n",
    "            # Regular training\n",
    "            with tf.GradientTape() as tape:\n",
    "                tape.watch(self.trainable_variables)\n",
    "                L = self.get_loss(X,Y)\n",
    "                g = tape.gradient(L, self.trainable_variables)            \n",
    "        \n",
    "        return g\n",
    "    \n",
    "    # perform gradient descent - regular\n",
    "    def network_learn(self,X,Y):\n",
    "        g = self.get_grad(X,Y)\n",
    "        self.train_op.apply_gradients(zip(g, self.trainable_variables))\n",
    "\n",
    "    # Train the model\n",
    "    def train_model(self):\n",
    "        plot_iter = 0\n",
    "        stop_iter = 0\n",
    "        patience = 100\n",
    "        best_valid_loss = np.inf # Some large number \n",
    "        swa_iter = 0\n",
    "\n",
    "        self.num_batches = 1\n",
    "        self.train_batch_size = int(self.ntrain/self.num_batches)\n",
    "        self.valid_batch_size = int(self.nvalid/self.num_batches)\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "            # Training loss\n",
    "            print('Training iteration:',i)\n",
    "            \n",
    "            for batch in range(self.num_batches):\n",
    "                input_batch = self.input_train_data[batch*self.train_batch_size:(batch+1)*self.train_batch_size]\n",
    "                output_batch = self.output_train_data[batch*self.train_batch_size:(batch+1)*self.train_batch_size]\n",
    "                self.network_learn(input_batch,output_batch)\n",
    "\n",
    "            # Validation loss\n",
    "            valid_loss = 0.0\n",
    "\n",
    "            for batch in range(self.num_batches):\n",
    "                input_batch = self.input_valid_data[batch*self.valid_batch_size:(batch+1)*self.valid_batch_size]\n",
    "                output_batch = self.output_valid_data[batch*self.valid_batch_size:(batch+1)*self.valid_batch_size]\n",
    "                valid_loss = valid_loss + self.get_loss(input_batch,output_batch).numpy()\n",
    "\n",
    "            valid_loss = valid_loss/self.nvalid\n",
    "\n",
    "            # Check early stopping criteria\n",
    "            if valid_loss < best_valid_loss:\n",
    "                \n",
    "                print('Improved validation loss from:',best_valid_loss,' to:', valid_loss)                \n",
    "                best_valid_loss = valid_loss\n",
    "                \n",
    "                if self.arch_type == 'baseline':\n",
    "                    self.save_weights('./checkpoints/baseline_checkpoint')\n",
    "                elif self.arch_type == 'dropout':\n",
    "                    self.save_weights('./checkpoints/dropout_checkpoint')\n",
    "                elif self.arch_type == 'mixture':\n",
    "                    self.save_weights('./checkpoints/mixture_checkpoint')\n",
    "                elif self.arch_type == 'ensemble':\n",
    "                    self.save_weights('./checkpoints/ensemble_checkpoint')\n",
    "                elif self.arch_type == 'swa':\n",
    "                    self.save_weights('./checkpoints/swa_checkpoint')\n",
    "                \n",
    "                stop_iter = 0\n",
    "            else:\n",
    "                print('Validation loss (no improvement):',valid_loss)\n",
    "                stop_iter = stop_iter + 1\n",
    "\n",
    "            if stop_iter == patience:\n",
    "                if self.arch_type == 'swa' and swa_iter < 5:\n",
    "                    self.save_weights('./checkpoints/swa_checkpoint_'+str(swa_iter))\n",
    "                    swa_iter +=1\n",
    "                    \n",
    "                    stop_iter = 0    \n",
    "                else:   \n",
    "                    break\n",
    "                \n",
    "    # Load weights\n",
    "    def restore_model(self):\n",
    "        if self.arch_type == 'baseline':\n",
    "            self.load_weights('./checkpoints/baseline_checkpoint')\n",
    "        elif self.arch_type == 'dropout':\n",
    "            self.load_weights('./checkpoints/dropout_checkpoint')\n",
    "        elif self.arch_type == 'mixture':\n",
    "            self.load_weights('./checkpoints/mixture_checkpoint')\n",
    "        elif self.arch_type == 'ensemble':\n",
    "            self.load_weights('./checkpoints/ensemble_checkpoint')\n",
    "\n",
    "    # Do some testing\n",
    "    def model_inference(self,mc_num=100):\n",
    "        # Restore from checkpoint\n",
    "        self.restore_model()\n",
    "\n",
    "        if self.arch_type == 'baseline':\n",
    "\n",
    "            predictions = self.call(self.input_test_data)\n",
    "            \n",
    "            np.save('Baseline_validation.npy',predictions.numpy())\n",
    "            \n",
    "            return None\n",
    "\n",
    "        \n",
    "        elif self.arch_type == 'dropout':\n",
    "            \n",
    "            prediction_list = []\n",
    "            for i in range(mc_num):\n",
    "                recon = self.call(self.input_test_data)\n",
    "                recon = recon.numpy()\n",
    "                prediction_list.append(recon)\n",
    "\n",
    "            prediction_list = np.asarray(prediction_list)\n",
    "            \n",
    "            np.save('Dropout_validation.npy',prediction_list)\n",
    "            \n",
    "            return None\n",
    "                    \n",
    "        elif self.arch_type == 'mixture' or self.arch_type == 'ensemble':       \n",
    "\n",
    "            mean, var = self.call(self.input_test_data)\n",
    "            mean = mean.numpy()\n",
    "            var = var.numpy()\n",
    "            \n",
    "            np.save(self.arch_type+'_mean_validation.npy',mean)\n",
    "            np.save(self.arch_type+'_var_validation.npy',var)\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        elif self.arch_type == 'swa':\n",
    "            # Average weights - initialization\n",
    "            self.load_weights('./checkpoints/swa_checkpoint')\n",
    "            \n",
    "            mean, logvar = self.call(self.input_test_data)\n",
    "            mean = mean.numpy()\n",
    "            var = np.exp(logvar.numpy()) + mean**2\n",
    "            \n",
    "            for i in range(5):\n",
    "                self.load_weights('./checkpoints/swa_checkpoint_'+str(i))\n",
    "            \n",
    "                t1, t2 = self.call(self.input_test_data)\n",
    "                mean = mean + t1.numpy()\n",
    "                var = var + np.exp(t2.numpy()) + mean**2\n",
    "                \n",
    "            ensemble_mean = mean/6.0\n",
    "            ensemble_var = var/6.0 - ensemble_mean**2\n",
    "            ensemble_std = np.sqrt(ensemble_var)\n",
    "            \n",
    "            np.save('SWA_ensembles_mean_multiple.npy',ensemble_mean)\n",
    "            np.save('SWA_ensembles_std_multiple.npy',ensemble_std)\n",
    "            \n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partition_data():\n",
    "    # Load data\n",
    "    x = np.random.uniform(low=-4.0,high=4.0,size=(200,1))\n",
    "    y = x**3 + np.random.normal(0,9)\n",
    "    \n",
    "    total_data = np.concatenate((x,y),axis=-1)\n",
    "    \n",
    "    train_data = total_data[:40]\n",
    "    valid_data = total_data[40:60]\n",
    "    test_data = total_data[60:]\n",
    "\n",
    "    data_tuple = (train_data, valid_data, test_data)\n",
    "\n",
    "    return data_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuple = load_partition_data()\n",
    "eps_range = 0.01*(np.max(data_tuple[0][:,0])-np.min(data_tuple[0][:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = krishnan_model(data_tuple,arch_type='baseline',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "dropout_model = krishnan_model(data_tuple,arch_type='dropout',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "mixture_model = krishnan_model(data_tuple,arch_type='mixture',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "swa_model = krishnan_model(data_tuple,arch_type='swa',lrate=0.01,num_epochs=4000,eps_range=eps_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 29.539276123046875\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 29.539276123046875  to: 28.959072875976563\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 28.959072875976563  to: 28.387847900390625\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 28.387847900390625  to: 27.826141357421875\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 27.826141357421875  to: 27.287051391601562\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 27.287051391601562  to: 26.756680297851563\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 26.756680297851563  to: 26.2314208984375\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 26.2314208984375  to: 25.70561828613281\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 25.70561828613281  to: 25.17650604248047\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 25.17650604248047  to: 24.642312622070314\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 24.642312622070314  to: 24.1021240234375\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 24.1021240234375  to: 23.55536651611328\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 23.55536651611328  to: 23.001435852050783\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 23.001435852050783  to: 22.44056091308594\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 22.44056091308594  to: 21.871533203125\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 21.871533203125  to: 21.293701171875\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 21.293701171875  to: 20.707002258300783\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 20.707002258300783  to: 20.11230010986328\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 20.11230010986328  to: 19.510433959960938\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 19.510433959960938  to: 18.90229949951172\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 18.90229949951172  to: 18.288377380371095\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 18.288377380371095  to: 17.668362426757813\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 17.668362426757813  to: 17.043423461914063\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 17.043423461914063  to: 16.41528778076172\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 16.41528778076172  to: 15.785591125488281\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 15.785591125488281  to: 15.15693359375\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 15.15693359375  to: 14.531370544433594\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 14.531370544433594  to: 13.911112976074218\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 13.911112976074218  to: 13.298860168457031\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 13.298860168457031  to: 12.69769515991211\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 12.69769515991211  to: 12.110103607177734\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 12.110103607177734  to: 11.53919677734375\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 11.53919677734375  to: 10.98818130493164\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 10.98818130493164  to: 10.460246276855468\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 10.460246276855468  to: 9.958570861816407\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 9.958570861816407  to: 9.486250305175782\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 9.486250305175782  to: 9.04624252319336\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 9.04624252319336  to: 8.641244506835937\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 8.641244506835937  to: 8.273696136474609\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 8.273696136474609  to: 7.945686340332031\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 7.945686340332031  to: 7.658852386474609\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 7.658852386474609  to: 7.4142906188964846\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 7.4142906188964846  to: 7.212577056884766\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 7.212577056884766  to: 7.053623962402344\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 7.053623962402344  to: 6.936643981933594\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 6.936643981933594  to: 6.860099029541016\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 6.860099029541016  to: 6.821687316894531\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 6.821687316894531  to: 6.818402862548828\n",
      "Training iteration: 48\n",
      "Validation loss (no improvement): 6.846561431884766\n",
      "Training iteration: 49\n",
      "Validation loss (no improvement): 6.901869201660157\n",
      "Training iteration: 50\n",
      "Validation loss (no improvement): 6.979634857177734\n",
      "Training iteration: 51\n",
      "Validation loss (no improvement): 7.074855804443359\n",
      "Training iteration: 52\n",
      "Validation loss (no improvement): 7.182489013671875\n",
      "Training iteration: 53\n",
      "Validation loss (no improvement): 7.297526550292969\n",
      "Training iteration: 54\n",
      "Validation loss (no improvement): 7.415238952636718\n",
      "Training iteration: 55\n",
      "Validation loss (no improvement): 7.53131332397461\n",
      "Training iteration: 56\n",
      "Validation loss (no improvement): 7.641957092285156\n",
      "Training iteration: 57\n",
      "Validation loss (no improvement): 7.743936920166016\n",
      "Training iteration: 58\n",
      "Validation loss (no improvement): 7.834882354736328\n",
      "Training iteration: 59\n",
      "Validation loss (no improvement): 7.913105773925781\n",
      "Training iteration: 60\n",
      "Validation loss (no improvement): 7.977559661865234\n",
      "Training iteration: 61\n",
      "Validation loss (no improvement): 8.027796936035156\n",
      "Training iteration: 62\n",
      "Validation loss (no improvement): 8.063909912109375\n",
      "Training iteration: 63\n",
      "Validation loss (no improvement): 8.086379241943359\n",
      "Training iteration: 64\n",
      "Validation loss (no improvement): 8.096098327636719\n",
      "Training iteration: 65\n",
      "Validation loss (no improvement): 8.094171142578125\n",
      "Training iteration: 66\n",
      "Validation loss (no improvement): 8.081817626953125\n",
      "Training iteration: 67\n",
      "Validation loss (no improvement): 8.060240173339844\n",
      "Training iteration: 68\n",
      "Validation loss (no improvement): 8.030860900878906\n",
      "Training iteration: 69\n",
      "Validation loss (no improvement): 7.994786071777344\n",
      "Training iteration: 70\n",
      "Validation loss (no improvement): 7.953284454345703\n",
      "Training iteration: 71\n",
      "Validation loss (no improvement): 7.907382965087891\n",
      "Training iteration: 72\n",
      "Validation loss (no improvement): 7.858073425292969\n",
      "Training iteration: 73\n",
      "Validation loss (no improvement): 7.806134796142578\n",
      "Training iteration: 74\n",
      "Validation loss (no improvement): 7.752280426025391\n",
      "Training iteration: 75\n",
      "Validation loss (no improvement): 7.696910095214844\n",
      "Training iteration: 76\n",
      "Validation loss (no improvement): 7.6407325744628904\n",
      "Training iteration: 77\n",
      "Validation loss (no improvement): 7.584180450439453\n",
      "Training iteration: 78\n",
      "Validation loss (no improvement): 7.527620697021485\n",
      "Training iteration: 79\n",
      "Validation loss (no improvement): 7.471469116210938\n",
      "Training iteration: 80\n",
      "Validation loss (no improvement): 7.415732574462891\n",
      "Training iteration: 81\n",
      "Validation loss (no improvement): 7.360612487792968\n",
      "Training iteration: 82\n",
      "Validation loss (no improvement): 7.306248474121094\n",
      "Training iteration: 83\n",
      "Validation loss (no improvement): 7.252806091308594\n",
      "Training iteration: 84\n",
      "Validation loss (no improvement): 7.200721740722656\n",
      "Training iteration: 85\n",
      "Validation loss (no improvement): 7.1499473571777346\n",
      "Training iteration: 86\n",
      "Validation loss (no improvement): 7.100572204589843\n",
      "Training iteration: 87\n",
      "Validation loss (no improvement): 7.052708435058594\n",
      "Training iteration: 88\n",
      "Validation loss (no improvement): 7.006358337402344\n",
      "Training iteration: 89\n",
      "Validation loss (no improvement): 6.961649322509766\n",
      "Training iteration: 90\n",
      "Validation loss (no improvement): 6.918649291992187\n",
      "Training iteration: 91\n",
      "Validation loss (no improvement): 6.877519226074218\n",
      "Training iteration: 92\n",
      "Validation loss (no improvement): 6.838339996337891\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 6.818402862548828  to: 6.80093994140625\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 6.80093994140625  to: 6.765245056152343\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 6.765245056152343  to: 6.731174468994141\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 6.731174468994141  to: 6.698661041259766\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 6.698661041259766  to: 6.667756652832031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 98\n",
      "Improved validation loss from: 6.667756652832031  to: 6.63829345703125\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 6.63829345703125  to: 6.60992660522461\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 6.60992660522461  to: 6.582463073730469\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 6.582463073730469  to: 6.555765533447266\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 6.555765533447266  to: 6.529615783691407\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 6.529615783691407  to: 6.50377197265625\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 6.50377197265625  to: 6.477980041503907\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 6.477980041503907  to: 6.452091217041016\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 6.452091217041016  to: 6.425978088378907\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 6.425978088378907  to: 6.39959831237793\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 6.39959831237793  to: 6.372657394409179\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 6.372657394409179  to: 6.3450767517089846\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 6.3450767517089846  to: 6.316609573364258\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 6.316609573364258  to: 6.287329864501953\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 6.287329864501953  to: 6.257244873046875\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 6.257244873046875  to: 6.226312255859375\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 6.226312255859375  to: 6.194448089599609\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 6.194448089599609  to: 6.16169319152832\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 6.16169319152832  to: 6.127891159057617\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 6.127891159057617  to: 6.093132400512696\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 6.093132400512696  to: 6.057542037963867\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 6.057542037963867  to: 6.0210693359375\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 6.0210693359375  to: 5.983641815185547\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 5.983641815185547  to: 5.9453369140625\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 5.9453369140625  to: 5.906259918212891\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 5.906259918212891  to: 5.866461944580078\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 5.866461944580078  to: 5.8263591766357425\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 5.8263591766357425  to: 5.78583984375\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 5.78583984375  to: 5.744940185546875\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 5.744940185546875  to: 5.703926849365234\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 5.703926849365234  to: 5.6627037048339846\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 5.6627037048339846  to: 5.621234130859375\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 5.621234130859375  to: 5.5795745849609375\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 5.5795745849609375  to: 5.537906646728516\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 5.537906646728516  to: 5.49639892578125\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 5.49639892578125  to: 5.455155563354492\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 5.455155563354492  to: 5.414328765869141\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 5.414328765869141  to: 5.373837280273437\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 5.373837280273437  to: 5.333545684814453\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 5.333545684814453  to: 5.293425369262695\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 5.293425369262695  to: 5.253488922119141\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 5.253488922119141  to: 5.213787078857422\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 5.213787078857422  to: 5.17444076538086\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 5.17444076538086  to: 5.135734939575196\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 5.135734939575196  to: 5.09729232788086\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 5.09729232788086  to: 5.058993148803711\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 5.058993148803711  to: 5.020804595947266\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 5.020804595947266  to: 4.982800674438477\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 4.982800674438477  to: 4.944865036010742\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 4.944865036010742  to: 4.907352066040039\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 4.907352066040039  to: 4.8700214385986325\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 4.8700214385986325  to: 4.8328704833984375\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 4.8328704833984375  to: 4.79571304321289\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 4.79571304321289  to: 4.758679962158203\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 4.758679962158203  to: 4.721891021728515\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 4.721891021728515  to: 4.685160064697266\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 4.685160064697266  to: 4.648416137695312\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 4.648416137695312  to: 4.611589813232422\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 4.611589813232422  to: 4.574848937988281\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 4.574848937988281  to: 4.538397216796875\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 4.538397216796875  to: 4.502094650268555\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 4.502094650268555  to: 4.465892791748047\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 4.465892791748047  to: 4.42984619140625\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 4.42984619140625  to: 4.394188690185547\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 4.394188690185547  to: 4.3589111328125\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 4.3589111328125  to: 4.323814773559571\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 4.323814773559571  to: 4.288924026489258\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 4.288924026489258  to: 4.254141998291016\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 4.254141998291016  to: 4.219570922851562\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 4.219570922851562  to: 4.1854087829589846\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 4.1854087829589846  to: 4.151764678955078\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 4.151764678955078  to: 4.118278121948242\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 4.118278121948242  to: 4.08490219116211\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 4.08490219116211  to: 4.05163688659668\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 4.05163688659668  to: 4.018608856201172\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 4.018608856201172  to: 3.9859569549560545\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 3.9859569549560545  to: 3.9537895202636717\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 3.9537895202636717  to: 3.922191619873047\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 3.922191619873047  to: 3.890929412841797\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 3.890929412841797  to: 3.85992546081543\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 3.85992546081543  to: 3.8292613983154298\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 3.8292613983154298  to: 3.798899459838867\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 3.798899459838867  to: 3.768726348876953\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 3.768726348876953  to: 3.738618087768555\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 3.738618087768555  to: 3.708591842651367\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 3.708591842651367  to: 3.678601837158203\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 3.678601837158203  to: 3.6486480712890623\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 3.6486480712890623  to: 3.619024658203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 186\n",
      "Improved validation loss from: 3.619024658203125  to: 3.589682388305664\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 3.589682388305664  to: 3.5606895446777345\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 3.5606895446777345  to: 3.5320343017578124\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 3.5320343017578124  to: 3.5035430908203127\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 3.5035430908203127  to: 3.475132369995117\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 3.475132369995117  to: 3.4468345642089844\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 3.4468345642089844  to: 3.418793869018555\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 3.418793869018555  to: 3.3908973693847657\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 3.3908973693847657  to: 3.3631053924560548\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 3.3631053924560548  to: 3.3353958129882812\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 3.3353958129882812  to: 3.307778167724609\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 3.307778167724609  to: 3.280290222167969\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 3.280290222167969  to: 3.2529834747314452\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 3.2529834747314452  to: 3.226012420654297\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 3.226012420654297  to: 3.1995534896850586\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 3.1995534896850586  to: 3.1733257293701174\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 3.1733257293701174  to: 3.1472253799438477\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 3.1472253799438477  to: 3.1212270736694334\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 3.1212270736694334  to: 3.095310592651367\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 3.095310592651367  to: 3.0695667266845703\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 3.0695667266845703  to: 3.0441076278686525\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 3.0441076278686525  to: 3.018888473510742\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 3.018888473510742  to: 2.9940399169921874\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 2.9940399169921874  to: 2.9695856094360353\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 2.9695856094360353  to: 2.945477294921875\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 2.945477294921875  to: 2.9215265274047852\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 2.9215265274047852  to: 2.89766902923584\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 2.89766902923584  to: 2.8737314224243162\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 2.8737314224243162  to: 2.8497081756591798\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 2.8497081756591798  to: 2.825532150268555\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 2.825532150268555  to: 2.8013662338256835\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 2.8013662338256835  to: 2.77724666595459\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 2.77724666595459  to: 2.75308780670166\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 2.75308780670166  to: 2.728922653198242\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 2.728922653198242  to: 2.704695129394531\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 2.704695129394531  to: 2.6805435180664063\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 2.6805435180664063  to: 2.6568553924560545\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 2.6568553924560545  to: 2.63406925201416\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 2.63406925201416  to: 2.61212043762207\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 2.61212043762207  to: 2.5907812118530273\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 2.5907812118530273  to: 2.5699840545654298\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 2.5699840545654298  to: 2.549665069580078\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 2.549665069580078  to: 2.52960319519043\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 2.52960319519043  to: 2.5097450256347655\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 2.5097450256347655  to: 2.49020938873291\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 2.49020938873291  to: 2.4708745956420897\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 2.4708745956420897  to: 2.451607894897461\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 2.451607894897461  to: 2.4322895050048827\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 2.4322895050048827  to: 2.4127756118774415\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 2.4127756118774415  to: 2.393022918701172\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 2.393022918701172  to: 2.3729726791381838\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 2.3729726791381838  to: 2.352647399902344\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 2.352647399902344  to: 2.33193302154541\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 2.33193302154541  to: 2.3108566284179686\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 2.3108566284179686  to: 2.2893781661987305\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 2.2893781661987305  to: 2.2676263809204102\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 2.2676263809204102  to: 2.2458248138427734\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 2.2458248138427734  to: 2.224064254760742\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 2.224064254760742  to: 2.202516555786133\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 2.202516555786133  to: 2.1812557220458983\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 2.1812557220458983  to: 2.160360336303711\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 2.160360336303711  to: 2.1398849487304688\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 2.1398849487304688  to: 2.120395469665527\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 2.120395469665527  to: 2.1017223358154298\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 2.1017223358154298  to: 2.0837663650512694\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 2.0837663650512694  to: 2.0664670944213865\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 2.0664670944213865  to: 2.0496978759765625\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 2.0496978759765625  to: 2.0333934783935548\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 2.0333934783935548  to: 2.0175735473632814\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 2.0175735473632814  to: 2.002071189880371\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 2.002071189880371  to: 1.9867870330810546\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 1.9867870330810546  to: 1.971540641784668\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 1.971540641784668  to: 1.9562337875366211\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 1.9562337875366211  to: 1.9411123275756836\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 1.9411123275756836  to: 1.9260530471801758\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 1.9260530471801758  to: 1.9110845565795898\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 1.9110845565795898  to: 1.8961071014404296\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 1.8961071014404296  to: 1.8810842514038086\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 1.8810842514038086  to: 1.86596622467041\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 1.86596622467041  to: 1.8507238388061524\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 1.8507238388061524  to: 1.8353439331054688\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 1.8353439331054688  to: 1.8198968887329101\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 1.8198968887329101  to: 1.8043531417846679\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 1.8043531417846679  to: 1.7887142181396485\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 1.7887142181396485  to: 1.7730751037597656\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 1.7730751037597656  to: 1.7574428558349608\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 1.7574428558349608  to: 1.741872787475586\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 1.741872787475586  to: 1.7264133453369142\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 1.7264133453369142  to: 1.7111114501953124\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 1.7111114501953124  to: 1.696012306213379\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 1.696012306213379  to: 1.6811580657958984\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 1.6811580657958984  to: 1.6665653228759765\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 1.6665653228759765  to: 1.6522777557373047\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 1.6522777557373047  to: 1.6382497787475585\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 1.6382497787475585  to: 1.624485397338867\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 1.624485397338867  to: 1.6109601974487304\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 1.6109601974487304  to: 1.5976455688476563\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 1.5976455688476563  to: 1.58450927734375\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 1.58450927734375  to: 1.5715222358703613\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 1.5715222358703613  to: 1.5586492538452148\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 1.5586492538452148  to: 1.5458793640136719\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 1.5458793640136719  to: 1.5331982612609862\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 1.5331982612609862  to: 1.5205598831176759\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 1.5205598831176759  to: 1.507948875427246\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 1.507948875427246  to: 1.49536190032959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 291\n",
      "Improved validation loss from: 1.49536190032959  to: 1.4828091621398927\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 1.4828091621398927  to: 1.4702993392944337\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 1.4702993392944337  to: 1.4578234672546386\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 1.4578234672546386  to: 1.4453943252563477\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 1.4453943252563477  to: 1.4330299377441407\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 1.4330299377441407  to: 1.4207565307617187\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 1.4207565307617187  to: 1.4086209297180177\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 1.4086209297180177  to: 1.396618366241455\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 1.396618366241455  to: 1.3847564697265624\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 1.3847564697265624  to: 1.373041820526123\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 1.373041820526123  to: 1.361492919921875\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 1.361492919921875  to: 1.350103759765625\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 1.350103759765625  to: 1.3387109756469726\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 1.3387109756469726  to: 1.3273386001586913\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 1.3273386001586913  to: 1.3160125732421875\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 1.3160125732421875  to: 1.3047589302062987\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 1.3047589302062987  to: 1.2935998916625977\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 1.2935998916625977  to: 1.2824930191040038\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 1.2824930191040038  to: 1.2714969635009765\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 1.2714969635009765  to: 1.2605844497680665\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 1.2605844497680665  to: 1.2497833251953125\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 1.2497833251953125  to: 1.2390630722045899\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 1.2390630722045899  to: 1.2284649848937987\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 1.2284649848937987  to: 1.2180233001708984\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 1.2180233001708984  to: 1.2077632904052735\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 1.2077632904052735  to: 1.1977020263671876\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 1.1977020263671876  to: 1.1878190994262696\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 1.1878190994262696  to: 1.1781208038330078\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 1.1781208038330078  to: 1.1684721946716308\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 1.1684721946716308  to: 1.1589014053344726\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 1.1589014053344726  to: 1.149433708190918\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 1.149433708190918  to: 1.1400912284851075\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 1.1400912284851075  to: 1.130834197998047\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 1.130834197998047  to: 1.1216554641723633\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 1.1216554641723633  to: 1.1125833511352539\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 1.1125833511352539  to: 1.1035967826843263\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 1.1035967826843263  to: 1.094722080230713\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 1.094722080230713  to: 1.0859875679016113\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 1.0859875679016113  to: 1.0774109840393067\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 1.0774109840393067  to: 1.0689958572387694\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 1.0689958572387694  to: 1.0607221603393555\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 1.0607221603393555  to: 1.0525912284851073\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 1.0525912284851073  to: 1.0446040153503418\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 1.0446040153503418  to: 1.0367444038391114\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 1.0367444038391114  to: 1.029012107849121\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 1.029012107849121  to: 1.0213987350463867\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 1.0213987350463867  to: 1.0138968467712401\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 1.0138968467712401  to: 1.0064974784851075\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 1.0064974784851075  to: 0.9991910934448243\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.9991910934448243  to: 0.9919683456420898\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.9919683456420898  to: 0.9848161697387695\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.9848161697387695  to: 0.9777281761169434\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.9777281761169434  to: 0.9706967353820801\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.9706967353820801  to: 0.9637086868286133\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.9637086868286133  to: 0.9567626953125\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.9567626953125  to: 0.949855899810791\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.949855899810791  to: 0.9429915428161622\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.9429915428161622  to: 0.9361770629882813\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.9361770629882813  to: 0.9294207572937012\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.9294207572937012  to: 0.9227300643920898\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.9227300643920898  to: 0.9161073684692382\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.9161073684692382  to: 0.9095541000366211\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.9095541000366211  to: 0.9030776977539062\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.9030776977539062  to: 0.8966807365417481\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.8966807365417481  to: 0.8903627395629883\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.8903627395629883  to: 0.884122657775879\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.884122657775879  to: 0.8779541015625\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.8779541015625  to: 0.871854591369629\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.871854591369629  to: 0.8658223152160645\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.8658223152160645  to: 0.859852409362793\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.859852409362793  to: 0.8539423942565918\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.8539423942565918  to: 0.8480875015258789\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.8480875015258789  to: 0.8422876358032226\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.8422876358032226  to: 0.8365400314331055\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.8365400314331055  to: 0.8308433532714844\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.8308433532714844  to: 0.8251967430114746\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.8251967430114746  to: 0.8196006774902344\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.8196006774902344  to: 0.8140539169311524\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.8140539169311524  to: 0.8085573196411133\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.8085573196411133  to: 0.8031133651733399\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.8031133651733399  to: 0.7977227687835693\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.7977227687835693  to: 0.7923803806304932\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.7923803806304932  to: 0.7870259761810303\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.7870259761810303  to: 0.7816913604736329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 375\n",
      "Improved validation loss from: 0.7816913604736329  to: 0.7763805389404297\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.7763805389404297  to: 0.771114730834961\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.771114730834961  to: 0.76589994430542\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.76589994430542  to: 0.7607252597808838\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.7607252597808838  to: 0.7555838108062745\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.7555838108062745  to: 0.7504570007324218\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.7504570007324218  to: 0.7453145027160645\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.7453145027160645  to: 0.7402133941650391\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.7402133941650391  to: 0.7351521968841552\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.7351521968841552  to: 0.7301361083984375\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.7301361083984375  to: 0.7251667976379395\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.7251667976379395  to: 0.7202452182769775\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.7202452182769775  to: 0.7153679847717285\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.7153679847717285  to: 0.7105369567871094\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.7105369567871094  to: 0.7057530403137207\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.7057530403137207  to: 0.7010179042816163\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.7010179042816163  to: 0.6963307380676269\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.6963307380676269  to: 0.691688060760498\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.691688060760498  to: 0.6870892524719239\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.6870892524719239  to: 0.6825309753417969\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.6825309753417969  to: 0.678012466430664\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.678012466430664  to: 0.6735320091247559\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.6735320091247559  to: 0.669088888168335\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.669088888168335  to: 0.6646836757659912\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.6646836757659912  to: 0.6603163242340088\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.6603163242340088  to: 0.655987548828125\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.655987548828125  to: 0.6516976833343506\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.6516976833343506  to: 0.6474467277526855\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.6474467277526855  to: 0.6432388305664063\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.6432388305664063  to: 0.6390751838684082\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.6390751838684082  to: 0.6349573612213135\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.6349573612213135  to: 0.6308454036712646\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.6308454036712646  to: 0.6267541885375977\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.6267541885375977  to: 0.6226977825164794\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.6226977825164794  to: 0.6186785697937012\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.6186785697937012  to: 0.6147106170654297\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.6147106170654297  to: 0.6107981681823731\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.6107981681823731  to: 0.6069509983062744\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.6069509983062744  to: 0.6031764507293701\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.6031764507293701  to: 0.599479866027832\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.599479866027832  to: 0.5958613395690918\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.5958613395690918  to: 0.5923164367675782\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.5923164367675782  to: 0.5888362884521484\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.5888362884521484  to: 0.585411548614502\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.585411548614502  to: 0.5820306301116943\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.5820306301116943  to: 0.5786789894104004\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.5786789894104004  to: 0.5753468990325927\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.5753468990325927  to: 0.5720292091369629\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.5720292091369629  to: 0.5686688423156738\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.5686688423156738  to: 0.5652714729309082\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.5652714729309082  to: 0.5618504524230957\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.5618504524230957  to: 0.5584236145019531\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.5584236145019531  to: 0.5550095558166503\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.5550095558166503  to: 0.5516269683837891\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.5516269683837891  to: 0.5482913970947265\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.5482913970947265  to: 0.5450161933898926\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.5450161933898926  to: 0.5418072700500488\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.5418072700500488  to: 0.53866605758667\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.53866605758667  to: 0.5355869770050049\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.5355869770050049  to: 0.5325613498687745\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.5325613498687745  to: 0.5295777320861816\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.5295777320861816  to: 0.5266257286071777\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.5266257286071777  to: 0.5236951828002929\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.5236951828002929  to: 0.5207768440246582\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.5207768440246582  to: 0.5178650379180908\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.5178650379180908  to: 0.5149568557739258\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.5149568557739258  to: 0.5120526313781738\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.5120526313781738  to: 0.5091548919677734\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.5091548919677734  to: 0.5062670230865478\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.5062670230865478  to: 0.5033950805664062\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.5033950805664062  to: 0.5005444526672364\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.5005444526672364  to: 0.4977219581604004\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.4977219581604004  to: 0.49493017196655276\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.49493017196655276  to: 0.4921736240386963\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.4921736240386963  to: 0.48945131301879885\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.48945131301879885  to: 0.48676314353942873\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.48676314353942873  to: 0.48410744667053224\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.48410744667053224  to: 0.48148131370544434\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.48148131370544434  to: 0.4788814544677734\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.4788814544677734  to: 0.4763047218322754\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.4763047218322754  to: 0.47374787330627444\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.47374787330627444  to: 0.47121849060058596\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.47121849060058596  to: 0.46870927810668944\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.46870927810668944  to: 0.4662197113037109\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.4662197113037109  to: 0.46374216079711916\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.46374216079711916  to: 0.4612706661224365\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.4612706661224365  to: 0.45880327224731443\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.45880327224731443  to: 0.45633845329284667\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.45633845329284667  to: 0.4538785457611084\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.4538785457611084  to: 0.4514345645904541\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.4514345645904541  to: 0.44901022911071775\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.44901022911071775  to: 0.4466094017028809\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.4466094017028809  to: 0.44423556327819824\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.44423556327819824  to: 0.4418903350830078\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.4418903350830078  to: 0.4395880222320557\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.4395880222320557  to: 0.4373642444610596\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.4373642444610596  to: 0.43520798683166506\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.43520798683166506  to: 0.4331029415130615\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.4331029415130615  to: 0.4310313701629639\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.4310313701629639  to: 0.42897462844848633\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.42897462844848633  to: 0.42691569328308104\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.42691569328308104  to: 0.42485437393188474\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.42485437393188474  to: 0.4227793216705322\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.4227793216705322  to: 0.42068328857421877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 479\n",
      "Improved validation loss from: 0.42068328857421877  to: 0.4185627460479736\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.4185627460479736  to: 0.4164191722869873\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.4164191722869873  to: 0.4142577648162842\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.4142577648162842  to: 0.41208643913269044\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.41208643913269044  to: 0.409988260269165\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.409988260269165  to: 0.4079486846923828\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.4079486846923828  to: 0.40599045753479\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.40599045753479  to: 0.4041476249694824\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.4041476249694824  to: 0.4023001670837402\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.4023001670837402  to: 0.4004383087158203\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.4004383087158203  to: 0.3985571384429932\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.3985571384429932  to: 0.396583104133606\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.396583104133606  to: 0.39453911781311035\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.39453911781311035  to: 0.3924560546875\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.3924560546875  to: 0.39037063121795657\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.39037063121795657  to: 0.3883160352706909\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.3883160352706909  to: 0.3862579345703125\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.3862579345703125  to: 0.38422203063964844\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.38422203063964844  to: 0.3821713924407959\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.3821713924407959  to: 0.38013222217559817\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.38013222217559817  to: 0.3781306028366089\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.3781306028366089  to: 0.3761822462081909\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.3761822462081909  to: 0.37429251670837405\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.37429251670837405  to: 0.37245903015136717\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.37245903015136717  to: 0.37067131996154784\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.37067131996154784  to: 0.3689144134521484\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.3689144134521484  to: 0.36717255115509034\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.36717255115509034  to: 0.36542975902557373\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.36542975902557373  to: 0.3636742830276489\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.3636742830276489  to: 0.3618972063064575\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.3618972063064575  to: 0.36009769439697265\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.36009769439697265  to: 0.3582806348800659\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.3582806348800659  to: 0.3564539670944214\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.3564539670944214  to: 0.3546300411224365\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.3546300411224365  to: 0.3528205156326294\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.3528205156326294  to: 0.35103442668914797\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.35103442668914797  to: 0.34927849769592284\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.34927849769592284  to: 0.3475563764572144\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.3475563764572144  to: 0.345879054069519\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.345879054069519  to: 0.3442413330078125\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.3442413330078125  to: 0.3426352024078369\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.3426352024078369  to: 0.3410648345947266\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.3410648345947266  to: 0.33953323364257815\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.33953323364257815  to: 0.33800544738769533\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.33800544738769533  to: 0.3364742279052734\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.3364742279052734  to: 0.3349373579025269\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.3349373579025269  to: 0.33336989879608153\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.33336989879608153  to: 0.33184630870819093\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.33184630870819093  to: 0.3303819179534912\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.3303819179534912  to: 0.32896201610565184\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.32896201610565184  to: 0.3275702714920044\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.3275702714920044  to: 0.32619011402130127\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.32619011402130127  to: 0.32480454444885254\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.32480454444885254  to: 0.3233990430831909\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.3233990430831909  to: 0.3219631195068359\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.3219631195068359  to: 0.3204843521118164\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.3204843521118164  to: 0.31894354820251464\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.31894354820251464  to: 0.317370867729187\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.317370867729187  to: 0.3157564640045166\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.3157564640045166  to: 0.31408960819244386\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.31408960819244386  to: 0.3124134302139282\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.3124134302139282  to: 0.31073009967803955\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.31073009967803955  to: 0.3090566873550415\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.3090566873550415  to: 0.30740957260131835\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.30740957260131835  to: 0.30580189228057864\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.30580189228057864  to: 0.304240083694458\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.304240083694458  to: 0.30272955894470216\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.30272955894470216  to: 0.3012660264968872\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.3012660264968872  to: 0.2998421907424927\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.2998421907424927  to: 0.298444128036499\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.298444128036499  to: 0.2971048355102539\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.2971048355102539  to: 0.29579477310180663\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.29579477310180663  to: 0.2944368839263916\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.2944368839263916  to: 0.2930168867111206\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.2930168867111206  to: 0.29153075218200686\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.29153075218200686  to: 0.28998818397521975\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.28998818397521975  to: 0.2884079456329346\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.2884079456329346  to: 0.28681511878967286\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.28681511878967286  to: 0.2852363109588623\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.2852363109588623  to: 0.2837196111679077\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.2837196111679077  to: 0.2822716236114502\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.2822716236114502  to: 0.28089027404785155\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.28089027404785155  to: 0.2795656681060791\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.2795656681060791  to: 0.27828383445739746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 563\n",
      "Improved validation loss from: 0.27828383445739746  to: 0.27704243659973143\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.27704243659973143  to: 0.27586994171142576\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.27586994171142576  to: 0.27467405796051025\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.27467405796051025  to: 0.2734398365020752\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.2734398365020752  to: 0.27216129302978515\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.27216129302978515  to: 0.2708427429199219\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.2708427429199219  to: 0.2694962978363037\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.2694962978363037  to: 0.2681387424468994\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.2681387424468994  to: 0.26678683757781985\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.26678683757781985  to: 0.26545860767364504\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.26545860767364504  to: 0.2641671895980835\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.2641671895980835  to: 0.26291909217834475\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.26291909217834475  to: 0.26171419620513914\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.26171419620513914  to: 0.260563325881958\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.260563325881958  to: 0.25945067405700684\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.25945067405700684  to: 0.2583580493927002\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.2583580493927002  to: 0.25730385780334475\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.25730385780334475  to: 0.2562253952026367\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.2562253952026367  to: 0.25512454509735105\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.25512454509735105  to: 0.25399150848388674\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.25399150848388674  to: 0.2528254985809326\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.2528254985809326  to: 0.2516479015350342\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.2516479015350342  to: 0.2504629611968994\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.2504629611968994  to: 0.24931349754333496\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.24931349754333496  to: 0.2481996536254883\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.2481996536254883  to: 0.24708266258239747\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.24708266258239747  to: 0.2459869146347046\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.2459869146347046  to: 0.24489457607269288\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.24489457607269288  to: 0.2438119888305664\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.2438119888305664  to: 0.24274399280548095\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.24274399280548095  to: 0.24169449806213378\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.24169449806213378  to: 0.24066710472106934\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.24066710472106934  to: 0.23966164588928224\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.23966164588928224  to: 0.23867549896240234\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.23867549896240234  to: 0.23770554065704347\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.23770554065704347  to: 0.236747407913208\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.236747407913208  to: 0.23582720756530762\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.23582720756530762  to: 0.23493106365203859\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.23493106365203859  to: 0.2340095281600952\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.2340095281600952  to: 0.2330566167831421\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.2330566167831421  to: 0.2320451021194458\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.2320451021194458  to: 0.2309898853302002\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.2309898853302002  to: 0.2299431800842285\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.2299431800842285  to: 0.22891852855682374\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.22891852855682374  to: 0.22789375782012938\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.22789375782012938  to: 0.22688066959381104\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.22688066959381104  to: 0.22588963508605958\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.22588963508605958  to: 0.2249279260635376\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.2249279260635376  to: 0.2239973545074463\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.2239973545074463  to: 0.22309563159942628\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.22309563159942628  to: 0.22221751213073732\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.22221751213073732  to: 0.22135493755340577\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.22135493755340577  to: 0.22049908638000487\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.22049908638000487  to: 0.21966905593872071\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.21966905593872071  to: 0.21886775493621827\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.21886775493621827  to: 0.21803569793701172\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.21803569793701172  to: 0.21716914176940919\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.21716914176940919  to: 0.21627318859100342\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.21627318859100342  to: 0.21535811424255372\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.21535811424255372  to: 0.2144629716873169\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.2144629716873169  to: 0.21359293460845946\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.21359293460845946  to: 0.21272454261779786\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.21272454261779786  to: 0.21186308860778807\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.21186308860778807  to: 0.21101391315460205\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.21101391315460205  to: 0.21018028259277344\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.21018028259277344  to: 0.20936369895935059\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.20936369895935059  to: 0.2085869789123535\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.2085869789123535  to: 0.2078186273574829\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.2078186273574829  to: 0.2070756196975708\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.2070756196975708  to: 0.20632259845733641\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.20632259845733641  to: 0.20555465221405028\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.20555465221405028  to: 0.20477137565612794\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.20477137565612794  to: 0.20398142337799072\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.20398142337799072  to: 0.20321471691131593\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.20321471691131593  to: 0.20246491432189942\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.20246491432189942  to: 0.20170981884002687\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.20170981884002687  to: 0.2009650468826294\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.2009650468826294  to: 0.20022211074829102\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.20022211074829102  to: 0.19950605630874635\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.19950605630874635  to: 0.1987945556640625\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1987945556640625  to: 0.1980884552001953\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.1980884552001953  to: 0.19740699529647826\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.19740699529647826  to: 0.19672613143920897\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.19672613143920897  to: 0.19604365825653075\n",
      "Training iteration: 647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.19604365825653075  to: 0.19537729024887085\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.19537729024887085  to: 0.19470113515853882\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.19470113515853882  to: 0.19401512145996094\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.19401512145996094  to: 0.193341064453125\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.193341064453125  to: 0.19267202615737916\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.19267202615737916  to: 0.19202167987823487\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.19202167987823487  to: 0.1913646340370178\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1913646340370178  to: 0.19069740772247315\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.19069740772247315  to: 0.19003727436065673\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.19003727436065673  to: 0.18936364650726317\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.18936364650726317  to: 0.18869686126708984\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.18869686126708984  to: 0.18803634643554687\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.18803634643554687  to: 0.18736560344696046\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.18736560344696046  to: 0.18668845891952515\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.18668845891952515  to: 0.1859950065612793\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.1859950065612793  to: 0.1853116273880005\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.1853116273880005  to: 0.1846318483352661\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.1846318483352661  to: 0.18394933938980101\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.18394933938980101  to: 0.18327243328094484\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.18327243328094484  to: 0.18260955810546875\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.18260955810546875  to: 0.18197920322418212\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.18197920322418212  to: 0.18136684894561766\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.18136684894561766  to: 0.18076956272125244\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.18076956272125244  to: 0.18018312454223634\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.18018312454223634  to: 0.17960227727890016\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.17960227727890016  to: 0.17904077768325805\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.17904077768325805  to: 0.17848879098892212\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.17848879098892212  to: 0.17792390584945678\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.17792390584945678  to: 0.17734096050262452\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.17734096050262452  to: 0.17674823999404907\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.17674823999404907  to: 0.17615823745727538\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.17615823745727538  to: 0.17557919025421143\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.17557919025421143  to: 0.1750080704689026\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.1750080704689026  to: 0.17444114685058593\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.17444114685058593  to: 0.17386581897735595\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.17386581897735595  to: 0.17328231334686278\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.17328231334686278  to: 0.17270476818084718\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.17270476818084718  to: 0.17213480472564696\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.17213480472564696  to: 0.17156282663345337\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.17156282663345337  to: 0.17099285125732422\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.17099285125732422  to: 0.1704277753829956\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.1704277753829956  to: 0.16987072229385375\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.16987072229385375  to: 0.16931517124176027\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.16931517124176027  to: 0.16876519918441774\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.16876519918441774  to: 0.16823066473007203\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.16823066473007203  to: 0.16770637035369873\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.16770637035369873  to: 0.16719200611114501\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.16719200611114501  to: 0.16669108867645263\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.16669108867645263  to: 0.16620270013809205\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.16620270013809205  to: 0.16572086811065673\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.16572086811065673  to: 0.16523993015289307\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.16523993015289307  to: 0.16475555896759034\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.16475555896759034  to: 0.16426578760147095\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.16426578760147095  to: 0.16377086639404298\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.16377086639404298  to: 0.1632826805114746\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.1632826805114746  to: 0.16279462575912476\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.16279462575912476  to: 0.1623099684715271\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.1623099684715271  to: 0.16183277368545532\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.16183277368545532  to: 0.161365282535553\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.161365282535553  to: 0.16090844869613646\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.16090844869613646  to: 0.1604621171951294\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.1604621171951294  to: 0.16002439260482787\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.16002439260482787  to: 0.15959266424179078\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.15959266424179078  to: 0.15916398763656617\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.15916398763656617  to: 0.15873613357543945\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.15873613357543945  to: 0.1583075761795044\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.1583075761795044  to: 0.1578775405883789\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.1578775405883789  to: 0.15745408535003663\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.15745408535003663  to: 0.15703699588775635\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.15703699588775635  to: 0.15662652254104614\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.15662652254104614  to: 0.15622150897979736\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.15622150897979736  to: 0.15585670471191407\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.15585670471191407  to: 0.1555253744125366\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.1555253744125366  to: 0.1552155137062073\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.1552155137062073  to: 0.1549142837524414\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.1549142837524414  to: 0.15460866689682007\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.15460866689682007  to: 0.15428757667541504\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.15428757667541504  to: 0.15394452810287476\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.15394452810287476  to: 0.15357666015625\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.15357666015625  to: 0.1531861901283264\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.1531861901283264  to: 0.1527793288230896\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1527793288230896  to: 0.15236375331878663\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.15236375331878663  to: 0.1519463300704956\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.1519463300704956  to: 0.15153884887695312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 731\n",
      "Improved validation loss from: 0.15153884887695312  to: 0.1511479139328003\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.1511479139328003  to: 0.1507746934890747\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.1507746934890747  to: 0.15042022466659546\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.15042022466659546  to: 0.15008153915405273\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.15008153915405273  to: 0.149754536151886\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.149754536151886  to: 0.14943327903747558\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.14943327903747558  to: 0.14911208152770997\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.14911208152770997  to: 0.14878517389297485\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.14878517389297485  to: 0.14844896793365478\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.14844896793365478  to: 0.14810127019882202\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.14810127019882202  to: 0.14774234294891359\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.14774234294891359  to: 0.1473742127418518\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.1473742127418518  to: 0.14700031280517578\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.14700031280517578  to: 0.14662500619888305\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.14662500619888305  to: 0.14625239372253418\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.14625239372253418  to: 0.14588623046875\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.14588623046875  to: 0.14552890062332152\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.14552890062332152  to: 0.14518134593963622\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.14518134593963622  to: 0.14484317302703859\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.14484317302703859  to: 0.14451425075531005\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.14451425075531005  to: 0.14419071674346923\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.14419071674346923  to: 0.14386955499649048\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.14386955499649048  to: 0.14354785680770873\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.14354785680770873  to: 0.14322344064712525\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.14322344064712525  to: 0.14289488792419433\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.14289488792419433  to: 0.14256235361099243\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.14256235361099243  to: 0.1422266960144043\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1422266960144043  to: 0.14188897609710693\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.14188897609710693  to: 0.1415517210960388\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.1415517210960388  to: 0.141216504573822\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.141216504573822  to: 0.14088541269302368\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.14088541269302368  to: 0.1405594229698181\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.1405594229698181  to: 0.14023952484130858\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.14023952484130858  to: 0.13992515802383423\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.13992515802383423  to: 0.1396157145500183\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.1396157145500183  to: 0.13931043148040773\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.13931043148040773  to: 0.13900786638259888\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.13900786638259888  to: 0.1387065529823303\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.1387065529823303  to: 0.13840552568435668\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.13840552568435668  to: 0.13810403347015382\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.13810403347015382  to: 0.13780239820480347\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.13780239820480347  to: 0.13750025033950805\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.13750025033950805  to: 0.1371986150741577\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.1371986150741577  to: 0.13689794540405273\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.13689794540405273  to: 0.1365993618965149\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.1365993618965149  to: 0.13630404472351074\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.13630404472351074  to: 0.13602275848388673\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.13602275848388673  to: 0.13575847148895265\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.13575847148895265  to: 0.13550963401794433\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.13550963401794433  to: 0.1352723240852356\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.1352723240852356  to: 0.13504279851913453\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.13504279851913453  to: 0.13481563329696655\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.13481563329696655  to: 0.13458664417266847\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.13458664417266847  to: 0.13435219526290892\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.13435219526290892  to: 0.13411033153533936\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.13411033153533936  to: 0.13386051654815673\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.13386051654815673  to: 0.13360331058502198\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.13360331058502198  to: 0.1333407759666443\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.1333407759666443  to: 0.133075749874115\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.133075749874115  to: 0.13281093835830687\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.13281093835830687  to: 0.1325497031211853\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.1325497031211853  to: 0.1322939157485962\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.1322939157485962  to: 0.1320446252822876\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.1320446252822876  to: 0.13180216550827026\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.13180216550827026  to: 0.13156508207321166\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.13156508207321166  to: 0.13133230209350585\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.13133230209350585  to: 0.1311017632484436\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.1311017632484436  to: 0.1308712601661682\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.1308712601661682  to: 0.13063900470733641\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.13063900470733641  to: 0.13040411472320557\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.13040411472320557  to: 0.13016577959060668\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.13016577959060668  to: 0.1299237608909607\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.1299237608909607  to: 0.12967956066131592\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12967956066131592  to: 0.12943347692489623\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12943347692489623  to: 0.12918730974197387\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12918730974197387  to: 0.12894251346588134\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12894251346588134  to: 0.12869974374771118\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12869974374771118  to: 0.1284605860710144\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.1284605860710144  to: 0.12822463512420654\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.12822463512420654  to: 0.12799203395843506\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12799203395843506  to: 0.12776215076446534\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12776215076446534  to: 0.1275342583656311\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.1275342583656311  to: 0.1273071885108948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 814\n",
      "Improved validation loss from: 0.1273071885108948  to: 0.12708054780960082\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12708054780960082  to: 0.126853346824646\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.126853346824646  to: 0.1266253709793091\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.1266253709793091  to: 0.1263965368270874\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.1263965368270874  to: 0.12616714239120483\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12616714239120483  to: 0.12593761682510377\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12593761682510377  to: 0.1257089853286743\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.1257089853286743  to: 0.1254812479019165\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.1254812479019165  to: 0.12525515556335448\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12525515556335448  to: 0.1250314950942993\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.1250314950942993  to: 0.1248096227645874\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.1248096227645874  to: 0.12458993196487426\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12458993196487426  to: 0.12437231540679931\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12437231540679931  to: 0.12415612936019897\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12415612936019897  to: 0.12394130229949951\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12394130229949951  to: 0.12372725009918213\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12372725009918213  to: 0.12351403236389161\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.12351403236389161  to: 0.12330077886581421\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12330077886581421  to: 0.12308790683746337\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12308790683746337  to: 0.1228752613067627\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1228752613067627  to: 0.12266287803649903\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12266287803649903  to: 0.12245134115219117\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12245134115219117  to: 0.1222407341003418\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.1222407341003418  to: 0.12203124761581421\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12203124761581421  to: 0.12182300090789795\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12182300090789795  to: 0.12161637544631958\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12161637544631958  to: 0.1214111328125\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1214111328125  to: 0.12120727300643921\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12120727300643921  to: 0.12100474834442139\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12100474834442139  to: 0.12080414295196533\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12080414295196533  to: 0.12060527801513672\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12060527801513672  to: 0.12040678262710572\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12040678262710572  to: 0.12020841836929322\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.12020841836929322  to: 0.12000977993011475\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12000977993011475  to: 0.11981017589569092\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.11981017589569092  to: 0.11961042881011963\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11961042881011963  to: 0.11941089630126953\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11941089630126953  to: 0.11921237707138062\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11921237707138062  to: 0.11901525259017945\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.11901525259017945  to: 0.1188198208808899\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.1188198208808899  to: 0.1186260461807251\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.1186260461807251  to: 0.11843410730361939\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.11843410730361939  to: 0.11824331283569336\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11824331283569336  to: 0.11805392503738403\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.11805392503738403  to: 0.11786472797393799\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.11786472797393799  to: 0.1176756501197815\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.1176756501197815  to: 0.11748676300048828\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.11748676300048828  to: 0.1172980546951294\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.1172980546951294  to: 0.11710937023162842\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.11710937023162842  to: 0.11692105531692505\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.11692105531692505  to: 0.11673336029052735\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.11673336029052735  to: 0.1165468692779541\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1165468692779541  to: 0.11636140346527099\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.11636140346527099  to: 0.11617708206176758\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.11617708206176758  to: 0.1159939169883728\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.1159939169883728  to: 0.11581165790557861\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.11581165790557861  to: 0.11563012599945069\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.11563012599945069  to: 0.11544917821884156\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.11544917821884156  to: 0.1152686357498169\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.1152686357498169  to: 0.11508857011795044\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.11508857011795044  to: 0.11490908861160279\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.11490908861160279  to: 0.1147301197052002\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.1147301197052002  to: 0.11455143690109253\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.11455143690109253  to: 0.11437394618988037\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.11437394618988037  to: 0.11419732570648193\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.11419732570648193  to: 0.1140211820602417\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.1140211820602417  to: 0.11384618282318115\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.11384618282318115  to: 0.113671875\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.113671875  to: 0.11349797248840332\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.11349797248840332  to: 0.11332486867904663\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.11332486867904663  to: 0.11315206289291382\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.11315206289291382  to: 0.11297996044158935\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.11297996044158935  to: 0.1128082275390625\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.1128082275390625  to: 0.11263729333877563\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.11263729333877563  to: 0.11246681213378906\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.11246681213378906  to: 0.11229698657989502\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.11229698657989502  to: 0.1121280312538147\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.1121280312538147  to: 0.11195946931838989\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.11195946931838989  to: 0.11179188489913941\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.11179188489913941  to: 0.1116248607635498\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.1116248607635498  to: 0.11145855188369751\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.11145855188369751  to: 0.11129258871078491\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.11129258871078491  to: 0.11112715005874634\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.11112715005874634  to: 0.11096242666244507\n",
      "Training iteration: 898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11096242666244507  to: 0.11079814434051513\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.11079814434051513  to: 0.11063457727432251\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.11063457727432251  to: 0.11047152280807496\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.11047152280807496  to: 0.1103089451789856\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.1103089451789856  to: 0.11014724969863891\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.11014724969863891  to: 0.1099860429763794\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.1099860429763794  to: 0.1098253846168518\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.1098253846168518  to: 0.1096653699874878\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.1096653699874878  to: 0.10950980186462403\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.10950980186462403  to: 0.10935759544372559\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.10935759544372559  to: 0.10920751094818115\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.10920751094818115  to: 0.10905859470367432\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.10905859470367432  to: 0.1089091420173645\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.1089091420173645  to: 0.10875861644744873\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.10875861644744873  to: 0.10860584974288941\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.10860584974288941  to: 0.1084509015083313\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.1084509015083313  to: 0.10829436779022217\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.10829436779022217  to: 0.10813663005828858\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.10813663005828858  to: 0.10797852277755737\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.10797852277755737  to: 0.10782115459442139\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.10782115459442139  to: 0.10766514539718627\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.10766514539718627  to: 0.10751106739044189\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.10751106739044189  to: 0.10735901594161987\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.10735901594161987  to: 0.10720891952514648\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.10720891952514648  to: 0.10706040859222413\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.10706040859222413  to: 0.10691264867782593\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.10691264867782593  to: 0.1067652702331543\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.1067652702331543  to: 0.10661804676055908\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.10661804676055908  to: 0.10647017955780029\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.10647017955780029  to: 0.10632164478302002\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.10632164478302002  to: 0.10617233514785766\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.10617233514785766  to: 0.10602284669876098\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.10602284669876098  to: 0.1058732271194458\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.1058732271194458  to: 0.10572410821914673\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.10572410821914673  to: 0.1055752992630005\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.1055752992630005  to: 0.10542752742767333\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.10542752742767333  to: 0.10528087615966797\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.10528087615966797  to: 0.10513566732406616\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.10513566732406616  to: 0.1049920678138733\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.1049920678138733  to: 0.10485014915466309\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.10485014915466309  to: 0.10470912456512452\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.10470912456512452  to: 0.10456879138946533\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.10456879138946533  to: 0.10442879199981689\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.10442879199981689  to: 0.10428913831710815\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.10428913831710815  to: 0.10414904356002808\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.10414904356002808  to: 0.10400886535644531\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.10400886535644531  to: 0.10386886596679687\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.10386886596679687  to: 0.10372920036315918\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.10372920036315918  to: 0.10358991622924804\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.10358991622924804  to: 0.10345183610916138\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.10345183610916138  to: 0.10331504344940186\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.10331504344940186  to: 0.10317938327789307\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.10317938327789307  to: 0.10304458141326904\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.10304458141326904  to: 0.10291098356246949\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.10291098356246949  to: 0.10277764797210694\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.10277764797210694  to: 0.10264469385147094\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.10264469385147094  to: 0.10251185894012452\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.10251185894012452  to: 0.1023790955543518\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.1023790955543518  to: 0.10224698781967163\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.10224698781967163  to: 0.1021154761314392\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.1021154761314392  to: 0.10198460817337036\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.10198460817337036  to: 0.10185507535934449\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.10185507535934449  to: 0.10172592401504517\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.10172592401504517  to: 0.10159789323806763\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.10159789323806763  to: 0.1014704942703247\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.1014704942703247  to: 0.10134375095367432\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.10134375095367432  to: 0.10121774673461914\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.10121774673461914  to: 0.1010931134223938\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.1010931134223938  to: 0.10096931457519531\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.10096931457519531  to: 0.10084565877914428\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.10084565877914428  to: 0.10072170495986939\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.10072170495986939  to: 0.10059715509414673\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.10059715509414673  to: 0.10047166347503662\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.10047166347503662  to: 0.10034562349319458\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.10034562349319458  to: 0.10021921396255493\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.10021921396255493  to: 0.10009276866912842\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.10009276866912842  to: 0.09996625781059265\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.09996625781059265  to: 0.09984076619148255\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.09984076619148255  to: 0.09971548914909363\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.09971548914909363  to: 0.09959136843681335\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.09959136843681335  to: 0.0994679570198059\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.0994679570198059  to: 0.09934532046318054\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.09934532046318054  to: 0.09922329187393189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 981\n",
      "Improved validation loss from: 0.09922329187393189  to: 0.09910200834274292\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.09910200834274292  to: 0.09898106455802917\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.09898106455802917  to: 0.09886037707328796\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.09886037707328796  to: 0.09873994588851928\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.09873994588851928  to: 0.09861981272697448\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.09861981272697448  to: 0.0984997272491455\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.0984997272491455  to: 0.09837978482246398\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.09837978482246398  to: 0.09826031923294068\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.09826031923294068  to: 0.09814144372940063\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.09814144372940063  to: 0.09802284240722656\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.09802284240722656  to: 0.09790515899658203\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.09790515899658203  to: 0.09778796434402466\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.09778796434402466  to: 0.09767155647277832\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.09767155647277832  to: 0.09755263328552247\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.09755263328552247  to: 0.0974319338798523\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.0974319338798523  to: 0.09731112718582154\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.09731112718582154  to: 0.09719129800796508\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.09719129800796508  to: 0.09707466363906861\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.09707466363906861  to: 0.09696058034896851\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.09696058034896851  to: 0.09684916734695434\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.09684916734695434  to: 0.09673874974250793\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.09673874974250793  to: 0.09662830233573913\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.09662830233573913  to: 0.09651695489883423\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.09651695489883423  to: 0.09640412330627442\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.09640412330627442  to: 0.09628992080688477\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.09628992080688477  to: 0.09617481231689454\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.09617481231689454  to: 0.09605981111526489\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.09605981111526489  to: 0.09594593048095704\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.09594593048095704  to: 0.09583374261856079\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.09583374261856079  to: 0.09572347402572631\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.09572347402572631  to: 0.09561471939086914\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.09561471939086914  to: 0.09550714492797852\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.09550714492797852  to: 0.0954000174999237\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.0954000174999237  to: 0.0952924907207489\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.0952924907207489  to: 0.09518418312072754\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.09518418312072754  to: 0.09507516026496887\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.09507516026496887  to: 0.09496566653251648\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.09496566653251648  to: 0.09485604166984558\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.09485604166984558  to: 0.0947468101978302\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.0947468101978302  to: 0.09463868141174317\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.09463868141174317  to: 0.09453177452087402\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.09453177452087402  to: 0.094426029920578\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.094426029920578  to: 0.0943210780620575\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.0943210780620575  to: 0.09421717524528503\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.09421717524528503  to: 0.09411330223083496\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.09411330223083496  to: 0.09400944709777832\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.09400944709777832  to: 0.09391885995864868\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.09391885995864868  to: 0.09384075999259948\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.09384075999259948  to: 0.09377185702323913\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.09377185702323913  to: 0.09370871782302856\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.09370871782302856  to: 0.09364711642265319\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.09364711642265319  to: 0.0935832142829895\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.0935832142829895  to: 0.09351431727409362\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.09351431727409362  to: 0.09343894124031067\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.09343894124031067  to: 0.0933568000793457\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.0933568000793457  to: 0.09326923489570618\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.09326923489570618  to: 0.09317823648452758\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.09317823648452758  to: 0.09308645129203796\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.09308645129203796  to: 0.09299659729003906\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.09299659729003906  to: 0.09291016459465026\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.09291016459465026  to: 0.09282814264297486\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.09282814264297486  to: 0.09275090098381042\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.09275090098381042  to: 0.09267700910568237\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.09267700910568237  to: 0.0926051139831543\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.0926051139831543  to: 0.0925329089164734\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.0925329089164734  to: 0.0924589455127716\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.0924589455127716  to: 0.09238139986991882\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.09238139986991882  to: 0.0922991931438446\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.0922991931438446  to: 0.0922126293182373\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.0922126293182373  to: 0.09212223291397095\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.09212223291397095  to: 0.09202906489372253\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.09202906489372253  to: 0.0919345498085022\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.0919345498085022  to: 0.09184049367904663\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.09184049367904663  to: 0.09174784421920776\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.09174784421920776  to: 0.09165767431259156\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.09165767431259156  to: 0.09156994819641114\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.09156994819641114  to: 0.09148479700088501\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.09148479700088501  to: 0.09140065312385559\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.09140065312385559  to: 0.09131744503974915\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.09131744503974915  to: 0.09123383760452271\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.09123383760452271  to: 0.09114938974380493\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.09114938974380493  to: 0.0910631537437439\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.0910631537437439  to: 0.09097549319267273\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.09097549319267273  to: 0.0908867061138153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.0908867061138153  to: 0.09079712033271789\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.09079712033271789  to: 0.0907072365283966\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.0907072365283966  to: 0.09061802029609681\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.09061802029609681  to: 0.09053013920783996\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.09053013920783996  to: 0.09044357538223266\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.09044357538223266  to: 0.0903582215309143\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.0903582215309143  to: 0.09027441143989563\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.09027441143989563  to: 0.09019168615341186\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.09019168615341186  to: 0.09010937809944153\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.09010937809944153  to: 0.09002714157104492\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.09002714157104492  to: 0.0899444878101349\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.0899444878101349  to: 0.08986116647720337\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.08986116647720337  to: 0.08977724313735962\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.08977724313735962  to: 0.08969236612319946\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.08969236612319946  to: 0.08960716128349304\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.08960716128349304  to: 0.08952170610427856\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.08952170610427856  to: 0.08943626284599304\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.08943626284599304  to: 0.08935108184814453\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.08935108184814453  to: 0.08926637768745423\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.08926637768745423  to: 0.0891845703125\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.0891845703125  to: 0.08911036252975464\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.08911036252975464  to: 0.08903678059577942\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.08903678059577942  to: 0.08896293640136718\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.08896293640136718  to: 0.08888923525810241\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.08888923525810241  to: 0.08881500363349915\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.08881500363349915  to: 0.088740473985672\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.088740473985672  to: 0.08866556882858276\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.08866556882858276  to: 0.08859007954597473\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.08859007954597473  to: 0.08851404190063476\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.08851404190063476  to: 0.08843797445297241\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.08843797445297241  to: 0.08836226463317871\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.08836226463317871  to: 0.08828624486923217\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.08828624486923217  to: 0.08821061849594117\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.08821061849594117  to: 0.08813511729240417\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.08813511729240417  to: 0.088060063123703\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.088060063123703  to: 0.0879850685596466\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.0879850685596466  to: 0.08791042566299438\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.08791042566299438  to: 0.08783559799194336\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.08783559799194336  to: 0.0877611756324768\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.0877611756324768  to: 0.08767260313034057\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.08767260313034057  to: 0.08757166862487793\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.08757166862487793  to: 0.08746269941329957\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.08746269941329957  to: 0.08735253214836121\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.08735253214836121  to: 0.08724666833877563\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.08724666833877563  to: 0.08714934587478637\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.08714934587478637  to: 0.08706094622612\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.08706094622612  to: 0.08697985410690308\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.08697985410690308  to: 0.08690239787101746\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.08690239787101746  to: 0.08682458996772766\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.08682458996772766  to: 0.08674321174621583\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.08674321174621583  to: 0.08665605783462524\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.08665605783462524  to: 0.08656464815139771\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.08656464815139771  to: 0.08647068738937377\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.08647068738937377  to: 0.08637562990188599\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.08637562990188599  to: 0.0862800121307373\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.0862800121307373  to: 0.08618521690368652\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.08618521690368652  to: 0.08609237670898437\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.08609237670898437  to: 0.08600319027900696\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.08600319027900696  to: 0.08591812252998351\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.08591812252998351  to: 0.08583704233169556\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.08583704233169556  to: 0.08575981259346008\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.08575981259346008  to: 0.08568414449691772\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.08568414449691772  to: 0.08560560941696167\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.08560560941696167  to: 0.08552231788635253\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.08552231788635253  to: 0.08543461561203003\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.08543461561203003  to: 0.0853446364402771\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.0853446364402771  to: 0.08525429964065552\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.08525429964065552  to: 0.08516538739204407\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.08516538739204407  to: 0.08507877588272095\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.08507877588272095  to: 0.08499452471733093\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.08499452471733093  to: 0.08491319417953491\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.08491319417953491  to: 0.08483386039733887\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.08483386039733887  to: 0.08475543856620789\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.08475543856620789  to: 0.08467475175857545\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.08467475175857545  to: 0.08459100723266602\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.08459100723266602  to: 0.08450530171394348\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.08450530171394348  to: 0.0844189167022705\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.0844189167022705  to: 0.08433359861373901\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.08433359861373901  to: 0.08425030708312989\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.08425030708312989  to: 0.08416849374771118\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.08416849374771118  to: 0.08408675193786622\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.08408675193786622  to: 0.084004145860672\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.084004145860672  to: 0.08392122983932496\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.08392122983932496  to: 0.0838389277458191\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.0838389277458191  to: 0.08375828862190246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.08375828862190246  to: 0.08367905616760254\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.08367905616760254  to: 0.08359916806221009\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.08359916806221009  to: 0.08351666331291199\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.08351666331291199  to: 0.08343232274055482\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.08343232274055482  to: 0.08334667086601258\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.08334667086601258  to: 0.08326241374015808\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.08326241374015808  to: 0.08318090438842773\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.08318090438842773  to: 0.08310271501541137\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.08310271501541137  to: 0.08302603960037232\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.08302603960037232  to: 0.08294876813888549\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.08294876813888549  to: 0.08286798596382142\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.08286798596382142  to: 0.0827839195728302\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.0827839195728302  to: 0.08269793391227723\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.08269793391227723  to: 0.08261293172836304\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.08261293172836304  to: 0.08253122568130493\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.08253122568130493  to: 0.08245360255241393\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.08245360255241393  to: 0.08237851858139038\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.08237851858139038  to: 0.08230406641960145\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.08230406641960145  to: 0.08222712278366089\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.08222712278366089  to: 0.08214601278305053\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.08214601278305053  to: 0.08206077814102172\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.08206077814102172  to: 0.08197396397590637\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.08197396397590637  to: 0.08188926577568054\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.08188926577568054  to: 0.08180816769599915\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.08180816769599915  to: 0.08173059225082398\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.08173059225082398  to: 0.08165499567985535\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.08165499567985535  to: 0.08157910108566284\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.08157910108566284  to: 0.08150181770324708\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.08150181770324708  to: 0.08142305612564087\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.08142305612564087  to: 0.08134398460388184\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.08134398460388184  to: 0.08126499056816101\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.08126499056816101  to: 0.08118605613708496\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.08118605613708496  to: 0.08110331296920777\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.08110331296920777  to: 0.08101696968078613\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.08101696968078613  to: 0.08092872500419616\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.08092872500419616  to: 0.08083783388137818\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.08083783388137818  to: 0.08074721097946166\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.08074721097946166  to: 0.08065902590751647\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.08065902590751647  to: 0.08057533502578736\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.08057533502578736  to: 0.0804961383342743\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.0804961383342743  to: 0.08041931390762329\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.08041931390762329  to: 0.08034318089485168\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.08034318089485168  to: 0.080265873670578\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.080265873670578  to: 0.08018630146980285\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.08018630146980285  to: 0.08010441064834595\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.08010441064834595  to: 0.08001968264579773\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.08001968264579773  to: 0.07993372082710266\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.07993372082710266  to: 0.07984830737113953\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.07984830737113953  to: 0.07976566553115845\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.07976566553115845  to: 0.07968686819076538\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.07968686819076538  to: 0.07961229085922242\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.07961229085922242  to: 0.0795406699180603\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.0795406699180603  to: 0.07946912050247193\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.07946912050247193  to: 0.07939528226852417\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.07939528226852417  to: 0.07931773662567139\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.07931773662567139  to: 0.07923652529716492\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.07923652529716492  to: 0.07915294766426087\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.07915294766426087  to: 0.07906922698020935\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.07906922698020935  to: 0.07898696660995483\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.07898696660995483  to: 0.07890734672546387\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.07890734672546387  to: 0.07883083820343018\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.07883083820343018  to: 0.07875648736953736\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.07875648736953736  to: 0.07868330478668213\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.07868330478668213  to: 0.0786096692085266\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.0786096692085266  to: 0.07853426337242127\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.07853426337242127  to: 0.07845736742019653\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.07845736742019653  to: 0.07837844491004944\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.07837844491004944  to: 0.07829848527908326\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.07829848527908326  to: 0.07821855545043946\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.07821855545043946  to: 0.07813924551010132\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.07813924551010132  to: 0.0780603289604187\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.0780603289604187  to: 0.07798182964324951\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.07798182964324951  to: 0.07790406346321106\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.07790406346321106  to: 0.07782685160636901\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.07782685160636901  to: 0.07774994373321534\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.07774994373321534  to: 0.07767308354377747\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.07767308354377747  to: 0.07759624719619751\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.07759624719619751  to: 0.07751851081848145\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.07751851081848145  to: 0.07743993997573853\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.07743993997573853  to: 0.07736044526100158\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.07736044526100158  to: 0.07728068828582764\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.07728068828582764  to: 0.07720110416412354\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.07720110416412354  to: 0.0771224319934845\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.0771224319934845  to: 0.07704429030418396\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.07704429030418396  to: 0.07696653008460999\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.07696653008460999  to: 0.07688883543014527\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.07688883543014527  to: 0.07681120634078979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.07681120634078979  to: 0.07673327326774597\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.07673327326774597  to: 0.07665562629699707\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.07665562629699707  to: 0.07657809257507324\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.07657809257507324  to: 0.07650081515312195\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.07650081515312195  to: 0.07642402648925781\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.07642402648925781  to: 0.07634721994400025\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.07634721994400025  to: 0.07627065777778626\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.07627065777778626  to: 0.0761938214302063\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.0761938214302063  to: 0.0761161744594574\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.0761161744594574  to: 0.07603791952133179\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.07603791952133179  to: 0.07595959901809693\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.07595959901809693  to: 0.07588157057762146\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.07588157057762146  to: 0.07580406665802002\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.07580406665802002  to: 0.07572736144065857\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.07572736144065857  to: 0.07565138936042785\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.07565138936042785  to: 0.07557583451271058\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.07557583451271058  to: 0.07550050020217895\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.07550050020217895  to: 0.07542482614517212\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.07542482614517212  to: 0.07534875273704529\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.07534875273704529  to: 0.07527216672897338\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.07527216672897338  to: 0.07519542574882507\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.07519542574882507  to: 0.07511822581291198\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.07511822581291198  to: 0.07504090666770935\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.07504090666770935  to: 0.07496377229690551\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.07496377229690551  to: 0.07488717436790467\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.07488717436790467  to: 0.07481110692024232\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.07481110692024232  to: 0.07473564147949219\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.07473564147949219  to: 0.0746606469154358\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.0746606469154358  to: 0.07458567023277282\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.07458567023277282  to: 0.07451063990592957\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.07451063990592957  to: 0.0744352638721466\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.0744352638721466  to: 0.07435985803604125\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.07435985803604125  to: 0.074284029006958\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.074284029006958  to: 0.07420811057090759\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.07420811057090759  to: 0.07413209676742553\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.07413209676742553  to: 0.0740561842918396\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.0740561842918396  to: 0.07398043870925904\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.07398043870925904  to: 0.07390757203102112\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.07390757203102112  to: 0.07383660078048707\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.07383660078048707  to: 0.0737663447856903\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.0737663447856903  to: 0.07369553446769714\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.07369553446769714  to: 0.0736229419708252\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.0736229419708252  to: 0.07354789972305298\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.07354789972305298  to: 0.07347009181976319\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.07347009181976319  to: 0.07338996529579163\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.07338996529579163  to: 0.07330861091613769\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.07330861091613769  to: 0.0732275664806366\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.0732275664806366  to: 0.07314774990081788\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.07314774990081788  to: 0.07306963801383973\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.07306963801383973  to: 0.07299399375915527\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.07299399375915527  to: 0.07292017340660095\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.07292017340660095  to: 0.07284777164459229\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.07284777164459229  to: 0.07277568578720092\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.07277568578720092  to: 0.0727035641670227\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.0727035641670227  to: 0.07263014316558838\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.07263014316558838  to: 0.07255536913871766\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.07255536913871766  to: 0.07247971296310425\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.07247971296310425  to: 0.07240347266197204\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.07240347266197204  to: 0.0723271906375885\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.0723271906375885  to: 0.07225155234336852\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.07225155234336852  to: 0.07217674851417541\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.07217674851417541  to: 0.07210298776626586\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.07210298776626586  to: 0.07203026413917542\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.07203026413917542  to: 0.07195800542831421\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.07195800542831421  to: 0.07188593149185181\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.07188593149185181  to: 0.07181367874145508\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.07181367874145508  to: 0.07174099683761596\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.07174099683761596  to: 0.07166819572448731\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.07166819572448731  to: 0.0715946912765503\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.0715946912765503  to: 0.07152097821235656\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.07152097821235656  to: 0.07144702076911927\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.07144702076911927  to: 0.0713729739189148\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.0713729739189148  to: 0.07129884958267212\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.07129884958267212  to: 0.07122489213943481\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.07122489213943481  to: 0.071151202917099\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.071151202917099  to: 0.07107776403427124\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.07107776403427124  to: 0.07100443243980407\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.07100443243980407  to: 0.07093170285224915\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.07093170285224915  to: 0.07085917592048645\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.07085917592048645  to: 0.07078648805618286\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.07078648805618286  to: 0.07071385383605958\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.07071385383605958  to: 0.07064071893692017\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.07064071893692017  to: 0.0705674409866333\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.0705674409866333  to: 0.07049392461776734\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.07049392461776734  to: 0.07042037844657897\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.07042037844657897  to: 0.0703467845916748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.0703467845916748  to: 0.07027356028556823\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.07027356028556823  to: 0.07020057439804077\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.07020057439804077  to: 0.07012799382209778\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.07012799382209778  to: 0.07005617022514343\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.07005617022514343  to: 0.06998381614685059\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.06998381614685059  to: 0.06991199851036071\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.06991199851036071  to: 0.06983928084373474\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.06983928084373474  to: 0.06976627111434937\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.06976627111434937  to: 0.06969306468963624\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.06969306468963624  to: 0.06961978673934936\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.06961978673934936  to: 0.06954666376113891\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.06954666376113891  to: 0.06947354674339294\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.06947354674339294  to: 0.06940046548843384\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.06940046548843384  to: 0.06932762861251832\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.06932762861251832  to: 0.06925499439239502\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.06925499439239502  to: 0.06918244361877442\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.06918244361877442  to: 0.06911001205444336\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.06911001205444336  to: 0.06903761029243469\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.06903761029243469  to: 0.06896529197692872\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.06896529197692872  to: 0.0688928484916687\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.0688928484916687  to: 0.06882047653198242\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.06882047653198242  to: 0.06874786615371704\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.06874786615371704  to: 0.06867550611495972\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.06867550611495972  to: 0.06860322952270508\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.06860322952270508  to: 0.06853080987930298\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.06853080987930298  to: 0.06845881342887879\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.06845881342887879  to: 0.06838679313659668\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.06838679313659668  to: 0.06831483840942383\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.06831483840942383  to: 0.06824293732643127\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.06824293732643127  to: 0.06817088127136231\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.06817088127136231  to: 0.06809892654418945\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.06809892654418945  to: 0.06802693009376526\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.06802693009376526  to: 0.06795513033866882\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.06795513033866882  to: 0.06788356900215149\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.06788356900215149  to: 0.0678122341632843\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.0678122341632843  to: 0.06774100065231323\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.06774100065231323  to: 0.06766992807388306\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.06766992807388306  to: 0.06759880781173706\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.06759880781173706  to: 0.06752749085426331\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.06752749085426331  to: 0.06745632886886596\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.06745632886886596  to: 0.06738483309745788\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.06738483309745788  to: 0.06731336712837219\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.06731336712837219  to: 0.06724209189414979\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.06724209189414979  to: 0.06717080473899842\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.06717080473899842  to: 0.06709965467453002\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.06709965467453002  to: 0.06702889800071717\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.06702889800071717  to: 0.066958487033844\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.066958487033844  to: 0.0668881893157959\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.0668881893157959  to: 0.06681776642799378\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.06681776642799378  to: 0.06674758195877076\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.06674758195877076  to: 0.06667712330818176\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.06667712330818176  to: 0.06660654544830322\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.06660654544830322  to: 0.06653594374656677\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.06653594374656677  to: 0.06646525859832764\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.06646525859832764  to: 0.06639455556869507\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.06639455556869507  to: 0.06632412672042846\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.06632412672042846  to: 0.0662537932395935\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.0662537932395935  to: 0.06618388295173645\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.06618388295173645  to: 0.06611393094062805\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.06611393094062805  to: 0.0660443663597107\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.0660443663597107  to: 0.06597476601600646\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.06597476601600646  to: 0.06590531468391418\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.06590531468391418  to: 0.06583559513092041\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.06583559513092041  to: 0.06576587557792664\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.06576587557792664  to: 0.06569621562957764\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.06569621562957764  to: 0.0656265676021576\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.0656265676021576  to: 0.06555685997009278\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.06555685997009278  to: 0.06548709869384765\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.06548709869384765  to: 0.06541755795478821\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.06541755795478821  to: 0.06534824371337891\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.06534824371337891  to: 0.06527896523475647\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.06527896523475647  to: 0.0652099609375\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.0652099609375  to: 0.06514117121696472\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.06514117121696472  to: 0.06507230997085571\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.06507230997085571  to: 0.0650036633014679\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.0650036633014679  to: 0.06493495106697082\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.06493495106697082  to: 0.0648660957813263\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.0648660957813263  to: 0.06479718685150146\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.06479718685150146  to: 0.06472820043563843\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.06472820043563843  to: 0.06465931534767151\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.06465931534767151  to: 0.06459051370620728\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.06459051370620728  to: 0.06452184915542603\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.06452184915542603  to: 0.06445325016975403\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.06445325016975403  to: 0.06438499093055725\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.06438499093055725  to: 0.06431662440299987\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.06431662440299987  to: 0.06424840688705444\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.06424840688705444  to: 0.06418023705482483\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.06418023705482483  to: 0.06411200165748596\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.06411200165748596  to: 0.06404387950897217\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.06404387950897217  to: 0.06397574543952941\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.06397574543952941  to: 0.06390761137008667\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.06390761137008667  to: 0.06383969187736512\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.06383969187736512  to: 0.0637715220451355\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.0637715220451355  to: 0.06370380520820618\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.06370380520820618  to: 0.06363595724105835\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.06363595724105835  to: 0.06356816291809082\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.06356816291809082  to: 0.06350038647651672\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.06350038647651672  to: 0.06343278884887696\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.06343278884887696  to: 0.06336537599563599\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.06336537599563599  to: 0.06329811215400696\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.06329811215400696  to: 0.06323066949844361\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.06323066949844361  to: 0.06316338181495666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.06316338181495666  to: 0.06309603452682495\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.06309603452682495  to: 0.06302881240844727\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.06302881240844727  to: 0.06296156048774719\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.06296156048774719  to: 0.06289435625076294\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.06289435625076294  to: 0.06282732486724854\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.06282732486724854  to: 0.0627600908279419\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.0627600908279419  to: 0.0626932680606842\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.0626932680606842  to: 0.06262636184692383\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.06262636184692383  to: 0.06255951523780823\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.06255951523780823  to: 0.06249274611473084\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.06249274611473084  to: 0.06242612600326538\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.06242612600326538  to: 0.06235962510108948\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.06235962510108948  to: 0.06229313611984253\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.06229313611984253  to: 0.062226617336273195\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.062226617336273195  to: 0.06215997934341431\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.06215997934341431  to: 0.06209352016448975\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.06209352016448975  to: 0.062027132511138915\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.062027132511138915  to: 0.06196073293685913\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.06196073293685913  to: 0.06189459562301636\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.06189459562301636  to: 0.061828404664993286\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.061828404664993286  to: 0.061762410402297976\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.061762410402297976  to: 0.06169646978378296\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.06169646978378296  to: 0.061630642414093016\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.061630642414093016  to: 0.06156481504440307\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.06156481504440307  to: 0.06149892807006836\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.06149892807006836  to: 0.06143307089805603\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.06143307089805603  to: 0.06136723756790161\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.06136723756790161  to: 0.06130167245864868\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.06130167245864868  to: 0.06123605966567993\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.06123605966567993  to: 0.061170512437820436\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.061170512437820436  to: 0.06110527515411377\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.06110527515411377  to: 0.061039960384368895\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.061039960384368895  to: 0.06097477078437805\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.06097477078437805  to: 0.06090952754020691\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.06090952754020691  to: 0.06084446310997009\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.06084446310997009  to: 0.060779517889022826\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.060779517889022826  to: 0.060714888572692874\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.060714888572692874  to: 0.060650336742401126\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.060650336742401126  to: 0.060586071014404295\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.060586071014404295  to: 0.06052190065383911\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.06052190065383911  to: 0.06047183275222778\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.06047183275222778  to: 0.06043100953102112\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.06043100953102112  to: 0.06039327383041382\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.06039327383041382  to: 0.06035141348838806\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.06035141348838806  to: 0.0603008508682251\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.0603008508682251  to: 0.06023930311203003\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.06023930311203003  to: 0.06016730070114136\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.06016730070114136  to: 0.060088354349136355\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.060088354349136355  to: 0.0600072979927063\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.0600072979927063  to: 0.059928447008132935\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.059928447008132935  to: 0.05985556840896607\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.05985556840896607  to: 0.059789592027664186\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.059789592027664186  to: 0.059730350971221924\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.059730350971221924  to: 0.05967543125152588\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.05967543125152588  to: 0.059620636701583865\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.059620636701583865  to: 0.05956321954727173\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.05956321954727173  to: 0.059500348567962644\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.059500348567962644  to: 0.059431493282318115\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.059431493282318115  to: 0.059357059001922605\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.059357059001922605  to: 0.05927909016609192\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.05927909016609192  to: 0.05920025706291199\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.05920025706291199  to: 0.05912346243858337\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.05912346243858337  to: 0.059050559997558594\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.059050559997558594  to: 0.05898207426071167\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.05898207426071167  to: 0.05891798138618469\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.05891798138618469  to: 0.05885663628578186\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.05885663628578186  to: 0.058796250820159913\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.058796250820159913  to: 0.05873504877090454\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.05873504877090454  to: 0.05867127776145935\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.05867127776145935  to: 0.058604764938354495\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.058604764938354495  to: 0.05853595733642578\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.05853595733642578  to: 0.05846589207649231\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.05846589207649231  to: 0.0583962082862854\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.0583962082862854  to: 0.05832797288894653\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.05832797288894653  to: 0.058262312412261964\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.058262312412261964  to: 0.0581992506980896\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.0581992506980896  to: 0.05813837647438049\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.05813837647438049  to: 0.05807904601097107\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.05807904601097107  to: 0.05801967978477478\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.05801967978477478  to: 0.05795941352844238\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.05795941352844238  to: 0.057897812128067015\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.057897812128067015  to: 0.05783442854881286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.05783442854881286  to: 0.05776960253715515\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.05776960253715515  to: 0.05770412683486938\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.05770412683486938  to: 0.05763874053955078\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.05763874053955078  to: 0.05757414698600769\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.05757414698600769  to: 0.05751085877418518\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.05751085877418518  to: 0.05744858980178833\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.05744858980178833  to: 0.05738765001296997\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.05738765001296997  to: 0.057326984405517575\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.057326984405517575  to: 0.05726613998413086\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.05726613998413086  to: 0.057204967737197875\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.057204967737197875  to: 0.057142794132232666\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.057142794132232666  to: 0.05707973837852478\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.05707973837852478  to: 0.057016235589981076\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.057016235589981076  to: 0.05695272088050842\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.05695272088050842  to: 0.05688910484313965\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.05688910484313965  to: 0.05682637691497803\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.05682637691497803  to: 0.056764519214630126\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.056764519214630126  to: 0.05670334100723266\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.05670334100723266  to: 0.05664293766021729\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.05664293766021729  to: 0.05658264756202698\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.05658264756202698  to: 0.05652239322662354\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.05652239322662354  to: 0.05646196603775024\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.05646196603775024  to: 0.05640082359313965\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.05640082359313965  to: 0.05633977651596069\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.05633977651596069  to: 0.056278270483016965\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.056278270483016965  to: 0.05621693134307861\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.05621693134307861  to: 0.0561558723449707\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.0561558723449707  to: 0.05609520673751831\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.05609520673751831  to: 0.056034809350967406\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.056034809350967406  to: 0.05597503781318665\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.05597503781318665  to: 0.05591540336608887\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.05591540336608887  to: 0.055855858325958255\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.055855858325958255  to: 0.05579637885093689\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.05579637885093689  to: 0.055736649036407473\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.055736649036407473  to: 0.05567673444747925\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.05567673444747925  to: 0.05561653971672058\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.05561653971672058  to: 0.05555649995803833\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.05555649995803833  to: 0.05549619197845459\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.05549619197845459  to: 0.05543631911277771\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.05543631911277771  to: 0.05537663102149963\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.05537663102149963  to: 0.055317074060440063\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.055317074060440063  to: 0.0552578866481781\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.0552578866481781  to: 0.05519875288009644\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.05519875288009644  to: 0.055140620470046996\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.055140620470046996  to: 0.05508301854133606\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.05508301854133606  to: 0.0550254762172699\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.0550254762172699  to: 0.05496757030487061\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.05496757030487061  to: 0.054909080266952515\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.054909080266952515  to: 0.054849940538406375\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.054849940538406375  to: 0.05479026436805725\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.05479026436805725  to: 0.054729956388473514\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.054729956388473514  to: 0.05466963648796082\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.05466963648796082  to: 0.054609769582748414\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.054609769582748414  to: 0.054550421237945554\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.054550421237945554  to: 0.05449144840240479\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.05449144840240479  to: 0.05443320274353027\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.05443320274353027  to: 0.05437542200088501\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.05437542200088501  to: 0.05431751012802124\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.05431751012802124  to: 0.054259699583053586\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.054259699583053586  to: 0.05420137047767639\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.05420137047767639  to: 0.05414279103279114\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.05414279103279114  to: 0.05408415198326111\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.05408415198326111  to: 0.054025578498840335\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.054025578498840335  to: 0.053967070579528806\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.053967070579528806  to: 0.05390917658805847\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.05390917658805847  to: 0.05385144352912903\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.05385144352912903  to: 0.05379432439804077\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.05379432439804077  to: 0.053737372159957886\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.053737372159957886  to: 0.05368066430091858\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.05368066430091858  to: 0.05362367630004883\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.05362367630004883  to: 0.05356680154800415\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.05356680154800415  to: 0.053509628772735594\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.053509628772735594  to: 0.05345260500907898\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.05345260500907898  to: 0.05339542031288147\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.05339542031288147  to: 0.05333830118179321\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.05333830118179321  to: 0.05328112840652466\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.05328112840652466  to: 0.053221702575683594\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.053221702575683594  to: 0.053162378072738645\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.053162378072738645  to: 0.05310321450233459\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.05310321450233459  to: 0.05304400324821472\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.05304400324821472  to: 0.05298501253128052\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.05298501253128052  to: 0.052925729751586915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.052925729751586915  to: 0.05286648869514465\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.05286648869514465  to: 0.05280710458755493\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.05280710458755493  to: 0.0527478814125061\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.0527478814125061  to: 0.05268851518630981\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.05268851518630981  to: 0.0526294469833374\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.0526294469833374  to: 0.05257017612457275\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.05257017612457275  to: 0.05251128077507019\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.05251128077507019  to: 0.05245236754417419\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.05245236754417419  to: 0.052393513917922976\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.052393513917922976  to: 0.05233486890792847\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.05233486890792847  to: 0.052276086807250974\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.052276086807250974  to: 0.05221737027168274\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.05221737027168274  to: 0.05215849876403809\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.05215849876403809  to: 0.05209977030754089\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.05209977030754089  to: 0.052041256427764894\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.052041256427764894  to: 0.051982653141021726\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.051982653141021726  to: 0.05192432403564453\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.05192432403564453  to: 0.05186585783958435\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.05186585783958435  to: 0.05180755853652954\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.05180755853652954  to: 0.05174921154975891\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.05174921154975891  to: 0.05169099569320679\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.05169099569320679  to: 0.05163273215293884\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.05163273215293884  to: 0.05157454609870911\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.05157454609870911  to: 0.051516282558441165\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.051516282558441165  to: 0.05145825147628784\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.05145825147628784  to: 0.05140010118484497\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.05140010118484497  to: 0.051342183351516725\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.051342183351516725  to: 0.05128411054611206\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.05128411054611206  to: 0.05122618675231934\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.05122618675231934  to: 0.05116814374923706\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.05116814374923706  to: 0.05111040472984314\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.05111040472984314  to: 0.051052522659301755\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.051052522659301755  to: 0.05099480748176575\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.05099480748176575  to: 0.050937104225158694\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.050937104225158694  to: 0.050879371166229245\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.050879371166229245  to: 0.05082176923751831\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.05082176923751831  to: 0.05076413154602051\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.05076413154602051  to: 0.05070651769638061\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.05070651769638061  to: 0.05064906477928162\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.05064906477928162  to: 0.050591576099395755\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.050591576099395755  to: 0.05053420662879944\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.05053420662879944  to: 0.05047696828842163\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.05047696828842163  to: 0.050419855117797854\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.050419855117797854  to: 0.05036251544952393\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.05036251544952393  to: 0.050305265188217166\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.050305265188217166  to: 0.05024811625480652\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.05024811625480652  to: 0.050190865993499756\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.050190865993499756  to: 0.050133705139160156\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.050133705139160156  to: 0.050076651573181155\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.050076651573181155  to: 0.05001977682113647\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.05001977682113647  to: 0.049962759017944336\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.049962759017944336  to: 0.04990590214729309\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.04990590214729309  to: 0.049849146604537965\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.049849146604537965  to: 0.049792543053627014\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.049792543053627014  to: 0.04973583221435547\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.04973583221435547  to: 0.049679034948349\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.049679034948349  to: 0.049622577428817746\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.049622577428817746  to: 0.04956574440002441\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.04956574440002441  to: 0.04950926303863525\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.04950926303863525  to: 0.04945257604122162\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.04945257604122162  to: 0.04939605593681336\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.04939605593681336  to: 0.04933978915214539\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.04933978915214539  to: 0.04928343892097473\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.04928343892097473  to: 0.04922711253166199\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.04922711253166199  to: 0.04917101263999939\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.04917101263999939  to: 0.04911511540412903\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.04911511540412903  to: 0.049059122800827026\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.049059122800827026  to: 0.049002963304519656\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.049002963304519656  to: 0.048946887254714966\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.048946887254714966  to: 0.04889083504676819\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.04889083504676819  to: 0.04883449673652649\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.04883449673652649  to: 0.04877846240997315\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.04877846240997315  to: 0.048722395300865175\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.048722395300865175  to: 0.04866644740104675\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.04866644740104675  to: 0.04861079752445221\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.04861079752445221  to: 0.048554936051368715\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.048554936051368715  to: 0.04849929809570312\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.04849929809570312  to: 0.04844377934932709\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.04844377934932709  to: 0.04838813841342926\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.04838813841342926  to: 0.0483325332403183\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.0483325332403183  to: 0.04827715456485748\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.04827715456485748  to: 0.04822155833244324\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.04822155833244324  to: 0.04816601276397705\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.04816601276397705  to: 0.04811074733734131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.04811074733734131  to: 0.04805528521537781\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.04805528521537781  to: 0.048000043630599974\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.048000043630599974  to: 0.047944658994674684\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.047944658994674684  to: 0.04788950383663178\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.04788950383663178  to: 0.04783456921577454\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.04783456921577454  to: 0.047779577970504764\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.047779577970504764  to: 0.04772465229034424\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.04772465229034424  to: 0.047669658064842226\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.047669658064842226  to: 0.047614821791648866\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.047614821791648866  to: 0.04755985736846924\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.04755985736846924  to: 0.047504919767379764\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.047504919767379764  to: 0.04745019376277924\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.04745019376277924  to: 0.04739545285701752\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.04739545285701752  to: 0.04734075963497162\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.04734075963497162  to: 0.04728626310825348\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.04728626310825348  to: 0.04723164141178131\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.04723164141178131  to: 0.047177156805992125\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.047177156805992125  to: 0.047122567892074585\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.047122567892074585  to: 0.04706826210021973\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.04706826210021973  to: 0.04701382517814636\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.04701382517814636  to: 0.0469595342874527\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.0469595342874527  to: 0.04690517783164978\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.04690517783164978  to: 0.04685071408748627\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.04685071408748627  to: 0.04679636061191559\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.04679636061191559  to: 0.04674195647239685\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.04674195647239685  to: 0.04668760299682617\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.04668760299682617  to: 0.04663334786891937\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.04663334786891937  to: 0.04657924771308899\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.04657924771308899  to: 0.04652517735958099\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.04652517735958099  to: 0.046471086144447324\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.046471086144447324  to: 0.04641712605953217\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.04641712605953217  to: 0.046363276243209836\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.046363276243209836  to: 0.046309319138526914\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.046309319138526914  to: 0.04625531733036041\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.04625531733036041  to: 0.04620128571987152\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.04620128571987152  to: 0.0461474746465683\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.0461474746465683  to: 0.0460937649011612\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.0460937649011612  to: 0.046040010452270505\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.046040010452270505  to: 0.04598640501499176\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.04598640501499176  to: 0.04593278467655182\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.04593278467655182  to: 0.04587925970554352\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.04587925970554352  to: 0.04582563042640686\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.04582563042640686  to: 0.04577227532863617\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.04577227532863617  to: 0.045718878507614136\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.045718878507614136  to: 0.0456653892993927\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.0456653892993927  to: 0.045612087845802306\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.045612087845802306  to: 0.04555874466896057\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.04555874466896057  to: 0.04550569951534271\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.04550569951534271  to: 0.045452603697776796\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.045452603697776796  to: 0.04539925456047058\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.04539925456047058  to: 0.04534624516963959\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.04534624516963959  to: 0.045293372869491574\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.045293372869491574  to: 0.04524039328098297\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.04524039328098297  to: 0.04518735408782959\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.04518735408782959  to: 0.045134696364402774\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.045134696364402774  to: 0.04508184492588043\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.04508184492588043  to: 0.04502911567687988\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.04502911567687988  to: 0.04497643411159515\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.04497643411159515  to: 0.044923797249794006\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.044923797249794006  to: 0.04487127661705017\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.04487127661705017  to: 0.044818726181983945\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.044818726181983945  to: 0.04476621747016907\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.04476621747016907  to: 0.044713693857192996\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.044713693857192996  to: 0.044661274552345274\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.044661274552345274  to: 0.044609054923057556\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.044609054923057556  to: 0.04455673098564148\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.04455673098564148  to: 0.04450453817844391\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.04450453817844391  to: 0.044452303647994997\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.044452303647994997  to: 0.044400197267532346\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.044400197267532346  to: 0.044348114728927614\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.044348114728927614  to: 0.04429604113101959\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.04429604113101959  to: 0.04424413740634918\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.04424413740634918  to: 0.0441922128200531\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.0441922128200531  to: 0.04414032995700836\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.04414032995700836  to: 0.04408847689628601\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.04408847689628601  to: 0.044036698341369626\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.044036698341369626  to: 0.043984952569007876\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.043984952569007876  to: 0.043933320045471194\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.043933320045471194  to: 0.04388157725334167\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.04388157725334167  to: 0.043830114603042605\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.043830114603042605  to: 0.043778452277183535\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.043778452277183535  to: 0.04372709691524505\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.04372709691524505  to: 0.04367569983005524\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.04367569983005524  to: 0.043624410033226015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.043624410033226015  to: 0.0435729444026947\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.0435729444026947  to: 0.043521752953529357\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.043521752953529357  to: 0.04347062110900879\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.04347062110900879  to: 0.043419352173805235\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.043419352173805235  to: 0.04336829781532288\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.04336829781532288  to: 0.043317285180091855\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.043317285180091855  to: 0.04326614439487457\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.04326614439487457  to: 0.04321528971195221\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.04321528971195221  to: 0.04316441416740417\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.04316441416740417  to: 0.04311351776123047\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.04311351776123047  to: 0.04306296408176422\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.04306296408176422  to: 0.043012142181396484\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.043012142181396484  to: 0.04296128153800964\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.04296128153800964  to: 0.04291069507598877\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.04291069507598877  to: 0.04286014139652252\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.04286014139652252  to: 0.042809733748435976\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.042809733748435976  to: 0.042759209871292114\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.042759209871292114  to: 0.04270873665809631\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.04270873665809631  to: 0.042658361792564395\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.042658361792564395  to: 0.04260796010494232\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.04260796010494232  to: 0.04255778193473816\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.04255778193473816  to: 0.04250754714012146\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.04250754714012146  to: 0.042457371950149536\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.042457371950149536  to: 0.042407312989234926\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.042407312989234926  to: 0.042357221245765686\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.042357221245765686  to: 0.04230723977088928\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.04230723977088928  to: 0.04225722849369049\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.04225722849369049  to: 0.04220735132694244\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.04220735132694244  to: 0.042157498002052304\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.042157498002052304  to: 0.042107772827148435\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.042107772827148435  to: 0.04205802977085114\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.04205802977085114  to: 0.04200831949710846\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.04200831949710846  to: 0.041958650946617125\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.041958650946617125  to: 0.04190918505191803\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.04190918505191803  to: 0.041859683394432065\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.041859683394432065  to: 0.04181005954742432\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.04181005954742432  to: 0.04176064133644104\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.04176064133644104  to: 0.041711336374282836\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.041711336374282836  to: 0.04166187644004822\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.04166187644004822  to: 0.041612499952316286\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.041612499952316286  to: 0.04156332910060882\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.04156332910060882  to: 0.04151422381401062\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.04151422381401062  to: 0.0414651483297348\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.0414651483297348  to: 0.04141611158847809\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.04141611158847809  to: 0.04136722087860108\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.04136722087860108  to: 0.041318416595458984\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.041318416595458984  to: 0.041269358992576596\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.041269358992576596  to: 0.041220512986183164\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.041220512986183164  to: 0.04117160439491272\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.04117160439491272  to: 0.04112275242805481\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.04112275242805481  to: 0.04107393622398377\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.04107393622398377  to: 0.04102541506290436\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.04102541506290436  to: 0.040976372361183164\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.040976372361183164  to: 0.04092724323272705\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.04092724323272705  to: 0.04087808132171631\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.04087808132171631  to: 0.04082881510257721\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.04082881510257721  to: 0.04077969193458557\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.04077969193458557  to: 0.0407303661108017\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.0407303661108017  to: 0.04068111479282379\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.04068111479282379  to: 0.04063192009925842\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.04063192009925842  to: 0.04058290123939514\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.04058290123939514  to: 0.0405339390039444\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.0405339390039444  to: 0.04048502445220947\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.04048502445220947  to: 0.040436011552810666\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.040436011552810666  to: 0.04038718640804291\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.04038718640804291  to: 0.040338411927223206\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.040338411927223206  to: 0.040289512276649474\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.040289512276649474  to: 0.04024081826210022\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.04024081826210022  to: 0.040192022919654846\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.040192022919654846  to: 0.0401435911655426\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.0401435911655426  to: 0.04009503424167633\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.04009503424167633  to: 0.040046533942222594\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.040046533942222594  to: 0.03999814093112945\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.03999814093112945  to: 0.0399495542049408\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.0399495542049408  to: 0.03990114331245422\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.03990114331245422  to: 0.03985278606414795\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.03985278606414795  to: 0.03980430364608765\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.03980430364608765  to: 0.03975588381290436\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.03975588381290436  to: 0.03970751166343689\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.03970751166343689  to: 0.0396593987941742\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.0396593987941742  to: 0.039611196517944335\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.039611196517944335  to: 0.03956301808357239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.03956301808357239  to: 0.03951504826545715\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.03951504826545715  to: 0.039467144012451175\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.039467144012451175  to: 0.039419031143188475\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.039419031143188475  to: 0.03937127888202667\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.03937127888202667  to: 0.03932318389415741\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.03932318389415741  to: 0.03927552103996277\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.03927552103996277  to: 0.03922781944274902\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.03922781944274902  to: 0.039180177450180056\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.039180177450180056  to: 0.03913266360759735\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.03913266360759735  to: 0.03908493816852569\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.03908493816852569  to: 0.03903756737709045\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.03903756737709045  to: 0.038990172743797305\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.038990172743797305  to: 0.03894276320934296\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.03894276320934296  to: 0.038895383477211\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.038895383477211  to: 0.03884804844856262\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.03884804844856262  to: 0.03880082666873932\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.03880082666873932  to: 0.038753679394721983\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.038753679394721983  to: 0.038706538081169126\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.038706538081169126  to: 0.03865962028503418\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.03865962028503418  to: 0.038612577319145205\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.038612577319145205  to: 0.03856567144393921\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.03856567144393921  to: 0.03851872980594635\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.03851872980594635  to: 0.03847191333770752\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.03847191333770752  to: 0.03842512965202331\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.03842512965202331  to: 0.03837854266166687\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.03837854266166687  to: 0.03833179771900177\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.03833179771900177  to: 0.038285306096076964\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.038285306096076964  to: 0.038238778710365295\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.038238778710365295  to: 0.03819237351417541\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.03819237351417541  to: 0.038145792484283444\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.038145792484283444  to: 0.038099414110183714\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.038099414110183714  to: 0.03805321753025055\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.03805321753025055  to: 0.0380068451166153\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.0380068451166153  to: 0.03796059489250183\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.03796059489250183  to: 0.037914511561393735\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.037914511561393735  to: 0.03786850273609162\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.03786850273609162  to: 0.037822380661964417\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.037822380661964417  to: 0.03777645826339722\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.03777645826339722  to: 0.037730664014816284\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.037730664014816284  to: 0.03768467009067535\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.03768467009067535  to: 0.03763903081417084\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.03763903081417084  to: 0.037593206763267516\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.037593206763267516  to: 0.03754756450653076\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.03754756450653076  to: 0.03750190734863281\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.03750190734863281  to: 0.03745644092559815\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.03745644092559815  to: 0.03741070628166199\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.03741070628166199  to: 0.03736540675163269\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.03736540675163269  to: 0.03732000291347504\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.03732000291347504  to: 0.03727471530437469\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.03727471530437469  to: 0.037229394912719725\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.037229394912719725  to: 0.037184348702430724\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.037184348702430724  to: 0.037138962745666505\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.037138962745666505  to: 0.03709391355514526\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.03709391355514526  to: 0.037048691511154176\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.037048691511154176  to: 0.037003666162490845\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.037003666162490845  to: 0.036958760023117064\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.036958760023117064  to: 0.036913889646530154\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.036913889646530154  to: 0.03686915338039398\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.03686915338039398  to: 0.0368244469165802\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.0368244469165802  to: 0.036779788136482236\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.036779788136482236  to: 0.0367351770401001\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.0367351770401001  to: 0.03669053018093109\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.03669053018093109  to: 0.03664585053920746\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.03664585053920746  to: 0.03660135865211487\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.03660135865211487  to: 0.03655691742897034\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.03655691742897034  to: 0.03651244938373566\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.03651244938373566  to: 0.036468306183815004\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.036468306183815004  to: 0.03642399907112122\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.03642399907112122  to: 0.0363798052072525\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.0363798052072525  to: 0.036335808038711545\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.036335808038711545  to: 0.03629176318645477\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.03629176318645477  to: 0.03624747097492218\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.03624747097492218  to: 0.03620359897613525\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.03620359897613525  to: 0.03615961968898773\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.03615961968898773  to: 0.036115771532058714\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.036115771532058714  to: 0.036071869730949405\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.036071869730949405  to: 0.036028170585632326\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.036028170585632326  to: 0.035984587669372556\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.035984587669372556  to: 0.035940971970558164\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.035940971970558164  to: 0.03589746654033661\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.03589746654033661  to: 0.03585392832756042\n",
      "Training iteration: 1920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03585392832756042  to: 0.03581044375896454\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.03581044375896454  to: 0.03576693534851074\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.03576693534851074  to: 0.035723596811294556\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.035723596811294556  to: 0.03568024933338165\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.03568024933338165  to: 0.03563700020313263\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.03563700020313263  to: 0.035593801736831666\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.035593801736831666  to: 0.035550788044929504\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.035550788044929504  to: 0.035507756471633914\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.035507756471633914  to: 0.03546483516693115\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.03546483516693115  to: 0.0354219526052475\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.0354219526052475  to: 0.035378959774971006\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.035378959774971006  to: 0.035336154699325564\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.035336154699325564  to: 0.035293099284172055\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.035293099284172055  to: 0.03525038361549378\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.03525038361549378  to: 0.03520779013633728\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.03520779013633728  to: 0.035165241360664366\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.035165241360664366  to: 0.03512287735939026\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.03512287735939026  to: 0.0350803941488266\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.0350803941488266  to: 0.03503796756267548\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.03503796756267548  to: 0.03499557971954346\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.03499557971954346  to: 0.0349531501531601\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.0349531501531601  to: 0.03491085767745972\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.03491085767745972  to: 0.03486859202384949\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.03486859202384949  to: 0.03482643961906433\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.03482643961906433  to: 0.03478440940380097\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.03478440940380097  to: 0.03474240899085999\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.03474240899085999  to: 0.0347004771232605\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.0347004771232605  to: 0.034658569097518924\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.034658569097518924  to: 0.03461677730083466\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.03461677730083466  to: 0.03457480072975159\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.03457480072975159  to: 0.034532946348190305\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.034532946348190305  to: 0.034491270780563354\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.034491270780563354  to: 0.034449723362922666\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.034449723362922666  to: 0.03440814018249512\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.03440814018249512  to: 0.03436667025089264\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.03436667025089264  to: 0.034325170516967776\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.034325170516967776  to: 0.03428364098072052\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.03428364098072052  to: 0.03424228429794311\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.03424228429794311  to: 0.034201040863990784\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.034201040863990784  to: 0.03415977954864502\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.03415977954864502  to: 0.034118634462356565\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.034118634462356565  to: 0.03407752811908722\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.03407752811908722  to: 0.03403638303279877\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.03403638303279877  to: 0.033995437622070315\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.033995437622070315  to: 0.03395451903343201\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.03395451903343201  to: 0.033913567662239075\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.033913567662239075  to: 0.03387280106544495\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.03387280106544495  to: 0.033831936120986936\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.033831936120986936  to: 0.0337912529706955\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.0337912529706955  to: 0.033750542998313905\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.033750542998313905  to: 0.03370987474918365\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.03370987474918365  to: 0.033669385313987735\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.033669385313987735  to: 0.033628880977630615\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.033628880977630615  to: 0.03358840346336365\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.03358840346336365  to: 0.03354804217815399\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.03354804217815399  to: 0.03350770771503449\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.03350770771503449  to: 0.03346729278564453\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.03346729278564453  to: 0.03342711329460144\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.03342711329460144  to: 0.03338705599308014\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.03338705599308014  to: 0.03334696888923645\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.03334696888923645  to: 0.03330692052841187\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.03330692052841187  to: 0.0332669198513031\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.0332669198513031  to: 0.033227008581161496\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.033227008581161496  to: 0.03318701684474945\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.03318701684474945  to: 0.03314706087112427\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.03314706087112427  to: 0.03310741782188416\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.03310741782188416  to: 0.03306761384010315\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.03306761384010315  to: 0.0330280601978302\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.0330280601978302  to: 0.032988399267196655\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.032988399267196655  to: 0.032949066162109374\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.032949066162109374  to: 0.03290963768959045\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.03290963768959045  to: 0.032870107889175416\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.032870107889175416  to: 0.03283074498176575\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.03283074498176575  to: 0.032791358232498166\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.032791358232498166  to: 0.03275201916694641\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.03275201916694641  to: 0.03271277844905853\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.03271277844905853  to: 0.0326737254858017\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.0326737254858017  to: 0.03263470530509949\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.03263470530509949  to: 0.03259572684764862\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.03259572684764862  to: 0.03255678713321686\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.03255678713321686  to: 0.03251774907112122\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.03251774907112122  to: 0.03247881829738617\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.03247881829738617  to: 0.03243979811668396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.03243979811668396  to: 0.03240109384059906\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.03240109384059906  to: 0.03236273229122162\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.03236273229122162  to: 0.03232434093952179\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.03232434093952179  to: 0.03228605985641479\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.03228605985641479  to: 0.03224759697914124\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.03224759697914124  to: 0.03220903277397156\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.03220903277397156  to: 0.032170307636260984\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.032170307636260984  to: 0.03213159143924713\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.03213159143924713  to: 0.032092800736427306\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.032092800736427306  to: 0.03205431401729584\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.03205431401729584  to: 0.03201588988304138\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.03201588988304138  to: 0.03197762966156006\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.03197762966156006  to: 0.03193947672843933\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.03193947672843933  to: 0.03190135657787323\n",
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.03190135657787323  to: 0.031863284111022946\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.031863284111022946  to: 0.03182502686977386\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.03182502686977386  to: 0.031786945462226865\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.031786945462226865  to: 0.031748971343040465\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.031748971343040465  to: 0.031710964441299436\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.031710964441299436  to: 0.031673195958137515\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.031673195958137515  to: 0.03163546919822693\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.03163546919822693  to: 0.03159806728363037\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.03159806728363037  to: 0.03156054317951203\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.03156054317951203  to: 0.031523081660270694\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.031523081660270694  to: 0.03148570358753204\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.03148570358753204  to: 0.03144824206829071\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.03144824206829071  to: 0.031410932540893555\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.031410932540893555  to: 0.031373602151870725\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.031373602151870725  to: 0.03133631944656372\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.03133631944656372  to: 0.0312992125749588\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.0312992125749588  to: 0.03126221001148224\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.03126221001148224  to: 0.031225305795669556\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.031225305795669556  to: 0.031188246607780457\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.031188246607780457  to: 0.031151288747787477\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.031151288747787477  to: 0.031114301085472106\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.031114301085472106  to: 0.031077343225479125\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.031077343225479125  to: 0.031040433049201965\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.031040433049201965  to: 0.031003695726394654\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.031003695726394654  to: 0.030966928601264952\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.030966928601264952  to: 0.03093027472496033\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.03093027472496033  to: 0.030893805623054504\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.030893805623054504  to: 0.030857345461845397\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.030857345461845397  to: 0.03082094490528107\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.03082094490528107  to: 0.0307844340801239\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.0307844340801239  to: 0.030748099088668823\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.030748099088668823  to: 0.03071172833442688\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.03071172833442688  to: 0.030675479769706727\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.030675479769706727  to: 0.030639320611953735\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.030639320611953735  to: 0.030603194236755372\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.030603194236755372  to: 0.030567115545272826\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.030567115545272826  to: 0.030531078577041626\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.030531078577041626  to: 0.030495211482048035\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.030495211482048035  to: 0.030459386110305787\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.030459386110305787  to: 0.030423516035079957\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.030423516035079957  to: 0.030387625098228455\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.030387625098228455  to: 0.030351907014846802\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.030351907014846802  to: 0.03031628727912903\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.03031628727912903  to: 0.03028051257133484\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.03028051257133484  to: 0.030245095491409302\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.030245095491409302  to: 0.030209538340568543\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.030209538340568543  to: 0.030174130201339723\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.030174130201339723  to: 0.030138638615608216\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.030138638615608216  to: 0.030103373527526855\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.030103373527526855  to: 0.03006795048713684\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.03006795048713684  to: 0.03003261685371399\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.03003261685371399  to: 0.029997393488883972\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.029997393488883972  to: 0.029962342977523804\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.029962342977523804  to: 0.029927343130111694\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.029927343130111694  to: 0.029892298579216003\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.029892298579216003  to: 0.029857298731803893\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.029857298731803893  to: 0.029822397232055663\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.029822397232055663  to: 0.029787522554397584\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.029787522554397584  to: 0.0297527551651001\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.0297527551651001  to: 0.02971802055835724\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.02971802055835724  to: 0.02968332767486572\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.02968332767486572  to: 0.029648739099502563\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.029648739099502563  to: 0.029614055156707765\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.029614055156707765  to: 0.02957960367202759\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.02957960367202759  to: 0.029545113444328308\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.029545113444328308  to: 0.02951059937477112\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.02951059937477112  to: 0.02947632670402527\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.02947632670402527  to: 0.029442018270492552\n",
      "Training iteration: 2085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.029442018270492552  to: 0.02940787374973297\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.02940787374973297  to: 0.029373767971992492\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.029373767971992492  to: 0.029339492321014404\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.029339492321014404  to: 0.029305389523506163\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.029305389523506163  to: 0.029271385073661803\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.029271385073661803  to: 0.029237282276153565\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.029237282276153565  to: 0.02920343279838562\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.02920343279838562  to: 0.029169750213623048\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.029169750213623048  to: 0.029135963320732115\n",
      "Training iteration: 2094\n",
      "Improved validation loss from: 0.029135963320732115  to: 0.029102268815040588\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.029102268815040588  to: 0.029068607091903686\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.029068607091903686  to: 0.029034847021102907\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.029034847021102907  to: 0.029001259803771974\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.029001259803771974  to: 0.028967636823654174\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.028967636823654174  to: 0.028934258222579955\n",
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.028934258222579955  to: 0.028900712728500366\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.028900712728500366  to: 0.028867465257644654\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.028867465257644654  to: 0.028834125399589537\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.028834125399589537  to: 0.028800949454307556\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.028800949454307556  to: 0.028767746686935425\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.028767746686935425  to: 0.028734633326530458\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.028734633326530458  to: 0.02870141565799713\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.02870141565799713  to: 0.028668352961540224\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.028668352961540224  to: 0.028635281324386596\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.028635281324386596  to: 0.02860238552093506\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.02860238552093506  to: 0.028569573163986207\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.028569573163986207  to: 0.028536805510520936\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.028536805510520936  to: 0.028504061698913574\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.028504061698913574  to: 0.028471362590789796\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.028471362590789796  to: 0.028438696265220643\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.028438696265220643  to: 0.02840607166290283\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.02840607166290283  to: 0.028373527526855468\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.028373527526855468  to: 0.028341028094291686\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.028341028094291686  to: 0.02830856144428253\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.02830856144428253  to: 0.02827618718147278\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.02827618718147278  to: 0.028243786096572875\n",
      "Training iteration: 2121\n",
      "Improved validation loss from: 0.028243786096572875  to: 0.028211620450019837\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.028211620450019837  to: 0.028179430961608888\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.028179430961608888  to: 0.028147274255752565\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.028147274255752565  to: 0.028115075826644898\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.028115075826644898  to: 0.028083094954490663\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.028083094954490663  to: 0.028051096200942992\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.028051096200942992  to: 0.02801913321018219\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.02801913321018219  to: 0.027987247705459593\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.027987247705459593  to: 0.027955415844917297\n",
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.027955415844917297  to: 0.027923601865768432\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.027923601865768432  to: 0.02789183259010315\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.02789183259010315  to: 0.027860099077224733\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.027860099077224733  to: 0.02782851755619049\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.02782851755619049  to: 0.027797093987464903\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.027797093987464903  to: 0.027765464782714844\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.027765464782714844  to: 0.027733975648880006\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.027733975648880006  to: 0.027702608704566957\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.027702608704566957  to: 0.02767125964164734\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.02767125964164734  to: 0.027639937400817872\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.027639937400817872  to: 0.027608716487884523\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.027608716487884523  to: 0.02757745683193207\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.02757745683193207  to: 0.027546373009681702\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.027546373009681702  to: 0.027515268325805663\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: 0.027515268325805663  to: 0.027484244108200072\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.027484244108200072  to: 0.027453389763832093\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.027453389763832093  to: 0.027422434091567992\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.027422434091567992  to: 0.02739151120185852\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.02739151120185852  to: 0.027360609173774718\n",
      "Training iteration: 2149\n",
      "Improved validation loss from: 0.027360609173774718  to: 0.027329808473587035\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.027329808473587035  to: 0.02729911208152771\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.02729911208152771  to: 0.02726844549179077\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.02726844549179077  to: 0.027237802743911743\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.027237802743911743  to: 0.027207207679748536\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.027207207679748536  to: 0.02717677354812622\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.02717677354812622  to: 0.02714642584323883\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.02714642584323883  to: 0.02711590826511383\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.02711590826511383  to: 0.02708561420440674\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.02708561420440674  to: 0.027055293321609497\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.027055293321609497  to: 0.02702501118183136\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.02702501118183136  to: 0.026994821429252625\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.026994821429252625  to: 0.026964730024337767\n",
      "Training iteration: 2162\n",
      "Improved validation loss from: 0.026964730024337767  to: 0.026934662461280824\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.026934662461280824  to: 0.02690463662147522\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.02690463662147522  to: 0.026874619722366332\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.026874619722366332  to: 0.026844656467437743\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.026844656467437743  to: 0.026814842224121095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.026814842224121095  to: 0.026785066723823546\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.026785066723823546  to: 0.026755255460739136\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.026755255460739136  to: 0.02672552764415741\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.02672552764415741  to: 0.026695841550827028\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: 0.026695841550827028  to: 0.026666176319122315\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.026666176319122315  to: 0.026636618375778198\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.026636618375778198  to: 0.02660708725452423\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.02660708725452423  to: 0.026577654480934142\n",
      "Training iteration: 2175\n",
      "Improved validation loss from: 0.026577654480934142  to: 0.02654837667942047\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.02654837667942047  to: 0.026519116759300233\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.026519116759300233  to: 0.026489713788032533\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.026489713788032533  to: 0.026460328698158266\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.026460328698158266  to: 0.026430970430374144\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.026430970430374144  to: 0.026401853561401366\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.026401853561401366  to: 0.026372873783111574\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.026372873783111574  to: 0.02634400427341461\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.02634400427341461  to: 0.02631496787071228\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.02631496787071228  to: 0.02628602385520935\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.02628602385520935  to: 0.026257234811782836\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.026257234811782836  to: 0.026228290796279908\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.026228290796279908  to: 0.026199299097061157\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.026199299097061157  to: 0.026170647144317626\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.026170647144317626  to: 0.02614192068576813\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.02614192068576813  to: 0.026113349199295043\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.026113349199295043  to: 0.02608475089073181\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.02608475089073181  to: 0.026056426763534545\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.026056426763534545  to: 0.0260280042886734\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.0260280042886734  to: 0.02599948048591614\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.02599948048591614  to: 0.025970906019210815\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.025970906019210815  to: 0.025942564010620117\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.025942564010620117  to: 0.025914257764816283\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.025914257764816283  to: 0.02588610053062439\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.02588610053062439  to: 0.025857993960380556\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.025857993960380556  to: 0.02582983374595642\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.02582983374595642  to: 0.025801834464073182\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.025801834464073182  to: 0.025773805379867554\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.025773805379867554  to: 0.025745850801467896\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.025745850801467896  to: 0.025717991590499877\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.025717991590499877  to: 0.025690045952796937\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.025690045952796937  to: 0.025662115216255187\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.025662115216255187  to: 0.02563435435295105\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.02563435435295105  to: 0.02560674250125885\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.02560674250125885  to: 0.025579094886779785\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.025579094886779785  to: 0.025551536679267885\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.025551536679267885  to: 0.025523948669433593\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.025523948669433593  to: 0.02549644708633423\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.02549644708633423  to: 0.025468966364860533\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.025468966364860533  to: 0.0254413366317749\n",
      "Training iteration: 2215\n",
      "Improved validation loss from: 0.0254413366317749  to: 0.025414034724235535\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.025414034724235535  to: 0.025386711955070494\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.025386711955070494  to: 0.02535955011844635\n",
      "Training iteration: 2218\n",
      "Improved validation loss from: 0.02535955011844635  to: 0.0253324031829834\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.0253324031829834  to: 0.025305289030075073\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.025305289030075073  to: 0.025278198719024658\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.025278198719024658  to: 0.0252510666847229\n",
      "Training iteration: 2222\n",
      "Improved validation loss from: 0.0252510666847229  to: 0.02522396445274353\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.02522396445274353  to: 0.025196897983551025\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.025196897983551025  to: 0.025170043110847473\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.025170043110847473  to: 0.025143152475357054\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.025143152475357054  to: 0.025116491317749023\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.025116491317749023  to: 0.025089773535728454\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.025089773535728454  to: 0.02506304085254669\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.02506304085254669  to: 0.025036314129829408\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.025036314129829408  to: 0.025009679794311523\n",
      "Training iteration: 2231\n",
      "Improved validation loss from: 0.025009679794311523  to: 0.024983076751232146\n",
      "Training iteration: 2232\n",
      "Improved validation loss from: 0.024983076751232146  to: 0.024956505000591277\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.024956505000591277  to: 0.024930015206336975\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.024930015206336975  to: 0.024903616309165953\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.024903616309165953  to: 0.024877181649208067\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.024877181649208067  to: 0.024850749969482423\n",
      "Training iteration: 2237\n",
      "Improved validation loss from: 0.024850749969482423  to: 0.02482442855834961\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.02482442855834961  to: 0.02479826956987381\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.02479826956987381  to: 0.02477220743894577\n",
      "Training iteration: 2240\n",
      "Improved validation loss from: 0.02477220743894577  to: 0.02474653720855713\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.02474653720855713  to: 0.02472052872180939\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.02472052872180939  to: 0.024694721400737762\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.024694721400737762  to: 0.024668756127357482\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.024668756127357482  to: 0.0246428519487381\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.0246428519487381  to: 0.024616846442222597\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.024616846442222597  to: 0.024590949714183807\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.024590949714183807  to: 0.024565255641937254\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.024565255641937254  to: 0.024539348483085633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2249\n",
      "Improved validation loss from: 0.024539348483085633  to: 0.024513666331768037\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.024513666331768037  to: 0.024488100409507753\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.024488100409507753  to: 0.024462446570396423\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.024462446570396423  to: 0.02443681061267853\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.02443681061267853  to: 0.024411198496818543\n",
      "Training iteration: 2254\n",
      "Improved validation loss from: 0.024411198496818543  to: 0.024385495483875273\n",
      "Training iteration: 2255\n",
      "Improved validation loss from: 0.024385495483875273  to: 0.02436001002788544\n",
      "Training iteration: 2256\n",
      "Improved validation loss from: 0.02436001002788544  to: 0.02433474063873291\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.02433474063873291  to: 0.024309559166431426\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.024309559166431426  to: 0.024284274876117708\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.024284274876117708  to: 0.024259133636951445\n",
      "Training iteration: 2260\n",
      "Improved validation loss from: 0.024259133636951445  to: 0.024234072864055635\n",
      "Training iteration: 2261\n",
      "Improved validation loss from: 0.024234072864055635  to: 0.024208991229534148\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.024208991229534148  to: 0.02418646365404129\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.02418646365404129  to: 0.024167077243328096\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.024167077243328096  to: 0.02414896935224533\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.02414896935224533  to: 0.024129334092140197\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.024129334092140197  to: 0.024105899035930634\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.024105899035930634  to: 0.02407882660627365\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: 0.02407882660627365  to: 0.02405058443546295\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.02405058443546295  to: 0.024023035168647768\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.024023035168647768  to: 0.023997700214385985\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: 0.023997700214385985  to: 0.023973926901817322\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.023973926901817322  to: 0.023950251936912536\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.023950251936912536  to: 0.023925189673900605\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.023925189673900605  to: 0.023898465931415556\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.023898465931415556  to: 0.02387091815471649\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.02387091815471649  to: 0.023843765258789062\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.023843765258789062  to: 0.023817917704582213\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.023817917704582213  to: 0.023795859515666963\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.023795859515666963  to: 0.023777408897876738\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.023777408897876738  to: 0.023759719729423524\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: 0.023759719729423524  to: 0.023739759624004365\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.023739759624004365  to: 0.02371538132429123\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.02371538132429123  to: 0.02368817329406738\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.02368817329406738  to: 0.023660580813884734\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.023660580813884734  to: 0.02363467961549759\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.02363467961549759  to: 0.023610945045948028\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.023610945045948028  to: 0.023588261008262633\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.023588261008262633  to: 0.023567390441894532\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.023567390441894532  to: 0.02354794293642044\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.02354794293642044  to: 0.02352779358625412\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.02352779358625412  to: 0.023505322635173798\n",
      "Training iteration: 2292\n",
      "Improved validation loss from: 0.023505322635173798  to: 0.023476704955101013\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.023476704955101013  to: 0.023446302115917205\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.023446302115917205  to: 0.023415696620941163\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.023415696620941163  to: 0.023385587334632873\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.023385587334632873  to: 0.023356823623180388\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.023356823623180388  to: 0.0233313649892807\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.0233313649892807  to: 0.0233088880777359\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.0233088880777359  to: 0.02328702211380005\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.02328702211380005  to: 0.023262667655944824\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.023262667655944824  to: 0.02323463708162308\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.02323463708162308  to: 0.0232038289308548\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.0232038289308548  to: 0.02317548543214798\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.02317548543214798  to: 0.023151406645774843\n",
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.023151406645774843  to: 0.02313058078289032\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.02313058078289032  to: 0.02310904711484909\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.02310904711484909  to: 0.023083150386810303\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.023083150386810303  to: 0.023052191734313963\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.023052191734313963  to: 0.023019225895404817\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.023019225895404817  to: 0.02298748940229416\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.02298748940229416  to: 0.02296120822429657\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.02296120822429657  to: 0.022940346598625184\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.022940346598625184  to: 0.022921216487884522\n",
      "Training iteration: 2314\n",
      "Improved validation loss from: 0.022921216487884522  to: 0.02289847582578659\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.02289847582578659  to: 0.022872230410575865\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.022872230410575865  to: 0.022844870388507844\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.022844870388507844  to: 0.022818319499492645\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.022818319499492645  to: 0.022793066501617432\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.022793066501617432  to: 0.022767806053161622\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.022767806053161622  to: 0.022740456461906432\n",
      "Training iteration: 2321\n",
      "Improved validation loss from: 0.022740456461906432  to: 0.022710511088371278\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.022710511088371278  to: 0.02268175780773163\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.02268175780773163  to: 0.02265615910291672\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.02265615910291672  to: 0.022633513808250426\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.022633513808250426  to: 0.02261088788509369\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.02261088788509369  to: 0.02258811742067337\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.02258811742067337  to: 0.022564685344696044\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.022564685344696044  to: 0.022539427876472472\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.022539427876472472  to: 0.02251160591840744\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.02251160591840744  to: 0.022481794655323028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.022481794655323028  to: 0.022451825439929962\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.022451825439929962  to: 0.02242540568113327\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.02242540568113327  to: 0.022403469681739806\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.022403469681739806  to: 0.022383245825767516\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.022383245825767516  to: 0.02236071527004242\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.02236071527004242  to: 0.02233566790819168\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.02233566790819168  to: 0.022309181094169617\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.022309181094169617  to: 0.02228224277496338\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.02228224277496338  to: 0.022255222499370574\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.022255222499370574  to: 0.02222849428653717\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.02222849428653717  to: 0.022203782200813295\n",
      "Training iteration: 2342\n",
      "Improved validation loss from: 0.022203782200813295  to: 0.022181601822376253\n",
      "Training iteration: 2343\n",
      "Improved validation loss from: 0.022181601822376253  to: 0.02215934991836548\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.02215934991836548  to: 0.022134459018707274\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.022134459018707274  to: 0.022108657658100127\n",
      "Training iteration: 2346\n",
      "Improved validation loss from: 0.022108657658100127  to: 0.022083564102649687\n",
      "Training iteration: 2347\n",
      "Improved validation loss from: 0.022083564102649687  to: 0.02205898016691208\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.02205898016691208  to: 0.02203354090452194\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.02203354090452194  to: 0.022009646892547606\n",
      "Training iteration: 2350\n",
      "Improved validation loss from: 0.022009646892547606  to: 0.021987056732177733\n",
      "Training iteration: 2351\n",
      "Improved validation loss from: 0.021987056732177733  to: 0.021964165568351745\n",
      "Training iteration: 2352\n",
      "Improved validation loss from: 0.021964165568351745  to: 0.02193859815597534\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.02193859815597534  to: 0.021909913420677184\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.021909913420677184  to: 0.0218821257352829\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.0218821257352829  to: 0.021860602498054504\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.021860602498054504  to: 0.02184510976076126\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.02184510976076126  to: 0.0218304306268692\n",
      "Training iteration: 2358\n",
      "Improved validation loss from: 0.0218304306268692  to: 0.021809370815753938\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.021809370815753938  to: 0.021778929233551025\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.021778929233551025  to: 0.02174243927001953\n",
      "Training iteration: 2361\n",
      "Improved validation loss from: 0.02174243927001953  to: 0.021706366539001466\n",
      "Training iteration: 2362\n",
      "Improved validation loss from: 0.021706366539001466  to: 0.02167811393737793\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.02167811393737793  to: 0.021658928692340852\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.021658928692340852  to: 0.021646413207054137\n",
      "Training iteration: 2365\n",
      "Improved validation loss from: 0.021646413207054137  to: 0.02163330018520355\n",
      "Training iteration: 2366\n",
      "Improved validation loss from: 0.02163330018520355  to: 0.021611592173576354\n",
      "Training iteration: 2367\n",
      "Improved validation loss from: 0.021611592173576354  to: 0.02157958298921585\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.02157958298921585  to: 0.02154625654220581\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.02154625654220581  to: 0.021518521010875702\n",
      "Training iteration: 2370\n",
      "Improved validation loss from: 0.021518521010875702  to: 0.021497371792793273\n",
      "Training iteration: 2371\n",
      "Improved validation loss from: 0.021497371792793273  to: 0.02147866487503052\n",
      "Training iteration: 2372\n",
      "Improved validation loss from: 0.02147866487503052  to: 0.021459560096263885\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.021459560096263885  to: 0.0214367151260376\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.0214367151260376  to: 0.0214089959859848\n",
      "Training iteration: 2375\n",
      "Improved validation loss from: 0.0214089959859848  to: 0.021381628513336182\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.021381628513336182  to: 0.021357671916484834\n",
      "Training iteration: 2377\n",
      "Improved validation loss from: 0.021357671916484834  to: 0.021336250007152557\n",
      "Training iteration: 2378\n",
      "Improved validation loss from: 0.021336250007152557  to: 0.02131441533565521\n",
      "Training iteration: 2379\n",
      "Improved validation loss from: 0.02131441533565521  to: 0.021291783452033995\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.021291783452033995  to: 0.021271185576915742\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.021271185576915742  to: 0.021252655982971193\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.021252655982971193  to: 0.021232572197914124\n",
      "Training iteration: 2383\n",
      "Improved validation loss from: 0.021232572197914124  to: 0.021208080649375915\n",
      "Training iteration: 2384\n",
      "Improved validation loss from: 0.021208080649375915  to: 0.021178527176380156\n",
      "Training iteration: 2385\n",
      "Improved validation loss from: 0.021178527176380156  to: 0.021145734190940856\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.021145734190940856  to: 0.021113574504852295\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.021113574504852295  to: 0.021087057888507843\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.021087057888507843  to: 0.021068951487541197\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.021068951487541197  to: 0.021059414744377135\n",
      "Training iteration: 2390\n",
      "Improved validation loss from: 0.021059414744377135  to: 0.021051569283008574\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.021051569283008574  to: 0.02103600949048996\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.02103600949048996  to: 0.02100772559642792\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.02100772559642792  to: 0.02097076177597046\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.02097076177597046  to: 0.020933334529399873\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.020933334529399873  to: 0.0209013894200325\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.0209013894200325  to: 0.0208768755197525\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.0208768755197525  to: 0.020859329402446745\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.020859329402446745  to: 0.02084570825099945\n",
      "Training iteration: 2399\n",
      "Improved validation loss from: 0.02084570825099945  to: 0.02083272933959961\n",
      "Training iteration: 2400\n",
      "Improved validation loss from: 0.02083272933959961  to: 0.02081633359193802\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: 0.02081633359193802  to: 0.020793998241424562\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.020793998241424562  to: 0.020765766501426697\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.020765766501426697  to: 0.020734651386737822\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: 0.020734651386737822  to: 0.020704102516174317\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: 0.020704102516174317  to: 0.020679020881652833\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.020679020881652833  to: 0.020663127303123474\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.020663127303123474  to: 0.020651955902576447\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.020651955902576447  to: 0.020636740326881408\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.020636740326881408  to: 0.02061101496219635\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.02061101496219635  to: 0.02057620733976364\n",
      "Training iteration: 2411\n",
      "Improved validation loss from: 0.02057620733976364  to: 0.020541779696941376\n",
      "Training iteration: 2412\n",
      "Improved validation loss from: 0.020541779696941376  to: 0.020517520606517792\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.020517520606517792  to: 0.020504312217235567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2414\n",
      "Improved validation loss from: 0.020504312217235567  to: 0.02049441635608673\n",
      "Training iteration: 2415\n",
      "Improved validation loss from: 0.02049441635608673  to: 0.0204768106341362\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: 0.0204768106341362  to: 0.02044958174228668\n",
      "Training iteration: 2417\n",
      "Improved validation loss from: 0.02044958174228668  to: 0.020417432487010955\n",
      "Training iteration: 2418\n",
      "Improved validation loss from: 0.020417432487010955  to: 0.02038974016904831\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.02038974016904831  to: 0.02037014961242676\n",
      "Training iteration: 2420\n",
      "Improved validation loss from: 0.02037014961242676  to: 0.020355208218097685\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.020355208218097685  to: 0.020337128639221193\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.020337128639221193  to: 0.020313660800457\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.020313660800457  to: 0.020288851857185364\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.020288851857185364  to: 0.020266199111938478\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.020266199111938478  to: 0.020245733857154845\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: 0.020245733857154845  to: 0.020224475860595705\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.020224475860595705  to: 0.020199528336524962\n",
      "Training iteration: 2428\n",
      "Improved validation loss from: 0.020199528336524962  to: 0.020173239707946777\n",
      "Training iteration: 2429\n",
      "Improved validation loss from: 0.020173239707946777  to: 0.020150148868560792\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: 0.020150148868560792  to: 0.02013184577226639\n",
      "Training iteration: 2431\n",
      "Improved validation loss from: 0.02013184577226639  to: 0.020114514231681823\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: 0.020114514231681823  to: 0.020093736052513123\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.020093736052513123  to: 0.020069947838783263\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: 0.020069947838783263  to: 0.020047572255134583\n",
      "Training iteration: 2435\n",
      "Improved validation loss from: 0.020047572255134583  to: 0.020028448104858397\n",
      "Training iteration: 2436\n",
      "Improved validation loss from: 0.020028448104858397  to: 0.020009391009807587\n",
      "Training iteration: 2437\n",
      "Improved validation loss from: 0.020009391009807587  to: 0.019987133145332337\n",
      "Training iteration: 2438\n",
      "Improved validation loss from: 0.019987133145332337  to: 0.019960075616836548\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.019960075616836548  to: 0.019932225346565247\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.019932225346565247  to: 0.019909921288490295\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.019909921288490295  to: 0.01989363133907318\n",
      "Training iteration: 2442\n",
      "Improved validation loss from: 0.01989363133907318  to: 0.019878533482551575\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.019878533482551575  to: 0.019861532747745513\n",
      "Training iteration: 2444\n",
      "Improved validation loss from: 0.019861532747745513  to: 0.019839395582675935\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.019839395582675935  to: 0.019812016189098357\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.019812016189098357  to: 0.019785100221633913\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.019785100221633913  to: 0.019762006402015687\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.019762006402015687  to: 0.019742222130298616\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.019742222130298616  to: 0.019725015759468077\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.019725015759468077  to: 0.019710275530815124\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.019710275530815124  to: 0.019694340229034425\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.019694340229034425  to: 0.0196733683347702\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.0196733683347702  to: 0.019645747542381287\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.019645747542381287  to: 0.01961481720209122\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.01961481720209122  to: 0.019587092101573944\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.019587092101573944  to: 0.019568035006523134\n",
      "Training iteration: 2457\n",
      "Improved validation loss from: 0.019568035006523134  to: 0.019559231400489808\n",
      "Training iteration: 2458\n",
      "Improved validation loss from: 0.019559231400489808  to: 0.019553887844085693\n",
      "Training iteration: 2459\n",
      "Improved validation loss from: 0.019553887844085693  to: 0.019540518522262573\n",
      "Training iteration: 2460\n",
      "Improved validation loss from: 0.019540518522262573  to: 0.019512732326984406\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.019512732326984406  to: 0.019474948942661285\n",
      "Training iteration: 2462\n",
      "Improved validation loss from: 0.019474948942661285  to: 0.019437763094902038\n",
      "Training iteration: 2463\n",
      "Improved validation loss from: 0.019437763094902038  to: 0.0194108247756958\n",
      "Training iteration: 2464\n",
      "Improved validation loss from: 0.0194108247756958  to: 0.0193973109126091\n",
      "Training iteration: 2465\n",
      "Improved validation loss from: 0.0193973109126091  to: 0.019394931197166444\n",
      "Training iteration: 2466\n",
      "Improved validation loss from: 0.019394931197166444  to: 0.019391696155071258\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.019391696155071258  to: 0.019374649226665496\n",
      "Training iteration: 2468\n",
      "Improved validation loss from: 0.019374649226665496  to: 0.01934150457382202\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.01934150457382202  to: 0.019302242994308473\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.019302242994308473  to: 0.019270190596580507\n",
      "Training iteration: 2471\n",
      "Improved validation loss from: 0.019270190596580507  to: 0.019253531098365785\n",
      "Training iteration: 2472\n",
      "Improved validation loss from: 0.019253531098365785  to: 0.019248780608177186\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.019248780608177186  to: 0.01924254298210144\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.01924254298210144  to: 0.019222021102905273\n",
      "Training iteration: 2475\n",
      "Improved validation loss from: 0.019222021102905273  to: 0.01918964833021164\n",
      "Training iteration: 2476\n",
      "Improved validation loss from: 0.01918964833021164  to: 0.019158142805099487\n",
      "Training iteration: 2477\n",
      "Improved validation loss from: 0.019158142805099487  to: 0.019138824939727784\n",
      "Training iteration: 2478\n",
      "Improved validation loss from: 0.019138824939727784  to: 0.01913200318813324\n",
      "Training iteration: 2479\n",
      "Improved validation loss from: 0.01913200318813324  to: 0.019127334654331207\n",
      "Training iteration: 2480\n",
      "Improved validation loss from: 0.019127334654331207  to: 0.01911034733057022\n",
      "Training iteration: 2481\n",
      "Improved validation loss from: 0.01911034733057022  to: 0.01907597780227661\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.01907597780227661  to: 0.019034852087497712\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.019034852087497712  to: 0.019002065062522888\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.019002065062522888  to: 0.018985120952129363\n",
      "Training iteration: 2485\n",
      "Improved validation loss from: 0.018985120952129363  to: 0.01898312121629715\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.018984739482402802\n",
      "Training iteration: 2487\n",
      "Improved validation loss from: 0.01898312121629715  to: 0.01897227466106415\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.01897227466106415  to: 0.018942886590957643\n",
      "Training iteration: 2489\n",
      "Improved validation loss from: 0.018942886590957643  to: 0.01890665590763092\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.01890665590763092  to: 0.018875423073768615\n",
      "Training iteration: 2491\n",
      "Improved validation loss from: 0.018875423073768615  to: 0.018853908777236937\n",
      "Training iteration: 2492\n",
      "Improved validation loss from: 0.018853908777236937  to: 0.018842051923274993\n",
      "Training iteration: 2493\n",
      "Improved validation loss from: 0.018842051923274993  to: 0.018835167586803436\n",
      "Training iteration: 2494\n",
      "Improved validation loss from: 0.018835167586803436  to: 0.018823300302028657\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.018823300302028657  to: 0.018803408741950987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.018803408741950987  to: 0.018777284026145934\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.018777284026145934  to: 0.018749849498271944\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.018749849498271944  to: 0.018724577128887178\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.018724577128887178  to: 0.018704742193222046\n",
      "Training iteration: 2500\n",
      "Improved validation loss from: 0.018704742193222046  to: 0.018691840767860412\n",
      "Training iteration: 2501\n",
      "Improved validation loss from: 0.018691840767860412  to: 0.0186807245016098\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.0186807245016098  to: 0.018663367629051207\n",
      "Training iteration: 2503\n",
      "Improved validation loss from: 0.018663367629051207  to: 0.01863957941532135\n",
      "Training iteration: 2504\n",
      "Improved validation loss from: 0.01863957941532135  to: 0.018613234162330627\n",
      "Training iteration: 2505\n",
      "Improved validation loss from: 0.018613234162330627  to: 0.018588264286518098\n",
      "Training iteration: 2506\n",
      "Improved validation loss from: 0.018588264286518098  to: 0.018568791449069977\n",
      "Training iteration: 2507\n",
      "Improved validation loss from: 0.018568791449069977  to: 0.018556468188762665\n",
      "Training iteration: 2508\n",
      "Improved validation loss from: 0.018556468188762665  to: 0.018546077609062194\n",
      "Training iteration: 2509\n",
      "Improved validation loss from: 0.018546077609062194  to: 0.01852998435497284\n",
      "Training iteration: 2510\n",
      "Improved validation loss from: 0.01852998435497284  to: 0.018507465720176697\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.018507465720176697  to: 0.01848154515028\n",
      "Training iteration: 2512\n",
      "Improved validation loss from: 0.01848154515028  to: 0.018456515669822694\n",
      "Training iteration: 2513\n",
      "Improved validation loss from: 0.018456515669822694  to: 0.018436796963214874\n",
      "Training iteration: 2514\n",
      "Improved validation loss from: 0.018436796963214874  to: 0.01842453479766846\n",
      "Training iteration: 2515\n",
      "Improved validation loss from: 0.01842453479766846  to: 0.018414755165576936\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.018414755165576936  to: 0.018398961424827574\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.018398961424827574  to: 0.018376275897026062\n",
      "Training iteration: 2518\n",
      "Improved validation loss from: 0.018376275897026062  to: 0.01835005134344101\n",
      "Training iteration: 2519\n",
      "Improved validation loss from: 0.01835005134344101  to: 0.018324394524097443\n",
      "Training iteration: 2520\n",
      "Improved validation loss from: 0.018324394524097443  to: 0.018304292857646943\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.018304292857646943  to: 0.01829216182231903\n",
      "Training iteration: 2522\n",
      "Improved validation loss from: 0.01829216182231903  to: 0.01828281581401825\n",
      "Training iteration: 2523\n",
      "Improved validation loss from: 0.01828281581401825  to: 0.01826755255460739\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: 0.01826755255460739  to: 0.018245014548301696\n",
      "Training iteration: 2525\n",
      "Improved validation loss from: 0.018245014548301696  to: 0.018218594789505004\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.018218594789505004  to: 0.018195968866348267\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.018195968866348267  to: 0.018179187178611757\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: 0.018179187178611757  to: 0.0181645005941391\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.0181645005941391  to: 0.018149155378341674\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.018149155378341674  to: 0.018132922053337098\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.018132922053337098  to: 0.018114778399467468\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: 0.018114778399467468  to: 0.018093398213386534\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.018093398213386534  to: 0.018069498240947723\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.018069498240947723  to: 0.018047448992729188\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.018047448992729188  to: 0.01803116351366043\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.01803116351366043  to: 0.01802258789539337\n",
      "Training iteration: 2537\n",
      "Improved validation loss from: 0.01802258789539337  to: 0.018015855550765993\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.018015855550765993  to: 0.018001365661621093\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: 0.018001365661621093  to: 0.017974846065044403\n",
      "Training iteration: 2540\n",
      "Improved validation loss from: 0.017974846065044403  to: 0.017940965294837952\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.017940965294837952  to: 0.01791176348924637\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.01791176348924637  to: 0.017894907295703887\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.017894907295703887  to: 0.017891478538513184\n",
      "Training iteration: 2544\n",
      "Improved validation loss from: 0.017891478538513184  to: 0.01789137125015259\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: 0.01789137125015259  to: 0.01787915974855423\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.01787915974855423  to: 0.017849162220954895\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.017849162220954895  to: 0.017813391983509064\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: 0.017813391983509064  to: 0.01778780072927475\n",
      "Training iteration: 2549\n",
      "Improved validation loss from: 0.01778780072927475  to: 0.017776189744472502\n",
      "Training iteration: 2550\n",
      "Improved validation loss from: 0.017776189744472502  to: 0.017771105468273162\n",
      "Training iteration: 2551\n",
      "Improved validation loss from: 0.017771105468273162  to: 0.01775972843170166\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: 0.01775972843170166  to: 0.01773689091205597\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: 0.01773689091205597  to: 0.017709414660930633\n",
      "Training iteration: 2554\n",
      "Improved validation loss from: 0.017709414660930633  to: 0.01768840253353119\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: 0.01768840253353119  to: 0.01767716258764267\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: 0.01767716258764267  to: 0.01766996681690216\n",
      "Training iteration: 2557\n",
      "Improved validation loss from: 0.01766996681690216  to: 0.017656461894512178\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: 0.017656461894512178  to: 0.017633961141109468\n",
      "Training iteration: 2559\n",
      "Improved validation loss from: 0.017633961141109468  to: 0.017608818411827088\n",
      "Training iteration: 2560\n",
      "Improved validation loss from: 0.017608818411827088  to: 0.01759113371372223\n",
      "Training iteration: 2561\n",
      "Improved validation loss from: 0.01759113371372223  to: 0.017582182586193085\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: 0.017582182586193085  to: 0.017574216425418853\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.017574216425418853  to: 0.01755679100751877\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.01755679100751877  to: 0.01752675473690033\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.01752675473690033  to: 0.01749403923749924\n",
      "Training iteration: 2566\n",
      "Improved validation loss from: 0.01749403923749924  to: 0.017470353841781618\n",
      "Training iteration: 2567\n",
      "Improved validation loss from: 0.017470353841781618  to: 0.017461414635181426\n",
      "Training iteration: 2568\n",
      "Improved validation loss from: 0.017461414635181426  to: 0.017461004853248595\n",
      "Training iteration: 2569\n",
      "Improved validation loss from: 0.017461004853248595  to: 0.017454694211483\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.017454694211483  to: 0.01743220239877701\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.01743220239877701  to: 0.017400079965591432\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: 0.017400079965591432  to: 0.017372597754001618\n",
      "Training iteration: 2573\n",
      "Improved validation loss from: 0.017372597754001618  to: 0.01736010015010834\n",
      "Training iteration: 2574\n",
      "Improved validation loss from: 0.01736010015010834  to: 0.017359964549541473\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.017359964549541473  to: 0.0173579066991806\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.0173579066991806  to: 0.017338719964027405\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: 0.017338719964027405  to: 0.017303583025932313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2578\n",
      "Improved validation loss from: 0.017303583025932313  to: 0.01726844161748886\n",
      "Training iteration: 2579\n",
      "Improved validation loss from: 0.01726844161748886  to: 0.017246858775615694\n",
      "Training iteration: 2580\n",
      "Improved validation loss from: 0.017246858775615694  to: 0.01724250614643097\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.01724696606397629\n",
      "Training iteration: 2582\n",
      "Improved validation loss from: 0.01724250614643097  to: 0.017241939902305603\n",
      "Training iteration: 2583\n",
      "Improved validation loss from: 0.017241939902305603  to: 0.017219045758247377\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.017219045758247377  to: 0.01718910187482834\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.01718910187482834  to: 0.01716431975364685\n",
      "Training iteration: 2586\n",
      "Improved validation loss from: 0.01716431975364685  to: 0.017148472368717194\n",
      "Training iteration: 2587\n",
      "Improved validation loss from: 0.017148472368717194  to: 0.017136594653129576\n",
      "Training iteration: 2588\n",
      "Improved validation loss from: 0.017136594653129576  to: 0.017123720049858092\n",
      "Training iteration: 2589\n",
      "Improved validation loss from: 0.017123720049858092  to: 0.017107924818992613\n",
      "Training iteration: 2590\n",
      "Improved validation loss from: 0.017107924818992613  to: 0.01708843559026718\n",
      "Training iteration: 2591\n",
      "Improved validation loss from: 0.01708843559026718  to: 0.017069627344608308\n",
      "Training iteration: 2592\n",
      "Improved validation loss from: 0.017069627344608308  to: 0.017053020000457764\n",
      "Training iteration: 2593\n",
      "Improved validation loss from: 0.017053020000457764  to: 0.01703660041093826\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.01703660041093826  to: 0.017020443081855775\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.017020443081855775  to: 0.01700633317232132\n",
      "Training iteration: 2596\n",
      "Improved validation loss from: 0.01700633317232132  to: 0.016992612183094023\n",
      "Training iteration: 2597\n",
      "Improved validation loss from: 0.016992612183094023  to: 0.016975635290145875\n",
      "Training iteration: 2598\n",
      "Improved validation loss from: 0.016975635290145875  to: 0.0169568732380867\n",
      "Training iteration: 2599\n",
      "Improved validation loss from: 0.0169568732380867  to: 0.01693806201219559\n",
      "Training iteration: 2600\n",
      "Improved validation loss from: 0.01693806201219559  to: 0.016921797394752504\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.016921797394752504  to: 0.016911205649375916\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.016911205649375916  to: 0.01690223962068558\n",
      "Training iteration: 2603\n",
      "Improved validation loss from: 0.01690223962068558  to: 0.0168873131275177\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.0168873131275177  to: 0.01686337888240814\n",
      "Training iteration: 2605\n",
      "Improved validation loss from: 0.01686337888240814  to: 0.016837258636951447\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.016837258636951447  to: 0.016817966103553773\n",
      "Training iteration: 2607\n",
      "Improved validation loss from: 0.016817966103553773  to: 0.016810940206050874\n",
      "Training iteration: 2608\n",
      "Improved validation loss from: 0.016810940206050874  to: 0.016809701919555664\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.016809701919555664  to: 0.016800558567047118\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.016800558567047118  to: 0.01677538752555847\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.01677538752555847  to: 0.016743239760398865\n",
      "Training iteration: 2612\n",
      "Improved validation loss from: 0.016743239760398865  to: 0.016729816794395447\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.016730761528015135\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): 0.016730210185050963\n",
      "Training iteration: 2615\n",
      "Improved validation loss from: 0.016729816794395447  to: 0.0167147234082222\n",
      "Training iteration: 2616\n",
      "Improved validation loss from: 0.0167147234082222  to: 0.01668267846107483\n",
      "Training iteration: 2617\n",
      "Improved validation loss from: 0.01668267846107483  to: 0.01664680689573288\n",
      "Training iteration: 2618\n",
      "Improved validation loss from: 0.01664680689573288  to: 0.016618865728378295\n",
      "Training iteration: 2619\n",
      "Improved validation loss from: 0.016618865728378295  to: 0.016602446138858796\n",
      "Training iteration: 2620\n",
      "Improved validation loss from: 0.016602446138858796  to: 0.016591784358024598\n",
      "Training iteration: 2621\n",
      "Improved validation loss from: 0.016591784358024598  to: 0.016578036546707153\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.016578036546707153  to: 0.016559170186519624\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.016559170186519624  to: 0.01653892397880554\n",
      "Training iteration: 2624\n",
      "Improved validation loss from: 0.01653892397880554  to: 0.016523237526416778\n",
      "Training iteration: 2625\n",
      "Improved validation loss from: 0.016523237526416778  to: 0.01651725620031357\n",
      "Training iteration: 2626\n",
      "Improved validation loss from: 0.01651725620031357  to: 0.01651591956615448\n",
      "Training iteration: 2627\n",
      "Improved validation loss from: 0.01651591956615448  to: 0.01650901734828949\n",
      "Training iteration: 2628\n",
      "Improved validation loss from: 0.01650901734828949  to: 0.016490885615348817\n",
      "Training iteration: 2629\n",
      "Improved validation loss from: 0.016490885615348817  to: 0.01646457612514496\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.01646457612514496  to: 0.01644041985273361\n",
      "Training iteration: 2631\n",
      "Improved validation loss from: 0.01644041985273361  to: 0.016425833106040955\n",
      "Training iteration: 2632\n",
      "Improved validation loss from: 0.016425833106040955  to: 0.016422979533672333\n",
      "Training iteration: 2633\n",
      "Improved validation loss from: 0.016422979533672333  to: 0.01642187833786011\n",
      "Training iteration: 2634\n",
      "Improved validation loss from: 0.01642187833786011  to: 0.0164081409573555\n",
      "Training iteration: 2635\n",
      "Improved validation loss from: 0.0164081409573555  to: 0.01637936681509018\n",
      "Training iteration: 2636\n",
      "Improved validation loss from: 0.01637936681509018  to: 0.016348595917224883\n",
      "Training iteration: 2637\n",
      "Improved validation loss from: 0.016348595917224883  to: 0.016328009963035583\n",
      "Training iteration: 2638\n",
      "Improved validation loss from: 0.016328009963035583  to: 0.01632288992404938\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.016324886679649354\n",
      "Training iteration: 2640\n",
      "Improved validation loss from: 0.01632288992404938  to: 0.016315914690494537\n",
      "Training iteration: 2641\n",
      "Improved validation loss from: 0.016315914690494537  to: 0.016286462545394897\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.016286462545394897  to: 0.016250693798065187\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.016250693798065187  to: 0.016226255893707277\n",
      "Training iteration: 2644\n",
      "Improved validation loss from: 0.016226255893707277  to: 0.016217748820781707\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.01622074991464615\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.016219767928123473\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.016217748820781707  to: 0.016204336285591127\n",
      "Training iteration: 2648\n",
      "Improved validation loss from: 0.016204336285591127  to: 0.016175642609596252\n",
      "Training iteration: 2649\n",
      "Improved validation loss from: 0.016175642609596252  to: 0.016145558655261995\n",
      "Training iteration: 2650\n",
      "Improved validation loss from: 0.016145558655261995  to: 0.016126100718975068\n",
      "Training iteration: 2651\n",
      "Improved validation loss from: 0.016126100718975068  to: 0.016121484339237213\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.016123056411743164\n",
      "Training iteration: 2653\n",
      "Improved validation loss from: 0.016121484339237213  to: 0.016114915907382964\n",
      "Training iteration: 2654\n",
      "Improved validation loss from: 0.016114915907382964  to: 0.01609179675579071\n",
      "Training iteration: 2655\n",
      "Improved validation loss from: 0.01609179675579071  to: 0.016063617169857027\n",
      "Training iteration: 2656\n",
      "Improved validation loss from: 0.016063617169857027  to: 0.016041335463523865\n",
      "Training iteration: 2657\n",
      "Improved validation loss from: 0.016041335463523865  to: 0.01602753847837448\n",
      "Training iteration: 2658\n",
      "Improved validation loss from: 0.01602753847837448  to: 0.016020268201828003\n",
      "Training iteration: 2659\n",
      "Improved validation loss from: 0.016020268201828003  to: 0.01601492613554001\n",
      "Training iteration: 2660\n",
      "Improved validation loss from: 0.01601492613554001  to: 0.016002777218818664\n",
      "Training iteration: 2661\n",
      "Improved validation loss from: 0.016002777218818664  to: 0.015979436039924622\n",
      "Training iteration: 2662\n",
      "Improved validation loss from: 0.015979436039924622  to: 0.015952780842781067\n",
      "Training iteration: 2663\n",
      "Improved validation loss from: 0.015952780842781067  to: 0.015934517979621886\n",
      "Training iteration: 2664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.015934517979621886  to: 0.015925855934619905\n",
      "Training iteration: 2665\n",
      "Improved validation loss from: 0.015925855934619905  to: 0.015918180346488953\n",
      "Training iteration: 2666\n",
      "Improved validation loss from: 0.015918180346488953  to: 0.015901871025562286\n",
      "Training iteration: 2667\n",
      "Improved validation loss from: 0.015901871025562286  to: 0.015877941250801088\n",
      "Training iteration: 2668\n",
      "Improved validation loss from: 0.015877941250801088  to: 0.015855349600315094\n",
      "Training iteration: 2669\n",
      "Improved validation loss from: 0.015855349600315094  to: 0.015842525660991667\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.01584298610687256\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.015846642851829528\n",
      "Training iteration: 2672\n",
      "Improved validation loss from: 0.015842525660991667  to: 0.015837569534778596\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.015837569534778596  to: 0.015809185802936554\n",
      "Training iteration: 2674\n",
      "Improved validation loss from: 0.015809185802936554  to: 0.01577105075120926\n",
      "Training iteration: 2675\n",
      "Improved validation loss from: 0.01577105075120926  to: 0.015741539001464844\n",
      "Training iteration: 2676\n",
      "Improved validation loss from: 0.015741539001464844  to: 0.01573009043931961\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.015735766291618346\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.015742120146751405\n",
      "Training iteration: 2679\n",
      "Improved validation loss from: 0.01573009043931961  to: 0.015728791058063508\n",
      "Training iteration: 2680\n",
      "Improved validation loss from: 0.015728791058063508  to: 0.015697643160820007\n",
      "Training iteration: 2681\n",
      "Improved validation loss from: 0.015697643160820007  to: 0.015664032101631163\n",
      "Training iteration: 2682\n",
      "Improved validation loss from: 0.015664032101631163  to: 0.015643587708473204\n",
      "Training iteration: 2683\n",
      "Improved validation loss from: 0.015643587708473204  to: 0.015637867152690887\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): 0.015640144050121308\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.01563954949378967\n",
      "Training iteration: 2686\n",
      "Improved validation loss from: 0.015637867152690887  to: 0.015625351667404176\n",
      "Training iteration: 2687\n",
      "Improved validation loss from: 0.015625351667404176  to: 0.015597805380821228\n",
      "Training iteration: 2688\n",
      "Improved validation loss from: 0.015597805380821228  to: 0.015566907823085785\n",
      "Training iteration: 2689\n",
      "Improved validation loss from: 0.015566907823085785  to: 0.015545403957366944\n",
      "Training iteration: 2690\n",
      "Improved validation loss from: 0.015545403957366944  to: 0.015538360178470611\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.015543141961097717\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.015544210374355317\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.015538360178470611  to: 0.015527196228504181\n",
      "Training iteration: 2694\n",
      "Improved validation loss from: 0.015527196228504181  to: 0.01549435555934906\n",
      "Training iteration: 2695\n",
      "Improved validation loss from: 0.01549435555934906  to: 0.015463247895240784\n",
      "Training iteration: 2696\n",
      "Improved validation loss from: 0.015463247895240784  to: 0.015447130799293518\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.015447130799293518  to: 0.015445415675640107\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.015448710322380066\n",
      "Training iteration: 2699\n",
      "Improved validation loss from: 0.015445415675640107  to: 0.015441277623176574\n",
      "Training iteration: 2700\n",
      "Improved validation loss from: 0.015441277623176574  to: 0.015419866144657134\n",
      "Training iteration: 2701\n",
      "Improved validation loss from: 0.015419866144657134  to: 0.01539265811443329\n",
      "Training iteration: 2702\n",
      "Improved validation loss from: 0.01539265811443329  to: 0.015373021364212036\n",
      "Training iteration: 2703\n",
      "Improved validation loss from: 0.015373021364212036  to: 0.015363487601280212\n",
      "Training iteration: 2704\n",
      "Improved validation loss from: 0.015363487601280212  to: 0.015356940031051636\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.015356940031051636  to: 0.01534648984670639\n",
      "Training iteration: 2706\n",
      "Improved validation loss from: 0.01534648984670639  to: 0.015330982208251954\n",
      "Training iteration: 2707\n",
      "Improved validation loss from: 0.015330982208251954  to: 0.015316334366798402\n",
      "Training iteration: 2708\n",
      "Improved validation loss from: 0.015316334366798402  to: 0.015305018424987793\n",
      "Training iteration: 2709\n",
      "Improved validation loss from: 0.015305018424987793  to: 0.015293699502944947\n",
      "Training iteration: 2710\n",
      "Improved validation loss from: 0.015293699502944947  to: 0.015277014672756195\n",
      "Training iteration: 2711\n",
      "Improved validation loss from: 0.015277014672756195  to: 0.015257282555103302\n",
      "Training iteration: 2712\n",
      "Improved validation loss from: 0.015257282555103302  to: 0.01524052619934082\n",
      "Training iteration: 2713\n",
      "Improved validation loss from: 0.01524052619934082  to: 0.015232667326927185\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.015232667326927185  to: 0.015229494869709015\n",
      "Training iteration: 2715\n",
      "Improved validation loss from: 0.015229494869709015  to: 0.015220078825950622\n",
      "Training iteration: 2716\n",
      "Improved validation loss from: 0.015220078825950622  to: 0.015198132395744324\n",
      "Training iteration: 2717\n",
      "Improved validation loss from: 0.015198132395744324  to: 0.015171501040458679\n",
      "Training iteration: 2718\n",
      "Improved validation loss from: 0.015171501040458679  to: 0.015152099728584289\n",
      "Training iteration: 2719\n",
      "Improved validation loss from: 0.015152099728584289  to: 0.015147173404693603\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.015149672329425812\n",
      "Training iteration: 2721\n",
      "Improved validation loss from: 0.015147173404693603  to: 0.015143653750419617\n",
      "Training iteration: 2722\n",
      "Improved validation loss from: 0.015143653750419617  to: 0.015120466053485871\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.015120466053485871  to: 0.015090498328208923\n",
      "Training iteration: 2724\n",
      "Improved validation loss from: 0.015090498328208923  to: 0.015069171786308289\n",
      "Training iteration: 2725\n",
      "Improved validation loss from: 0.015069171786308289  to: 0.015065023303031921\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.015070734918117524\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.01506788730621338\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.015065023303031921  to: 0.015045157074928284\n",
      "Training iteration: 2729\n",
      "Improved validation loss from: 0.015045157074928284  to: 0.015013264119625091\n",
      "Training iteration: 2730\n",
      "Improved validation loss from: 0.015013264119625091  to: 0.014990246295928955\n",
      "Training iteration: 2731\n",
      "Improved validation loss from: 0.014990246295928955  to: 0.014985613524913788\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.014992633461952209\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.014991845190525054\n",
      "Training iteration: 2734\n",
      "Improved validation loss from: 0.014985613524913788  to: 0.014973077178001403\n",
      "Training iteration: 2735\n",
      "Improved validation loss from: 0.014973077178001403  to: 0.014946597814559936\n",
      "Training iteration: 2736\n",
      "Improved validation loss from: 0.014946597814559936  to: 0.014925149083137513\n",
      "Training iteration: 2737\n",
      "Improved validation loss from: 0.014925149083137513  to: 0.014911969006061555\n",
      "Training iteration: 2738\n",
      "Improved validation loss from: 0.014911969006061555  to: 0.014902374148368836\n",
      "Training iteration: 2739\n",
      "Improved validation loss from: 0.014902374148368836  to: 0.014891822636127473\n",
      "Training iteration: 2740\n",
      "Improved validation loss from: 0.014891822636127473  to: 0.014879018068313599\n",
      "Training iteration: 2741\n",
      "Improved validation loss from: 0.014879018068313599  to: 0.014866861701011657\n",
      "Training iteration: 2742\n",
      "Improved validation loss from: 0.014866861701011657  to: 0.014860633015632629\n",
      "Training iteration: 2743\n",
      "Improved validation loss from: 0.014860633015632629  to: 0.014857280254364013\n",
      "Training iteration: 2744\n",
      "Improved validation loss from: 0.014857280254364013  to: 0.014848171174526215\n",
      "Training iteration: 2745\n",
      "Improved validation loss from: 0.014848171174526215  to: 0.014827018976211548\n",
      "Training iteration: 2746\n",
      "Improved validation loss from: 0.014827018976211548  to: 0.014797821640968323\n",
      "Training iteration: 2747\n",
      "Improved validation loss from: 0.014797821640968323  to: 0.014773544669151307\n",
      "Training iteration: 2748\n",
      "Improved validation loss from: 0.014773544669151307  to: 0.014762987196445466\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.014765766263008118\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.01476937234401703\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.0147629976272583\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.014762987196445466  to: 0.014742283523082734\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.014742283523082734  to: 0.014714087545871734\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.014714087545871734  to: 0.014692427217960357\n",
      "Training iteration: 2755\n",
      "Improved validation loss from: 0.014692427217960357  to: 0.014682020246982574\n",
      "Training iteration: 2756\n",
      "Improved validation loss from: 0.014682020246982574  to: 0.014680440723896026\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.014680837094783784\n",
      "Training iteration: 2758\n",
      "Improved validation loss from: 0.014680440723896026  to: 0.014671990275382995\n",
      "Training iteration: 2759\n",
      "Improved validation loss from: 0.014671990275382995  to: 0.014650039374828339\n",
      "Training iteration: 2760\n",
      "Improved validation loss from: 0.014650039374828339  to: 0.014624950289726258\n",
      "Training iteration: 2761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.014624950289726258  to: 0.014609861373901366\n",
      "Training iteration: 2762\n",
      "Improved validation loss from: 0.014609861373901366  to: 0.014605246484279633\n",
      "Training iteration: 2763\n",
      "Improved validation loss from: 0.014605246484279633  to: 0.014600905776023864\n",
      "Training iteration: 2764\n",
      "Improved validation loss from: 0.014600905776023864  to: 0.014589592814445496\n",
      "Training iteration: 2765\n",
      "Improved validation loss from: 0.014589592814445496  to: 0.014573016762733459\n",
      "Training iteration: 2766\n",
      "Improved validation loss from: 0.014573016762733459  to: 0.014555466175079346\n",
      "Training iteration: 2767\n",
      "Improved validation loss from: 0.014555466175079346  to: 0.014539192616939544\n",
      "Training iteration: 2768\n",
      "Improved validation loss from: 0.014539192616939544  to: 0.014527240395545959\n",
      "Training iteration: 2769\n",
      "Improved validation loss from: 0.014527240395545959  to: 0.01452091634273529\n",
      "Training iteration: 2770\n",
      "Improved validation loss from: 0.01452091634273529  to: 0.014514195919036865\n",
      "Training iteration: 2771\n",
      "Improved validation loss from: 0.014514195919036865  to: 0.014500381052494049\n",
      "Training iteration: 2772\n",
      "Improved validation loss from: 0.014500381052494049  to: 0.014481939375400543\n",
      "Training iteration: 2773\n",
      "Improved validation loss from: 0.014481939375400543  to: 0.014466977119445801\n",
      "Training iteration: 2774\n",
      "Improved validation loss from: 0.014466977119445801  to: 0.014457356929779053\n",
      "Training iteration: 2775\n",
      "Improved validation loss from: 0.014457356929779053  to: 0.014453032612800598\n",
      "Training iteration: 2776\n",
      "Improved validation loss from: 0.014453032612800598  to: 0.014446313679218292\n",
      "Training iteration: 2777\n",
      "Improved validation loss from: 0.014446313679218292  to: 0.01442989856004715\n",
      "Training iteration: 2778\n",
      "Improved validation loss from: 0.01442989856004715  to: 0.014408285915851592\n",
      "Training iteration: 2779\n",
      "Improved validation loss from: 0.014408285915851592  to: 0.014391952753067016\n",
      "Training iteration: 2780\n",
      "Improved validation loss from: 0.014391952753067016  to: 0.014383435249328613\n",
      "Training iteration: 2781\n",
      "Improved validation loss from: 0.014383435249328613  to: 0.014381150901317596\n",
      "Training iteration: 2782\n",
      "Improved validation loss from: 0.014381150901317596  to: 0.014375272393226623\n",
      "Training iteration: 2783\n",
      "Improved validation loss from: 0.014375272393226623  to: 0.01435815989971161\n",
      "Training iteration: 2784\n",
      "Improved validation loss from: 0.01435815989971161  to: 0.014335279166698457\n",
      "Training iteration: 2785\n",
      "Improved validation loss from: 0.014335279166698457  to: 0.014318230748176574\n",
      "Training iteration: 2786\n",
      "Improved validation loss from: 0.014318230748176574  to: 0.01431494951248169\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.014318791031837464\n",
      "Training iteration: 2788\n",
      "Improved validation loss from: 0.01431494951248169  to: 0.014313453435897827\n",
      "Training iteration: 2789\n",
      "Improved validation loss from: 0.014313453435897827  to: 0.014289912581443787\n",
      "Training iteration: 2790\n",
      "Improved validation loss from: 0.014289912581443787  to: 0.014259928464889526\n",
      "Training iteration: 2791\n",
      "Improved validation loss from: 0.014259928464889526  to: 0.014240334928035735\n",
      "Training iteration: 2792\n",
      "Improved validation loss from: 0.014240334928035735  to: 0.014239633083343506\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.014248219132423402\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.014245076477527619\n",
      "Training iteration: 2795\n",
      "Improved validation loss from: 0.014239633083343506  to: 0.014219929277896882\n",
      "Training iteration: 2796\n",
      "Improved validation loss from: 0.014219929277896882  to: 0.014186924695968628\n",
      "Training iteration: 2797\n",
      "Improved validation loss from: 0.014186924695968628  to: 0.014166226983070374\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): 0.014166301488876343\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.014177668094635009\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.014177630841732024\n",
      "Training iteration: 2801\n",
      "Improved validation loss from: 0.014166226983070374  to: 0.014157280325889587\n",
      "Training iteration: 2802\n",
      "Improved validation loss from: 0.014157280325889587  to: 0.014130307734012604\n",
      "Training iteration: 2803\n",
      "Improved validation loss from: 0.014130307734012604  to: 0.014111208915710449\n",
      "Training iteration: 2804\n",
      "Improved validation loss from: 0.014111208915710449  to: 0.01410219371318817\n",
      "Training iteration: 2805\n",
      "Improved validation loss from: 0.01410219371318817  to: 0.014096502959728242\n",
      "Training iteration: 2806\n",
      "Improved validation loss from: 0.014096502959728242  to: 0.014087259769439697\n",
      "Training iteration: 2807\n",
      "Improved validation loss from: 0.014087259769439697  to: 0.014073681831359864\n",
      "Training iteration: 2808\n",
      "Improved validation loss from: 0.014073681831359864  to: 0.014061211049556733\n",
      "Training iteration: 2809\n",
      "Improved validation loss from: 0.014061211049556733  to: 0.014056578278541565\n",
      "Training iteration: 2810\n",
      "Improved validation loss from: 0.014056578278541565  to: 0.01405656635761261\n",
      "Training iteration: 2811\n",
      "Improved validation loss from: 0.01405656635761261  to: 0.014050242304801942\n",
      "Training iteration: 2812\n",
      "Improved validation loss from: 0.014050242304801942  to: 0.01402953565120697\n",
      "Training iteration: 2813\n",
      "Improved validation loss from: 0.01402953565120697  to: 0.013999782502651215\n",
      "Training iteration: 2814\n",
      "Improved validation loss from: 0.013999782502651215  to: 0.013975979387760162\n",
      "Training iteration: 2815\n",
      "Improved validation loss from: 0.013975979387760162  to: 0.013967616856098175\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.01397358626127243\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.013984107971191406\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.013982175290584565\n",
      "Training iteration: 2819\n",
      "Improved validation loss from: 0.013967616856098175  to: 0.013959532976150513\n",
      "Training iteration: 2820\n",
      "Improved validation loss from: 0.013959532976150513  to: 0.013926026225090028\n",
      "Training iteration: 2821\n",
      "Improved validation loss from: 0.013926026225090028  to: 0.01389807164669037\n",
      "Training iteration: 2822\n",
      "Improved validation loss from: 0.01389807164669037  to: 0.013886377215385437\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.013890616595745087\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.013899974524974823\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.013901983201503754\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.013888588547706604\n",
      "Training iteration: 2827\n",
      "Improved validation loss from: 0.013886377215385437  to: 0.013863858580589295\n",
      "Training iteration: 2828\n",
      "Improved validation loss from: 0.013863858580589295  to: 0.013837772607803344\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.013837772607803344  to: 0.013818110525608062\n",
      "Training iteration: 2830\n",
      "Improved validation loss from: 0.013818110525608062  to: 0.013809046149253846\n",
      "Training iteration: 2831\n",
      "Improved validation loss from: 0.013809046149253846  to: 0.013808497786521911\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): 0.01380966603755951\n",
      "Training iteration: 2833\n",
      "Improved validation loss from: 0.013808497786521911  to: 0.013807877898216248\n",
      "Training iteration: 2834\n",
      "Improved validation loss from: 0.013807877898216248  to: 0.013798364996910095\n",
      "Training iteration: 2835\n",
      "Improved validation loss from: 0.013798364996910095  to: 0.013780151307582856\n",
      "Training iteration: 2836\n",
      "Improved validation loss from: 0.013780151307582856  to: 0.013756935298442841\n",
      "Training iteration: 2837\n",
      "Improved validation loss from: 0.013756935298442841  to: 0.013734784722328187\n",
      "Training iteration: 2838\n",
      "Improved validation loss from: 0.013734784722328187  to: 0.013721121847629547\n",
      "Training iteration: 2839\n",
      "Improved validation loss from: 0.013721121847629547  to: 0.013717825710773467\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.013720943033695221\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.013723742961883546\n",
      "Training iteration: 2842\n",
      "Improved validation loss from: 0.013717825710773467  to: 0.013717307150363922\n",
      "Training iteration: 2843\n",
      "Improved validation loss from: 0.013717307150363922  to: 0.01369827538728714\n",
      "Training iteration: 2844\n",
      "Improved validation loss from: 0.01369827538728714  to: 0.013672032952308654\n",
      "Training iteration: 2845\n",
      "Improved validation loss from: 0.013672032952308654  to: 0.013648569583892822\n",
      "Training iteration: 2846\n",
      "Improved validation loss from: 0.013648569583892822  to: 0.013636447489261627\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.013636562228202819\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.013642147183418274\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.01364436000585556\n",
      "Training iteration: 2850\n",
      "Improved validation loss from: 0.013636447489261627  to: 0.01363515704870224\n",
      "Training iteration: 2851\n",
      "Improved validation loss from: 0.01363515704870224  to: 0.013614071905612946\n",
      "Training iteration: 2852\n",
      "Improved validation loss from: 0.013614071905612946  to: 0.013588348031044006\n",
      "Training iteration: 2853\n",
      "Improved validation loss from: 0.013588348031044006  to: 0.013567119836807251\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.013567119836807251  to: 0.013556967675685882\n",
      "Training iteration: 2855\n",
      "Validation loss (no improvement): 0.013557226955890655\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): 0.013560888171195985\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.013561175763607025\n",
      "Training iteration: 2858\n",
      "Improved validation loss from: 0.013556967675685882  to: 0.013551783561706544\n",
      "Training iteration: 2859\n",
      "Improved validation loss from: 0.013551783561706544  to: 0.013532425463199615\n",
      "Training iteration: 2860\n",
      "Improved validation loss from: 0.013532425463199615  to: 0.013508348166942597\n",
      "Training iteration: 2861\n",
      "Improved validation loss from: 0.013508348166942597  to: 0.013487523794174195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2862\n",
      "Improved validation loss from: 0.013487523794174195  to: 0.013476432859897613\n",
      "Training iteration: 2863\n",
      "Improved validation loss from: 0.013476432859897613  to: 0.013475584983825683\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.013479186594486237\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.013480624556541443\n",
      "Training iteration: 2866\n",
      "Improved validation loss from: 0.013475584983825683  to: 0.013472741842269898\n",
      "Training iteration: 2867\n",
      "Improved validation loss from: 0.013472741842269898  to: 0.013453876972198487\n",
      "Training iteration: 2868\n",
      "Improved validation loss from: 0.013453876972198487  to: 0.013429459929466248\n",
      "Training iteration: 2869\n",
      "Improved validation loss from: 0.013429459929466248  to: 0.013408111035823822\n",
      "Training iteration: 2870\n",
      "Improved validation loss from: 0.013408111035823822  to: 0.01339697539806366\n",
      "Training iteration: 2871\n",
      "Improved validation loss from: 0.01339697539806366  to: 0.01339675486087799\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.013401240110397339\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.013403069972991944\n",
      "Training iteration: 2874\n",
      "Improved validation loss from: 0.01339675486087799  to: 0.013394807279109956\n",
      "Training iteration: 2875\n",
      "Improved validation loss from: 0.013394807279109956  to: 0.013375556468963623\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.013375556468963623  to: 0.013351163268089295\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.013351163268089295  to: 0.01333034485578537\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.01333034485578537  to: 0.013319982588291169\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.013320119678974151\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.013324351608753204\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.013325521349906921\n",
      "Training iteration: 2882\n",
      "Improved validation loss from: 0.013319982588291169  to: 0.013317027688026428\n",
      "Training iteration: 2883\n",
      "Improved validation loss from: 0.013317027688026428  to: 0.013298101723194122\n",
      "Training iteration: 2884\n",
      "Improved validation loss from: 0.013298101723194122  to: 0.01327449530363083\n",
      "Training iteration: 2885\n",
      "Improved validation loss from: 0.01327449530363083  to: 0.013258042931556701\n",
      "Training iteration: 2886\n",
      "Improved validation loss from: 0.013258042931556701  to: 0.01325550526380539\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.01325814425945282\n",
      "Training iteration: 2888\n",
      "Improved validation loss from: 0.01325550526380539  to: 0.013251264393329621\n",
      "Training iteration: 2889\n",
      "Improved validation loss from: 0.013251264393329621  to: 0.013233187794685363\n",
      "Training iteration: 2890\n",
      "Improved validation loss from: 0.013233187794685363  to: 0.013214804232120514\n",
      "Training iteration: 2891\n",
      "Improved validation loss from: 0.013214804232120514  to: 0.013208156824111939\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.013211102783679962\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.013208909332752228\n",
      "Training iteration: 2894\n",
      "Improved validation loss from: 0.013208156824111939  to: 0.013191373646259308\n",
      "Training iteration: 2895\n",
      "Improved validation loss from: 0.013191373646259308  to: 0.013167014718055725\n",
      "Training iteration: 2896\n",
      "Improved validation loss from: 0.013167014718055725  to: 0.013151296973228454\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.013152463734149933\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.013160961866378783\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.013157600164413452\n",
      "Training iteration: 2900\n",
      "Improved validation loss from: 0.013151296973228454  to: 0.013138078153133392\n",
      "Training iteration: 2901\n",
      "Improved validation loss from: 0.013138078153133392  to: 0.013117194175720215\n",
      "Training iteration: 2902\n",
      "Improved validation loss from: 0.013117194175720215  to: 0.013105425238609313\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.013105425238609313  to: 0.013100716471672057\n",
      "Training iteration: 2904\n",
      "Improved validation loss from: 0.013100716471672057  to: 0.013098329305648804\n",
      "Training iteration: 2905\n",
      "Improved validation loss from: 0.013098329305648804  to: 0.013093990087509156\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: 0.013093990087509156  to: 0.013082480430603028\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.013082480430603028  to: 0.013064119219779968\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.013064119219779968  to: 0.013048170506954193\n",
      "Training iteration: 2909\n",
      "Improved validation loss from: 0.013048170506954193  to: 0.01304219514131546\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.013048143684864044\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.01305299997329712\n",
      "Training iteration: 2912\n",
      "Improved validation loss from: 0.01304219514131546  to: 0.0130410298705101\n",
      "Training iteration: 2913\n",
      "Improved validation loss from: 0.0130410298705101  to: 0.01301223337650299\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.01301223337650299  to: 0.01298595815896988\n",
      "Training iteration: 2915\n",
      "Improved validation loss from: 0.01298595815896988  to: 0.012977316975593567\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.012988622486591338\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.013001129031181335\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.012990668416023254\n",
      "Training iteration: 2919\n",
      "Improved validation loss from: 0.012977316975593567  to: 0.012957721948623657\n",
      "Training iteration: 2920\n",
      "Improved validation loss from: 0.012957721948623657  to: 0.012927128374576569\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.012927128374576569  to: 0.012917836010456086\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.012931945919990539\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.012948957085609437\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.012941770255565643\n",
      "Training iteration: 2925\n",
      "Improved validation loss from: 0.012917836010456086  to: 0.012912550568580627\n",
      "Training iteration: 2926\n",
      "Improved validation loss from: 0.012912550568580627  to: 0.012887004017829894\n",
      "Training iteration: 2927\n",
      "Improved validation loss from: 0.012887004017829894  to: 0.012879088521003723\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): 0.012883439660072327\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.012886925041675568\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.012879744172096252\n",
      "Training iteration: 2931\n",
      "Improved validation loss from: 0.012879088521003723  to: 0.012867099046707154\n",
      "Training iteration: 2932\n",
      "Improved validation loss from: 0.012867099046707154  to: 0.012854938209056855\n",
      "Training iteration: 2933\n",
      "Improved validation loss from: 0.012854938209056855  to: 0.012844081223011016\n",
      "Training iteration: 2934\n",
      "Improved validation loss from: 0.012844081223011016  to: 0.01283046305179596\n",
      "Training iteration: 2935\n",
      "Improved validation loss from: 0.01283046305179596  to: 0.012816858291625977\n",
      "Training iteration: 2936\n",
      "Improved validation loss from: 0.012816858291625977  to: 0.012808135151863098\n",
      "Training iteration: 2937\n",
      "Improved validation loss from: 0.012808135151863098  to: 0.01280764639377594\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.012807948887348175\n",
      "Training iteration: 2939\n",
      "Improved validation loss from: 0.01280764639377594  to: 0.012797877192497253\n",
      "Training iteration: 2940\n",
      "Improved validation loss from: 0.012797877192497253  to: 0.012776462733745575\n",
      "Training iteration: 2941\n",
      "Improved validation loss from: 0.012776462733745575  to: 0.012755852937698365\n",
      "Training iteration: 2942\n",
      "Improved validation loss from: 0.012755852937698365  to: 0.01274782419204712\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.012754562497138976\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.012769261002540588\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.012772977352142334\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.012754321098327637\n",
      "Training iteration: 2947\n",
      "Improved validation loss from: 0.01274782419204712  to: 0.012722223997116089\n",
      "Training iteration: 2948\n",
      "Improved validation loss from: 0.012722223997116089  to: 0.012693950533866882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2949\n",
      "Improved validation loss from: 0.012693950533866882  to: 0.01268327534198761\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.01269042193889618\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.012702833116054534\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.012706713378429412\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.01269414871931076\n",
      "Training iteration: 2954\n",
      "Improved validation loss from: 0.01268327534198761  to: 0.012670494616031647\n",
      "Training iteration: 2955\n",
      "Improved validation loss from: 0.012670494616031647  to: 0.012646842002868652\n",
      "Training iteration: 2956\n",
      "Improved validation loss from: 0.012646842002868652  to: 0.01263096034526825\n",
      "Training iteration: 2957\n",
      "Improved validation loss from: 0.01263096034526825  to: 0.012626096606254578\n",
      "Training iteration: 2958\n",
      "Validation loss (no improvement): 0.012628667056560516\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.012631595134735107\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): 0.012630787491798402\n",
      "Training iteration: 2961\n",
      "Improved validation loss from: 0.012626096606254578  to: 0.012623336911201478\n",
      "Training iteration: 2962\n",
      "Improved validation loss from: 0.012623336911201478  to: 0.012608753144741058\n",
      "Training iteration: 2963\n",
      "Improved validation loss from: 0.012608753144741058  to: 0.012589780986309052\n",
      "Training iteration: 2964\n",
      "Improved validation loss from: 0.012589780986309052  to: 0.01257605105638504\n",
      "Training iteration: 2965\n",
      "Improved validation loss from: 0.01257605105638504  to: 0.012573492527008057\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.012575286626815795\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.012573698163032531\n",
      "Training iteration: 2968\n",
      "Improved validation loss from: 0.012573492527008057  to: 0.012562012672424317\n",
      "Training iteration: 2969\n",
      "Improved validation loss from: 0.012562012672424317  to: 0.012546046078205109\n",
      "Training iteration: 2970\n",
      "Improved validation loss from: 0.012546046078205109  to: 0.012531821429729462\n",
      "Training iteration: 2971\n",
      "Improved validation loss from: 0.012531821429729462  to: 0.012525448203086853\n",
      "Training iteration: 2972\n",
      "Improved validation loss from: 0.012525448203086853  to: 0.012522681057453156\n",
      "Training iteration: 2973\n",
      "Improved validation loss from: 0.012522681057453156  to: 0.012519288063049316\n",
      "Training iteration: 2974\n",
      "Improved validation loss from: 0.012519288063049316  to: 0.012515302002429961\n",
      "Training iteration: 2975\n",
      "Improved validation loss from: 0.012515302002429961  to: 0.012507626414299011\n",
      "Training iteration: 2976\n",
      "Improved validation loss from: 0.012507626414299011  to: 0.01249295026063919\n",
      "Training iteration: 2977\n",
      "Improved validation loss from: 0.01249295026063919  to: 0.012473913282155991\n",
      "Training iteration: 2978\n",
      "Improved validation loss from: 0.012473913282155991  to: 0.01245945319533348\n",
      "Training iteration: 2979\n",
      "Improved validation loss from: 0.01245945319533348  to: 0.012455983459949494\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.012462326139211655\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.012472091615200043\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.012471561133861542\n",
      "Training iteration: 2983\n",
      "Improved validation loss from: 0.012455983459949494  to: 0.012453605979681015\n",
      "Training iteration: 2984\n",
      "Improved validation loss from: 0.012453605979681015  to: 0.012425188720226289\n",
      "Training iteration: 2985\n",
      "Improved validation loss from: 0.012425188720226289  to: 0.01240060180425644\n",
      "Training iteration: 2986\n",
      "Improved validation loss from: 0.01240060180425644  to: 0.0123914435505867\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.012397544085979461\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.012408030033111573\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.012411540746688843\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.012400640547275544\n",
      "Training iteration: 2991\n",
      "Improved validation loss from: 0.0123914435505867  to: 0.012379224598407745\n",
      "Training iteration: 2992\n",
      "Improved validation loss from: 0.012379224598407745  to: 0.012356767803430558\n",
      "Training iteration: 2993\n",
      "Improved validation loss from: 0.012356767803430558  to: 0.012345880270004272\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.01234993189573288\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.012355337291955948\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.012351179122924804\n",
      "Training iteration: 2997\n",
      "Improved validation loss from: 0.012345880270004272  to: 0.012333863973617553\n",
      "Training iteration: 2998\n",
      "Improved validation loss from: 0.012333863973617553  to: 0.012315809726715088\n",
      "Training iteration: 2999\n",
      "Improved validation loss from: 0.012315809726715088  to: 0.012304885685443879\n",
      "Training iteration: 3000\n",
      "Improved validation loss from: 0.012304885685443879  to: 0.012304157018661499\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.012304530292749406\n",
      "Training iteration: 3002\n",
      "Improved validation loss from: 0.012304157018661499  to: 0.012299185991287232\n",
      "Training iteration: 3003\n",
      "Improved validation loss from: 0.012299185991287232  to: 0.012291043996810913\n",
      "Training iteration: 3004\n",
      "Improved validation loss from: 0.012291043996810913  to: 0.012281636148691178\n",
      "Training iteration: 3005\n",
      "Improved validation loss from: 0.012281636148691178  to: 0.012269797176122666\n",
      "Training iteration: 3006\n",
      "Improved validation loss from: 0.012269797176122666  to: 0.012259687483310699\n",
      "Training iteration: 3007\n",
      "Improved validation loss from: 0.012259687483310699  to: 0.012256566435098648\n",
      "Training iteration: 3008\n",
      "Improved validation loss from: 0.012256566435098648  to: 0.012254719436168671\n",
      "Training iteration: 3009\n",
      "Improved validation loss from: 0.012254719436168671  to: 0.012244890630245208\n",
      "Training iteration: 3010\n",
      "Improved validation loss from: 0.012244890630245208  to: 0.012230418622493744\n",
      "Training iteration: 3011\n",
      "Improved validation loss from: 0.012230418622493744  to: 0.012220755964517594\n",
      "Training iteration: 3012\n",
      "Improved validation loss from: 0.012220755964517594  to: 0.012216730415821076\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.01221684217453003\n",
      "Training iteration: 3014\n",
      "Improved validation loss from: 0.012216730415821076  to: 0.0122118279337883\n",
      "Training iteration: 3015\n",
      "Improved validation loss from: 0.0122118279337883  to: 0.012201813608407974\n",
      "Training iteration: 3016\n",
      "Improved validation loss from: 0.012201813608407974  to: 0.01218850612640381\n",
      "Training iteration: 3017\n",
      "Improved validation loss from: 0.01218850612640381  to: 0.012179753929376601\n",
      "Training iteration: 3018\n",
      "Improved validation loss from: 0.012179753929376601  to: 0.012174788862466812\n",
      "Training iteration: 3019\n",
      "Improved validation loss from: 0.012174788862466812  to: 0.012171872705221177\n",
      "Training iteration: 3020\n",
      "Improved validation loss from: 0.012171872705221177  to: 0.012164872884750367\n",
      "Training iteration: 3021\n",
      "Improved validation loss from: 0.012164872884750367  to: 0.012154797464609146\n",
      "Training iteration: 3022\n",
      "Improved validation loss from: 0.012154797464609146  to: 0.012148746103048325\n",
      "Training iteration: 3023\n",
      "Improved validation loss from: 0.012148746103048325  to: 0.012145687639713288\n",
      "Training iteration: 3024\n",
      "Improved validation loss from: 0.012145687639713288  to: 0.012138012796640396\n",
      "Training iteration: 3025\n",
      "Improved validation loss from: 0.012138012796640396  to: 0.012126706540584564\n",
      "Training iteration: 3026\n",
      "Improved validation loss from: 0.012126706540584564  to: 0.012119007110595704\n",
      "Training iteration: 3027\n",
      "Improved validation loss from: 0.012119007110595704  to: 0.012113998085260392\n",
      "Training iteration: 3028\n",
      "Improved validation loss from: 0.012113998085260392  to: 0.012105977535247803\n",
      "Training iteration: 3029\n",
      "Improved validation loss from: 0.012105977535247803  to: 0.012095648050308227\n",
      "Training iteration: 3030\n",
      "Improved validation loss from: 0.012095648050308227  to: 0.012088676542043686\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.012090758234262467\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.012093734741210938\n",
      "Training iteration: 3033\n",
      "Improved validation loss from: 0.012088676542043686  to: 0.012084533274173737\n",
      "Training iteration: 3034\n",
      "Improved validation loss from: 0.012084533274173737  to: 0.012061107158660888\n",
      "Training iteration: 3035\n",
      "Improved validation loss from: 0.012061107158660888  to: 0.012039729207754136\n",
      "Training iteration: 3036\n",
      "Improved validation loss from: 0.012039729207754136  to: 0.01203446239233017\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.0120466448366642\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.012057441473007201\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.01204584464430809\n",
      "Training iteration: 3040\n",
      "Improved validation loss from: 0.01203446239233017  to: 0.01201978772878647\n",
      "Training iteration: 3041\n",
      "Improved validation loss from: 0.01201978772878647  to: 0.012003292888402938\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.012003469467163085\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.012008152902126312\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.012005116790533066\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: 0.012003292888402938  to: 0.011993557214736938\n",
      "Training iteration: 3046\n",
      "Improved validation loss from: 0.011993557214736938  to: 0.011985105276107789\n",
      "Training iteration: 3047\n",
      "Improved validation loss from: 0.011985105276107789  to: 0.011982697248458862\n",
      "Training iteration: 3048\n",
      "Improved validation loss from: 0.011982697248458862  to: 0.01197824701666832\n",
      "Training iteration: 3049\n",
      "Improved validation loss from: 0.01197824701666832  to: 0.011963532865047454\n",
      "Training iteration: 3050\n",
      "Improved validation loss from: 0.011963532865047454  to: 0.011945609748363496\n",
      "Training iteration: 3051\n",
      "Improved validation loss from: 0.011945609748363496  to: 0.011936334520578384\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.01194138303399086\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.011948943138122559\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.011941975355148316\n",
      "Training iteration: 3055\n",
      "Improved validation loss from: 0.011936334520578384  to: 0.011922793090343475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3056\n",
      "Improved validation loss from: 0.011922793090343475  to: 0.011908493936061859\n",
      "Training iteration: 3057\n",
      "Improved validation loss from: 0.011908493936061859  to: 0.011905360221862792\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.01190546602010727\n",
      "Training iteration: 3059\n",
      "Improved validation loss from: 0.011905360221862792  to: 0.011901749670505524\n",
      "Training iteration: 3060\n",
      "Improved validation loss from: 0.011901749670505524  to: 0.011894302070140838\n",
      "Training iteration: 3061\n",
      "Improved validation loss from: 0.011894302070140838  to: 0.011890751123428345\n",
      "Training iteration: 3062\n",
      "Improved validation loss from: 0.011890751123428345  to: 0.011889483779668808\n",
      "Training iteration: 3063\n",
      "Improved validation loss from: 0.011889483779668808  to: 0.011881764978170395\n",
      "Training iteration: 3064\n",
      "Improved validation loss from: 0.011881764978170395  to: 0.01186332106590271\n",
      "Training iteration: 3065\n",
      "Improved validation loss from: 0.01186332106590271  to: 0.011845213174819947\n",
      "Training iteration: 3066\n",
      "Improved validation loss from: 0.011845213174819947  to: 0.011839282512664796\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): 0.011848165839910507\n",
      "Training iteration: 3068\n",
      "Validation loss (no improvement): 0.011856234073638916\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.011845852434635162\n",
      "Training iteration: 3070\n",
      "Improved validation loss from: 0.011839282512664796  to: 0.011823301017284394\n",
      "Training iteration: 3071\n",
      "Improved validation loss from: 0.011823301017284394  to: 0.011809666454792023\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.011809935420751571\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.011812616884708405\n",
      "Training iteration: 3074\n",
      "Improved validation loss from: 0.011809666454792023  to: 0.011808128654956817\n",
      "Training iteration: 3075\n",
      "Improved validation loss from: 0.011808128654956817  to: 0.011797760426998139\n",
      "Training iteration: 3076\n",
      "Improved validation loss from: 0.011797760426998139  to: 0.011792647838592529\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.011792650073766708\n",
      "Training iteration: 3078\n",
      "Improved validation loss from: 0.011792647838592529  to: 0.011787800490856171\n",
      "Training iteration: 3079\n",
      "Improved validation loss from: 0.011787800490856171  to: 0.011770756542682647\n",
      "Training iteration: 3080\n",
      "Improved validation loss from: 0.011770756542682647  to: 0.011751803010702134\n",
      "Training iteration: 3081\n",
      "Improved validation loss from: 0.011751803010702134  to: 0.011744455993175506\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.011752712726593017\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.01176200658082962\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.011753375828266143\n",
      "Training iteration: 3085\n",
      "Improved validation loss from: 0.011744455993175506  to: 0.011731795221567153\n",
      "Training iteration: 3086\n",
      "Improved validation loss from: 0.011731795221567153  to: 0.011717579513788223\n",
      "Training iteration: 3087\n",
      "Improved validation loss from: 0.011717579513788223  to: 0.011716966331005097\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.01171964555978775\n",
      "Training iteration: 3089\n",
      "Improved validation loss from: 0.011716966331005097  to: 0.011716109514236451\n",
      "Training iteration: 3090\n",
      "Improved validation loss from: 0.011716109514236451  to: 0.011706911027431488\n",
      "Training iteration: 3091\n",
      "Improved validation loss from: 0.011706911027431488  to: 0.01170218214392662\n",
      "Training iteration: 3092\n",
      "Improved validation loss from: 0.01170218214392662  to: 0.011701633781194687\n",
      "Training iteration: 3093\n",
      "Improved validation loss from: 0.011701633781194687  to: 0.01169610396027565\n",
      "Training iteration: 3094\n",
      "Improved validation loss from: 0.01169610396027565  to: 0.011679200083017349\n",
      "Training iteration: 3095\n",
      "Improved validation loss from: 0.011679200083017349  to: 0.011661241948604583\n",
      "Training iteration: 3096\n",
      "Improved validation loss from: 0.011661241948604583  to: 0.011654695123434066\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.011663345247507095\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.011672113835811616\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.011662833392620087\n",
      "Training iteration: 3100\n",
      "Improved validation loss from: 0.011654695123434066  to: 0.011641395092010499\n",
      "Training iteration: 3101\n",
      "Improved validation loss from: 0.011641395092010499  to: 0.011628232151269912\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.01162886843085289\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.01163158044219017\n",
      "Training iteration: 3104\n",
      "Improved validation loss from: 0.011628232151269912  to: 0.011627249419689178\n",
      "Training iteration: 3105\n",
      "Improved validation loss from: 0.011627249419689178  to: 0.011617457866668702\n",
      "Training iteration: 3106\n",
      "Improved validation loss from: 0.011617457866668702  to: 0.011613365262746811\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.011614276468753815\n",
      "Training iteration: 3108\n",
      "Improved validation loss from: 0.011613365262746811  to: 0.011609365046024323\n",
      "Training iteration: 3109\n",
      "Improved validation loss from: 0.011609365046024323  to: 0.011591901630163192\n",
      "Training iteration: 3110\n",
      "Improved validation loss from: 0.011591901630163192  to: 0.011573077738285064\n",
      "Training iteration: 3111\n",
      "Improved validation loss from: 0.011573077738285064  to: 0.011566714942455291\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.011576464027166366\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.01158628910779953\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.011576911062002182\n",
      "Training iteration: 3115\n",
      "Improved validation loss from: 0.011566714942455291  to: 0.01155463233590126\n",
      "Training iteration: 3116\n",
      "Improved validation loss from: 0.01155463233590126  to: 0.011541272699832916\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.011542576551437377\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.01154642105102539\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.01154250130057335\n",
      "Training iteration: 3120\n",
      "Improved validation loss from: 0.011541272699832916  to: 0.011532481014728545\n",
      "Training iteration: 3121\n",
      "Improved validation loss from: 0.011532481014728545  to: 0.011527875810861588\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.011528805643320084\n",
      "Training iteration: 3123\n",
      "Improved validation loss from: 0.011527875810861588  to: 0.011524500697851181\n",
      "Training iteration: 3124\n",
      "Improved validation loss from: 0.011524500697851181  to: 0.01150769591331482\n",
      "Training iteration: 3125\n",
      "Improved validation loss from: 0.01150769591331482  to: 0.011489231884479523\n",
      "Training iteration: 3126\n",
      "Improved validation loss from: 0.011489231884479523  to: 0.011482898145914078\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.011492649465799332\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.011502452194690704\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.01149316281080246\n",
      "Training iteration: 3130\n",
      "Improved validation loss from: 0.011482898145914078  to: 0.011471326649188995\n",
      "Training iteration: 3131\n",
      "Improved validation loss from: 0.011471326649188995  to: 0.011458618938922882\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.011460361629724502\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.011464180052280426\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.011460056155920028\n",
      "Training iteration: 3135\n",
      "Improved validation loss from: 0.011458618938922882  to: 0.01145000010728836\n",
      "Training iteration: 3136\n",
      "Improved validation loss from: 0.01145000010728836  to: 0.011445925384759904\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.011447489261627197\n",
      "Training iteration: 3138\n",
      "Improved validation loss from: 0.011445925384759904  to: 0.011443375051021576\n",
      "Training iteration: 3139\n",
      "Improved validation loss from: 0.011443375051021576  to: 0.01142624169588089\n",
      "Training iteration: 3140\n",
      "Improved validation loss from: 0.01142624169588089  to: 0.011407647281885147\n",
      "Training iteration: 3141\n",
      "Improved validation loss from: 0.011407647281885147  to: 0.011401776224374771\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.011412174999713897\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.011422417312860488\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.01141282171010971\n",
      "Training iteration: 3145\n",
      "Improved validation loss from: 0.011401776224374771  to: 0.011390785872936248\n",
      "Training iteration: 3146\n",
      "Improved validation loss from: 0.011390785872936248  to: 0.01137838587164879\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.011380910873413086\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.0113852821290493\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.011380908638238907\n",
      "Training iteration: 3150\n",
      "Improved validation loss from: 0.01137838587164879  to: 0.011370432376861573\n",
      "Training iteration: 3151\n",
      "Improved validation loss from: 0.011370432376861573  to: 0.011366498470306397\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.011368757486343384\n",
      "Training iteration: 3153\n",
      "Improved validation loss from: 0.011366498470306397  to: 0.011365226656198501\n",
      "Training iteration: 3154\n",
      "Improved validation loss from: 0.011365226656198501  to: 0.011348072439432144\n",
      "Training iteration: 3155\n",
      "Improved validation loss from: 0.011348072439432144  to: 0.01132921725511551\n",
      "Training iteration: 3156\n",
      "Improved validation loss from: 0.01132921725511551  to: 0.011323444545269012\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.011334381252527236\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.011345074325799943\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.011335720866918563\n",
      "Training iteration: 3160\n",
      "Improved validation loss from: 0.011323444545269012  to: 0.011313354969024659\n",
      "Training iteration: 3161\n",
      "Improved validation loss from: 0.011313354969024659  to: 0.011301068961620331\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.011304030567407608\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.01130894273519516\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.011304749548435212\n",
      "Training iteration: 3165\n",
      "Improved validation loss from: 0.011301068961620331  to: 0.011294148117303848\n",
      "Training iteration: 3166\n",
      "Improved validation loss from: 0.011294148117303848  to: 0.011290303617715835\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.011292944103479386\n",
      "Training iteration: 3168\n",
      "Improved validation loss from: 0.011290303617715835  to: 0.011289636045694352\n",
      "Training iteration: 3169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.011289636045694352  to: 0.011272595077753068\n",
      "Training iteration: 3170\n",
      "Improved validation loss from: 0.011272595077753068  to: 0.011253736168146133\n",
      "Training iteration: 3171\n",
      "Improved validation loss from: 0.011253736168146133  to: 0.011248280107975007\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.01125970333814621\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.011270521581172943\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.01126086339354515\n",
      "Training iteration: 3175\n",
      "Improved validation loss from: 0.011248280107975007  to: 0.01123853176832199\n",
      "Training iteration: 3176\n",
      "Improved validation loss from: 0.01123853176832199  to: 0.011226780712604523\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.011230424791574479\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.011235632747411729\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): 0.011231215298175811\n",
      "Training iteration: 3180\n",
      "Improved validation loss from: 0.011226780712604523  to: 0.011220308393239975\n",
      "Training iteration: 3181\n",
      "Improved validation loss from: 0.011220308393239975  to: 0.011216723918914795\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.011219974607229233\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.011217130720615387\n",
      "Training iteration: 3184\n",
      "Improved validation loss from: 0.011216723918914795  to: 0.011199815571308136\n",
      "Training iteration: 3185\n",
      "Improved validation loss from: 0.011199815571308136  to: 0.011180917173624039\n",
      "Training iteration: 3186\n",
      "Improved validation loss from: 0.011180917173624039  to: 0.01117568239569664\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.011187724769115448\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.011198838800191879\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.011189005523920059\n",
      "Training iteration: 3190\n",
      "Improved validation loss from: 0.01117568239569664  to: 0.011166538298130035\n",
      "Training iteration: 3191\n",
      "Improved validation loss from: 0.011166538298130035  to: 0.01115509420633316\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.011159525066614152\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.01116505116224289\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.011160377413034439\n",
      "Training iteration: 3195\n",
      "Improved validation loss from: 0.01115509420633316  to: 0.011149223148822784\n",
      "Training iteration: 3196\n",
      "Improved validation loss from: 0.011149223148822784  to: 0.011145839840173722\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.011149682104587555\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.011147264391183853\n",
      "Training iteration: 3199\n",
      "Improved validation loss from: 0.011145839840173722  to: 0.011129913479089737\n",
      "Training iteration: 3200\n",
      "Improved validation loss from: 0.011129913479089737  to: 0.011110764741897584\n",
      "Training iteration: 3201\n",
      "Improved validation loss from: 0.011110764741897584  to: 0.01110580787062645\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.01111837849020958\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.011129853874444961\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.011119858920574188\n",
      "Training iteration: 3205\n",
      "Improved validation loss from: 0.01110580787062645  to: 0.01109721213579178\n",
      "Training iteration: 3206\n",
      "Improved validation loss from: 0.01109721213579178  to: 0.011086104065179824\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.011091159284114837\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.01109715923666954\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.011092226207256316\n",
      "Training iteration: 3210\n",
      "Improved validation loss from: 0.011086104065179824  to: 0.0110807403922081\n",
      "Training iteration: 3211\n",
      "Improved validation loss from: 0.0110807403922081  to: 0.011077654361724854\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.011082103103399276\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.011079947650432586\n",
      "Training iteration: 3214\n",
      "Improved validation loss from: 0.011077654361724854  to: 0.011062459647655487\n",
      "Training iteration: 3215\n",
      "Improved validation loss from: 0.011062459647655487  to: 0.011043158918619156\n",
      "Training iteration: 3216\n",
      "Improved validation loss from: 0.011043158918619156  to: 0.011038605123758316\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.011051653325557709\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.011063389480113983\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.011053170263767242\n",
      "Training iteration: 3220\n",
      "Improved validation loss from: 0.011038605123758316  to: 0.011030365526676179\n",
      "Training iteration: 3221\n",
      "Improved validation loss from: 0.011030365526676179  to: 0.011019699275493622\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.011025406420230865\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.011031784862279893\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.011026642471551894\n",
      "Training iteration: 3225\n",
      "Improved validation loss from: 0.011019699275493622  to: 0.011014854907989502\n",
      "Training iteration: 3226\n",
      "Improved validation loss from: 0.011014854907989502  to: 0.011011910438537598\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.011017074435949325\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.011015273630619049\n",
      "Training iteration: 3229\n",
      "Improved validation loss from: 0.011011910438537598  to: 0.010997619479894638\n",
      "Training iteration: 3230\n",
      "Improved validation loss from: 0.010997619479894638  to: 0.010978142917156219\n",
      "Training iteration: 3231\n",
      "Improved validation loss from: 0.010978142917156219  to: 0.0109737791121006\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.010987380892038346\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.01099950522184372\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.010989125818014145\n",
      "Training iteration: 3235\n",
      "Improved validation loss from: 0.0109737791121006  to: 0.01096617579460144\n",
      "Training iteration: 3236\n",
      "Improved validation loss from: 0.01096617579460144  to: 0.010955812036991119\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.010962142795324325\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.010968854278326034\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.010963429510593415\n",
      "Training iteration: 3240\n",
      "Improved validation loss from: 0.010955812036991119  to: 0.010951463133096695\n",
      "Training iteration: 3241\n",
      "Improved validation loss from: 0.010951463133096695  to: 0.010948763787746429\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.010954544693231583\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.010953070968389511\n",
      "Training iteration: 3244\n",
      "Improved validation loss from: 0.010948763787746429  to: 0.010935109853744508\n",
      "Training iteration: 3245\n",
      "Improved validation loss from: 0.010935109853744508  to: 0.010915454477071762\n",
      "Training iteration: 3246\n",
      "Improved validation loss from: 0.010915454477071762  to: 0.010911446809768677\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.010925735533237457\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.01093805804848671\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.010934758186340331\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.010915924608707429\n",
      "Training iteration: 3251\n",
      "Improved validation loss from: 0.010911446809768677  to: 0.010894950479269028\n",
      "Training iteration: 3252\n",
      "Improved validation loss from: 0.010894950479269028  to: 0.010889075696468353\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.010901747643947602\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.010911653190851212\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.010905998945236205\n",
      "Training iteration: 3256\n",
      "Improved validation loss from: 0.010889075696468353  to: 0.010886278003454208\n",
      "Training iteration: 3257\n",
      "Improved validation loss from: 0.010886278003454208  to: 0.010873322188854218\n",
      "Training iteration: 3258\n",
      "Improved validation loss from: 0.010873322188854218  to: 0.010872530937194824\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.010880448669195176\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.010880432277917861\n",
      "Training iteration: 3261\n",
      "Improved validation loss from: 0.010872530937194824  to: 0.010869846493005753\n",
      "Training iteration: 3262\n",
      "Improved validation loss from: 0.010869846493005753  to: 0.010862101614475251\n",
      "Training iteration: 3263\n",
      "Improved validation loss from: 0.010862101614475251  to: 0.010861023515462875\n",
      "Training iteration: 3264\n",
      "Improved validation loss from: 0.010861023515462875  to: 0.010858215391635895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3265\n",
      "Improved validation loss from: 0.010858215391635895  to: 0.010852295160293578\n",
      "Training iteration: 3266\n",
      "Improved validation loss from: 0.010852295160293578  to: 0.0108485147356987\n",
      "Training iteration: 3267\n",
      "Improved validation loss from: 0.0108485147356987  to: 0.010845606029033662\n",
      "Training iteration: 3268\n",
      "Improved validation loss from: 0.010845606029033662  to: 0.010845576226711274\n",
      "Training iteration: 3269\n",
      "Improved validation loss from: 0.010845576226711274  to: 0.010841862857341766\n",
      "Training iteration: 3270\n",
      "Improved validation loss from: 0.010841862857341766  to: 0.010836567729711533\n",
      "Training iteration: 3271\n",
      "Improved validation loss from: 0.010836567729711533  to: 0.010828526318073272\n",
      "Training iteration: 3272\n",
      "Improved validation loss from: 0.010828526318073272  to: 0.01082346886396408\n",
      "Training iteration: 3273\n",
      "Improved validation loss from: 0.01082346886396408  to: 0.010819715261459351\n",
      "Training iteration: 3274\n",
      "Improved validation loss from: 0.010819715261459351  to: 0.01081806793808937\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.010820873826742173\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.010820118337869644\n",
      "Training iteration: 3277\n",
      "Improved validation loss from: 0.01081806793808937  to: 0.010808111727237701\n",
      "Training iteration: 3278\n",
      "Improved validation loss from: 0.010808111727237701  to: 0.010794736444950104\n",
      "Training iteration: 3279\n",
      "Improved validation loss from: 0.010794736444950104  to: 0.010793572664260865\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.010799292474985123\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.010796524584293365\n",
      "Training iteration: 3282\n",
      "Improved validation loss from: 0.010793572664260865  to: 0.010784576088190079\n",
      "Training iteration: 3283\n",
      "Improved validation loss from: 0.010784576088190079  to: 0.010776283591985703\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.010782923549413681\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.010793739557266235\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.01078798547387123\n",
      "Training iteration: 3287\n",
      "Improved validation loss from: 0.010776283591985703  to: 0.010764014720916749\n",
      "Training iteration: 3288\n",
      "Improved validation loss from: 0.010764014720916749  to: 0.010744750499725342\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.010747145116329192\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.010767813026905059\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.010789195448160172\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.010785683244466781\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.010757581144571305\n",
      "Training iteration: 3294\n",
      "Improved validation loss from: 0.010744750499725342  to: 0.010728245973587036\n",
      "Training iteration: 3295\n",
      "Improved validation loss from: 0.010728245973587036  to: 0.010714657604694366\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.010720745474100114\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.01073424369096756\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.010739525407552719\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.010735370963811875\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.010728089511394501\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.010720529407262803\n",
      "Training iteration: 3302\n",
      "Improved validation loss from: 0.010714657604694366  to: 0.010711290687322617\n",
      "Training iteration: 3303\n",
      "Improved validation loss from: 0.010711290687322617  to: 0.010705651342868805\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.010707827657461167\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.010717759281396866\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.010721218585968018\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.010706250369548798\n",
      "Training iteration: 3308\n",
      "Improved validation loss from: 0.010705651342868805  to: 0.01068781167268753\n",
      "Training iteration: 3309\n",
      "Improved validation loss from: 0.01068781167268753  to: 0.01067839115858078\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.010683035850524903\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.010695376247167588\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.010696097463369369\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.01068502888083458\n",
      "Training iteration: 3314\n",
      "Improved validation loss from: 0.01067839115858078  to: 0.010669942945241928\n",
      "Training iteration: 3315\n",
      "Improved validation loss from: 0.010669942945241928  to: 0.010658515989780426\n",
      "Training iteration: 3316\n",
      "Improved validation loss from: 0.010658515989780426  to: 0.010657654702663421\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.01066601425409317\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.010677099227905273\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.010676021873950958\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.01065765842795372\n",
      "Training iteration: 3321\n",
      "Improved validation loss from: 0.010657654702663421  to: 0.010634738206863403\n",
      "Training iteration: 3322\n",
      "Improved validation loss from: 0.010634738206863403  to: 0.010626684129238128\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.01063869372010231\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.010657697916030884\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.010669025033712387\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.010660393536090851\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.010637588798999786\n",
      "Training iteration: 3328\n",
      "Improved validation loss from: 0.010626684129238128  to: 0.010615646839141846\n",
      "Training iteration: 3329\n",
      "Improved validation loss from: 0.010615646839141846  to: 0.010603699833154678\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.010605070739984512\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.010613200813531875\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.010619236528873444\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.010620512068271637\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.010616663843393326\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.01060691624879837\n",
      "Training iteration: 3336\n",
      "Improved validation loss from: 0.010603699833154678  to: 0.01059427410364151\n",
      "Training iteration: 3337\n",
      "Improved validation loss from: 0.01059427410364151  to: 0.010588455200195312\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.010594954341650009\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.010608100891113281\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.01061028689146042\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.010591886192560195\n",
      "Training iteration: 3342\n",
      "Improved validation loss from: 0.010588455200195312  to: 0.010572437942028046\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.010574301332235336\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.010587368905544282\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.010585460811853409\n",
      "Training iteration: 3346\n",
      "Improved validation loss from: 0.010572437942028046  to: 0.010567043721675873\n",
      "Training iteration: 3347\n",
      "Improved validation loss from: 0.010567043721675873  to: 0.010553321987390517\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.010562248528003693\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.010579116642475128\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.010574408620595933\n",
      "Training iteration: 3351\n",
      "Improved validation loss from: 0.010553321987390517  to: 0.010545456409454345\n",
      "Training iteration: 3352\n",
      "Improved validation loss from: 0.010545456409454345  to: 0.010522345453500748\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.010528121143579483\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.010555851459503173\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.010580656677484512\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.010573574155569077\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.010541461408138275\n",
      "Training iteration: 3358\n",
      "Improved validation loss from: 0.010522345453500748  to: 0.01051332801580429\n",
      "Training iteration: 3359\n",
      "Improved validation loss from: 0.01051332801580429  to: 0.010505791008472442\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.010516820847988129\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.010528729856014251\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.010529007762670517\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.010523296892642975\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.010519955307245255\n",
      "Training iteration: 3365\n",
      "Validation loss (no improvement): 0.010516701638698578\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.010507466644048691\n",
      "Training iteration: 3367\n",
      "Improved validation loss from: 0.010505791008472442  to: 0.010497720539569854\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.010497951507568359\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.010510022938251495\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.010516367852687836\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.010502728074789048\n",
      "Training iteration: 3372\n",
      "Improved validation loss from: 0.010497720539569854  to: 0.0104828342795372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3373\n",
      "Improved validation loss from: 0.0104828342795372  to: 0.010480896383523942\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.010491570085287094\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.010492227226495742\n",
      "Training iteration: 3376\n",
      "Improved validation loss from: 0.010480896383523942  to: 0.010478679835796357\n",
      "Training iteration: 3377\n",
      "Improved validation loss from: 0.010478679835796357  to: 0.01046721711754799\n",
      "Training iteration: 3378\n",
      "Validation loss (no improvement): 0.010474498569965362\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.010488309711217881\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.010483241081237793\n",
      "Training iteration: 3381\n",
      "Improved validation loss from: 0.01046721711754799  to: 0.01045711487531662\n",
      "Training iteration: 3382\n",
      "Improved validation loss from: 0.01045711487531662  to: 0.010435976833105088\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.010442550480365752\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.010468907654285431\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): 0.010490991920232774\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.01048392802476883\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.010454189777374268\n",
      "Training iteration: 3388\n",
      "Improved validation loss from: 0.010435976833105088  to: 0.010426972806453706\n",
      "Training iteration: 3389\n",
      "Improved validation loss from: 0.010426972806453706  to: 0.010418768227100372\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.010428550094366074\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.010439378023147584\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.01044081300497055\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.01043703556060791\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.010445404052734374\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.01045733317732811\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.01045246347784996\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.010425561666488647\n",
      "Training iteration: 3398\n",
      "Improved validation loss from: 0.010418768227100372  to: 0.010394270718097686\n",
      "Training iteration: 3399\n",
      "Improved validation loss from: 0.010394270718097686  to: 0.010386021435260772\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.010403318703174591\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): 0.010424528270959854\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.010429714620113373\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.0104144848883152\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.010394080728292465\n",
      "Training iteration: 3405\n",
      "Improved validation loss from: 0.010386021435260772  to: 0.010381648689508438\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.01038663387298584\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): 0.01040094867348671\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.010411284118890762\n",
      "Training iteration: 3409\n",
      "Validation loss (no improvement): 0.010404545068740844\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): 0.01038215383887291\n",
      "Training iteration: 3411\n",
      "Improved validation loss from: 0.010381648689508438  to: 0.010361939668655396\n",
      "Training iteration: 3412\n",
      "Improved validation loss from: 0.010361939668655396  to: 0.010360423475503922\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.01037609800696373\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.01039215326309204\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.010398226976394653\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.010390007495880127\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): 0.010372743755578995\n",
      "Training iteration: 3418\n",
      "Improved validation loss from: 0.010360423475503922  to: 0.01035430058836937\n",
      "Training iteration: 3419\n",
      "Improved validation loss from: 0.01035430058836937  to: 0.010341255366802216\n",
      "Training iteration: 3420\n",
      "Improved validation loss from: 0.010341255366802216  to: 0.010339288413524628\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): 0.010346810519695281\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): 0.0103564515709877\n",
      "Training iteration: 3423\n",
      "Validation loss (no improvement): 0.01036272495985031\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): 0.010371412336826324\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.010375503450632095\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.010366203635931015\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.010344529151916504\n",
      "Training iteration: 3428\n",
      "Improved validation loss from: 0.010339288413524628  to: 0.010320033878087997\n",
      "Training iteration: 3429\n",
      "Improved validation loss from: 0.010320033878087997  to: 0.01031302660703659\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.010325530916452408\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.010341064631938934\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.010348290205001831\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.010339824855327607\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.01032164841890335\n",
      "Training iteration: 3435\n",
      "Improved validation loss from: 0.01031302660703659  to: 0.010304652154445648\n",
      "Training iteration: 3436\n",
      "Improved validation loss from: 0.010304652154445648  to: 0.010303713381290436\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.010317765176296234\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.010333915054798127\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.010330839455127716\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.010306211560964585\n",
      "Training iteration: 3441\n",
      "Improved validation loss from: 0.010303713381290436  to: 0.010289348661899567\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.010290579497814178\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.010303640365600586\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.010313139110803605\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.010315899550914765\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.010309191048145294\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.010293958336114883\n",
      "Training iteration: 3448\n",
      "Improved validation loss from: 0.010289348661899567  to: 0.010275200754404069\n",
      "Training iteration: 3449\n",
      "Improved validation loss from: 0.010275200754404069  to: 0.010268521308898926\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.01027868390083313\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.010296143591403961\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.010296936333179473\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.01027405858039856\n",
      "Training iteration: 3454\n",
      "Improved validation loss from: 0.010268521308898926  to: 0.010254964977502824\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.01026361584663391\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.010283295065164566\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.010280152410268783\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.010257421433925629\n",
      "Training iteration: 3459\n",
      "Improved validation loss from: 0.010254964977502824  to: 0.010245301574468613\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.010262644290924073\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.010285277664661408\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.010274839401245118\n",
      "Training iteration: 3463\n",
      "Improved validation loss from: 0.010245301574468613  to: 0.010237978398799896\n",
      "Training iteration: 3464\n",
      "Improved validation loss from: 0.010237978398799896  to: 0.010215703397989273\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.010231788456439971\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.01026654988527298\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.010286551713943482\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.010269198566675186\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.010236092656850816\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.010216566175222397\n",
      "Training iteration: 3471\n",
      "Improved validation loss from: 0.010215703397989273  to: 0.010215387493371964\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.010223500430583954\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.010227391868829728\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.010225322097539902\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.01022767573595047\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.01023380532860756\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.010229922831058502\n",
      "Training iteration: 3478\n",
      "Improved validation loss from: 0.010215387493371964  to: 0.010212121903896332\n",
      "Training iteration: 3479\n",
      "Improved validation loss from: 0.010212121903896332  to: 0.010198915004730224\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.010206709057092667\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.010229297727346421\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.01023661345243454\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.010212807357311249\n",
      "Training iteration: 3484\n",
      "Improved validation loss from: 0.010198915004730224  to: 0.010187699645757674\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.010193946212530136\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.010215524584054947\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.010214613378047943\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.010190186649560928\n",
      "Training iteration: 3489\n",
      "Improved validation loss from: 0.010187699645757674  to: 0.010174797475337982\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.010190881788730621\n",
      "Training iteration: 3491\n",
      "Validation loss (no improvement): 0.010215425491333007\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): 0.010207335650920867\n",
      "Training iteration: 3493\n",
      "Validation loss (no improvement): 0.010178256034851074\n",
      "Training iteration: 3494\n",
      "Improved validation loss from: 0.010174797475337982  to: 0.010159492492675781\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.010169029235839844\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.010196204483509063\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): 0.01020188331604004\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.010182658582925797\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): 0.010161195695400239\n",
      "Training iteration: 3500\n",
      "Improved validation loss from: 0.010159492492675781  to: 0.010153661668300628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3501\n",
      "Validation loss (no improvement): 0.01016377955675125\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): 0.010177693516016006\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.010183695703744888\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.010174806416034698\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.010156305879354477\n",
      "Training iteration: 3506\n",
      "Improved validation loss from: 0.010153661668300628  to: 0.010140280425548553\n",
      "Training iteration: 3507\n",
      "Improved validation loss from: 0.010140280425548553  to: 0.01014000028371811\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.010153146833181382\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.010166486352682113\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.010174617916345597\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.01017061024904251\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.010154278576374054\n",
      "Training iteration: 3513\n",
      "Improved validation loss from: 0.01014000028371811  to: 0.01013336405158043\n",
      "Training iteration: 3514\n",
      "Improved validation loss from: 0.01013336405158043  to: 0.010118669271469117\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.010118921846151352\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.010130462795495987\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.010141609609127045\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.010145752131938935\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.010153236240148545\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.010160151869058609\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.010153557360172271\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.010130844265222549\n",
      "Training iteration: 3523\n",
      "Improved validation loss from: 0.010118669271469117  to: 0.01010473519563675\n",
      "Training iteration: 3524\n",
      "Improved validation loss from: 0.01010473519563675  to: 0.01009758859872818\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.01011221632361412\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.010130681842565537\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.010137075185775756\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.010126008838415145\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.01010749340057373\n",
      "Training iteration: 3530\n",
      "Improved validation loss from: 0.01009758859872818  to: 0.010093843936920166\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.010096216201782226\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.01011059433221817\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.010123971849679947\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.010118816792964936\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.010095580667257308\n",
      "Training iteration: 3536\n",
      "Improved validation loss from: 0.010093843936920166  to: 0.010082592815160751\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.010098739713430404\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): 0.010114838182926179\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.010102073848247527\n",
      "Training iteration: 3540\n",
      "Improved validation loss from: 0.010082592815160751  to: 0.010074988752603532\n",
      "Training iteration: 3541\n",
      "Improved validation loss from: 0.010074988752603532  to: 0.010069829225540162\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.010097181797027588\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.010117616504430771\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.010096901655197143\n",
      "Training iteration: 3545\n",
      "Improved validation loss from: 0.010069829225540162  to: 0.010064119100570678\n",
      "Training iteration: 3546\n",
      "Improved validation loss from: 0.010064119100570678  to: 0.010053716599941254\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): 0.010074218362569809\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): 0.010098984092473983\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.010093142837285995\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.01006964072585106\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.01005522981286049\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.010057231038808822\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.010067136585712433\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.010072362422943116\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.01007320061326027\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.01006961464881897\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.010058462619781494\n",
      "Training iteration: 3558\n",
      "Improved validation loss from: 0.010053716599941254  to: 0.010052573680877686\n",
      "Training iteration: 3559\n",
      "Improved validation loss from: 0.010052573680877686  to: 0.01005043014883995\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): 0.010052002966403961\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.010057958215475083\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.010057449340820312\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.010054437816143036\n",
      "Training iteration: 3564\n",
      "Improved validation loss from: 0.01005043014883995  to: 0.010046931356191635\n",
      "Training iteration: 3565\n",
      "Improved validation loss from: 0.010046931356191635  to: 0.010034338384866715\n",
      "Training iteration: 3566\n",
      "Improved validation loss from: 0.010034338384866715  to: 0.01002887636423111\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): 0.010037809610366821\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.010056015104055405\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.010058712959289551\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.010036207735538483\n",
      "Training iteration: 3571\n",
      "Improved validation loss from: 0.01002887636423111  to: 0.010017921030521394\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.010030844062566758\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.010051276534795761\n",
      "Training iteration: 3574\n",
      "Validation loss (no improvement): 0.010041918605566025\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.010017921030521394  to: 0.01001359596848488\n",
      "Training iteration: 3576\n",
      "Improved validation loss from: 0.01001359596848488  to: 0.010005497932434082\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.010033418238162995\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.01005607694387436\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.010036154091358185\n",
      "Training iteration: 3580\n",
      "Improved validation loss from: 0.010005497932434082  to: 0.010001158714294434\n",
      "Training iteration: 3581\n",
      "Improved validation loss from: 0.010001158714294434  to: 0.009989618510007858\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.010010609775781632\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.010037555545568465\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.01003192663192749\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.010006288439035416\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.009991621971130371\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.009994439035654067\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.010005156695842742\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.010010236501693725\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.01001029759645462\n",
      "Training iteration: 3591\n",
      "Validation loss (no improvement): 0.010006406158208848\n",
      "Training iteration: 3592\n",
      "Validation loss (no improvement): 0.009996317327022552\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.009991376101970673\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.01000174880027771\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.010008220374584199\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.009992663562297822\n",
      "Training iteration: 3597\n",
      "Improved validation loss from: 0.009989618510007858  to: 0.009974660724401474\n",
      "Training iteration: 3598\n",
      "Validation loss (no improvement): 0.009977977722883224\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.010005225986242294\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.010016779601573943\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.009991078823804855\n",
      "Training iteration: 3602\n",
      "Improved validation loss from: 0.009974660724401474  to: 0.009953749179840089\n",
      "Training iteration: 3603\n",
      "Improved validation loss from: 0.009953749179840089  to: 0.009947817772626877\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.009977133572101593\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.010007323324680328\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.010010818392038346\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.00999012365937233\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.00997033268213272\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.009961120784282684\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.009955247491598129\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.009950390458106995\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.009950699657201767\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.0099592424929142\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.009973242133855819\n",
      "Training iteration: 3615\n",
      "Validation loss (no improvement): 0.009990999847650528\n",
      "Training iteration: 3616\n",
      "Validation loss (no improvement): 0.009995181113481522\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.009977513551712036\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.009951423108577728\n",
      "Training iteration: 3619\n",
      "Improved validation loss from: 0.009947817772626877  to: 0.009933088719844819\n",
      "Training iteration: 3620\n",
      "Validation loss (no improvement): 0.00993514508008957\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.009950152039527893\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.009960301220417023\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.009962205588817597\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.009957195818424225\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.009946034848690033\n",
      "Training iteration: 3626\n",
      "Improved validation loss from: 0.009933088719844819  to: 0.00993150845170021\n",
      "Training iteration: 3627\n",
      "Improved validation loss from: 0.00993150845170021  to: 0.00992758497595787\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.00993911772966385\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.009956615418195725\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.009955903142690658\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.00993208885192871\n",
      "Training iteration: 3632\n",
      "Improved validation loss from: 0.00992758497595787  to: 0.009916804730892181\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.009932076930999756\n",
      "Training iteration: 3634\n",
      "Validation loss (no improvement): 0.00995083898305893\n",
      "Training iteration: 3635\n",
      "Validation loss (no improvement): 0.0099379763007164\n",
      "Training iteration: 3636\n",
      "Improved validation loss from: 0.009916804730892181  to: 0.009911197423934936\n",
      "Training iteration: 3637\n",
      "Improved validation loss from: 0.009911197423934936  to: 0.009907834231853485\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.009937568008899689\n",
      "Training iteration: 3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.009958341717720032\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.009933267533779145\n",
      "Training iteration: 3641\n",
      "Improved validation loss from: 0.009907834231853485  to: 0.009901255369186401\n",
      "Training iteration: 3642\n",
      "Validation loss (no improvement): 0.0099080391228199\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.00993654653429985\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.009934626519680023\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.009903977811336517\n",
      "Training iteration: 3646\n",
      "Improved validation loss from: 0.009901255369186401  to: 0.00989045724272728\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.009916831552982331\n",
      "Training iteration: 3648\n",
      "Validation loss (no improvement): 0.009945511072874069\n",
      "Training iteration: 3649\n",
      "Validation loss (no improvement): 0.009926319867372514\n",
      "Training iteration: 3650\n",
      "Improved validation loss from: 0.00989045724272728  to: 0.009889422357082367\n",
      "Training iteration: 3651\n",
      "Improved validation loss from: 0.009889422357082367  to: 0.009888628870248795\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.009916197508573532\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.009922568500041962\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.009896162897348404\n",
      "Training iteration: 3655\n",
      "Improved validation loss from: 0.009888628870248795  to: 0.009877792745828628\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.009896419942378998\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.00992487296462059\n",
      "Training iteration: 3658\n",
      "Validation loss (no improvement): 0.00991482213139534\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.009882428497076035\n",
      "Training iteration: 3660\n",
      "Improved validation loss from: 0.009877792745828628  to: 0.009876126796007157\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.009896516799926758\n",
      "Training iteration: 3662\n",
      "Validation loss (no improvement): 0.009917112439870835\n",
      "Training iteration: 3663\n",
      "Validation loss (no improvement): 0.009903192520141602\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.009878069162368774\n",
      "Training iteration: 3665\n",
      "Improved validation loss from: 0.009876126796007157  to: 0.009869636595249176\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.009885996580123901\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.009910327196121217\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.009905319660902023\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.009872030466794968\n",
      "Training iteration: 3670\n",
      "Improved validation loss from: 0.009869636595249176  to: 0.009855750948190689\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.00987795665860176\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.009901755303144456\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.009894628822803498\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.009865420311689377\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.009855987131595611\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.009869740158319474\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.009885761886835098\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.009886975586414336\n",
      "Training iteration: 3679\n",
      "Validation loss (no improvement): 0.009871011227369308\n",
      "Training iteration: 3680\n",
      "Improved validation loss from: 0.009855750948190689  to: 0.009853225946426392\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.009854181855916976\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.009873367846012115\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.009894482046365737\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.009888424724340438\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.009858362376689911\n",
      "Training iteration: 3686\n",
      "Improved validation loss from: 0.009853225946426392  to: 0.009833605587482452\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.009838412702083587\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.009863529354333878\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.009878990054130555\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.009879173338413238\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.009869877994060517\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.009857745468616485\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.009842462092638015\n",
      "Training iteration: 3694\n",
      "Improved validation loss from: 0.009833605587482452  to: 0.009827519953250884\n",
      "Training iteration: 3695\n",
      "Improved validation loss from: 0.009827519953250884  to: 0.009823935478925705\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.009833545237779618\n",
      "Training iteration: 3697\n",
      "Validation loss (no improvement): 0.00984734296798706\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.009855665266513824\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.009865187108516693\n",
      "Training iteration: 3700\n",
      "Validation loss (no improvement): 0.009871069341897964\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.009862281382083893\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.009838856756687164\n",
      "Training iteration: 3703\n",
      "Improved validation loss from: 0.009823935478925705  to: 0.009815527498722077\n",
      "Training iteration: 3704\n",
      "Improved validation loss from: 0.009815527498722077  to: 0.009813114255666732\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.009829817712306977\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.00984557718038559\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.009848462045192718\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.009839420020580292\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.009825588017702103\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.009814654290676118\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.009816521406173706\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.009829311072826386\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.009842356294393539\n",
      "Training iteration: 3714\n",
      "Validation loss (no improvement): 0.009837482124567032\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.009815981239080429\n",
      "Training iteration: 3716\n",
      "Improved validation loss from: 0.009813114255666732  to: 0.009807132184505463\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.009826002269983291\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.009840227663516998\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.009820719063282014\n",
      "Training iteration: 3720\n",
      "Improved validation loss from: 0.009807132184505463  to: 0.009795299917459487\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.009799917042255402\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.009834842383861541\n",
      "Training iteration: 3723\n",
      "Validation loss (no improvement): 0.009849213063716888\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.009815908968448639\n",
      "Training iteration: 3725\n",
      "Improved validation loss from: 0.009795299917459487  to: 0.009786667674779892\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.009803904592990876\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.00983581766486168\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.009823537617921829\n",
      "Training iteration: 3729\n",
      "Improved validation loss from: 0.009786667674779892  to: 0.009786514937877655\n",
      "Training iteration: 3730\n",
      "Improved validation loss from: 0.009786514937877655  to: 0.009779610484838486\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.009817011654376984\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.00984400361776352\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.009811989963054657\n",
      "Training iteration: 3734\n",
      "Improved validation loss from: 0.009779610484838486  to: 0.009773465991020202\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.009783761203289032\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.009819872677326202\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.009830696135759353\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.009796370565891267\n",
      "Training iteration: 3739\n",
      "Improved validation loss from: 0.009773465991020202  to: 0.009771598875522614\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.009780688583850861\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.009808214008808136\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.009819342195987702\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.009796545654535294\n",
      "Training iteration: 3744\n",
      "Improved validation loss from: 0.009771598875522614  to: 0.00976852998137474\n",
      "Training iteration: 3745\n",
      "Validation loss (no improvement): 0.009770729392766953\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.009800820052623749\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.009827262163162232\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.009812279045581818\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.009774570167064667\n",
      "Training iteration: 3750\n",
      "Improved validation loss from: 0.00976852998137474  to: 0.009754224121570588\n",
      "Training iteration: 3751\n",
      "Validation loss (no improvement): 0.009768912196159362\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.009796367585659027\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.009802306443452835\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.009796424210071564\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.009792742133140565\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.0097880020737648\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.009770803153514862\n",
      "Training iteration: 3758\n",
      "Improved validation loss from: 0.009754224121570588  to: 0.009749168157577514\n",
      "Training iteration: 3759\n",
      "Improved validation loss from: 0.009749168157577514  to: 0.009744111448526382\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.009758996218442917\n",
      "Training iteration: 3761\n",
      "Validation loss (no improvement): 0.00977712869644165\n",
      "Training iteration: 3762\n",
      "Validation loss (no improvement): 0.009782636165618896\n",
      "Training iteration: 3763\n",
      "Validation loss (no improvement): 0.009786908328533173\n",
      "Training iteration: 3764\n",
      "Validation loss (no improvement): 0.009793534129858016\n",
      "Training iteration: 3765\n",
      "Validation loss (no improvement): 0.009789803624153137\n",
      "Training iteration: 3766\n",
      "Validation loss (no improvement): 0.00976775512099266\n",
      "Training iteration: 3767\n",
      "Improved validation loss from: 0.009744111448526382  to: 0.009741699695587159\n",
      "Training iteration: 3768\n",
      "Improved validation loss from: 0.009741699695587159  to: 0.009737825393676758\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.009756533056497574\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.009774751961231232\n",
      "Training iteration: 3771\n",
      "Validation loss (no improvement): 0.009776593744754791\n",
      "Training iteration: 3772\n",
      "Validation loss (no improvement): 0.009781838208436967\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.009790418297052383\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.009784193336963653\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.009758049249649048\n",
      "Training iteration: 3776\n",
      "Improved validation loss from: 0.009737825393676758  to: 0.009731059521436691\n",
      "Training iteration: 3777\n",
      "Improved validation loss from: 0.009731059521436691  to: 0.009729965031147004\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.009751129150390624\n",
      "Training iteration: 3779\n",
      "Validation loss (no improvement): 0.009767301380634308\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.00976553037762642\n",
      "Training iteration: 3781\n",
      "Validation loss (no improvement): 0.009754055738449096\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.00974402204155922\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.009737672656774521\n",
      "Training iteration: 3784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.00973852425813675\n",
      "Training iteration: 3785\n",
      "Validation loss (no improvement): 0.009746357798576355\n",
      "Training iteration: 3786\n",
      "Validation loss (no improvement): 0.009757070988416671\n",
      "Training iteration: 3787\n",
      "Validation loss (no improvement): 0.009771446883678436\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.00977252572774887\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.009751556813716889\n",
      "Training iteration: 3790\n",
      "Improved validation loss from: 0.009729965031147004  to: 0.009724320471286773\n",
      "Training iteration: 3791\n",
      "Improved validation loss from: 0.009724320471286773  to: 0.009722315520048142\n",
      "Training iteration: 3792\n",
      "Validation loss (no improvement): 0.009745508432388306\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.009767290204763412\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.009753461182117461\n",
      "Training iteration: 3795\n",
      "Improved validation loss from: 0.009722315520048142  to: 0.009719448536634446\n",
      "Training iteration: 3796\n",
      "Improved validation loss from: 0.009719448536634446  to: 0.009713133424520492\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.009745700657367707\n",
      "Training iteration: 3798\n",
      "Validation loss (no improvement): 0.009763599187135697\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.009744417667388917\n",
      "Training iteration: 3800\n",
      "Validation loss (no improvement): 0.00971551313996315\n",
      "Training iteration: 3801\n",
      "Validation loss (no improvement): 0.009719491004943848\n",
      "Training iteration: 3802\n",
      "Validation loss (no improvement): 0.009739787876605987\n",
      "Training iteration: 3803\n",
      "Validation loss (no improvement): 0.009745105355978011\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.009734852612018586\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.009722781926393509\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.009715928137302399\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.009719685465097428\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.009730227291584015\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.009742479771375656\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.009739451110363007\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.009717734158039093\n",
      "Training iteration: 3812\n",
      "Improved validation loss from: 0.009713133424520492  to: 0.009696877002716065\n",
      "Training iteration: 3813\n",
      "Validation loss (no improvement): 0.009699072688817978\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.009719829261302947\n",
      "Training iteration: 3815\n",
      "Validation loss (no improvement): 0.009735528379678726\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.009739278256893158\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.009732229262590408\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.009718835353851318\n",
      "Training iteration: 3819\n",
      "Validation loss (no improvement): 0.009703071415424347\n",
      "Training iteration: 3820\n",
      "Validation loss (no improvement): 0.009702052175998687\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.009716801345348358\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.009733837842941285\n",
      "Training iteration: 3823\n",
      "Validation loss (no improvement): 0.009725935757160187\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.009697513282299041\n",
      "Training iteration: 3825\n",
      "Improved validation loss from: 0.009696877002716065  to: 0.00968935266137123\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.009715348482131958\n",
      "Training iteration: 3827\n",
      "Validation loss (no improvement): 0.009731053560972213\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.009715938568115234\n",
      "Training iteration: 3829\n",
      "Validation loss (no improvement): 0.009690452367067337\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.009692232310771941\n",
      "Training iteration: 3831\n",
      "Validation loss (no improvement): 0.009708978980779649\n",
      "Training iteration: 3832\n",
      "Validation loss (no improvement): 0.009714777767658233\n",
      "Training iteration: 3833\n",
      "Validation loss (no improvement): 0.009707999229431153\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.009697450697422028\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.0096893772482872\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.009692107886075973\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.00970405787229538\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.009718822687864304\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.009715674072504043\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.009691739082336425\n",
      "Training iteration: 3841\n",
      "Improved validation loss from: 0.00968935266137123  to: 0.00967010036110878\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.00967426672577858\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.009696820378303527\n",
      "Training iteration: 3844\n",
      "Validation loss (no improvement): 0.00971105545759201\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.009712501615285873\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.009705469012260437\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.00969398468732834\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.009678763151168824\n",
      "Training iteration: 3849\n",
      "Validation loss (no improvement): 0.009676353633403778\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.009689871966838837\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.009707064926624298\n",
      "Training iteration: 3852\n",
      "Validation loss (no improvement): 0.009699992835521698\n",
      "Training iteration: 3853\n",
      "Validation loss (no improvement): 0.009672266244888306\n",
      "Training iteration: 3854\n",
      "Improved validation loss from: 0.00967010036110878  to: 0.009664420783519746\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.009690076112747192\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.009705135226249694\n",
      "Training iteration: 3857\n",
      "Validation loss (no improvement): 0.009689873456954956\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.009665580093860626\n",
      "Training iteration: 3859\n",
      "Validation loss (no improvement): 0.009668544679880143\n",
      "Training iteration: 3860\n",
      "Validation loss (no improvement): 0.009701572358608246\n",
      "Training iteration: 3861\n",
      "Validation loss (no improvement): 0.00970984697341919\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.009671969711780548\n",
      "Training iteration: 3863\n",
      "Improved validation loss from: 0.009664420783519746  to: 0.009647095203399658\n",
      "Training iteration: 3864\n",
      "Validation loss (no improvement): 0.00967046469449997\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.009699450433254242\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.009689961373806\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.009669868648052216\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.009668251127004623\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.009673448652029038\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.009674018621444702\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.00967283546924591\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.00966816246509552\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.009669831395149231\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.009668782353401184\n",
      "Training iteration: 3875\n",
      "Validation loss (no improvement): 0.009666845947504044\n",
      "Training iteration: 3876\n",
      "Validation loss (no improvement): 0.009675167500972748\n",
      "Training iteration: 3877\n",
      "Validation loss (no improvement): 0.009678103029727936\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.009661176055669785\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.009648794680833817\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.009660805761814117\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.009690360724925995\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.009691216051578522\n",
      "Training iteration: 3883\n",
      "Validation loss (no improvement): 0.009655178338289262\n",
      "Training iteration: 3884\n",
      "Improved validation loss from: 0.009647095203399658  to: 0.009626378864049911\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.009636929631233216\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.009669406712055207\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.009680765122175217\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.009671978652477264\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.009664491564035416\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.009661475569009781\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.009649412333965301\n",
      "Training iteration: 3892\n",
      "Validation loss (no improvement): 0.009641294926404953\n",
      "Training iteration: 3893\n",
      "Validation loss (no improvement): 0.009649650007486344\n",
      "Training iteration: 3894\n",
      "Validation loss (no improvement): 0.009669186174869537\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.009667159616947174\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.009639158099889755\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.009627938270568848\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.009652032703161239\n",
      "Training iteration: 3899\n",
      "Validation loss (no improvement): 0.009688691794872284\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.009680914878845214\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.009635890275239945\n",
      "Training iteration: 3902\n",
      "Improved validation loss from: 0.009626378864049911  to: 0.009610478579998017\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.009630506485700607\n",
      "Training iteration: 3904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.009665156155824662\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.00966716855764389\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.009653538465499878\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.00965137556195259\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.009653930366039277\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.00963839441537857\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.009623972326517105\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.009632066637277604\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.009656599164009095\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.009656136482954025\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.00962512642145157\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.009612318128347397\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.009638331830501556\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.009676878154277802\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.009667082130908966\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.009620648622512818\n",
      "Training iteration: 3920\n",
      "Improved validation loss from: 0.009610478579998017  to: 0.009597247838974\n",
      "Training iteration: 3921\n",
      "Validation loss (no improvement): 0.009619678556919097\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.009653080254793167\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.0096517875790596\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.009638601541519165\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.009639862924814224\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.009643247723579407\n",
      "Training iteration: 3927\n",
      "Validation loss (no improvement): 0.009624600410461426\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.009608610719442367\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.009618984162807464\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.009646382927894593\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.009644464403390885\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.00961054340004921\n",
      "Training iteration: 3933\n",
      "Validation loss (no improvement): 0.00959828644990921\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.00962762013077736\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.009667514264583588\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.009654366970062256\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.009606113284826278\n",
      "Training iteration: 3938\n",
      "Improved validation loss from: 0.009597247838974  to: 0.009584639966487885\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.009609848260879517\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.009642384946346283\n",
      "Training iteration: 3941\n",
      "Validation loss (no improvement): 0.00963762030005455\n",
      "Training iteration: 3942\n",
      "Validation loss (no improvement): 0.00962441712617874\n",
      "Training iteration: 3943\n",
      "Validation loss (no improvement): 0.009628597646951675\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.009632693976163864\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.009611610323190689\n",
      "Training iteration: 3946\n",
      "Validation loss (no improvement): 0.009594404697418213\n",
      "Training iteration: 3947\n",
      "Validation loss (no improvement): 0.009606574475765229\n",
      "Training iteration: 3948\n",
      "Validation loss (no improvement): 0.00963549017906189\n",
      "Training iteration: 3949\n",
      "Validation loss (no improvement): 0.009632232040166855\n",
      "Training iteration: 3950\n",
      "Validation loss (no improvement): 0.009611008316278457\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.009601910412311555\n",
      "Training iteration: 3952\n",
      "Validation loss (no improvement): 0.009604856371879578\n",
      "Training iteration: 3953\n",
      "Validation loss (no improvement): 0.009610502421855927\n",
      "Training iteration: 3954\n",
      "Validation loss (no improvement): 0.009612790495157241\n",
      "Training iteration: 3955\n",
      "Validation loss (no improvement): 0.009618540853261947\n",
      "Training iteration: 3956\n",
      "Validation loss (no improvement): 0.00961914137005806\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.009605122357606887\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.009597840160131455\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.009612766653299331\n",
      "Training iteration: 3960\n",
      "Validation loss (no improvement): 0.009620174020528793\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.009612083435058594\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.009596775472164153\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.009598435461521148\n",
      "Training iteration: 3964\n",
      "Validation loss (no improvement): 0.009605507552623748\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.009606319665908813\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.009607193619012832\n",
      "Training iteration: 3967\n",
      "Validation loss (no improvement): 0.009604451805353164\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.009608419239521026\n",
      "Training iteration: 3969\n",
      "Validation loss (no improvement): 0.009605538100004196\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.009589062631130218\n",
      "Training iteration: 3971\n",
      "Improved validation loss from: 0.009584639966487885  to: 0.009581955522298813\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.009596841037273407\n",
      "Training iteration: 3973\n",
      "Validation loss (no improvement): 0.009618723392486572\n",
      "Training iteration: 3974\n",
      "Validation loss (no improvement): 0.00961248055100441\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.00958397388458252\n",
      "Training iteration: 3976\n",
      "Improved validation loss from: 0.009581955522298813  to: 0.009578759968280792\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.009608320891857147\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.009619607031345368\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.009598965942859649\n",
      "Training iteration: 3980\n",
      "Improved validation loss from: 0.009578759968280792  to: 0.009577175974845887\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.009588091820478439\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.009604273736476899\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.009599149227142334\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.009589769691228867\n",
      "Training iteration: 3985\n",
      "Validation loss (no improvement): 0.009588569402694702\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.009586475789546967\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.009583946317434311\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.009589192271232606\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.009606174379587173\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.009608830511569976\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.009584162384271622\n",
      "Training iteration: 3992\n",
      "Improved validation loss from: 0.009577175974845887  to: 0.00957198366522789\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.009596078097820282\n",
      "Training iteration: 3994\n",
      "Validation loss (no improvement): 0.009609712660312653\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.009580007195472718\n",
      "Training iteration: 3996\n",
      "Improved validation loss from: 0.00957198366522789  to: 0.009555478394031525\n",
      "Training iteration: 3997\n",
      "Validation loss (no improvement): 0.00957394391298294\n",
      "Training iteration: 3998\n",
      "Validation loss (no improvement): 0.009614560753107071\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.009609777480363846\n"
     ]
    }
   ],
   "source": [
    "baseline_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 30.025592041015624\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 30.025592041015624  to: 29.183856201171874\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 29.183856201171874  to: 29.160659790039062\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 29.160659790039062  to: 28.470611572265625\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 28.470611572265625  to: 27.797076416015624\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 27.797076416015624  to: 27.548233032226562\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 27.548233032226562  to: 27.003457641601564\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 27.003457641601564  to: 26.580133056640626\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 26.580133056640626  to: 26.545138549804687\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 26.545138549804687  to: 26.0802734375\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 26.0802734375  to: 25.44574737548828\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 25.44574737548828  to: 24.77008056640625\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 24.77008056640625  to: 24.25171356201172\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 24.25171356201172  to: 24.11907196044922\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 24.11907196044922  to: 23.60650177001953\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 23.60650177001953  to: 22.95177307128906\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 22.95177307128906  to: 22.501889038085938\n",
      "Training iteration: 17\n",
      "Validation loss (no improvement): 22.74109802246094\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 22.501889038085938  to: 21.680703735351564\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 21.680703735351564  to: 21.087870788574218\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 21.087870788574218  to: 20.718685913085938\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 20.718685913085938  to: 20.616439819335938\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 20.616439819335938  to: 19.75130615234375\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 19.75130615234375  to: 19.67328796386719\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 19.67328796386719  to: 18.944001770019533\n",
      "Training iteration: 25\n",
      "Validation loss (no improvement): 19.06693878173828\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 18.944001770019533  to: 18.09619140625\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 18.09619140625  to: 17.51805725097656\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 17.51805725097656  to: 17.151858520507812\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 17.151858520507812  to: 17.092391967773438\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 17.092391967773438  to: 16.377656555175783\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 16.377656555175783  to: 16.01921691894531\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 16.01921691894531  to: 15.666482543945312\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 15.666482543945312  to: 15.148210144042968\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 15.148210144042968  to: 14.930859375\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 14.930859375  to: 14.173159790039062\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 14.173159790039062  to: 14.131654357910156\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 14.131654357910156  to: 13.923786926269532\n",
      "Training iteration: 38\n",
      "Validation loss (no improvement): 14.034429931640625\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 13.923786926269532  to: 13.619429016113282\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 13.619429016113282  to: 12.946873474121094\n",
      "Training iteration: 41\n",
      "Validation loss (no improvement): 13.716268920898438\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 12.946873474121094  to: 12.78672103881836\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 12.78672103881836  to: 12.58798828125\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 12.58798828125  to: 12.54446792602539\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 12.54446792602539  to: 11.72125244140625\n",
      "Training iteration: 46\n",
      "Validation loss (no improvement): 11.84342269897461\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 11.72125244140625  to: 11.348240661621094\n",
      "Training iteration: 48\n",
      "Validation loss (no improvement): 11.68176040649414\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 11.348240661621094  to: 11.299725341796876\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 11.299725341796876  to: 11.025633239746094\n",
      "Training iteration: 51\n",
      "Validation loss (no improvement): 11.41219711303711\n",
      "Training iteration: 52\n",
      "Validation loss (no improvement): 11.413448333740234\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 11.025633239746094  to: 9.361540222167969\n",
      "Training iteration: 54\n",
      "Validation loss (no improvement): 10.74379653930664\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 9.361540222167969  to: 9.309355163574219\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 9.309355163574219  to: 8.565079498291016\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 8.565079498291016  to: 7.988820648193359\n",
      "Training iteration: 58\n",
      "Validation loss (no improvement): 8.127323150634766\n",
      "Training iteration: 59\n",
      "Validation loss (no improvement): 9.043203735351563\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 7.988820648193359  to: 7.565339660644531\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 7.565339660644531  to: 7.1928764343261715\n",
      "Training iteration: 62\n",
      "Validation loss (no improvement): 8.61797866821289\n",
      "Training iteration: 63\n",
      "Validation loss (no improvement): 7.595855712890625\n",
      "Training iteration: 64\n",
      "Validation loss (no improvement): 7.700840759277344\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 7.1928764343261715  to: 7.165061187744141\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 7.165061187744141  to: 7.145169830322265\n",
      "Training iteration: 67\n",
      "Validation loss (no improvement): 7.321443176269531\n",
      "Training iteration: 68\n",
      "Validation loss (no improvement): 7.560478210449219\n",
      "Training iteration: 69\n",
      "Validation loss (no improvement): 7.867649841308594\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 7.145169830322265  to: 6.2048389434814455\n",
      "Training iteration: 71\n",
      "Validation loss (no improvement): 7.665618896484375\n",
      "Training iteration: 72\n",
      "Validation loss (no improvement): 7.148085021972657\n",
      "Training iteration: 73\n",
      "Validation loss (no improvement): 8.659104919433593\n",
      "Training iteration: 74\n",
      "Validation loss (no improvement): 6.833900451660156\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 6.2048389434814455  to: 6.2017261505126955\n",
      "Training iteration: 76\n",
      "Validation loss (no improvement): 6.5050193786621096\n",
      "Training iteration: 77\n",
      "Validation loss (no improvement): 6.8943939208984375\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 6.2017261505126955  to: 6.185161590576172\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 6.185161590576172  to: 5.860821533203125\n",
      "Training iteration: 80\n",
      "Validation loss (no improvement): 7.396470642089843\n",
      "Training iteration: 81\n",
      "Validation loss (no improvement): 8.007637786865235\n",
      "Training iteration: 82\n",
      "Validation loss (no improvement): 6.1573333740234375\n",
      "Training iteration: 83\n",
      "Validation loss (no improvement): 6.404548645019531\n",
      "Training iteration: 84\n",
      "Validation loss (no improvement): 7.221940612792968\n",
      "Training iteration: 85\n",
      "Validation loss (no improvement): 6.441263580322266\n",
      "Training iteration: 86\n",
      "Validation loss (no improvement): 6.42889175415039\n",
      "Training iteration: 87\n",
      "Validation loss (no improvement): 6.810527038574219\n",
      "Training iteration: 88\n",
      "Validation loss (no improvement): 7.479881286621094\n",
      "Training iteration: 89\n",
      "Validation loss (no improvement): 6.280927658081055\n",
      "Training iteration: 90\n",
      "Validation loss (no improvement): 5.899094772338867\n",
      "Training iteration: 91\n",
      "Validation loss (no improvement): 6.30694694519043\n",
      "Training iteration: 92\n",
      "Validation loss (no improvement): 7.337303161621094\n",
      "Training iteration: 93\n",
      "Validation loss (no improvement): 7.071154022216797\n",
      "Training iteration: 94\n",
      "Validation loss (no improvement): 7.643257141113281\n",
      "Training iteration: 95\n",
      "Validation loss (no improvement): 6.575968933105469\n",
      "Training iteration: 96\n",
      "Validation loss (no improvement): 6.792059326171875\n",
      "Training iteration: 97\n",
      "Validation loss (no improvement): 6.187461090087891\n",
      "Training iteration: 98\n",
      "Validation loss (no improvement): 7.234292602539062\n",
      "Training iteration: 99\n",
      "Validation loss (no improvement): 7.137287139892578\n",
      "Training iteration: 100\n",
      "Validation loss (no improvement): 7.223075103759766\n",
      "Training iteration: 101\n",
      "Validation loss (no improvement): 6.504429626464844\n",
      "Training iteration: 102\n",
      "Validation loss (no improvement): 7.7582237243652346\n",
      "Training iteration: 103\n",
      "Validation loss (no improvement): 6.804426574707032\n",
      "Training iteration: 104\n",
      "Validation loss (no improvement): 6.518707275390625\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 5.860821533203125  to: 5.795052337646484\n",
      "Training iteration: 106\n",
      "Validation loss (no improvement): 6.130222320556641\n",
      "Training iteration: 107\n",
      "Validation loss (no improvement): 7.245856475830078\n",
      "Training iteration: 108\n",
      "Validation loss (no improvement): 6.060814285278321\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 5.795052337646484  to: 5.446635818481445\n",
      "Training iteration: 110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 5.9112895965576175\n",
      "Training iteration: 111\n",
      "Validation loss (no improvement): 5.898348999023438\n",
      "Training iteration: 112\n",
      "Validation loss (no improvement): 6.356414031982422\n",
      "Training iteration: 113\n",
      "Validation loss (no improvement): 5.899881744384766\n",
      "Training iteration: 114\n",
      "Validation loss (no improvement): 5.932521820068359\n",
      "Training iteration: 115\n",
      "Validation loss (no improvement): 6.3755615234375\n",
      "Training iteration: 116\n",
      "Validation loss (no improvement): 5.994960784912109\n",
      "Training iteration: 117\n",
      "Validation loss (no improvement): 5.71296501159668\n",
      "Training iteration: 118\n",
      "Validation loss (no improvement): 6.0081535339355465\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 5.446635818481445  to: 5.350532150268554\n",
      "Training iteration: 120\n",
      "Validation loss (no improvement): 6.119072341918946\n",
      "Training iteration: 121\n",
      "Validation loss (no improvement): 5.874407958984375\n",
      "Training iteration: 122\n",
      "Validation loss (no improvement): 5.7453468322753904\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 5.350532150268554  to: 5.056784057617188\n",
      "Training iteration: 124\n",
      "Validation loss (no improvement): 5.549506759643554\n",
      "Training iteration: 125\n",
      "Validation loss (no improvement): 5.306284713745117\n",
      "Training iteration: 126\n",
      "Validation loss (no improvement): 5.825901031494141\n",
      "Training iteration: 127\n",
      "Validation loss (no improvement): 5.682260894775391\n",
      "Training iteration: 128\n",
      "Validation loss (no improvement): 5.58763313293457\n",
      "Training iteration: 129\n",
      "Validation loss (no improvement): 5.92804069519043\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 5.056784057617188  to: 4.882427215576172\n",
      "Training iteration: 131\n",
      "Validation loss (no improvement): 6.14105110168457\n",
      "Training iteration: 132\n",
      "Validation loss (no improvement): 5.265999603271484\n",
      "Training iteration: 133\n",
      "Validation loss (no improvement): 5.548282623291016\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 4.882427215576172  to: 4.612490844726563\n",
      "Training iteration: 135\n",
      "Validation loss (no improvement): 5.07231330871582\n",
      "Training iteration: 136\n",
      "Validation loss (no improvement): 5.612709808349609\n",
      "Training iteration: 137\n",
      "Validation loss (no improvement): 4.7414501190185545\n",
      "Training iteration: 138\n",
      "Validation loss (no improvement): 4.61693115234375\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 4.612490844726563  to: 4.461282730102539\n",
      "Training iteration: 140\n",
      "Validation loss (no improvement): 4.738118743896484\n",
      "Training iteration: 141\n",
      "Validation loss (no improvement): 4.858882904052734\n",
      "Training iteration: 142\n",
      "Validation loss (no improvement): 4.988401412963867\n",
      "Training iteration: 143\n",
      "Validation loss (no improvement): 5.070589065551758\n",
      "Training iteration: 144\n",
      "Validation loss (no improvement): 4.472045135498047\n",
      "Training iteration: 145\n",
      "Validation loss (no improvement): 5.8490447998046875\n",
      "Training iteration: 146\n",
      "Validation loss (no improvement): 4.951901245117187\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 4.461282730102539  to: 4.126355743408203\n",
      "Training iteration: 148\n",
      "Validation loss (no improvement): 4.293800735473633\n",
      "Training iteration: 149\n",
      "Validation loss (no improvement): 4.417198944091797\n",
      "Training iteration: 150\n",
      "Validation loss (no improvement): 4.175662231445313\n",
      "Training iteration: 151\n",
      "Validation loss (no improvement): 4.840861129760742\n",
      "Training iteration: 152\n",
      "Validation loss (no improvement): 4.3880561828613285\n",
      "Training iteration: 153\n",
      "Validation loss (no improvement): 4.2937774658203125\n",
      "Training iteration: 154\n",
      "Validation loss (no improvement): 4.132517623901367\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 4.126355743408203  to: 3.7609722137451174\n",
      "Training iteration: 156\n",
      "Validation loss (no improvement): 5.38289794921875\n",
      "Training iteration: 157\n",
      "Validation loss (no improvement): 3.7620235443115235\n",
      "Training iteration: 158\n",
      "Validation loss (no improvement): 5.294588470458985\n",
      "Training iteration: 159\n",
      "Validation loss (no improvement): 5.000769805908203\n",
      "Training iteration: 160\n",
      "Validation loss (no improvement): 4.010612487792969\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 3.7609722137451174  to: 3.556876373291016\n",
      "Training iteration: 162\n",
      "Validation loss (no improvement): 3.5734222412109373\n",
      "Training iteration: 163\n",
      "Validation loss (no improvement): 4.040468215942383\n",
      "Training iteration: 164\n",
      "Validation loss (no improvement): 3.6514251708984373\n",
      "Training iteration: 165\n",
      "Validation loss (no improvement): 3.9304168701171873\n",
      "Training iteration: 166\n",
      "Validation loss (no improvement): 3.6337860107421873\n",
      "Training iteration: 167\n",
      "Validation loss (no improvement): 4.591806030273437\n",
      "Training iteration: 168\n",
      "Validation loss (no improvement): 3.664136505126953\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 3.556876373291016  to: 3.480463409423828\n",
      "Training iteration: 170\n",
      "Validation loss (no improvement): 3.545396423339844\n",
      "Training iteration: 171\n",
      "Validation loss (no improvement): 3.519074249267578\n",
      "Training iteration: 172\n",
      "Validation loss (no improvement): 4.451258850097656\n",
      "Training iteration: 173\n",
      "Validation loss (no improvement): 4.1092887878417965\n",
      "Training iteration: 174\n",
      "Validation loss (no improvement): 3.927080535888672\n",
      "Training iteration: 175\n",
      "Validation loss (no improvement): 3.700508880615234\n",
      "Training iteration: 176\n",
      "Validation loss (no improvement): 3.8762298583984376\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 3.480463409423828  to: 3.3434829711914062\n",
      "Training iteration: 178\n",
      "Validation loss (no improvement): 3.723227310180664\n",
      "Training iteration: 179\n",
      "Validation loss (no improvement): 4.0451408386230465\n",
      "Training iteration: 180\n",
      "Validation loss (no improvement): 3.617484283447266\n",
      "Training iteration: 181\n",
      "Validation loss (no improvement): 3.9364059448242186\n",
      "Training iteration: 182\n",
      "Validation loss (no improvement): 4.239949798583984\n",
      "Training iteration: 183\n",
      "Validation loss (no improvement): 4.020949935913086\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 3.3434829711914062  to: 2.915534019470215\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 2.915534019470215  to: 2.8748615264892576\n",
      "Training iteration: 186\n",
      "Validation loss (no improvement): 3.1784313201904295\n",
      "Training iteration: 187\n",
      "Validation loss (no improvement): 2.9899993896484376\n",
      "Training iteration: 188\n",
      "Validation loss (no improvement): 3.274973678588867\n",
      "Training iteration: 189\n",
      "Validation loss (no improvement): 3.769195556640625\n",
      "Training iteration: 190\n",
      "Validation loss (no improvement): 3.3666961669921873\n",
      "Training iteration: 191\n",
      "Validation loss (no improvement): 4.439445114135742\n",
      "Training iteration: 192\n",
      "Validation loss (no improvement): 3.551399993896484\n",
      "Training iteration: 193\n",
      "Validation loss (no improvement): 4.70947036743164\n",
      "Training iteration: 194\n",
      "Validation loss (no improvement): 3.235332489013672\n",
      "Training iteration: 195\n",
      "Validation loss (no improvement): 3.496240234375\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 2.8748615264892576  to: 2.676523971557617\n",
      "Training iteration: 197\n",
      "Validation loss (no improvement): 3.5511405944824217\n",
      "Training iteration: 198\n",
      "Validation loss (no improvement): 3.0053619384765624\n",
      "Training iteration: 199\n",
      "Validation loss (no improvement): 3.2325206756591798\n",
      "Training iteration: 200\n",
      "Validation loss (no improvement): 4.01628532409668\n",
      "Training iteration: 201\n",
      "Validation loss (no improvement): 3.2973751068115233\n",
      "Training iteration: 202\n",
      "Validation loss (no improvement): 3.628887176513672\n",
      "Training iteration: 203\n",
      "Validation loss (no improvement): 4.152294158935547\n",
      "Training iteration: 204\n",
      "Validation loss (no improvement): 2.8098785400390627\n",
      "Training iteration: 205\n",
      "Validation loss (no improvement): 3.3532413482666015\n",
      "Training iteration: 206\n",
      "Validation loss (no improvement): 4.253578186035156\n",
      "Training iteration: 207\n",
      "Validation loss (no improvement): 2.908201217651367\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 2.676523971557617  to: 2.4709243774414062\n",
      "Training iteration: 209\n",
      "Validation loss (no improvement): 2.732124900817871\n",
      "Training iteration: 210\n",
      "Validation loss (no improvement): 3.5064525604248047\n",
      "Training iteration: 211\n",
      "Validation loss (no improvement): 3.524715805053711\n",
      "Training iteration: 212\n",
      "Validation loss (no improvement): 3.5607154846191404\n",
      "Training iteration: 213\n",
      "Validation loss (no improvement): 3.2714607238769533\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 2.4709243774414062  to: 2.3353174209594725\n",
      "Training iteration: 215\n",
      "Validation loss (no improvement): 2.6089990615844725\n",
      "Training iteration: 216\n",
      "Validation loss (no improvement): 2.9126602172851563\n",
      "Training iteration: 217\n",
      "Validation loss (no improvement): 3.5251743316650392\n",
      "Training iteration: 218\n",
      "Validation loss (no improvement): 3.6486831665039063\n",
      "Training iteration: 219\n",
      "Validation loss (no improvement): 2.807313919067383\n",
      "Training iteration: 220\n",
      "Validation loss (no improvement): 2.554757308959961\n",
      "Training iteration: 221\n",
      "Validation loss (no improvement): 3.9552101135253905\n",
      "Training iteration: 222\n",
      "Validation loss (no improvement): 2.3769109725952147\n",
      "Training iteration: 223\n",
      "Validation loss (no improvement): 4.231438064575196\n",
      "Training iteration: 224\n",
      "Validation loss (no improvement): 3.0970367431640624\n",
      "Training iteration: 225\n",
      "Validation loss (no improvement): 3.321192169189453\n",
      "Training iteration: 226\n",
      "Validation loss (no improvement): 2.4709264755249025\n",
      "Training iteration: 227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 2.537128448486328\n",
      "Training iteration: 228\n",
      "Validation loss (no improvement): 2.4492359161376953\n",
      "Training iteration: 229\n",
      "Validation loss (no improvement): 2.81243782043457\n",
      "Training iteration: 230\n",
      "Validation loss (no improvement): 2.8147113800048826\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 2.3353174209594725  to: 2.10430965423584\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 2.10430965423584  to: 2.026350975036621\n",
      "Training iteration: 233\n",
      "Validation loss (no improvement): 2.7533315658569335\n",
      "Training iteration: 234\n",
      "Validation loss (no improvement): 2.460453224182129\n",
      "Training iteration: 235\n",
      "Validation loss (no improvement): 3.230219268798828\n",
      "Training iteration: 236\n",
      "Validation loss (no improvement): 2.5108333587646485\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 2.026350975036621  to: 1.7165538787841796\n",
      "Training iteration: 238\n",
      "Validation loss (no improvement): 2.8722997665405274\n",
      "Training iteration: 239\n",
      "Validation loss (no improvement): 3.11020565032959\n",
      "Training iteration: 240\n",
      "Validation loss (no improvement): 3.1701471328735353\n",
      "Training iteration: 241\n",
      "Validation loss (no improvement): 2.2392093658447267\n",
      "Training iteration: 242\n",
      "Validation loss (no improvement): 2.4091876983642577\n",
      "Training iteration: 243\n",
      "Validation loss (no improvement): 2.1843145370483397\n",
      "Training iteration: 244\n",
      "Validation loss (no improvement): 2.209339904785156\n",
      "Training iteration: 245\n",
      "Validation loss (no improvement): 2.2362464904785155\n",
      "Training iteration: 246\n",
      "Validation loss (no improvement): 2.711818504333496\n",
      "Training iteration: 247\n",
      "Validation loss (no improvement): 2.7284265518188477\n",
      "Training iteration: 248\n",
      "Validation loss (no improvement): 1.9007074356079101\n",
      "Training iteration: 249\n",
      "Validation loss (no improvement): 1.9612045288085938\n",
      "Training iteration: 250\n",
      "Validation loss (no improvement): 2.4949228286743166\n",
      "Training iteration: 251\n",
      "Validation loss (no improvement): 1.876290512084961\n",
      "Training iteration: 252\n",
      "Validation loss (no improvement): 3.559452438354492\n",
      "Training iteration: 253\n",
      "Validation loss (no improvement): 3.0736242294311524\n",
      "Training iteration: 254\n",
      "Validation loss (no improvement): 2.460092544555664\n",
      "Training iteration: 255\n",
      "Validation loss (no improvement): 2.6065078735351563\n",
      "Training iteration: 256\n",
      "Validation loss (no improvement): 2.0108226776123046\n",
      "Training iteration: 257\n",
      "Validation loss (no improvement): 2.1178258895874023\n",
      "Training iteration: 258\n",
      "Validation loss (no improvement): 1.8394977569580078\n",
      "Training iteration: 259\n",
      "Validation loss (no improvement): 2.0230464935302734\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 1.7165538787841796  to: 1.5600080490112305\n",
      "Training iteration: 261\n",
      "Validation loss (no improvement): 1.9167251586914062\n",
      "Training iteration: 262\n",
      "Validation loss (no improvement): 1.8365848541259766\n",
      "Training iteration: 263\n",
      "Validation loss (no improvement): 1.8944736480712892\n",
      "Training iteration: 264\n",
      "Validation loss (no improvement): 2.9226633071899415\n",
      "Training iteration: 265\n",
      "Validation loss (no improvement): 3.0258270263671876\n",
      "Training iteration: 266\n",
      "Validation loss (no improvement): 2.7184581756591797\n",
      "Training iteration: 267\n",
      "Validation loss (no improvement): 2.374409866333008\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 1.5600080490112305  to: 1.5051183700561523\n",
      "Training iteration: 269\n",
      "Validation loss (no improvement): 1.8171669006347657\n",
      "Training iteration: 270\n",
      "Validation loss (no improvement): 1.6205093383789062\n",
      "Training iteration: 271\n",
      "Validation loss (no improvement): 1.7376089096069336\n",
      "Training iteration: 272\n",
      "Validation loss (no improvement): 1.9808746337890626\n",
      "Training iteration: 273\n",
      "Validation loss (no improvement): 2.4595455169677733\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 1.5051183700561523  to: 1.3425442695617675\n",
      "Training iteration: 275\n",
      "Validation loss (no improvement): 1.6385528564453125\n",
      "Training iteration: 276\n",
      "Validation loss (no improvement): 1.9033809661865235\n",
      "Training iteration: 277\n",
      "Validation loss (no improvement): 1.9683895111083984\n",
      "Training iteration: 278\n",
      "Validation loss (no improvement): 1.3972378730773927\n",
      "Training iteration: 279\n",
      "Validation loss (no improvement): 1.8957103729248046\n",
      "Training iteration: 280\n",
      "Validation loss (no improvement): 4.345479583740234\n",
      "Training iteration: 281\n",
      "Validation loss (no improvement): 1.9913049697875977\n",
      "Training iteration: 282\n",
      "Validation loss (no improvement): 2.091434097290039\n",
      "Training iteration: 283\n",
      "Validation loss (no improvement): 2.7057750701904295\n",
      "Training iteration: 284\n",
      "Validation loss (no improvement): 2.0626972198486326\n",
      "Training iteration: 285\n",
      "Validation loss (no improvement): 2.914402389526367\n",
      "Training iteration: 286\n",
      "Validation loss (no improvement): 2.6385854721069335\n",
      "Training iteration: 287\n",
      "Validation loss (no improvement): 2.070515441894531\n",
      "Training iteration: 288\n",
      "Validation loss (no improvement): 2.0046899795532225\n",
      "Training iteration: 289\n",
      "Validation loss (no improvement): 1.4127172470092773\n",
      "Training iteration: 290\n",
      "Validation loss (no improvement): 1.898767852783203\n",
      "Training iteration: 291\n",
      "Validation loss (no improvement): 2.2695571899414064\n",
      "Training iteration: 292\n",
      "Validation loss (no improvement): 2.3225793838500977\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 1.3425442695617675  to: 1.2112525939941405\n",
      "Training iteration: 294\n",
      "Validation loss (no improvement): 2.702971267700195\n",
      "Training iteration: 295\n",
      "Validation loss (no improvement): 1.4891477584838868\n",
      "Training iteration: 296\n",
      "Validation loss (no improvement): 1.4503949165344239\n",
      "Training iteration: 297\n",
      "Validation loss (no improvement): 1.8979934692382812\n",
      "Training iteration: 298\n",
      "Validation loss (no improvement): 2.0555255889892576\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 1.2112525939941405  to: 1.1082664489746095\n",
      "Training iteration: 300\n",
      "Validation loss (no improvement): 2.432258415222168\n",
      "Training iteration: 301\n",
      "Validation loss (no improvement): 1.9138980865478517\n",
      "Training iteration: 302\n",
      "Validation loss (no improvement): 1.723590087890625\n",
      "Training iteration: 303\n",
      "Validation loss (no improvement): 1.7690448760986328\n",
      "Training iteration: 304\n",
      "Validation loss (no improvement): 1.744375991821289\n",
      "Training iteration: 305\n",
      "Validation loss (no improvement): 2.6198617935180666\n",
      "Training iteration: 306\n",
      "Validation loss (no improvement): 1.7905179977416992\n",
      "Training iteration: 307\n",
      "Validation loss (no improvement): 1.7257770538330077\n",
      "Training iteration: 308\n",
      "Validation loss (no improvement): 1.2914227485656737\n",
      "Training iteration: 309\n",
      "Validation loss (no improvement): 1.2119192123413085\n",
      "Training iteration: 310\n",
      "Validation loss (no improvement): 1.1588037490844727\n",
      "Training iteration: 311\n",
      "Validation loss (no improvement): 2.147638511657715\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 1.1082664489746095  to: 1.055859088897705\n",
      "Training iteration: 313\n",
      "Validation loss (no improvement): 1.915536880493164\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 1.055859088897705  to: 0.9830755233764649\n",
      "Training iteration: 315\n",
      "Validation loss (no improvement): 1.643423080444336\n",
      "Training iteration: 316\n",
      "Validation loss (no improvement): 1.4135966300964355\n",
      "Training iteration: 317\n",
      "Validation loss (no improvement): 1.3628430366516113\n",
      "Training iteration: 318\n",
      "Validation loss (no improvement): 1.8580835342407227\n",
      "Training iteration: 319\n",
      "Validation loss (no improvement): 1.8016460418701172\n",
      "Training iteration: 320\n",
      "Validation loss (no improvement): 1.3698077201843262\n",
      "Training iteration: 321\n",
      "Validation loss (no improvement): 1.4044735908508301\n",
      "Training iteration: 322\n",
      "Validation loss (no improvement): 1.949068260192871\n",
      "Training iteration: 323\n",
      "Validation loss (no improvement): 1.7691314697265625\n",
      "Training iteration: 324\n",
      "Validation loss (no improvement): 1.1858815193176269\n",
      "Training iteration: 325\n",
      "Validation loss (no improvement): 2.3727748870849608\n",
      "Training iteration: 326\n",
      "Validation loss (no improvement): 1.0213664054870606\n",
      "Training iteration: 327\n",
      "Validation loss (no improvement): 1.3760757446289062\n",
      "Training iteration: 328\n",
      "Validation loss (no improvement): 1.7098600387573242\n",
      "Training iteration: 329\n",
      "Validation loss (no improvement): 1.7871347427368165\n",
      "Training iteration: 330\n",
      "Validation loss (no improvement): 1.1344515800476074\n",
      "Training iteration: 331\n",
      "Validation loss (no improvement): 1.2624290466308594\n",
      "Training iteration: 332\n",
      "Validation loss (no improvement): 1.6659526824951172\n",
      "Training iteration: 333\n",
      "Validation loss (no improvement): 1.7897705078125\n",
      "Training iteration: 334\n",
      "Validation loss (no improvement): 1.5348370552062989\n",
      "Training iteration: 335\n",
      "Validation loss (no improvement): 2.935451126098633\n",
      "Training iteration: 336\n",
      "Validation loss (no improvement): 1.4556756973266602\n",
      "Training iteration: 337\n",
      "Validation loss (no improvement): 1.9133087158203126\n",
      "Training iteration: 338\n",
      "Validation loss (no improvement): 1.187668800354004\n",
      "Training iteration: 339\n",
      "Validation loss (no improvement): 1.2017852783203125\n",
      "Training iteration: 340\n",
      "Validation loss (no improvement): 1.2556225776672363\n",
      "Training iteration: 341\n",
      "Validation loss (no improvement): 1.0537113189697265\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.9830755233764649  to: 0.9016685485839844\n",
      "Training iteration: 343\n",
      "Validation loss (no improvement): 1.5174975395202637\n",
      "Training iteration: 344\n",
      "Validation loss (no improvement): 1.1588491439819335\n",
      "Training iteration: 345\n",
      "Validation loss (no improvement): 1.1125421524047852\n",
      "Training iteration: 346\n",
      "Validation loss (no improvement): 1.59956693649292\n",
      "Training iteration: 347\n",
      "Validation loss (no improvement): 1.1728957176208497\n",
      "Training iteration: 348\n",
      "Validation loss (no improvement): 0.9796211242675781\n",
      "Training iteration: 349\n",
      "Validation loss (no improvement): 1.6312780380249023\n",
      "Training iteration: 350\n",
      "Validation loss (no improvement): 1.5231653213500977\n",
      "Training iteration: 351\n",
      "Validation loss (no improvement): 1.6503801345825195\n",
      "Training iteration: 352\n",
      "Validation loss (no improvement): 1.9596254348754882\n",
      "Training iteration: 353\n",
      "Validation loss (no improvement): 1.3120965957641602\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.9016685485839844  to: 0.8050367355346679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 355\n",
      "Validation loss (no improvement): 0.825928783416748\n",
      "Training iteration: 356\n",
      "Validation loss (no improvement): 1.4088915824890136\n",
      "Training iteration: 357\n",
      "Validation loss (no improvement): 1.127507209777832\n",
      "Training iteration: 358\n",
      "Validation loss (no improvement): 2.2846487045288084\n",
      "Training iteration: 359\n",
      "Validation loss (no improvement): 1.0910521507263184\n",
      "Training iteration: 360\n",
      "Validation loss (no improvement): 1.4536059379577637\n",
      "Training iteration: 361\n",
      "Validation loss (no improvement): 0.8317426681518555\n",
      "Training iteration: 362\n",
      "Validation loss (no improvement): 1.2689061164855957\n",
      "Training iteration: 363\n",
      "Validation loss (no improvement): 1.03002986907959\n",
      "Training iteration: 364\n",
      "Validation loss (no improvement): 1.2806655883789062\n",
      "Training iteration: 365\n",
      "Validation loss (no improvement): 1.2331515312194825\n",
      "Training iteration: 366\n",
      "Validation loss (no improvement): 2.148899459838867\n",
      "Training iteration: 367\n",
      "Validation loss (no improvement): 1.1039146423339843\n",
      "Training iteration: 368\n",
      "Validation loss (no improvement): 1.2745497703552247\n",
      "Training iteration: 369\n",
      "Validation loss (no improvement): 1.3964795112609862\n",
      "Training iteration: 370\n",
      "Validation loss (no improvement): 1.0483808517456055\n",
      "Training iteration: 371\n",
      "Validation loss (no improvement): 1.4093908309936523\n",
      "Training iteration: 372\n",
      "Validation loss (no improvement): 0.9834539413452148\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.8050367355346679  to: 0.7994879722595215\n",
      "Training iteration: 374\n",
      "Validation loss (no improvement): 0.9862069129943848\n",
      "Training iteration: 375\n",
      "Validation loss (no improvement): 2.115127372741699\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.7994879722595215  to: 0.7465952396392822\n",
      "Training iteration: 377\n",
      "Validation loss (no improvement): 1.4819325447082519\n",
      "Training iteration: 378\n",
      "Validation loss (no improvement): 0.9444883346557618\n",
      "Training iteration: 379\n",
      "Validation loss (no improvement): 2.096250534057617\n",
      "Training iteration: 380\n",
      "Validation loss (no improvement): 1.5063339233398438\n",
      "Training iteration: 381\n",
      "Validation loss (no improvement): 0.9876371383666992\n",
      "Training iteration: 382\n",
      "Validation loss (no improvement): 1.3618560791015626\n",
      "Training iteration: 383\n",
      "Validation loss (no improvement): 0.9705916404724121\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.7465952396392822  to: 0.6446404933929444\n",
      "Training iteration: 385\n",
      "Validation loss (no improvement): 0.856151008605957\n",
      "Training iteration: 386\n",
      "Validation loss (no improvement): 0.8396011352539062\n",
      "Training iteration: 387\n",
      "Validation loss (no improvement): 1.005373191833496\n",
      "Training iteration: 388\n",
      "Validation loss (no improvement): 1.3286038398742677\n",
      "Training iteration: 389\n",
      "Validation loss (no improvement): 1.5058425903320312\n",
      "Training iteration: 390\n",
      "Validation loss (no improvement): 1.8497682571411134\n",
      "Training iteration: 391\n",
      "Validation loss (no improvement): 1.0942873001098632\n",
      "Training iteration: 392\n",
      "Validation loss (no improvement): 0.8507041931152344\n",
      "Training iteration: 393\n",
      "Validation loss (no improvement): 1.1224299430847169\n",
      "Training iteration: 394\n",
      "Validation loss (no improvement): 2.00662899017334\n",
      "Training iteration: 395\n",
      "Validation loss (no improvement): 0.9663260459899903\n",
      "Training iteration: 396\n",
      "Validation loss (no improvement): 1.3531600952148437\n",
      "Training iteration: 397\n",
      "Validation loss (no improvement): 1.231447982788086\n",
      "Training iteration: 398\n",
      "Validation loss (no improvement): 1.3678703308105469\n",
      "Training iteration: 399\n",
      "Validation loss (no improvement): 1.3911508560180663\n",
      "Training iteration: 400\n",
      "Validation loss (no improvement): 0.9174664497375489\n",
      "Training iteration: 401\n",
      "Validation loss (no improvement): 1.0785248756408692\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.6446404933929444  to: 0.5136056900024414\n",
      "Training iteration: 403\n",
      "Validation loss (no improvement): 0.8169981002807617\n",
      "Training iteration: 404\n",
      "Validation loss (no improvement): 0.819025993347168\n",
      "Training iteration: 405\n",
      "Validation loss (no improvement): 1.3491355895996093\n",
      "Training iteration: 406\n",
      "Validation loss (no improvement): 1.095585823059082\n",
      "Training iteration: 407\n",
      "Validation loss (no improvement): 0.5353566169738769\n",
      "Training iteration: 408\n",
      "Validation loss (no improvement): 0.752838134765625\n",
      "Training iteration: 409\n",
      "Validation loss (no improvement): 1.1129740715026855\n",
      "Training iteration: 410\n",
      "Validation loss (no improvement): 0.9562633514404297\n",
      "Training iteration: 411\n",
      "Validation loss (no improvement): 0.6795939445495606\n",
      "Training iteration: 412\n",
      "Validation loss (no improvement): 0.6077738761901855\n",
      "Training iteration: 413\n",
      "Validation loss (no improvement): 1.048048973083496\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.5136056900024414  to: 0.49149141311645506\n",
      "Training iteration: 415\n",
      "Validation loss (no improvement): 0.8181515693664551\n",
      "Training iteration: 416\n",
      "Validation loss (no improvement): 2.1469051361083986\n",
      "Training iteration: 417\n",
      "Validation loss (no improvement): 2.038152503967285\n",
      "Training iteration: 418\n",
      "Validation loss (no improvement): 1.0269869804382323\n",
      "Training iteration: 419\n",
      "Validation loss (no improvement): 1.1515901565551758\n",
      "Training iteration: 420\n",
      "Validation loss (no improvement): 1.2732088088989257\n",
      "Training iteration: 421\n",
      "Validation loss (no improvement): 0.7354374885559082\n",
      "Training iteration: 422\n",
      "Validation loss (no improvement): 1.2558466911315918\n",
      "Training iteration: 423\n",
      "Validation loss (no improvement): 1.3624911308288574\n",
      "Training iteration: 424\n",
      "Validation loss (no improvement): 0.7827500820159912\n",
      "Training iteration: 425\n",
      "Validation loss (no improvement): 2.8155628204345704\n",
      "Training iteration: 426\n",
      "Validation loss (no improvement): 0.9332420349121093\n",
      "Training iteration: 427\n",
      "Validation loss (no improvement): 0.9627823829650879\n",
      "Training iteration: 428\n",
      "Validation loss (no improvement): 1.1404090881347657\n",
      "Training iteration: 429\n",
      "Validation loss (no improvement): 1.1414128303527833\n",
      "Training iteration: 430\n",
      "Validation loss (no improvement): 0.7137277126312256\n",
      "Training iteration: 431\n",
      "Validation loss (no improvement): 0.8306562423706054\n",
      "Training iteration: 432\n",
      "Validation loss (no improvement): 1.0033671379089355\n",
      "Training iteration: 433\n",
      "Validation loss (no improvement): 0.9722448348999023\n",
      "Training iteration: 434\n",
      "Validation loss (no improvement): 0.8772192001342773\n",
      "Training iteration: 435\n",
      "Validation loss (no improvement): 0.9505945205688476\n",
      "Training iteration: 436\n",
      "Validation loss (no improvement): 0.5055319309234619\n",
      "Training iteration: 437\n",
      "Validation loss (no improvement): 0.859182071685791\n",
      "Training iteration: 438\n",
      "Validation loss (no improvement): 1.5214006423950195\n",
      "Training iteration: 439\n",
      "Validation loss (no improvement): 0.7689038276672363\n",
      "Training iteration: 440\n",
      "Validation loss (no improvement): 0.9213335990905762\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.49149141311645506  to: 0.4867713451385498\n",
      "Training iteration: 442\n",
      "Validation loss (no improvement): 1.1047983169555664\n",
      "Training iteration: 443\n",
      "Validation loss (no improvement): 0.7396605014801025\n",
      "Training iteration: 444\n",
      "Validation loss (no improvement): 0.737028980255127\n",
      "Training iteration: 445\n",
      "Validation loss (no improvement): 2.5871465682983397\n",
      "Training iteration: 446\n",
      "Validation loss (no improvement): 0.6899679660797119\n",
      "Training iteration: 447\n",
      "Validation loss (no improvement): 0.9383123397827149\n",
      "Training iteration: 448\n",
      "Validation loss (no improvement): 0.8353731155395507\n",
      "Training iteration: 449\n",
      "Validation loss (no improvement): 2.652652931213379\n",
      "Training iteration: 450\n",
      "Validation loss (no improvement): 0.9906608581542968\n",
      "Training iteration: 451\n",
      "Validation loss (no improvement): 1.0476037979125976\n",
      "Training iteration: 452\n",
      "Validation loss (no improvement): 1.2050825119018556\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.4867713451385498  to: 0.4461820125579834\n",
      "Training iteration: 454\n",
      "Validation loss (no improvement): 0.9042796134948731\n",
      "Training iteration: 455\n",
      "Validation loss (no improvement): 0.7756192684173584\n",
      "Training iteration: 456\n",
      "Validation loss (no improvement): 0.7443351745605469\n",
      "Training iteration: 457\n",
      "Validation loss (no improvement): 0.8661322593688965\n",
      "Training iteration: 458\n",
      "Validation loss (no improvement): 0.8722917556762695\n",
      "Training iteration: 459\n",
      "Validation loss (no improvement): 1.019570255279541\n",
      "Training iteration: 460\n",
      "Validation loss (no improvement): 0.78018217086792\n",
      "Training iteration: 461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 1.0144798278808593\n",
      "Training iteration: 462\n",
      "Validation loss (no improvement): 0.5968884468078614\n",
      "Training iteration: 463\n",
      "Validation loss (no improvement): 0.7896766662597656\n",
      "Training iteration: 464\n",
      "Validation loss (no improvement): 1.0284001350402832\n",
      "Training iteration: 465\n",
      "Validation loss (no improvement): 0.6983839988708496\n",
      "Training iteration: 466\n",
      "Validation loss (no improvement): 1.0160043716430665\n",
      "Training iteration: 467\n",
      "Validation loss (no improvement): 0.46665639877319337\n",
      "Training iteration: 468\n",
      "Validation loss (no improvement): 0.9994328498840332\n",
      "Training iteration: 469\n",
      "Validation loss (no improvement): 1.3701624870300293\n",
      "Training iteration: 470\n",
      "Validation loss (no improvement): 1.2345691680908204\n",
      "Training iteration: 471\n",
      "Validation loss (no improvement): 0.5265948295593261\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.4461820125579834  to: 0.3267852783203125\n",
      "Training iteration: 473\n",
      "Validation loss (no improvement): 0.6225359916687012\n",
      "Training iteration: 474\n",
      "Validation loss (no improvement): 0.9301828384399414\n",
      "Training iteration: 475\n",
      "Validation loss (no improvement): 0.8430261611938477\n",
      "Training iteration: 476\n",
      "Validation loss (no improvement): 0.6999644279479981\n",
      "Training iteration: 477\n",
      "Validation loss (no improvement): 0.6554995536804199\n",
      "Training iteration: 478\n",
      "Validation loss (no improvement): 1.3553756713867187\n",
      "Training iteration: 479\n",
      "Validation loss (no improvement): 0.3546092987060547\n",
      "Training iteration: 480\n",
      "Validation loss (no improvement): 0.8121435165405273\n",
      "Training iteration: 481\n",
      "Validation loss (no improvement): 0.4772916316986084\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.3267852783203125  to: 0.32154083251953125\n",
      "Training iteration: 483\n",
      "Validation loss (no improvement): 1.2636411666870118\n",
      "Training iteration: 484\n",
      "Validation loss (no improvement): 1.8339406967163085\n",
      "Training iteration: 485\n",
      "Validation loss (no improvement): 0.7513243198394776\n",
      "Training iteration: 486\n",
      "Validation loss (no improvement): 0.8107070922851562\n",
      "Training iteration: 487\n",
      "Validation loss (no improvement): 0.5354230403900146\n",
      "Training iteration: 488\n",
      "Validation loss (no improvement): 1.5040830612182616\n",
      "Training iteration: 489\n",
      "Validation loss (no improvement): 0.47678194046020506\n",
      "Training iteration: 490\n",
      "Validation loss (no improvement): 0.6349658966064453\n",
      "Training iteration: 491\n",
      "Validation loss (no improvement): 0.5759932041168213\n",
      "Training iteration: 492\n",
      "Validation loss (no improvement): 0.38965868949890137\n",
      "Training iteration: 493\n",
      "Validation loss (no improvement): 1.9882793426513672\n",
      "Training iteration: 494\n",
      "Validation loss (no improvement): 1.007172203063965\n",
      "Training iteration: 495\n",
      "Validation loss (no improvement): 0.6314909934997559\n",
      "Training iteration: 496\n",
      "Validation loss (no improvement): 0.5222779750823975\n",
      "Training iteration: 497\n",
      "Validation loss (no improvement): 0.9976588249206543\n",
      "Training iteration: 498\n",
      "Validation loss (no improvement): 1.085662841796875\n",
      "Training iteration: 499\n",
      "Validation loss (no improvement): 0.6204188346862793\n",
      "Training iteration: 500\n",
      "Validation loss (no improvement): 0.34887936115264895\n",
      "Training iteration: 501\n",
      "Validation loss (no improvement): 0.9366643905639649\n",
      "Training iteration: 502\n",
      "Validation loss (no improvement): 0.6354562282562256\n",
      "Training iteration: 503\n",
      "Validation loss (no improvement): 0.9822286605834961\n",
      "Training iteration: 504\n",
      "Validation loss (no improvement): 0.9616332054138184\n",
      "Training iteration: 505\n",
      "Validation loss (no improvement): 0.523552417755127\n",
      "Training iteration: 506\n",
      "Validation loss (no improvement): 1.6975387573242187\n",
      "Training iteration: 507\n",
      "Validation loss (no improvement): 1.0736903190612792\n",
      "Training iteration: 508\n",
      "Validation loss (no improvement): 0.9410722732543946\n",
      "Training iteration: 509\n",
      "Validation loss (no improvement): 0.3758860111236572\n",
      "Training iteration: 510\n",
      "Validation loss (no improvement): 0.5816392421722412\n",
      "Training iteration: 511\n",
      "Validation loss (no improvement): 0.709604024887085\n",
      "Training iteration: 512\n",
      "Validation loss (no improvement): 1.041273307800293\n",
      "Training iteration: 513\n",
      "Validation loss (no improvement): 0.37340469360351564\n",
      "Training iteration: 514\n",
      "Validation loss (no improvement): 0.5954883575439454\n",
      "Training iteration: 515\n",
      "Validation loss (no improvement): 0.9142743110656738\n",
      "Training iteration: 516\n",
      "Validation loss (no improvement): 1.1937223434448243\n",
      "Training iteration: 517\n",
      "Validation loss (no improvement): 1.0273578643798829\n",
      "Training iteration: 518\n",
      "Validation loss (no improvement): 0.8786448478698731\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.32154083251953125  to: 0.31756815910339353\n",
      "Training iteration: 520\n",
      "Validation loss (no improvement): 0.7409749031066895\n",
      "Training iteration: 521\n",
      "Validation loss (no improvement): 0.6644232273101807\n",
      "Training iteration: 522\n",
      "Validation loss (no improvement): 1.2575370788574218\n",
      "Training iteration: 523\n",
      "Validation loss (no improvement): 0.3820288419723511\n",
      "Training iteration: 524\n",
      "Validation loss (no improvement): 1.4323092460632325\n",
      "Training iteration: 525\n",
      "Validation loss (no improvement): 0.6435255527496337\n",
      "Training iteration: 526\n",
      "Validation loss (no improvement): 0.42575979232788086\n",
      "Training iteration: 527\n",
      "Validation loss (no improvement): 0.7290190696716309\n",
      "Training iteration: 528\n",
      "Validation loss (no improvement): 0.434050178527832\n",
      "Training iteration: 529\n",
      "Validation loss (no improvement): 0.6288545608520508\n",
      "Training iteration: 530\n",
      "Validation loss (no improvement): 0.5248476505279541\n",
      "Training iteration: 531\n",
      "Validation loss (no improvement): 0.8285963058471679\n",
      "Training iteration: 532\n",
      "Validation loss (no improvement): 0.6163125038146973\n",
      "Training iteration: 533\n",
      "Validation loss (no improvement): 0.6466481208801269\n",
      "Training iteration: 534\n",
      "Validation loss (no improvement): 0.440195894241333\n",
      "Training iteration: 535\n",
      "Validation loss (no improvement): 0.5597567081451416\n",
      "Training iteration: 536\n",
      "Validation loss (no improvement): 0.4841768741607666\n",
      "Training iteration: 537\n",
      "Validation loss (no improvement): 1.2521085739135742\n",
      "Training iteration: 538\n",
      "Validation loss (no improvement): 0.8258410453796386\n",
      "Training iteration: 539\n",
      "Validation loss (no improvement): 0.5682229042053223\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.31756815910339353  to: 0.30246577262878416\n",
      "Training iteration: 541\n",
      "Validation loss (no improvement): 1.305128002166748\n",
      "Training iteration: 542\n",
      "Validation loss (no improvement): 0.5221116065979003\n",
      "Training iteration: 543\n",
      "Validation loss (no improvement): 0.8757877349853516\n",
      "Training iteration: 544\n",
      "Validation loss (no improvement): 0.3648542881011963\n",
      "Training iteration: 545\n",
      "Validation loss (no improvement): 1.3313638687133789\n",
      "Training iteration: 546\n",
      "Validation loss (no improvement): 0.642181396484375\n",
      "Training iteration: 547\n",
      "Validation loss (no improvement): 0.5477899551391602\n",
      "Training iteration: 548\n",
      "Validation loss (no improvement): 0.5979884147644043\n",
      "Training iteration: 549\n",
      "Validation loss (no improvement): 0.38392364978790283\n",
      "Training iteration: 550\n",
      "Validation loss (no improvement): 0.8803590774536133\n",
      "Training iteration: 551\n",
      "Validation loss (no improvement): 0.44971508979797364\n",
      "Training iteration: 552\n",
      "Validation loss (no improvement): 1.205882740020752\n",
      "Training iteration: 553\n",
      "Validation loss (no improvement): 1.2485105514526367\n",
      "Training iteration: 554\n",
      "Validation loss (no improvement): 0.3840109586715698\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.30246577262878416  to: 0.2167975425720215\n",
      "Training iteration: 556\n",
      "Validation loss (no improvement): 0.641195821762085\n",
      "Training iteration: 557\n",
      "Validation loss (no improvement): 0.37850399017333985\n",
      "Training iteration: 558\n",
      "Validation loss (no improvement): 0.5198227882385253\n",
      "Training iteration: 559\n",
      "Validation loss (no improvement): 0.6297753334045411\n",
      "Training iteration: 560\n",
      "Validation loss (no improvement): 0.42885560989379884\n",
      "Training iteration: 561\n",
      "Validation loss (no improvement): 0.47272334098815916\n",
      "Training iteration: 562\n",
      "Validation loss (no improvement): 0.5095380306243896\n",
      "Training iteration: 563\n",
      "Validation loss (no improvement): 0.596239709854126\n",
      "Training iteration: 564\n",
      "Validation loss (no improvement): 0.37679038047790525\n",
      "Training iteration: 565\n",
      "Validation loss (no improvement): 1.2120434761047363\n",
      "Training iteration: 566\n",
      "Validation loss (no improvement): 0.5063495635986328\n",
      "Training iteration: 567\n",
      "Validation loss (no improvement): 0.30211124420166013\n",
      "Training iteration: 568\n",
      "Validation loss (no improvement): 1.1900732040405273\n",
      "Training iteration: 569\n",
      "Validation loss (no improvement): 0.9417104721069336\n",
      "Training iteration: 570\n",
      "Validation loss (no improvement): 0.4543132781982422\n",
      "Training iteration: 571\n",
      "Validation loss (no improvement): 0.4992654800415039\n",
      "Training iteration: 572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 1.5553613662719727\n",
      "Training iteration: 573\n",
      "Validation loss (no improvement): 0.5793945789337158\n",
      "Training iteration: 574\n",
      "Validation loss (no improvement): 0.5330490112304688\n",
      "Training iteration: 575\n",
      "Validation loss (no improvement): 0.44431581497192385\n",
      "Training iteration: 576\n",
      "Validation loss (no improvement): 0.47216148376464845\n",
      "Training iteration: 577\n",
      "Validation loss (no improvement): 0.9032369613647461\n",
      "Training iteration: 578\n",
      "Validation loss (no improvement): 0.2541360378265381\n",
      "Training iteration: 579\n",
      "Validation loss (no improvement): 0.3487377166748047\n",
      "Training iteration: 580\n",
      "Validation loss (no improvement): 0.45491514205932615\n",
      "Training iteration: 581\n",
      "Validation loss (no improvement): 0.8146799087524415\n",
      "Training iteration: 582\n",
      "Validation loss (no improvement): 0.28382458686828616\n",
      "Training iteration: 583\n",
      "Validation loss (no improvement): 0.4443087577819824\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.2167975425720215  to: 0.1951512098312378\n",
      "Training iteration: 585\n",
      "Validation loss (no improvement): 0.8267402648925781\n",
      "Training iteration: 586\n",
      "Validation loss (no improvement): 0.2676721096038818\n",
      "Training iteration: 587\n",
      "Validation loss (no improvement): 0.8898678779602051\n",
      "Training iteration: 588\n",
      "Validation loss (no improvement): 0.936297607421875\n",
      "Training iteration: 589\n",
      "Validation loss (no improvement): 0.6830320835113526\n",
      "Training iteration: 590\n",
      "Validation loss (no improvement): 2.1222675323486326\n",
      "Training iteration: 591\n",
      "Validation loss (no improvement): 1.718211555480957\n",
      "Training iteration: 592\n",
      "Validation loss (no improvement): 0.3165518045425415\n",
      "Training iteration: 593\n",
      "Validation loss (no improvement): 1.1973950386047363\n",
      "Training iteration: 594\n",
      "Validation loss (no improvement): 0.273531436920166\n",
      "Training iteration: 595\n",
      "Validation loss (no improvement): 0.6851832389831543\n",
      "Training iteration: 596\n",
      "Validation loss (no improvement): 1.065208625793457\n",
      "Training iteration: 597\n",
      "Validation loss (no improvement): 0.43242464065551756\n",
      "Training iteration: 598\n",
      "Validation loss (no improvement): 0.5902731418609619\n",
      "Training iteration: 599\n",
      "Validation loss (no improvement): 0.2832892894744873\n",
      "Training iteration: 600\n",
      "Validation loss (no improvement): 0.4604164123535156\n",
      "Training iteration: 601\n",
      "Validation loss (no improvement): 0.6150009632110596\n",
      "Training iteration: 602\n",
      "Validation loss (no improvement): 0.2683734655380249\n",
      "Training iteration: 603\n",
      "Validation loss (no improvement): 0.9010551452636719\n",
      "Training iteration: 604\n",
      "Validation loss (no improvement): 0.45757369995117186\n",
      "Training iteration: 605\n",
      "Validation loss (no improvement): 0.22096467018127441\n",
      "Training iteration: 606\n",
      "Validation loss (no improvement): 0.6541010856628418\n",
      "Training iteration: 607\n",
      "Validation loss (no improvement): 0.9861319541931153\n",
      "Training iteration: 608\n",
      "Validation loss (no improvement): 0.33388237953186034\n",
      "Training iteration: 609\n",
      "Validation loss (no improvement): 0.40876259803771975\n",
      "Training iteration: 610\n",
      "Validation loss (no improvement): 0.49196724891662597\n",
      "Training iteration: 611\n",
      "Validation loss (no improvement): 0.27172064781188965\n",
      "Training iteration: 612\n",
      "Validation loss (no improvement): 0.2931622266769409\n",
      "Training iteration: 613\n",
      "Validation loss (no improvement): 0.6022537231445313\n",
      "Training iteration: 614\n",
      "Validation loss (no improvement): 0.33592727184295657\n",
      "Training iteration: 615\n",
      "Validation loss (no improvement): 0.21784021854400634\n",
      "Training iteration: 616\n",
      "Validation loss (no improvement): 0.29008235931396487\n",
      "Training iteration: 617\n",
      "Validation loss (no improvement): 0.3231964111328125\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.1951512098312378  to: 0.19053274393081665\n",
      "Training iteration: 619\n",
      "Validation loss (no improvement): 0.4118810176849365\n",
      "Training iteration: 620\n",
      "Validation loss (no improvement): 0.1949436902999878\n",
      "Training iteration: 621\n",
      "Validation loss (no improvement): 0.7815211296081543\n",
      "Training iteration: 622\n",
      "Validation loss (no improvement): 0.24618611335754395\n",
      "Training iteration: 623\n",
      "Validation loss (no improvement): 0.3250089168548584\n",
      "Training iteration: 624\n",
      "Validation loss (no improvement): 0.677890396118164\n",
      "Training iteration: 625\n",
      "Validation loss (no improvement): 0.2808291673660278\n",
      "Training iteration: 626\n",
      "Validation loss (no improvement): 0.23875651359558106\n",
      "Training iteration: 627\n",
      "Validation loss (no improvement): 0.7094688415527344\n",
      "Training iteration: 628\n",
      "Validation loss (no improvement): 0.4962300777435303\n",
      "Training iteration: 629\n",
      "Validation loss (no improvement): 0.3536397457122803\n",
      "Training iteration: 630\n",
      "Validation loss (no improvement): 0.5674168586730957\n",
      "Training iteration: 631\n",
      "Validation loss (no improvement): 0.25647768974304197\n",
      "Training iteration: 632\n",
      "Validation loss (no improvement): 0.8317755699157715\n",
      "Training iteration: 633\n",
      "Validation loss (no improvement): 0.4660759449005127\n",
      "Training iteration: 634\n",
      "Validation loss (no improvement): 0.3324778079986572\n",
      "Training iteration: 635\n",
      "Validation loss (no improvement): 0.7392926216125488\n",
      "Training iteration: 636\n",
      "Validation loss (no improvement): 1.4488535881042481\n",
      "Training iteration: 637\n",
      "Validation loss (no improvement): 0.8167119026184082\n",
      "Training iteration: 638\n",
      "Validation loss (no improvement): 0.565523624420166\n",
      "Training iteration: 639\n",
      "Validation loss (no improvement): 0.7121274471282959\n",
      "Training iteration: 640\n",
      "Validation loss (no improvement): 0.2679359674453735\n",
      "Training iteration: 641\n",
      "Validation loss (no improvement): 0.4417665958404541\n",
      "Training iteration: 642\n",
      "Validation loss (no improvement): 0.7989242553710938\n",
      "Training iteration: 643\n",
      "Validation loss (no improvement): 0.7075159549713135\n",
      "Training iteration: 644\n",
      "Validation loss (no improvement): 0.37081332206726075\n",
      "Training iteration: 645\n",
      "Validation loss (no improvement): 0.9808627128601074\n",
      "Training iteration: 646\n",
      "Validation loss (no improvement): 0.23142571449279786\n",
      "Training iteration: 647\n",
      "Validation loss (no improvement): 0.4484855175018311\n",
      "Training iteration: 648\n",
      "Validation loss (no improvement): 0.6159018039703369\n",
      "Training iteration: 649\n",
      "Validation loss (no improvement): 0.5751936435699463\n",
      "Training iteration: 650\n",
      "Validation loss (no improvement): 1.0243212699890136\n",
      "Training iteration: 651\n",
      "Validation loss (no improvement): 0.29782848358154296\n",
      "Training iteration: 652\n",
      "Validation loss (no improvement): 0.2303410291671753\n",
      "Training iteration: 653\n",
      "Validation loss (no improvement): 0.3235144138336182\n",
      "Training iteration: 654\n",
      "Validation loss (no improvement): 0.4540429592132568\n",
      "Training iteration: 655\n",
      "Validation loss (no improvement): 1.0264776229858399\n",
      "Training iteration: 656\n",
      "Validation loss (no improvement): 0.5298293113708497\n",
      "Training iteration: 657\n",
      "Validation loss (no improvement): 0.3789988040924072\n",
      "Training iteration: 658\n",
      "Validation loss (no improvement): 0.7542918205261231\n",
      "Training iteration: 659\n",
      "Validation loss (no improvement): 0.3216078519821167\n",
      "Training iteration: 660\n",
      "Validation loss (no improvement): 0.28919219970703125\n",
      "Training iteration: 661\n",
      "Validation loss (no improvement): 0.9400389671325684\n",
      "Training iteration: 662\n",
      "Validation loss (no improvement): 0.3782495975494385\n",
      "Training iteration: 663\n",
      "Validation loss (no improvement): 0.46117677688598635\n",
      "Training iteration: 664\n",
      "Validation loss (no improvement): 0.36707851886749265\n",
      "Training iteration: 665\n",
      "Validation loss (no improvement): 0.5281373023986816\n",
      "Training iteration: 666\n",
      "Validation loss (no improvement): 0.526149845123291\n",
      "Training iteration: 667\n",
      "Validation loss (no improvement): 0.20959362983703614\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.19053274393081665  to: 0.16188523769378663\n",
      "Training iteration: 669\n",
      "Validation loss (no improvement): 0.2045295238494873\n",
      "Training iteration: 670\n",
      "Validation loss (no improvement): 1.0326478958129883\n",
      "Training iteration: 671\n",
      "Validation loss (no improvement): 0.6527869224548339\n",
      "Training iteration: 672\n",
      "Validation loss (no improvement): 0.6830826759338379\n",
      "Training iteration: 673\n",
      "Validation loss (no improvement): 0.2421705722808838\n",
      "Training iteration: 674\n",
      "Validation loss (no improvement): 0.21687424182891846\n",
      "Training iteration: 675\n",
      "Validation loss (no improvement): 0.5386075973510742\n",
      "Training iteration: 676\n",
      "Validation loss (no improvement): 0.22238442897796631\n",
      "Training iteration: 677\n",
      "Validation loss (no improvement): 0.30587742328643797\n",
      "Training iteration: 678\n",
      "Validation loss (no improvement): 0.39422879219055174\n",
      "Training iteration: 679\n",
      "Validation loss (no improvement): 0.29902896881103513\n",
      "Training iteration: 680\n",
      "Validation loss (no improvement): 0.34664015769958495\n",
      "Training iteration: 681\n",
      "Validation loss (no improvement): 0.17241814136505126\n",
      "Training iteration: 682\n",
      "Validation loss (no improvement): 0.5885882377624512\n",
      "Training iteration: 683\n",
      "Validation loss (no improvement): 0.4623254299163818\n",
      "Training iteration: 684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.6419209480285645\n",
      "Training iteration: 685\n",
      "Validation loss (no improvement): 0.5635902881622314\n",
      "Training iteration: 686\n",
      "Validation loss (no improvement): 0.5085769653320312\n",
      "Training iteration: 687\n",
      "Validation loss (no improvement): 0.7098185539245605\n",
      "Training iteration: 688\n",
      "Validation loss (no improvement): 0.22192807197570802\n",
      "Training iteration: 689\n",
      "Validation loss (no improvement): 0.425399112701416\n",
      "Training iteration: 690\n",
      "Validation loss (no improvement): 0.3116799592971802\n",
      "Training iteration: 691\n",
      "Validation loss (no improvement): 0.23135464191436766\n",
      "Training iteration: 692\n",
      "Validation loss (no improvement): 0.1720349073410034\n",
      "Training iteration: 693\n",
      "Validation loss (no improvement): 0.2621987581253052\n",
      "Training iteration: 694\n",
      "Validation loss (no improvement): 0.9895844459533691\n",
      "Training iteration: 695\n",
      "Validation loss (no improvement): 0.24441676139831542\n",
      "Training iteration: 696\n",
      "Validation loss (no improvement): 0.2733100175857544\n",
      "Training iteration: 697\n",
      "Validation loss (no improvement): 0.20290095806121827\n",
      "Training iteration: 698\n",
      "Validation loss (no improvement): 0.6627874851226807\n",
      "Training iteration: 699\n",
      "Validation loss (no improvement): 0.32007861137390137\n",
      "Training iteration: 700\n",
      "Validation loss (no improvement): 0.21288573741912842\n",
      "Training iteration: 701\n",
      "Validation loss (no improvement): 0.43712739944458007\n",
      "Training iteration: 702\n",
      "Validation loss (no improvement): 0.1805661916732788\n",
      "Training iteration: 703\n",
      "Validation loss (no improvement): 0.2689947605133057\n",
      "Training iteration: 704\n",
      "Validation loss (no improvement): 0.3410497188568115\n",
      "Training iteration: 705\n",
      "Validation loss (no improvement): 0.873621654510498\n",
      "Training iteration: 706\n",
      "Validation loss (no improvement): 1.0120462417602538\n",
      "Training iteration: 707\n",
      "Validation loss (no improvement): 0.3929121494293213\n",
      "Training iteration: 708\n",
      "Validation loss (no improvement): 0.2980167865753174\n",
      "Training iteration: 709\n",
      "Validation loss (no improvement): 0.5419898986816406\n",
      "Training iteration: 710\n",
      "Validation loss (no improvement): 0.24951610565185547\n",
      "Training iteration: 711\n",
      "Validation loss (no improvement): 0.4486572265625\n",
      "Training iteration: 712\n",
      "Validation loss (no improvement): 0.2814762115478516\n",
      "Training iteration: 713\n",
      "Validation loss (no improvement): 0.3393054246902466\n",
      "Training iteration: 714\n",
      "Validation loss (no improvement): 0.8120316505432129\n",
      "Training iteration: 715\n",
      "Validation loss (no improvement): 0.2198819637298584\n",
      "Training iteration: 716\n",
      "Validation loss (no improvement): 0.38517882823944094\n",
      "Training iteration: 717\n",
      "Validation loss (no improvement): 0.3175955295562744\n",
      "Training iteration: 718\n",
      "Validation loss (no improvement): 0.2909615278244019\n",
      "Training iteration: 719\n",
      "Validation loss (no improvement): 0.3449972152709961\n",
      "Training iteration: 720\n",
      "Validation loss (no improvement): 0.4305127143859863\n",
      "Training iteration: 721\n",
      "Validation loss (no improvement): 0.5589924812316894\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.16188523769378663  to: 0.1519261598587036\n",
      "Training iteration: 723\n",
      "Validation loss (no improvement): 0.4021132469177246\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.1519261598587036  to: 0.09053834080696106\n",
      "Training iteration: 725\n",
      "Validation loss (no improvement): 0.3514086246490479\n",
      "Training iteration: 726\n",
      "Validation loss (no improvement): 0.59543776512146\n",
      "Training iteration: 727\n",
      "Validation loss (no improvement): 1.1422143936157227\n",
      "Training iteration: 728\n",
      "Validation loss (no improvement): 0.18565473556518555\n",
      "Training iteration: 729\n",
      "Validation loss (no improvement): 0.6295947551727294\n",
      "Training iteration: 730\n",
      "Validation loss (no improvement): 0.6120918273925782\n",
      "Training iteration: 731\n",
      "Validation loss (no improvement): 0.37897658348083496\n",
      "Training iteration: 732\n",
      "Validation loss (no improvement): 0.4949494361877441\n",
      "Training iteration: 733\n",
      "Validation loss (no improvement): 0.22254929542541504\n",
      "Training iteration: 734\n",
      "Validation loss (no improvement): 0.6133686542510987\n",
      "Training iteration: 735\n",
      "Validation loss (no improvement): 0.15970203876495362\n",
      "Training iteration: 736\n",
      "Validation loss (no improvement): 0.5470304965972901\n",
      "Training iteration: 737\n",
      "Validation loss (no improvement): 0.961154842376709\n",
      "Training iteration: 738\n",
      "Validation loss (no improvement): 0.5170760154724121\n",
      "Training iteration: 739\n",
      "Validation loss (no improvement): 0.6607309818267822\n",
      "Training iteration: 740\n",
      "Validation loss (no improvement): 0.47848081588745117\n",
      "Training iteration: 741\n",
      "Validation loss (no improvement): 0.2373790979385376\n",
      "Training iteration: 742\n",
      "Validation loss (no improvement): 0.22121694087982177\n",
      "Training iteration: 743\n",
      "Validation loss (no improvement): 0.25291595458984373\n",
      "Training iteration: 744\n",
      "Validation loss (no improvement): 0.12498973608016968\n",
      "Training iteration: 745\n",
      "Validation loss (no improvement): 0.8853039741516113\n",
      "Training iteration: 746\n",
      "Validation loss (no improvement): 0.1821030020713806\n",
      "Training iteration: 747\n",
      "Validation loss (no improvement): 0.6107243061065674\n",
      "Training iteration: 748\n",
      "Validation loss (no improvement): 0.41636018753051757\n",
      "Training iteration: 749\n",
      "Validation loss (no improvement): 0.5268013954162598\n",
      "Training iteration: 750\n",
      "Validation loss (no improvement): 0.5734461784362793\n",
      "Training iteration: 751\n",
      "Validation loss (no improvement): 0.6762419223785401\n",
      "Training iteration: 752\n",
      "Validation loss (no improvement): 0.4830470085144043\n",
      "Training iteration: 753\n",
      "Validation loss (no improvement): 0.32979531288146974\n",
      "Training iteration: 754\n",
      "Validation loss (no improvement): 0.18321834802627562\n",
      "Training iteration: 755\n",
      "Validation loss (no improvement): 0.257749080657959\n",
      "Training iteration: 756\n",
      "Validation loss (no improvement): 0.3932385206222534\n",
      "Training iteration: 757\n",
      "Validation loss (no improvement): 1.6370040893554687\n",
      "Training iteration: 758\n",
      "Validation loss (no improvement): 0.2321767807006836\n",
      "Training iteration: 759\n",
      "Validation loss (no improvement): 0.26667606830596924\n",
      "Training iteration: 760\n",
      "Validation loss (no improvement): 0.3129563570022583\n",
      "Training iteration: 761\n",
      "Validation loss (no improvement): 0.27569420337677003\n",
      "Training iteration: 762\n",
      "Validation loss (no improvement): 0.3411848545074463\n",
      "Training iteration: 763\n",
      "Validation loss (no improvement): 0.5490301609039306\n",
      "Training iteration: 764\n",
      "Validation loss (no improvement): 0.5558419704437256\n",
      "Training iteration: 765\n",
      "Validation loss (no improvement): 0.2534403562545776\n",
      "Training iteration: 766\n",
      "Validation loss (no improvement): 0.2085641622543335\n",
      "Training iteration: 767\n",
      "Validation loss (no improvement): 0.22154250144958496\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.09053834080696106  to: 0.08914044499397278\n",
      "Training iteration: 769\n",
      "Validation loss (no improvement): 0.15797789096832277\n",
      "Training iteration: 770\n",
      "Validation loss (no improvement): 0.592430591583252\n",
      "Training iteration: 771\n",
      "Validation loss (no improvement): 0.4067532539367676\n",
      "Training iteration: 772\n",
      "Validation loss (no improvement): 0.485382080078125\n",
      "Training iteration: 773\n",
      "Validation loss (no improvement): 0.4899073600769043\n",
      "Training iteration: 774\n",
      "Validation loss (no improvement): 0.33298041820526125\n",
      "Training iteration: 775\n",
      "Validation loss (no improvement): 0.15920755863189698\n",
      "Training iteration: 776\n",
      "Validation loss (no improvement): 0.8291502952575683\n",
      "Training iteration: 777\n",
      "Validation loss (no improvement): 0.47884283065795896\n",
      "Training iteration: 778\n",
      "Validation loss (no improvement): 0.39805610179901124\n",
      "Training iteration: 779\n",
      "Validation loss (no improvement): 0.21628971099853517\n",
      "Training iteration: 780\n",
      "Validation loss (no improvement): 0.22111172676086427\n",
      "Training iteration: 781\n",
      "Validation loss (no improvement): 0.5151127815246582\n",
      "Training iteration: 782\n",
      "Validation loss (no improvement): 0.17384999990463257\n",
      "Training iteration: 783\n",
      "Validation loss (no improvement): 0.3124632596969604\n",
      "Training iteration: 784\n",
      "Validation loss (no improvement): 0.17320096492767334\n",
      "Training iteration: 785\n",
      "Validation loss (no improvement): 0.17571430206298827\n",
      "Training iteration: 786\n",
      "Validation loss (no improvement): 1.192964744567871\n",
      "Training iteration: 787\n",
      "Validation loss (no improvement): 0.21867477893829346\n",
      "Training iteration: 788\n",
      "Validation loss (no improvement): 1.4643648147583008\n",
      "Training iteration: 789\n",
      "Validation loss (no improvement): 0.602438497543335\n",
      "Training iteration: 790\n",
      "Validation loss (no improvement): 0.5712624549865722\n",
      "Training iteration: 791\n",
      "Validation loss (no improvement): 0.5803235530853271\n",
      "Training iteration: 792\n",
      "Validation loss (no improvement): 0.27911303043365476\n",
      "Training iteration: 793\n",
      "Validation loss (no improvement): 0.24939770698547364\n",
      "Training iteration: 794\n",
      "Validation loss (no improvement): 0.4364619255065918\n",
      "Training iteration: 795\n",
      "Validation loss (no improvement): 0.2748842239379883\n",
      "Training iteration: 796\n",
      "Validation loss (no improvement): 0.3281308650970459\n",
      "Training iteration: 797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.37961678504943847\n",
      "Training iteration: 798\n",
      "Validation loss (no improvement): 0.18757808208465576\n",
      "Training iteration: 799\n",
      "Validation loss (no improvement): 0.14652234315872192\n",
      "Training iteration: 800\n",
      "Validation loss (no improvement): 0.13688349723815918\n",
      "Training iteration: 801\n",
      "Validation loss (no improvement): 0.1223940134048462\n",
      "Training iteration: 802\n",
      "Validation loss (no improvement): 0.18555240631103515\n",
      "Training iteration: 803\n",
      "Validation loss (no improvement): 0.42608232498168946\n",
      "Training iteration: 804\n",
      "Validation loss (no improvement): 0.2739758253097534\n",
      "Training iteration: 805\n",
      "Validation loss (no improvement): 0.3358618974685669\n",
      "Training iteration: 806\n",
      "Validation loss (no improvement): 1.127711868286133\n",
      "Training iteration: 807\n",
      "Validation loss (no improvement): 0.18961198329925538\n",
      "Training iteration: 808\n",
      "Validation loss (no improvement): 0.20584166049957275\n",
      "Training iteration: 809\n",
      "Validation loss (no improvement): 0.3125256776809692\n",
      "Training iteration: 810\n",
      "Validation loss (no improvement): 0.20966410636901855\n",
      "Training iteration: 811\n",
      "Validation loss (no improvement): 0.31219370365142823\n",
      "Training iteration: 812\n",
      "Validation loss (no improvement): 0.4566277027130127\n",
      "Training iteration: 813\n",
      "Validation loss (no improvement): 2.115805244445801\n",
      "Training iteration: 814\n",
      "Validation loss (no improvement): 0.28128321170806886\n",
      "Training iteration: 815\n",
      "Validation loss (no improvement): 1.1539212226867677\n",
      "Training iteration: 816\n",
      "Validation loss (no improvement): 0.37937188148498535\n",
      "Training iteration: 817\n",
      "Validation loss (no improvement): 0.13226083517074586\n",
      "Training iteration: 818\n",
      "Validation loss (no improvement): 0.588299560546875\n",
      "Training iteration: 819\n",
      "Validation loss (no improvement): 0.5547863006591797\n",
      "Training iteration: 820\n",
      "Validation loss (no improvement): 0.5022659778594971\n",
      "Training iteration: 821\n",
      "Validation loss (no improvement): 0.40416269302368163\n",
      "Training iteration: 822\n",
      "Validation loss (no improvement): 0.5194972038269043\n",
      "Training iteration: 823\n",
      "Validation loss (no improvement): 0.7381660461425781\n",
      "Training iteration: 824\n",
      "Validation loss (no improvement): 0.16017587184906007\n",
      "Training iteration: 825\n",
      "Validation loss (no improvement): 0.19942623376846313\n",
      "Training iteration: 826\n",
      "Validation loss (no improvement): 0.1890789747238159\n",
      "Training iteration: 827\n",
      "Validation loss (no improvement): 0.5145938873291016\n",
      "Training iteration: 828\n",
      "Validation loss (no improvement): 0.3691342115402222\n",
      "Training iteration: 829\n",
      "Validation loss (no improvement): 0.3065199851989746\n",
      "Training iteration: 830\n",
      "Validation loss (no improvement): 0.12320337295532227\n",
      "Training iteration: 831\n",
      "Validation loss (no improvement): 0.16948140859603883\n",
      "Training iteration: 832\n",
      "Validation loss (no improvement): 0.2587914943695068\n",
      "Training iteration: 833\n",
      "Validation loss (no improvement): 0.3691419124603271\n",
      "Training iteration: 834\n",
      "Validation loss (no improvement): 0.2738192081451416\n",
      "Training iteration: 835\n",
      "Validation loss (no improvement): 0.7499948024749756\n",
      "Training iteration: 836\n",
      "Validation loss (no improvement): 0.5039067268371582\n",
      "Training iteration: 837\n",
      "Validation loss (no improvement): 0.309271502494812\n",
      "Training iteration: 838\n",
      "Validation loss (no improvement): 0.3060049057006836\n",
      "Training iteration: 839\n",
      "Validation loss (no improvement): 0.6016207695007324\n",
      "Training iteration: 840\n",
      "Validation loss (no improvement): 0.3914205074310303\n",
      "Training iteration: 841\n",
      "Validation loss (no improvement): 0.21054205894470215\n",
      "Training iteration: 842\n",
      "Validation loss (no improvement): 0.9129566192626953\n",
      "Training iteration: 843\n",
      "Validation loss (no improvement): 0.25064613819122317\n",
      "Training iteration: 844\n",
      "Validation loss (no improvement): 0.27078590393066404\n",
      "Training iteration: 845\n",
      "Validation loss (no improvement): 0.3082128047943115\n",
      "Training iteration: 846\n",
      "Validation loss (no improvement): 0.3497276306152344\n",
      "Training iteration: 847\n",
      "Validation loss (no improvement): 0.27850680351257323\n",
      "Training iteration: 848\n",
      "Validation loss (no improvement): 0.8000539779663086\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.08914044499397278  to: 0.08854423761367798\n",
      "Training iteration: 850\n",
      "Validation loss (no improvement): 0.4149938583374023\n",
      "Training iteration: 851\n",
      "Validation loss (no improvement): 0.2349327325820923\n",
      "Training iteration: 852\n",
      "Validation loss (no improvement): 0.27122886180877687\n",
      "Training iteration: 853\n",
      "Validation loss (no improvement): 0.27616584300994873\n",
      "Training iteration: 854\n",
      "Validation loss (no improvement): 0.8694547653198242\n",
      "Training iteration: 855\n",
      "Validation loss (no improvement): 0.20962531566619874\n",
      "Training iteration: 856\n",
      "Validation loss (no improvement): 0.20746102333068847\n",
      "Training iteration: 857\n",
      "Validation loss (no improvement): 0.22021732330322266\n",
      "Training iteration: 858\n",
      "Validation loss (no improvement): 0.21709208488464354\n",
      "Training iteration: 859\n",
      "Validation loss (no improvement): 0.8904693603515625\n",
      "Training iteration: 860\n",
      "Validation loss (no improvement): 0.3326153516769409\n",
      "Training iteration: 861\n",
      "Validation loss (no improvement): 0.21659209728240966\n",
      "Training iteration: 862\n",
      "Validation loss (no improvement): 0.3851050138473511\n",
      "Training iteration: 863\n",
      "Validation loss (no improvement): 0.12944430112838745\n",
      "Training iteration: 864\n",
      "Validation loss (no improvement): 0.744773006439209\n",
      "Training iteration: 865\n",
      "Validation loss (no improvement): 0.3564403533935547\n",
      "Training iteration: 866\n",
      "Validation loss (no improvement): 0.46030101776123045\n",
      "Training iteration: 867\n",
      "Validation loss (no improvement): 0.1448573350906372\n",
      "Training iteration: 868\n",
      "Validation loss (no improvement): 0.5774779319763184\n",
      "Training iteration: 869\n",
      "Validation loss (no improvement): 0.14551804065704346\n",
      "Training iteration: 870\n",
      "Validation loss (no improvement): 0.39518303871154786\n",
      "Training iteration: 871\n",
      "Validation loss (no improvement): 0.14040935039520264\n",
      "Training iteration: 872\n",
      "Validation loss (no improvement): 0.31454493999481203\n",
      "Training iteration: 873\n",
      "Validation loss (no improvement): 0.09974930882453918\n",
      "Training iteration: 874\n",
      "Validation loss (no improvement): 1.0967307090759277\n",
      "Training iteration: 875\n",
      "Validation loss (no improvement): 0.2569313049316406\n",
      "Training iteration: 876\n",
      "Validation loss (no improvement): 0.2805637359619141\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.08854423761367798  to: 0.048391562700271604\n",
      "Training iteration: 878\n",
      "Validation loss (no improvement): 0.16020673513412476\n",
      "Training iteration: 879\n",
      "Validation loss (no improvement): 0.6936952114105225\n",
      "Training iteration: 880\n",
      "Validation loss (no improvement): 0.3150728940963745\n",
      "Training iteration: 881\n",
      "Validation loss (no improvement): 0.2194849967956543\n",
      "Training iteration: 882\n",
      "Validation loss (no improvement): 0.7601665496826172\n",
      "Training iteration: 883\n",
      "Validation loss (no improvement): 0.18586139678955077\n",
      "Training iteration: 884\n",
      "Validation loss (no improvement): 1.018127155303955\n",
      "Training iteration: 885\n",
      "Validation loss (no improvement): 0.2562330961227417\n",
      "Training iteration: 886\n",
      "Validation loss (no improvement): 0.5321721076965332\n",
      "Training iteration: 887\n",
      "Validation loss (no improvement): 0.24309000968933106\n",
      "Training iteration: 888\n",
      "Validation loss (no improvement): 0.29381527900695803\n",
      "Training iteration: 889\n",
      "Validation loss (no improvement): 0.35844085216522215\n",
      "Training iteration: 890\n",
      "Validation loss (no improvement): 0.28973495960235596\n",
      "Training iteration: 891\n",
      "Validation loss (no improvement): 0.2141176223754883\n",
      "Training iteration: 892\n",
      "Validation loss (no improvement): 0.32156569957733155\n",
      "Training iteration: 893\n",
      "Validation loss (no improvement): 0.48537459373474123\n",
      "Training iteration: 894\n",
      "Validation loss (no improvement): 0.30105721950531006\n",
      "Training iteration: 895\n",
      "Validation loss (no improvement): 0.8932546615600586\n",
      "Training iteration: 896\n",
      "Validation loss (no improvement): 0.21486687660217285\n",
      "Training iteration: 897\n",
      "Validation loss (no improvement): 0.09691910743713379\n",
      "Training iteration: 898\n",
      "Validation loss (no improvement): 0.42304315567016604\n",
      "Training iteration: 899\n",
      "Validation loss (no improvement): 0.5195194721221924\n",
      "Training iteration: 900\n",
      "Validation loss (no improvement): 0.24203963279724122\n",
      "Training iteration: 901\n",
      "Validation loss (no improvement): 0.19757617712020875\n",
      "Training iteration: 902\n",
      "Validation loss (no improvement): 0.20200059413909913\n",
      "Training iteration: 903\n",
      "Validation loss (no improvement): 0.21508898735046386\n",
      "Training iteration: 904\n",
      "Validation loss (no improvement): 0.13207502365112306\n",
      "Training iteration: 905\n",
      "Validation loss (no improvement): 0.12326900959014893\n",
      "Training iteration: 906\n",
      "Validation loss (no improvement): 0.42324438095092776\n",
      "Training iteration: 907\n",
      "Validation loss (no improvement): 0.2174689531326294\n",
      "Training iteration: 908\n",
      "Validation loss (no improvement): 0.19430668354034425\n",
      "Training iteration: 909\n",
      "Validation loss (no improvement): 0.1914054274559021\n",
      "Training iteration: 910\n",
      "Validation loss (no improvement): 0.09024902582168579\n",
      "Training iteration: 911\n",
      "Validation loss (no improvement): 0.7519718170166015\n",
      "Training iteration: 912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.31925458908081056\n",
      "Training iteration: 913\n",
      "Validation loss (no improvement): 0.4730783462524414\n",
      "Training iteration: 914\n",
      "Validation loss (no improvement): 0.4233663082122803\n",
      "Training iteration: 915\n",
      "Validation loss (no improvement): 0.21272616386413573\n",
      "Training iteration: 916\n",
      "Validation loss (no improvement): 0.390319299697876\n",
      "Training iteration: 917\n",
      "Validation loss (no improvement): 0.17499539852142335\n",
      "Training iteration: 918\n",
      "Validation loss (no improvement): 1.1335383415222169\n",
      "Training iteration: 919\n",
      "Validation loss (no improvement): 0.11428571939468384\n",
      "Training iteration: 920\n",
      "Validation loss (no improvement): 0.48311362266540525\n",
      "Training iteration: 921\n",
      "Validation loss (no improvement): 0.236694860458374\n",
      "Training iteration: 922\n",
      "Validation loss (no improvement): 0.24147138595581055\n",
      "Training iteration: 923\n",
      "Validation loss (no improvement): 0.4082831382751465\n",
      "Training iteration: 924\n",
      "Validation loss (no improvement): 0.4666707992553711\n",
      "Training iteration: 925\n",
      "Validation loss (no improvement): 0.6466856479644776\n",
      "Training iteration: 926\n",
      "Validation loss (no improvement): 0.11307402849197387\n",
      "Training iteration: 927\n",
      "Validation loss (no improvement): 0.05942007899284363\n",
      "Training iteration: 928\n",
      "Validation loss (no improvement): 0.08248463869094849\n",
      "Training iteration: 929\n",
      "Validation loss (no improvement): 0.08980472683906555\n",
      "Training iteration: 930\n",
      "Validation loss (no improvement): 0.12106330394744873\n",
      "Training iteration: 931\n",
      "Validation loss (no improvement): 0.9165521621704101\n",
      "Training iteration: 932\n",
      "Validation loss (no improvement): 0.4477218151092529\n",
      "Training iteration: 933\n",
      "Validation loss (no improvement): 1.3997400283813477\n",
      "Training iteration: 934\n",
      "Validation loss (no improvement): 0.10201337337493896\n",
      "Training iteration: 935\n",
      "Validation loss (no improvement): 0.2585299491882324\n",
      "Training iteration: 936\n",
      "Validation loss (no improvement): 0.43136253356933596\n",
      "Training iteration: 937\n",
      "Validation loss (no improvement): 0.3177491664886475\n",
      "Training iteration: 938\n",
      "Validation loss (no improvement): 0.18135697841644288\n",
      "Training iteration: 939\n",
      "Validation loss (no improvement): 0.2393787384033203\n",
      "Training iteration: 940\n",
      "Validation loss (no improvement): 1.2093095779418945\n",
      "Training iteration: 941\n",
      "Validation loss (no improvement): 0.40944876670837405\n",
      "Training iteration: 942\n",
      "Validation loss (no improvement): 0.08236079216003418\n",
      "Training iteration: 943\n",
      "Validation loss (no improvement): 0.5263817787170411\n",
      "Training iteration: 944\n",
      "Validation loss (no improvement): 0.15860130786895751\n",
      "Training iteration: 945\n",
      "Validation loss (no improvement): 0.2832325458526611\n",
      "Training iteration: 946\n",
      "Validation loss (no improvement): 0.40742011070251466\n",
      "Training iteration: 947\n",
      "Validation loss (no improvement): 0.29793567657470704\n",
      "Training iteration: 948\n",
      "Validation loss (no improvement): 0.19569065570831298\n",
      "Training iteration: 949\n",
      "Validation loss (no improvement): 0.3352424144744873\n",
      "Training iteration: 950\n",
      "Validation loss (no improvement): 0.1525510787963867\n",
      "Training iteration: 951\n",
      "Validation loss (no improvement): 0.34865403175354004\n",
      "Training iteration: 952\n",
      "Validation loss (no improvement): 0.164736008644104\n",
      "Training iteration: 953\n",
      "Validation loss (no improvement): 0.17226598262786866\n",
      "Training iteration: 954\n",
      "Validation loss (no improvement): 0.39557483196258547\n",
      "Training iteration: 955\n",
      "Validation loss (no improvement): 0.3452263593673706\n",
      "Training iteration: 956\n",
      "Validation loss (no improvement): 0.8934854507446289\n",
      "Training iteration: 957\n",
      "Validation loss (no improvement): 0.20605244636535644\n",
      "Training iteration: 958\n",
      "Validation loss (no improvement): 0.6685303688049317\n",
      "Training iteration: 959\n",
      "Validation loss (no improvement): 0.47483010292053224\n",
      "Training iteration: 960\n",
      "Validation loss (no improvement): 0.24387211799621583\n",
      "Training iteration: 961\n",
      "Validation loss (no improvement): 0.1793531060218811\n",
      "Training iteration: 962\n",
      "Validation loss (no improvement): 0.15648181438446046\n",
      "Training iteration: 963\n",
      "Validation loss (no improvement): 0.07916259169578552\n",
      "Training iteration: 964\n",
      "Validation loss (no improvement): 0.49994306564331054\n",
      "Training iteration: 965\n",
      "Validation loss (no improvement): 0.20558159351348876\n",
      "Training iteration: 966\n",
      "Validation loss (no improvement): 0.21872267723083497\n",
      "Training iteration: 967\n",
      "Validation loss (no improvement): 0.23427529335021974\n",
      "Training iteration: 968\n",
      "Validation loss (no improvement): 0.08260771632194519\n",
      "Training iteration: 969\n",
      "Validation loss (no improvement): 0.31443607807159424\n",
      "Training iteration: 970\n",
      "Validation loss (no improvement): 0.1843758225440979\n",
      "Training iteration: 971\n",
      "Validation loss (no improvement): 1.3305822372436524\n",
      "Training iteration: 972\n",
      "Validation loss (no improvement): 0.28917946815490725\n",
      "Training iteration: 973\n",
      "Validation loss (no improvement): 0.19310554265975952\n",
      "Training iteration: 974\n",
      "Validation loss (no improvement): 0.1825728178024292\n",
      "Training iteration: 975\n",
      "Validation loss (no improvement): 0.635254955291748\n",
      "Training iteration: 976\n",
      "Validation loss (no improvement): 0.2913954496383667\n",
      "Training iteration: 977\n",
      "Validation loss (no improvement): 0.2301104784011841\n"
     ]
    }
   ],
   "source": [
    "dropout_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 11.703658294677734\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 11.703658294677734  to: 8.950230407714844\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 8.950230407714844  to: 6.909260559082031\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 6.909260559082031  to: 5.39338493347168\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 5.39338493347168  to: 4.293907546997071\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 4.293907546997071  to: 3.4594615936279296\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 3.4594615936279296  to: 2.8196857452392576\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 2.8196857452392576  to: 2.324397087097168\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 2.324397087097168  to: 1.9301166534423828\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.9301166534423828  to: 1.6144500732421876\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.6144500732421876  to: 1.3597905158996582\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 1.3597905158996582  to: 1.153759765625\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.153759765625  to: 0.9865246772766113\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.9865246772766113  to: 0.8503961563110352\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.8503961563110352  to: 0.7391556739807129\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.7391556739807129  to: 0.6479999542236328\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.6479999542236328  to: 0.5729885101318359\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.5729885101318359  to: 0.5111930847167969\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.5111930847167969  to: 0.46001949310302737\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.46001949310302737  to: 0.4174381732940674\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.4174381732940674  to: 0.3818331480026245\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.3818331480026245  to: 0.3519749164581299\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.3519749164581299  to: 0.32672092914581297\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.32672092914581297  to: 0.3052481412887573\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.3052481412887573  to: 0.2869203329086304\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.2869203329086304  to: 0.2712165117263794\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.2712165117263794  to: 0.2577093839645386\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.2577093839645386  to: 0.24604613780975343\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.24604613780975343  to: 0.23593907356262206\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.23593907356262206  to: 0.22714786529541015\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.22714786529541015  to: 0.2194741487503052\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.2194741487503052  to: 0.21275160312652588\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.21275160312652588  to: 0.20684237480163575\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.20684237480163575  to: 0.20163052082061766\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.20163052082061766  to: 0.19701833724975587\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.19701833724975587  to: 0.19292287826538085\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.19292287826538085  to: 0.18927459716796874\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.18927459716796874  to: 0.18601446151733397\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.18601446151733397  to: 0.18309221267700196\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.18309221267700196  to: 0.18046481609344484\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.18046481609344484  to: 0.1780955672264099\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.1780955672264099  to: 0.17595307826995848\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.17595307826995848  to: 0.1740102529525757\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1740102529525757  to: 0.17224349975585937\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.17224349975585937  to: 0.1706321120262146\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.1706321120262146  to: 0.1691584587097168\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.1691584587097168  to: 0.167807674407959\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.167807674407959  to: 0.16656605005264283\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.16656605005264283  to: 0.16542171239852904\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.16542171239852904  to: 0.16436464786529542\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.16436464786529542  to: 0.16338558197021485\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.16338558197021485  to: 0.1624765634536743\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.1624765634536743  to: 0.1616305947303772\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.1616305947303772  to: 0.16084150075912476\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.16084150075912476  to: 0.16010383367538453\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.16010383367538453  to: 0.1594127893447876\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.1594127893447876  to: 0.15876398086547852\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.15876398086547852  to: 0.15815360546112062\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.15815360546112062  to: 0.1575782537460327\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.1575782537460327  to: 0.15703483819961547\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.15703483819961547  to: 0.15652060508728027\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.15652060508728027  to: 0.1560332179069519\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.1560332179069519  to: 0.1555704116821289\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.1555704116821289  to: 0.15513017177581787\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.15513017177581787  to: 0.15471073389053344\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.15471073389053344  to: 0.15431047677993776\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.15431047677993776  to: 0.15392796993255614\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.15392796993255614  to: 0.15356190204620362\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.15356190204620362  to: 0.15321109294891358\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.15321109294891358  to: 0.15287444591522217\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.15287444591522217  to: 0.15255101919174194\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.15255101919174194  to: 0.1522399067878723\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.1522399067878723  to: 0.15194031000137329\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.15194031000137329  to: 0.15165125131607055\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.15165125131607055  to: 0.15137217044830323\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.15137217044830323  to: 0.15110259056091307\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.15110259056091307  to: 0.15084196329116822\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.15084196329116822  to: 0.15058975219726561\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.15058975219726561  to: 0.15034517049789428\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.15034517049789428  to: 0.15010746717453002\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.15010746717453002  to: 0.14987690448760987\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14987690448760987  to: 0.14965307712554932\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.14965307712554932  to: 0.1494356870651245\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1494356870651245  to: 0.1492244005203247\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.1492244005203247  to: 0.14901889562606813\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.14901889562606813  to: 0.1488189458847046\n",
      "Training iteration: 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1488189458847046  to: 0.1486242890357971\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1486242890357971  to: 0.14843467473983765\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14843467473983765  to: 0.14824988842010497\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.14824988842010497  to: 0.14806973934173584\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.14806973934173584  to: 0.147894024848938\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.147894024848938  to: 0.1477225661277771\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1477225661277771  to: 0.14755523204803467\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14755523204803467  to: 0.1473918080329895\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.1473918080329895  to: 0.14723216295242308\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14723216295242308  to: 0.14707620143890382\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.14707620143890382  to: 0.14692370891571044\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14692370891571044  to: 0.1467746376991272\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.1467746376991272  to: 0.14662882089614868\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14662882089614868  to: 0.14648613929748536\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14648613929748536  to: 0.1463465452194214\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.1463465452194214  to: 0.14620987176895142\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.14620987176895142  to: 0.14607608318328857\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14607608318328857  to: 0.1459450602531433\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.1459450602531433  to: 0.14581673145294188\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14581673145294188  to: 0.14569100141525268\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14569100141525268  to: 0.14556782245635985\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14556782245635985  to: 0.1454470634460449\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1454470634460449  to: 0.14532868862152098\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14532868862152098  to: 0.14521262645721436\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14521262645721436  to: 0.14509881734848024\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14509881734848024  to: 0.14498717784881593\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14498717784881593  to: 0.1448776602745056\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.1448776602745056  to: 0.14477020502090454\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14477020502090454  to: 0.14466432332992554\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14466432332992554  to: 0.14456020593643187\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14456020593643187  to: 0.14445797204971314\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14445797204971314  to: 0.14435758590698242\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14435758590698242  to: 0.14425899982452392\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14425899982452392  to: 0.14416216611862182\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14416216611862182  to: 0.14406702518463135\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14406702518463135  to: 0.14397355318069457\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14397355318069457  to: 0.1438816785812378\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.1438816785812378  to: 0.14379141330718995\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14379141330718995  to: 0.14370275735855104\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14370275735855104  to: 0.14361556768417358\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14361556768417358  to: 0.14352984428405763\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14352984428405763  to: 0.14344552755355836\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14344552755355836  to: 0.14336258172988892\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.14336258172988892  to: 0.1432809829711914\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.1432809829711914  to: 0.14320067167282105\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14320067167282105  to: 0.14312163591384888\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14312163591384888  to: 0.14304383993148803\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14304383993148803  to: 0.14296724796295165\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14296724796295165  to: 0.14289183616638185\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14289183616638185  to: 0.1428175687789917\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.1428175687789917  to: 0.14274442195892334\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14274442195892334  to: 0.14267234802246093\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14267234802246093  to: 0.14260133504867553\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14260133504867553  to: 0.14253135919570922\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14253135919570922  to: 0.1424623966217041\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.1424623966217041  to: 0.1423943519592285\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1423943519592285  to: 0.1423271656036377\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.1423271656036377  to: 0.14226090908050537\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14226090908050537  to: 0.14219554662704467\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14219554662704467  to: 0.1421310782432556\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.1421310782432556  to: 0.14206746816635132\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14206746816635132  to: 0.14200470447540284\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14200470447540284  to: 0.1419427514076233\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1419427514076233  to: 0.14188159704208375\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14188159704208375  to: 0.14182122945785522\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14182122945785522  to: 0.14176160097122192\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14176160097122192  to: 0.1417027235031128\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.1417027235031128  to: 0.14164457321166993\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14164457321166993  to: 0.1415871262550354\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.1415871262550354  to: 0.1415303587913513\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.1415303587913513  to: 0.14147427082061767\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14147427082061767  to: 0.14141883850097656\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14141883850097656  to: 0.1413640260696411\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.1413640260696411  to: 0.14130985736846924\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14130985736846924  to: 0.1412562608718872\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.1412562608718872  to: 0.14120328426361084\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14120328426361084  to: 0.14115089178085327\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14115089178085327  to: 0.14109904766082765\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14109904766082765  to: 0.14104776382446288\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14104776382446288  to: 0.14099700450897218\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14099700450897218  to: 0.14094675779342652\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.14094675779342652  to: 0.14089703559875488\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.14089703559875488  to: 0.1408478021621704\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1408478021621704  to: 0.1407990574836731\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.1407990574836731  to: 0.14075078964233398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14075078964233398  to: 0.14070297479629518\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14070297479629518  to: 0.14065558910369874\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14065558910369874  to: 0.14060862064361573\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14060862064361573  to: 0.1405620574951172\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.1405620574951172  to: 0.14051588773727416\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14051588773727416  to: 0.14047012329101563\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14047012329101563  to: 0.1404247522354126\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1404247522354126  to: 0.14037973880767823\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14037973880767823  to: 0.1403351068496704\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.1403351068496704  to: 0.14029080867767335\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14029080867767335  to: 0.14024689197540283\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14024689197540283  to: 0.14020328521728515\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14020328521728515  to: 0.14016004800796508\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14016004800796508  to: 0.1401171088218689\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.1401171088218689  to: 0.14007450342178346\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.14007450342178346  to: 0.14003220796585084\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.14003220796585084  to: 0.13999019861221312\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13999019861221312  to: 0.13994851112365722\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13994851112365722  to: 0.13990710973739623\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.13990710973739623  to: 0.1398659825325012\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.1398659825325012  to: 0.13982514142990113\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13982514142990113  to: 0.13978456258773803\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13978456258773803  to: 0.13974425792694092\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13974425792694092  to: 0.13970422744750977\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13970422744750977  to: 0.13966442346572877\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13966442346572877  to: 0.13962486982345582\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13962486982345582  to: 0.13958556652069093\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.13958556652069093  to: 0.13954650163650512\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.13954650163650512  to: 0.13950765132904053\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13950765132904053  to: 0.13946902751922607\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13946902751922607  to: 0.13943064212799072\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13943064212799072  to: 0.13939244747161866\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13939244747161866  to: 0.13935447931289674\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.13935447931289674  to: 0.13931671380996705\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13931671380996705  to: 0.13927913904190065\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13927913904190065  to: 0.13924176692962648\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13924176692962648  to: 0.13920458555221557\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13920458555221557  to: 0.13916759490966796\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13916759490966796  to: 0.1391307830810547\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.1391307830810547  to: 0.13909412622451783\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.13909412622451783  to: 0.13905760049819946\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.13905760049819946  to: 0.13902124166488647\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.13902124166488647  to: 0.13898504972457887\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13898504972457887  to: 0.1389490246772766\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.1389490246772766  to: 0.13891317844390869\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13891317844390869  to: 0.1388774633407593\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.1388774633407593  to: 0.1388419270515442\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.1388419270515442  to: 0.13880650997161864\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.13880650997161864  to: 0.13877127170562745\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.13877127170562745  to: 0.13873616456985474\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13873616456985474  to: 0.13870121240615846\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13870121240615846  to: 0.1386663794517517\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.1386663794517517  to: 0.1386317014694214\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1386317014694214  to: 0.13859713077545166\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13859713077545166  to: 0.1385627031326294\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.1385627031326294  to: 0.1385284185409546\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.1385284185409546  to: 0.13849424123764037\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13849424123764037  to: 0.13846020698547362\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13846020698547362  to: 0.13842628002166749\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13842628002166749  to: 0.13839247226715087\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13839247226715087  to: 0.13835878372192384\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.13835878372192384  to: 0.13832520246505736\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.13832520246505736  to: 0.13829174041748046\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13829174041748046  to: 0.13825838565826415\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13825838565826415  to: 0.13822513818740845\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.13822513818740845  to: 0.13819199800491333\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13819199800491333  to: 0.1381589651107788\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1381589651107788  to: 0.13812601566314697\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13812601566314697  to: 0.1380931854248047\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.1380931854248047  to: 0.13806045055389404\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13806045055389404  to: 0.138027822971344\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.138027822971344  to: 0.13799526691436767\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13799526691436767  to: 0.13796284198760986\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13796284198760986  to: 0.13793047666549682\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13793047666549682  to: 0.1378982186317444\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.1378982186317444  to: 0.13786604404449462\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13786604404449462  to: 0.13783395290374756\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.13783395290374756  to: 0.13780196905136108\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13780196905136108  to: 0.13777004480361937\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13777004480361937  to: 0.13773821592330932\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13773821592330932  to: 0.1377064824104309\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.1377064824104309  to: 0.13767483234405517\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13767483234405517  to: 0.13764325380325318\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13764325380325318  to: 0.13761175870895387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13761175870895387  to: 0.13758035898208618\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13758035898208618  to: 0.1375490188598633\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1375490188598633  to: 0.13751776218414308\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13751776218414308  to: 0.13748658895492555\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13748658895492555  to: 0.1374554991722107\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.1374554991722107  to: 0.1374244809150696\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.1374244809150696  to: 0.1373935341835022\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1373935341835022  to: 0.1373626708984375\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.1373626708984375  to: 0.13733186721801757\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13733186721801757  to: 0.13730113506317138\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13730113506317138  to: 0.1372704863548279\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.1372704863548279  to: 0.1372399091720581\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.1372399091720581  to: 0.13720940351486205\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.13720940351486205  to: 0.13717896938323976\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13717896938323976  to: 0.1371485948562622\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.1371485948562622  to: 0.13711830377578735\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13711830377578735  to: 0.1370880722999573\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1370880722999573  to: 0.13705791234970094\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13705791234970094  to: 0.1370278239250183\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.1370278239250183  to: 0.1369978070259094\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.1369978070259094  to: 0.13696784973144532\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13696784973144532  to: 0.13693795204162598\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13693795204162598  to: 0.13690812587738038\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13690812587738038  to: 0.1368783712387085\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1368783712387085  to: 0.13684866428375245\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13684866428375245  to: 0.13681905269622802\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13681905269622802  to: 0.13678946495056152\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13678946495056152  to: 0.13675997257232667\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13675997257232667  to: 0.13673051595687866\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13673051595687866  to: 0.13670114278793336\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13670114278793336  to: 0.1366718292236328\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.1366718292236328  to: 0.13664257526397705\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13664257526397705  to: 0.13661338090896608\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13661338090896608  to: 0.13658425807952881\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13658425807952881  to: 0.13655517101287842\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13655517101287842  to: 0.1365261673927307\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.1365261673927307  to: 0.13649723529815674\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13649723529815674  to: 0.13646833896636962\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13646833896636962  to: 0.13643951416015626\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13643951416015626  to: 0.13641073703765869\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13641073703765869  to: 0.13638203144073485\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13638203144073485  to: 0.13635339736938476\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13635339736938476  to: 0.13632481098175048\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13632481098175048  to: 0.13629629611968994\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13629629611968994  to: 0.13626781702041627\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13626781702041627  to: 0.1362394094467163\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.1362394094467163  to: 0.1362110495567322\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.1362110495567322  to: 0.13618276119232178\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13618276119232178  to: 0.13615453243255615\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13615453243255615  to: 0.13612637519836426\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13612637519836426  to: 0.13609826564788818\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13609826564788818  to: 0.13607022762298585\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13607022762298585  to: 0.13604220151901245\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13604220151901245  to: 0.13601417541503907\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13601417541503907  to: 0.13598620891571045\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13598620891571045  to: 0.13595831394195557\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13595831394195557  to: 0.1359304666519165\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.1359304666519165  to: 0.13590269088745116\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13590269088745116  to: 0.13587496280670167\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13587496280670167  to: 0.13584729433059692\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13584729433059692  to: 0.1358196973800659\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.1358196973800659  to: 0.13579214811325074\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13579214811325074  to: 0.13576465845108032\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13576465845108032  to: 0.13573722839355468\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13573722839355468  to: 0.13570988178253174\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13570988178253174  to: 0.1356825590133667\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.1356825590133667  to: 0.13565530776977539\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13565530776977539  to: 0.13562811613082887\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13562811613082887  to: 0.13560099601745607\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13560099601745607  to: 0.13557392358779907\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13557392358779907  to: 0.13554691076278685\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13554691076278685  to: 0.13551994562149047\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13551994562149047  to: 0.13549305200576783\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13549305200576783  to: 0.13546621799468994\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13546621799468994  to: 0.1354394316673279\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.1354394316673279  to: 0.13541271686553955\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13541271686553955  to: 0.135386061668396\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.135386061668396  to: 0.13535945415496825\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13535945415496825  to: 0.13533291816711426\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13533291816711426  to: 0.13530642986297609\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13530642986297609  to: 0.13528001308441162\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13528001308441162  to: 0.135253643989563\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.135253643989563  to: 0.13522733449935914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13522733449935914  to: 0.13520108461380004\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13520108461380004  to: 0.1351749062538147\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.1351749062538147  to: 0.13514878749847412\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13514878749847412  to: 0.13512274026870727\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13512274026870727  to: 0.13509674072265626\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13509674072265626  to: 0.13507080078125\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13507080078125  to: 0.13504493236541748\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13504493236541748  to: 0.13501912355422974\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13501912355422974  to: 0.13499338626861573\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13499338626861573  to: 0.13496768474578857\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13496768474578857  to: 0.13494205474853516\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13494205474853516  to: 0.13491647243499755\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13491647243499755  to: 0.13489097356796265\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13489097356796265  to: 0.13486553430557252\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.13486553430557252  to: 0.1348401427268982\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.1348401427268982  to: 0.13481481075286866\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13481481075286866  to: 0.13478955030441284\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13478955030441284  to: 0.13476433753967285\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.13476433753967285  to: 0.1347391963005066\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.1347391963005066  to: 0.1347141146659851\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1347141146659851  to: 0.13468908071517943\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13468908071517943  to: 0.1346641182899475\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.1346641182899475  to: 0.1346392035484314\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1346392035484314  to: 0.13461436033248902\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13461436033248902  to: 0.1345895767211914\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.1345895767211914  to: 0.13456485271453858\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13456485271453858  to: 0.13454020023345947\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.13454020023345947  to: 0.1345155954360962\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.1345155954360962  to: 0.13449103832244874\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13449103832244874  to: 0.1344665765762329\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.1344665765762329  to: 0.13444215059280396\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.13444215059280396  to: 0.13441779613494872\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13441779613494872  to: 0.13439348936080933\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.13439348936080933  to: 0.13436925411224365\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.13436925411224365  to: 0.13434504270553588\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13434504270553588  to: 0.134320867061615\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.134320867061615  to: 0.1342967629432678\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.1342967629432678  to: 0.13427270650863649\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13427270650863649  to: 0.1342487096786499\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1342487096786499  to: 0.13422478437423707\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13422478437423707  to: 0.13420093059539795\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13420093059539795  to: 0.1341771364212036\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.1341771364212036  to: 0.1341533899307251\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.1341533899307251  to: 0.13412971496582032\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13412971496582032  to: 0.1341060996055603\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.1341060996055603  to: 0.134082567691803\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.134082567691803  to: 0.13405908346176149\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.13405908346176149  to: 0.13403565883636476\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13403565883636476  to: 0.13401230573654174\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13401230573654174  to: 0.13398902416229247\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13398902416229247  to: 0.133965802192688\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.133965802192688  to: 0.1339426279067993\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1339426279067993  to: 0.1339195489883423\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.1339195489883423  to: 0.13389651775360106\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.13389651775360106  to: 0.1338735580444336\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.1338735580444336  to: 0.13385066986083985\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13385066986083985  to: 0.13382784128189087\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13382784128189087  to: 0.13380507230758668\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13380507230758668  to: 0.1337823748588562\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.1337823748588562  to: 0.13375974893569947\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.13375974893569947  to: 0.13373717069625854\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.13373717069625854  to: 0.1337146520614624\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.1337146520614624  to: 0.13369220495224\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13369220495224  to: 0.13366982936859131\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13366982936859131  to: 0.1336475133895874\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.1336475133895874  to: 0.1336252808570862\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.1336252808570862  to: 0.13360309600830078\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13360309600830078  to: 0.13358099460601808\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13358099460601808  to: 0.1335589647293091\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.1335589647293091  to: 0.13353703022003174\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13353703022003174  to: 0.1335151195526123\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.1335151195526123  to: 0.1334933042526245\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.1334933042526245  to: 0.13347158432006836\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13347158432006836  to: 0.133449923992157\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.133449923992157  to: 0.13342832326889037\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13342832326889037  to: 0.13340680599212645\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.13340680599212645  to: 0.13338533639907837\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.13338533639907837  to: 0.13336396217346191\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13336396217346191  to: 0.13334262371063232\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13334262371063232  to: 0.13332139253616332\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.13332139253616332  to: 0.1333001971244812\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.1333001971244812  to: 0.13327908515930176\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13327908515930176  to: 0.13325803279876708\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.13325803279876708  to: 0.1332370400428772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1332370400428772  to: 0.13321611881256104\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13321611881256104  to: 0.1331952691078186\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.1331952691078186  to: 0.13317447900772095\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13317447900772095  to: 0.13315374851226808\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13315374851226808  to: 0.13313307762145996\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13313307762145996  to: 0.13311247825622557\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.13311247825622557  to: 0.13309195041656494\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13309195041656494  to: 0.1330714464187622\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1330714464187622  to: 0.1330510377883911\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1330510377883911  to: 0.13303067684173583\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.13303067684173583  to: 0.1330103874206543\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1330103874206543  to: 0.13299016952514647\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.13299016952514647  to: 0.1329699993133545\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.1329699993133545  to: 0.13294990062713624\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13294990062713624  to: 0.1329298734664917\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.1329298734664917  to: 0.13290990591049195\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13290990591049195  to: 0.13288999795913697\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13288999795913697  to: 0.13287014961242677\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.13287014961242677  to: 0.13285038471221924\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.13285038471221924  to: 0.1328306555747986\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.1328306555747986  to: 0.13281099796295165\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13281099796295165  to: 0.1327913999557495\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.1327913999557495  to: 0.132771897315979\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.132771897315979  to: 0.13275240659713744\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13275240659713744  to: 0.13273301124572753\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13273301124572753  to: 0.13271366357803344\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.13271366357803344  to: 0.13269443511962892\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13269443511962892  to: 0.13267530202865602\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13267530202865602  to: 0.1326562523841858\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.1326562523841858  to: 0.13263725042343139\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13263725042343139  to: 0.13261833190917968\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.13261833190917968  to: 0.13259947299957275\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13259947299957275  to: 0.13258068561553954\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.13258068561553954  to: 0.13256195783615113\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13256195783615113  to: 0.13254330158233643\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13254330158233643  to: 0.13252465724945067\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.13252465724945067  to: 0.13250603675842285\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13250603675842285  to: 0.1324874758720398\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1324874758720398  to: 0.13246898651123046\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.13246898651123046  to: 0.13245055675506592\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13245055675506592  to: 0.1324322462081909\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1324322462081909  to: 0.13241398334503174\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13241398334503174  to: 0.13239579200744628\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13239579200744628  to: 0.13237766027450562\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13237766027450562  to: 0.13235960006713868\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13235960006713868  to: 0.1323415994644165\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.1323415994644165  to: 0.13232367038726806\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.13232367038726806  to: 0.13230578899383544\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13230578899383544  to: 0.13228797912597656\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13228797912597656  to: 0.1322702169418335\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.1322702169418335  to: 0.1322525382041931\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.1322525382041931  to: 0.1322349190711975\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1322349190711975  to: 0.1322173595428467\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.1322173595428467  to: 0.13219985961914063\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13219985961914063  to: 0.1321824312210083\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.1321824312210083  to: 0.1321650505065918\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1321650505065918  to: 0.13214774131774903\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13214774131774903  to: 0.13213049173355101\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13213049173355101  to: 0.13211328983306886\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.13211328983306886  to: 0.1320961594581604\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.1320961594581604  to: 0.1320791006088257\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.1320791006088257  to: 0.13206207752227783\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.13206207752227783  to: 0.13204511404037475\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13204511404037475  to: 0.1320281982421875\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.1320281982421875  to: 0.13201135396957397\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.13201135396957397  to: 0.13199456930160522\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13199456930160522  to: 0.13197784423828124\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.13197784423828124  to: 0.1319611668586731\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.1319611668586731  to: 0.13194453716278076\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.13194453716278076  to: 0.1319279909133911\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.1319279909133911  to: 0.13191146850585939\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.13191146850585939  to: 0.13189502954483032\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13189502954483032  to: 0.13187861442565918\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13187861442565918  to: 0.13186228275299072\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.13186228275299072  to: 0.13184597492218017\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.13184597492218017  to: 0.13182976245880126\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.13182976245880126  to: 0.13181356191635132\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.13181356191635132  to: 0.1317974328994751\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.1317974328994751  to: 0.13178136348724365\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.13178136348724365  to: 0.13176534175872803\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13176534175872803  to: 0.13174937963485717\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.13174937963485717  to: 0.1317334771156311\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.1317334771156311  to: 0.13171761035919188\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.13171761035919188  to: 0.13170180320739747\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.13170180320739747  to: 0.13168604373931886\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.13168604373931886  to: 0.13167035579681396\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.13167035579681396  to: 0.13165470361709594\n",
      "Training iteration: 509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13165470361709594  to: 0.1316391110420227\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.1316391110420227  to: 0.13162357807159425\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.13162357807159425  to: 0.13160808086395265\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.13160808086395265  to: 0.13159265518188476\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.13159265518188476  to: 0.13157728910446168\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.13157728910446168  to: 0.1315619707107544\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.1315619707107544  to: 0.13154672384262084\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.13154672384262084  to: 0.13153154850006105\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.13153154850006105  to: 0.13151642084121704\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.13151642084121704  to: 0.13150134086608886\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.13150134086608886  to: 0.13148632049560546\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.13148632049560546  to: 0.1314713716506958\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.1314713716506958  to: 0.13145647048950196\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13145647048950196  to: 0.1314416289329529\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.1314416289329529  to: 0.1314268469810486\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.1314268469810486  to: 0.1314121127128601\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1314121127128601  to: 0.13139746189117432\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.13139746189117432  to: 0.1313828706741333\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.1313828706741333  to: 0.1313683271408081\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.1313683271408081  to: 0.13135385513305664\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.13135385513305664  to: 0.131339430809021\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.131339430809021  to: 0.13132508993148803\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.13132508993148803  to: 0.13131080865859984\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13131080865859984  to: 0.1312965750694275\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.1312965750694275  to: 0.13128241300582885\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13128241300582885  to: 0.131268310546875\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.131268310546875  to: 0.13125426769256593\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.13125426769256593  to: 0.13124027252197265\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.13124027252197265  to: 0.1312263250350952\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.1312263250350952  to: 0.1312124252319336\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.1312124252319336  to: 0.13119843006134033\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.13119843006134033  to: 0.13118438720703124\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13118438720703124  to: 0.13117038011550902\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.13117038011550902  to: 0.13115642070770264\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13115642070770264  to: 0.131142520904541\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.131142520904541  to: 0.13112865686416625\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.13112865686416625  to: 0.13111484050750732\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13111484050750732  to: 0.13110105991363524\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.13110105991363524  to: 0.131087327003479\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.131087327003479  to: 0.13107362985610962\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.13107362985610962  to: 0.131059992313385\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.131059992313385  to: 0.13104636669158937\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13104636669158937  to: 0.13103283643722535\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.13103283643722535  to: 0.13101933002471924\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13101933002471924  to: 0.1310058832168579\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.1310058832168579  to: 0.13099247217178345\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.13099247217178345  to: 0.13097909688949586\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.13097909688949586  to: 0.13096579313278198\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.13096579313278198  to: 0.13095251321792603\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.13095251321792603  to: 0.1309392809867859\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.1309392809867859  to: 0.13092615604400634\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.13092615604400634  to: 0.13091306686401366\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.13091306686401366  to: 0.13089990615844727\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13089990615844727  to: 0.1308867812156677\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.1308867812156677  to: 0.1308736801147461\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.1308736801147461  to: 0.13086063861846925\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.13086063861846925  to: 0.13084762096405028\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.13084762096405028  to: 0.1308346390724182\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.1308346390724182  to: 0.130821692943573\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.130821692943573  to: 0.1308087706565857\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1308087706565857  to: 0.13079588413238524\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.13079588413238524  to: 0.13078304529190063\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13078304529190063  to: 0.13077021837234498\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.13077021837234498  to: 0.13075743913650512\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.13075743913650512  to: 0.13074467182159424\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.13074467182159424  to: 0.13073195219039918\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.13073195219039918  to: 0.13071924448013306\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.13071924448013306  to: 0.13070658445358277\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.13070658445358277  to: 0.1306939482688904\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.1306939482688904  to: 0.1306813597679138\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1306813597679138  to: 0.1306688070297241\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.1306688070297241  to: 0.13065629005432128\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.13065629005432128  to: 0.1306437849998474\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.1306437849998474  to: 0.13063137531280516\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.13063137531280516  to: 0.13061898946762085\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.13061898946762085  to: 0.1306066632270813\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1306066632270813  to: 0.1305943489074707\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.1305943489074707  to: 0.13058207035064698\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13058207035064698  to: 0.1305698037147522\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.1305698037147522  to: 0.13055758476257323\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.13055758476257323  to: 0.1305453896522522\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.1305453896522522  to: 0.13053324222564697\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.13053324222564697  to: 0.13052107095718385\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.13052107095718385  to: 0.13050874471664428\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.13050874471664428  to: 0.1304964542388916\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.1304964542388916  to: 0.1304841995239258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 595\n",
      "Improved validation loss from: 0.1304841995239258  to: 0.13047195672988893\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.13047195672988893  to: 0.1304597496986389\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.1304597496986389  to: 0.13044755458831786\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.13044755458831786  to: 0.13043540716171265\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.13043540716171265  to: 0.13042328357696534\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.13042328357696534  to: 0.13041117191314697\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.13041117191314697  to: 0.13039907217025756\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.13039907217025756  to: 0.130387020111084\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.130387020111084  to: 0.13037502765655518\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.13037502765655518  to: 0.13036309480667113\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.13036309480667113  to: 0.13035118579864502\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.13035118579864502  to: 0.13033931255340575\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.13033931255340575  to: 0.1303274393081665\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.1303274393081665  to: 0.13031561374664308\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.13031561374664308  to: 0.13030381202697755\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.13030381202697755  to: 0.13029201030731202\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.13029201030731202  to: 0.13028026819229127\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.13028026819229127  to: 0.13026851415634155\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.13026851415634155  to: 0.1302567958831787\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.1302567958831787  to: 0.13024508953094482\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.13024508953094482  to: 0.1302334189414978\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1302334189414978  to: 0.13022174835205078\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.13022174835205078  to: 0.1302100896835327\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.1302100896835327  to: 0.1301984667778015\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.1301984667778015  to: 0.13018686771392823\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.13018686771392823  to: 0.13017524480819703\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.13017524480819703  to: 0.1301636815071106\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.1301636815071106  to: 0.13015210628509521\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.13015210628509521  to: 0.1301405668258667\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.1301405668258667  to: 0.1301290512084961\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.1301290512084961  to: 0.13011754751205445\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.13011754751205445  to: 0.1301060676574707\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.1301060676574707  to: 0.13009461164474487\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.13009461164474487  to: 0.13008317947387696\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.13008317947387696  to: 0.13007179498672486\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.13007179498672486  to: 0.13006041049957276\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.13006041049957276  to: 0.13004906177520753\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.13004906177520753  to: 0.13003772497177124\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.13003772497177124  to: 0.13002642393112182\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.13002642393112182  to: 0.13001513481140137\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.13001513481140137  to: 0.13000386953353882\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.13000386953353882  to: 0.12999261617660524\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12999261617660524  to: 0.12998138666152953\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12998138666152953  to: 0.12997015714645385\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12997015714645385  to: 0.12995895147323608\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12995895147323608  to: 0.12994775772094727\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12994775772094727  to: 0.1299365758895874\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.1299365758895874  to: 0.1299254298210144\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1299254298210144  to: 0.12991429567337037\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12991429567337037  to: 0.12990317344665528\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12990317344665528  to: 0.12989206314086915\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12989206314086915  to: 0.12988097667694093\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12988097667694093  to: 0.12986990213394164\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12986990213394164  to: 0.12985881567001342\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12985881567001342  to: 0.1298477530479431\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.1298477530479431  to: 0.12983671426773072\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12983671426773072  to: 0.12982566356658937\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12982566356658937  to: 0.12981464862823486\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12981464862823486  to: 0.1298036217689514\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1298036217689514  to: 0.12979261875152587\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12979261875152587  to: 0.12978161573410035\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12978161573410035  to: 0.1297703504562378\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.1297703504562378  to: 0.1297589659690857\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.1297589659690857  to: 0.12974765300750732\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12974765300750732  to: 0.12973631620407106\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12973631620407106  to: 0.12972500324249267\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12972500324249267  to: 0.12971370220184325\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12971370220184325  to: 0.12970244884490967\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12970244884490967  to: 0.1296912908554077\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.1296912908554077  to: 0.12968013286590577\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12968013286590577  to: 0.1296689748764038\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.1296689748764038  to: 0.12965781688690187\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12965781688690187  to: 0.12964667081832887\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12964667081832887  to: 0.12963554859161378\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.12963554859161378  to: 0.12962440252304078\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.12962440252304078  to: 0.12961328029632568\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12961328029632568  to: 0.12960214614868165\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12960214614868165  to: 0.1295910120010376\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1295910120010376  to: 0.1295798897743225\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.1295798897743225  to: 0.12956876754760743\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12956876754760743  to: 0.12955763339996337\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12955763339996337  to: 0.12954632043838502\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12954632043838502  to: 0.12953479290008546\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12953479290008546  to: 0.12952327728271484\n",
      "Training iteration: 679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12952327728271484  to: 0.1295117974281311\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.1295117974281311  to: 0.12950034141540528\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.12950034141540528  to: 0.12948887348175048\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12948887348175048  to: 0.12947741746902466\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.12947741746902466  to: 0.12946596145629882\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12946596145629882  to: 0.12945449352264404\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12945449352264404  to: 0.1294430375099182\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.1294430375099182  to: 0.12943156957626342\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12943156957626342  to: 0.12942008972167968\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12942008972167968  to: 0.1294086217880249\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.1294086217880249  to: 0.12939714193344115\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12939714193344115  to: 0.12938567399978637\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12938567399978637  to: 0.12937419414520263\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12937419414520263  to: 0.12936270236968994\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12936270236968994  to: 0.12935121059417726\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.12935121059417726  to: 0.12933971881866455\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12933971881866455  to: 0.12932822704315186\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.12932822704315186  to: 0.1293167233467102\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.1293167233467102  to: 0.12930521965026856\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12930521965026856  to: 0.12929370403289794\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12929370403289794  to: 0.12928221225738526\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12928221225738526  to: 0.12927069664001464\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.12927069664001464  to: 0.12925918102264405\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.12925918102264405  to: 0.1292476773262024\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.1292476773262024  to: 0.12923617362976075\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12923617362976075  to: 0.12922465801239014\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12922465801239014  to: 0.12921316623687745\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12921316623687745  to: 0.12920162677764893\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12920162677764893  to: 0.12919013500213622\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12919013500213622  to: 0.12917859554290773\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12917859554290773  to: 0.12916706800460814\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12916706800460814  to: 0.1291555404663086\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.1291555404663086  to: 0.12914397716522216\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.12914397716522216  to: 0.12913243770599364\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12913243770599364  to: 0.12912088632583618\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12912088632583618  to: 0.12910932302474976\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12910932302474976  to: 0.12909775972366333\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12909775972366333  to: 0.12908618450164794\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12908618450164794  to: 0.12907460927963257\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12907460927963257  to: 0.1290630578994751\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.1290630578994751  to: 0.12905150651931763\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12905150651931763  to: 0.1290399432182312\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.1290399432182312  to: 0.12902837991714478\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12902837991714478  to: 0.12901680469512938\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12901680469512938  to: 0.12900521755218505\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12900521755218505  to: 0.12899363040924072\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12899363040924072  to: 0.12898203134536743\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12898203134536743  to: 0.12897043228149413\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12897043228149413  to: 0.1289588212966919\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1289588212966919  to: 0.1289471983909607\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.1289471983909607  to: 0.1289355754852295\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.1289355754852295  to: 0.1289239525794983\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.1289239525794983  to: 0.12891229391098022\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12891229391098022  to: 0.1289006471633911\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.1289006471633911  to: 0.12888901233673095\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12888901233673095  to: 0.12887732982635497\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12887732982635497  to: 0.12886565923690796\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12886565923690796  to: 0.12885398864746095\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12885398864746095  to: 0.128842294216156\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.128842294216156  to: 0.12883059978485106\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12883059978485106  to: 0.12881890535354615\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12881890535354615  to: 0.1288071870803833\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.1288071870803833  to: 0.12879549264907836\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12879549264907836  to: 0.12878376245498657\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12878376245498657  to: 0.12877204418182372\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12877204418182372  to: 0.12876030206680297\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12876030206680297  to: 0.12874858379364013\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12874858379364013  to: 0.12873693704605102\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12873693704605102  to: 0.12872530221939088\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12872530221939088  to: 0.12871363162994384\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12871363162994384  to: 0.12870198488235474\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12870198488235474  to: 0.12869032621383666\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12869032621383666  to: 0.1286786675453186\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.1286786675453186  to: 0.12866700887680055\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12866700887680055  to: 0.12865535020828248\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12865535020828248  to: 0.12864367961883544\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12864367961883544  to: 0.12863199710845946\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12863199710845946  to: 0.12862033843994142\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12862033843994142  to: 0.12860865592956544\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12860865592956544  to: 0.1285969614982605\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.1285969614982605  to: 0.12858532667160033\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12858532667160033  to: 0.12857370376586913\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12857370376586913  to: 0.128562068939209\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.128562068939209  to: 0.1285504460334778\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.1285504460334778  to: 0.12853879928588868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12853879928588868  to: 0.1285271406173706\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.1285271406173706  to: 0.12851548194885254\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12851548194885254  to: 0.12850388288497924\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12850388288497924  to: 0.12849242687225343\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12849242687225343  to: 0.12848100662231446\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12848100662231446  to: 0.1284695863723755\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.1284695863723755  to: 0.12845815420150758\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12845815420150758  to: 0.12844672203063964\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12844672203063964  to: 0.12843530178070067\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12843530178070067  to: 0.12842388153076173\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12842388153076173  to: 0.12841246128082276\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12841246128082276  to: 0.12840102910995482\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12840102910995482  to: 0.12838960886001588\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12838960886001588  to: 0.12837817668914794\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12837817668914794  to: 0.12836673259735107\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12836673259735107  to: 0.1283552885055542\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1283552885055542  to: 0.12834384441375732\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.12834384441375732  to: 0.1283323884010315\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1283323884010315  to: 0.12832090854644776\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12832090854644776  to: 0.12830941677093505\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12830941677093505  to: 0.12829792499542236\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12829792499542236  to: 0.1282864212989807\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.1282864212989807  to: 0.1282749056816101\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.1282749056816101  to: 0.1282633900642395\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.1282633900642395  to: 0.12825186252593995\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12825186252593995  to: 0.12824031114578247\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12824031114578247  to: 0.128228759765625\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.128228759765625  to: 0.1282171845436096\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.1282171845436096  to: 0.12820560932159425\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12820560932159425  to: 0.12819401025772095\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12819401025772095  to: 0.12818241119384766\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12818241119384766  to: 0.12817080020904542\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12817080020904542  to: 0.12815916538238525\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12815916538238525  to: 0.12814751863479615\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12814751863479615  to: 0.12813587188720704\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12813587188720704  to: 0.12812420129776\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12812420129776  to: 0.128112530708313\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.128112530708313  to: 0.12810083627700805\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12810083627700805  to: 0.12808916568756104\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12808916568756104  to: 0.1280774712562561\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.1280774712562561  to: 0.12806577682495118\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12806577682495118  to: 0.12805405855178834\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12805405855178834  to: 0.12804235219955445\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12804235219955445  to: 0.12803066968917848\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12803066968917848  to: 0.1280190110206604\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.1280190110206604  to: 0.12800734043121337\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.12800734043121337  to: 0.12799568176269532\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12799568176269532  to: 0.12798402309417725\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12798402309417725  to: 0.12797237634658815\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12797237634658815  to: 0.12796071767807007\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12796071767807007  to: 0.12794907093048097\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12794907093048097  to: 0.1279374361038208\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.1279374361038208  to: 0.12792577743530273\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.12792577743530273  to: 0.12791413068771362\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12791413068771362  to: 0.12790248394012452\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12790248394012452  to: 0.1278908371925354\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.1278908371925354  to: 0.12787916660308837\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12787916660308837  to: 0.1278675079345703\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.1278675079345703  to: 0.12785587310791016\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12785587310791016  to: 0.1278442144393921\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.1278442144393921  to: 0.12783255577087402\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12783255577087402  to: 0.12782089710235595\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12782089710235595  to: 0.1278092384338379\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.1278092384338379  to: 0.12779757976531983\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12779757976531983  to: 0.12778592109680176\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12778592109680176  to: 0.12777426242828369\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12777426242828369  to: 0.1277625799179077\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.1277625799179077  to: 0.1277509093284607\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.1277509093284607  to: 0.1277392625808716\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.1277392625808716  to: 0.12772758007049562\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.12772758007049562  to: 0.12771589756011964\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12771589756011964  to: 0.12770421504974366\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12770421504974366  to: 0.1276925325393677\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.1276925325393677  to: 0.12768082618713378\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12768082618713378  to: 0.12766910791397096\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12766910791397096  to: 0.12765740156173705\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12765740156173705  to: 0.12764570713043213\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12764570713043213  to: 0.1276339888572693\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.1276339888572693  to: 0.12762229442596434\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12762229442596434  to: 0.12761058807373046\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12761058807373046  to: 0.12759888172149658\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12759888172149658  to: 0.12758716344833373\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12758716344833373  to: 0.1275754451751709\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.1275754451751709  to: 0.127563738822937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 848\n",
      "Improved validation loss from: 0.127563738822937  to: 0.12755200862884522\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12755200862884522  to: 0.12754026651382447\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.12754026651382447  to: 0.12752851247787475\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12752851247787475  to: 0.12751682996749877\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.12751682996749877  to: 0.12750513553619386\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.12750513553619386  to: 0.127493417263031\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.127493417263031  to: 0.12748167514801026\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.12748167514801026  to: 0.12746992111206054\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.12746992111206054  to: 0.12745815515518188\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12745815515518188  to: 0.12744637727737426\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12744637727737426  to: 0.1274345874786377\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.1274345874786377  to: 0.1274227738380432\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.1274227738380432  to: 0.12741096019744874\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12741096019744874  to: 0.12739911079406738\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12739911079406738  to: 0.12738723754882814\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.12738723754882814  to: 0.12737534046173096\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12737534046173096  to: 0.1273634433746338\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.1273634433746338  to: 0.1273514986038208\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.1273514986038208  to: 0.12733956575393676\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12733956575393676  to: 0.12732759714126587\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.12732759714126587  to: 0.12731561660766602\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12731561660766602  to: 0.1273036241531372\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.1273036241531372  to: 0.12729164361953735\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12729164361953735  to: 0.1272796392440796\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.1272796392440796  to: 0.12726762294769287\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12726762294769287  to: 0.12725560665130614\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.12725560665130614  to: 0.12724357843399048\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12724357843399048  to: 0.12723157405853272\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12723157405853272  to: 0.12721954584121703\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.12721954584121703  to: 0.12720760107040405\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12720760107040405  to: 0.12719576358795165\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12719576358795165  to: 0.12718396186828612\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.12718396186828612  to: 0.1271722435951233\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.1271722435951233  to: 0.12716054916381836\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.12716054916381836  to: 0.1271489143371582\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.1271489143371582  to: 0.12713733911514283\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12713733911514283  to: 0.12712578773498534\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12712578773498534  to: 0.12711429595947266\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.12711429595947266  to: 0.12710285186767578\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12710285186767578  to: 0.12709147930145265\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12709147930145265  to: 0.12708014249801636\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12708014249801636  to: 0.1270688772201538\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.1270688772201538  to: 0.12705767154693604\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12705767154693604  to: 0.12704646587371826\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.12704646587371826  to: 0.12703533172607423\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.12703533172607423  to: 0.127024245262146\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.127024245262146  to: 0.1270131826400757\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.1270131826400757  to: 0.12700214385986328\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.12700214385986328  to: 0.12699114084243773\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12699114084243773  to: 0.12698013782501222\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.12698013782501222  to: 0.1269691228866577\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.1269691228866577  to: 0.12695811986923217\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12695811986923217  to: 0.12694709300994872\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.12694709300994872  to: 0.12693603038787843\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.12693603038787843  to: 0.1269249677658081\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.1269249677658081  to: 0.12691383361816405\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12691383361816405  to: 0.12690268754959105\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12690268754959105  to: 0.12689149379730225\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12689149379730225  to: 0.1268802285194397\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.1268802285194397  to: 0.12686891555786134\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.12686891555786134  to: 0.12685753107070924\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12685753107070924  to: 0.12684611082077027\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.12684611082077027  to: 0.12683460712432862\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12683460712432862  to: 0.12682303190231323\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12682303190231323  to: 0.126811420917511\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.126811420917511  to: 0.12679972648620605\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12679972648620605  to: 0.12678797245025636\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.12678797245025636  to: 0.12677617073059083\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.12677617073059083  to: 0.1267642855644226\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.1267642855644226  to: 0.12675235271453858\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.12675235271453858  to: 0.1267404317855835\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.1267404317855835  to: 0.12672841548919678\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12672841548919678  to: 0.12671635150909424\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12671635150909424  to: 0.12670421600341797\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.12670421600341797  to: 0.126691997051239\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.126691997051239  to: 0.12667973041534425\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12667973041534425  to: 0.1266673684120178\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.1266673684120178  to: 0.12665494680404663\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12665494680404663  to: 0.12664258480072021\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12664258480072021  to: 0.12663023471832274\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12663023471832274  to: 0.1266179084777832\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.1266179084777832  to: 0.12660562992095947\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.12660562992095947  to: 0.12659332752227784\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.12659332752227784  to: 0.12658101320266724\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.12658101320266724  to: 0.12656869888305664\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12656869888305664  to: 0.1265563726425171\n",
      "Training iteration: 934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1265563726425171  to: 0.12654402256011962\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12654402256011962  to: 0.1265316367149353\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.1265316367149353  to: 0.12651922702789306\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.12651922702789306  to: 0.12650678157806397\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12650678157806397  to: 0.12649428844451904\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.12649428844451904  to: 0.12648173570632934\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.12648173570632934  to: 0.12646912336349486\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12646912336349486  to: 0.12645648717880248\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12645648717880248  to: 0.12644379138946532\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12644379138946532  to: 0.12643102407455445\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12643102407455445  to: 0.12641819715499877\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.12641819715499877  to: 0.12640531063079835\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12640531063079835  to: 0.12639236450195312\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12639236450195312  to: 0.12637935876846312\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12637935876846312  to: 0.1263662099838257\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.1263662099838257  to: 0.1263529419898987\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.1263529419898987  to: 0.1263395667076111\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.1263395667076111  to: 0.1263260841369629\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.1263260841369629  to: 0.12631250619888307\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.12631250619888307  to: 0.1262989044189453\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.1262989044189453  to: 0.12628519535064697\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12628519535064697  to: 0.1262714147567749\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.1262714147567749  to: 0.126257586479187\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.126257586479187  to: 0.12624365091323853\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12624365091323853  to: 0.1262296438217163\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.1262296438217163  to: 0.12621560096740722\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12621560096740722  to: 0.12620147466659545\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.12620147466659545  to: 0.12618730068206788\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.12618730068206788  to: 0.12617307901382446\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.12617307901382446  to: 0.12615878582000734\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.12615878582000734  to: 0.12614448070526124\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12614448070526124  to: 0.1261301279067993\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.1261301279067993  to: 0.12611572742462157\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12611572742462157  to: 0.12610130310058593\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.12610130310058593  to: 0.12608684301376344\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12608684301376344  to: 0.12607239484786986\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12607239484786986  to: 0.12605798244476318\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12605798244476318  to: 0.1260435938835144\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.1260435938835144  to: 0.12602924108505248\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12602924108505248  to: 0.12601491212844848\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12601491212844848  to: 0.1260009527206421\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.1260009527206421  to: 0.12598707675933837\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12598707675933837  to: 0.12597317695617677\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12597317695617677  to: 0.12595846652984619\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12595846652984619  to: 0.12594306468963623\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12594306468963623  to: 0.12592761516571044\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.12592761516571044  to: 0.12591218948364258\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12591218948364258  to: 0.12589675188064575\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12589675188064575  to: 0.12588129043579102\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12588129043579102  to: 0.12586584091186523\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12586584091186523  to: 0.12585041522979737\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12585041522979737  to: 0.1258350133895874\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.1258350133895874  to: 0.12581961154937743\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12581961154937743  to: 0.12580424547195435\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12580424547195435  to: 0.12578890323638917\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12578890323638917  to: 0.1257735848426819\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.1257735848426819  to: 0.12575826644897461\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12575826644897461  to: 0.12574295997619628\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.12574295997619628  to: 0.12572768926620484\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12572768926620484  to: 0.12571243047714234\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.12571243047714234  to: 0.12569719552993774\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12569719552993774  to: 0.1256819486618042\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.1256819486618042  to: 0.1256667971611023\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.1256667971611023  to: 0.12565197944641113\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12565197944641113  to: 0.12563717365264893\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12563717365264893  to: 0.12562234401702882\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.12562234401702882  to: 0.12560752630233765\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.12560752630233765  to: 0.12559242248535157\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.12559242248535157  to: 0.12557684183120726\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12557684183120726  to: 0.1255612850189209\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.1255612850189209  to: 0.12554569244384767\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12554569244384767  to: 0.12553011178970336\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.12553011178970336  to: 0.12551450729370117\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12551450729370117  to: 0.12549889087677002\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12549889087677002  to: 0.12548325061798096\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12548325061798096  to: 0.12546758651733397\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12546758651733397  to: 0.1254518985748291\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.1254518985748291  to: 0.1254361867904663\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.1254361867904663  to: 0.1254204511642456\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.1254204511642456  to: 0.1254046678543091\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.1254046678543091  to: 0.12538884878158568\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12538884878158568  to: 0.1253730058670044\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.1253730058670044  to: 0.12535712718963624\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.12535712718963624  to: 0.1253412127494812\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.1253412127494812  to: 0.1253252625465393\n",
      "Training iteration: 1019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1253252625465393  to: 0.12530925273895263\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12530925273895263  to: 0.1252932071685791\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.1252932071685791  to: 0.1252771258354187\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.1252771258354187  to: 0.12526099681854247\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.12526099681854247  to: 0.1252448320388794\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.1252448320388794  to: 0.1252286195755005\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.1252286195755005  to: 0.12521237134933472\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.12521237134933472  to: 0.12519605159759523\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.12519605159759523  to: 0.1251797080039978\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.1251797080039978  to: 0.12516332864761354\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.12516332864761354  to: 0.12514688968658447\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.12514688968658447  to: 0.12513041496276855\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.12513041496276855  to: 0.1251138925552368\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.1251138925552368  to: 0.1250973105430603\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.1250973105430603  to: 0.1250806927680969\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.1250806927680969  to: 0.12506402730941774\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12506402730941774  to: 0.12504732608795166\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.12504732608795166  to: 0.12503057718276978\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.12503057718276978  to: 0.12501378059387208\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.12501378059387208  to: 0.12499692440032958\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12499692440032958  to: 0.1249800443649292\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.1249800443649292  to: 0.12496311664581299\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.12496311664581299  to: 0.12494614124298095\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12494614124298095  to: 0.12492911815643311\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.12492911815643311  to: 0.12491204738616943\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.12491204738616943  to: 0.12489495277404786\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.12489495277404786  to: 0.12487783432006835\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12487783432006835  to: 0.12486068010330201\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.12486068010330201  to: 0.1248435139656067\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.1248435139656067  to: 0.12482630014419556\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12482630014419556  to: 0.12480908632278442\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12480908632278442  to: 0.12479183673858643\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.12479183673858643  to: 0.12477457523345947\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.12477457523345947  to: 0.1247572660446167\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.1247572660446167  to: 0.12473995685577392\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.12473995685577392  to: 0.12472264766693116\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.12472264766693116  to: 0.12470537424087524\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.12470537424087524  to: 0.12468807697296143\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.12468807697296143  to: 0.12467079162597657\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12467079162597657  to: 0.1246535062789917\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.1246535062789917  to: 0.12463624477386474\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12463624477386474  to: 0.12461897134780883\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.12461897134780883  to: 0.12460172176361084\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12460172176361084  to: 0.1245844841003418\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.1245844841003418  to: 0.12456724643707276\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.12456724643707276  to: 0.12454955577850342\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.12454955577850342  to: 0.12453148365020753\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12453148365020753  to: 0.12451339960098266\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.12451339960098266  to: 0.12449531555175782\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.12449531555175782  to: 0.12447724342346192\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12447724342346192  to: 0.12445917129516601\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.12445917129516601  to: 0.12444108724594116\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.12444108724594116  to: 0.12442299127578735\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12442299127578735  to: 0.12440485954284668\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.12440485954284668  to: 0.1243867039680481\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.1243867039680481  to: 0.12436853647232056\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.12436853647232056  to: 0.12435034513473511\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.12435034513473511  to: 0.12433216571807862\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.12433216571807862  to: 0.12431398630142212\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.12431398630142212  to: 0.12429579496383666\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.12429579496383666  to: 0.12427761554718017\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.12427761554718017  to: 0.12425944805145264\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12425944805145264  to: 0.12424126863479615\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.12424126863479615  to: 0.12422308921813965\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12422308921813965  to: 0.12420492172241211\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12420492172241211  to: 0.12418674230575562\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.12418674230575562  to: 0.12416857481002808\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12416857481002808  to: 0.12415040731430053\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.12415040731430053  to: 0.12413225173950196\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.12413225173950196  to: 0.12411407232284546\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12411407232284546  to: 0.12409589290618897\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.12409589290618897  to: 0.12407772541046143\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12407772541046143  to: 0.12405951023101806\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12405951023101806  to: 0.12404128313064575\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12404128313064575  to: 0.12402304410934448\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.12402304410934448  to: 0.12400481700897217\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.12400481700897217  to: 0.12398656606674194\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12398656606674194  to: 0.12396833896636963\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.12396833896636963  to: 0.12395012378692627\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.12395012378692627  to: 0.12393192052841187\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.12393192052841187  to: 0.12391376495361328\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12391376495361328  to: 0.12389566898345947\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.12389566898345947  to: 0.12387815713882447\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12387815713882447  to: 0.12386066913604736\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12386066913604736  to: 0.12384320497512817\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.12384320497512817  to: 0.12382571697235108\n",
      "Training iteration: 1105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12382571697235108  to: 0.12380822896957397\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.12380822896957397  to: 0.12379076480865478\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.12379076480865478  to: 0.12377331256866456\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12377331256866456  to: 0.1237558126449585\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.1237558126449585  to: 0.12373828887939453\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.12373828887939453  to: 0.12372074127197266\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.12372074127197266  to: 0.12370316982269287\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.12370316982269287  to: 0.12368559837341309\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.12368559837341309  to: 0.12366797924041747\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12366797924041747  to: 0.12365036010742188\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.12365036010742188  to: 0.12363269329071044\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.12363269329071044  to: 0.12361501455307007\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.12361501455307007  to: 0.12359726428985596\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12359726428985596  to: 0.12357950210571289\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.12357950210571289  to: 0.12356172800064087\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.12356172800064087  to: 0.12354390621185303\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.12354390621185303  to: 0.12352609634399414\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.12352609634399414  to: 0.12350819110870362\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.12350819110870362  to: 0.12349036931991578\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12349036931991578  to: 0.12347260713577271\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.12347260713577271  to: 0.1234548807144165\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.1234548807144165  to: 0.12343721389770508\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.12343721389770508  to: 0.12341957092285157\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.12341957092285157  to: 0.12340197563171387\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.12340197563171387  to: 0.12338440418243408\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.12338440418243408  to: 0.12336678504943847\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.12336678504943847  to: 0.12334916591644288\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.12334916591644288  to: 0.12333157062530517\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.12333157062530517  to: 0.1233139991760254\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.1233139991760254  to: 0.12329646348953247\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.12329646348953247  to: 0.12327892780303955\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.12327892780303955  to: 0.12326138019561768\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.12326138019561768  to: 0.1232438325881958\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.1232438325881958  to: 0.1232262372970581\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.1232262372970581  to: 0.12320867776870728\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.12320867776870728  to: 0.12319101095199585\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.12319101095199585  to: 0.12317330837249756\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.12317330837249756  to: 0.12315552234649658\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.12315552234649658  to: 0.12313770055770874\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.12313770055770874  to: 0.12311975955963135\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.12311975955963135  to: 0.12310173511505126\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.12310173511505126  to: 0.12308362722396851\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.12308362722396851  to: 0.12306544780731202\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.12306544780731202  to: 0.12304719686508178\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.12304719686508178  to: 0.12302887439727783\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.12302887439727783  to: 0.12301048040390014\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.12301048040390014  to: 0.12299206256866455\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.12299206256866455  to: 0.12297359704971314\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.12297359704971314  to: 0.12295507192611695\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.12295507192611695  to: 0.12293651103973388\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.12293651103973388  to: 0.12291789054870605\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.12291789054870605  to: 0.12289915084838868\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.12289915084838868  to: 0.12288030385971069\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.12288030385971069  to: 0.12286136150360108\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.12286136150360108  to: 0.12284238338470459\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.12284238338470459  to: 0.12282330989837646\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.12282330989837646  to: 0.12280417680740356\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.12280417680740356  to: 0.12278544902801514\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.12278544902801514  to: 0.12276679277420044\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.12276679277420044  to: 0.12274816036224365\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.12274816036224365  to: 0.1227296233177185\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.1227296233177185  to: 0.12271112203598022\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.12271112203598022  to: 0.12269264459609985\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.12269264459609985  to: 0.12267415523529053\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.12267415523529053  to: 0.12265561819076538\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.12265561819076538  to: 0.12263710498809814\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.12263710498809814  to: 0.12261865139007569\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.12261865139007569  to: 0.1226002335548401\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.1226002335548401  to: 0.12258180379867553\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.12258180379867553  to: 0.12256338596343994\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.12256338596343994  to: 0.12254493236541748\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.12254493236541748  to: 0.12252651453018189\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.12252651453018189  to: 0.12250813245773315\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.12250813245773315  to: 0.12248977422714233\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.12248977422714233  to: 0.12247146368026733\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.12247146368026733  to: 0.12245322465896606\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.12245322465896606  to: 0.12243497371673584\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.12243497371673584  to: 0.12241668701171875\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.12241668701171875  to: 0.12239835262298585\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.12239835262298585  to: 0.12238004207611083\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.12238004207611083  to: 0.12236173152923584\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.12236173152923584  to: 0.12234342098236084\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.12234342098236084  to: 0.1223254919052124\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.1223254919052124  to: 0.12230767011642456\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.12230767011642456  to: 0.12228982448577881\n",
      "Training iteration: 1190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12228982448577881  to: 0.12227195501327515\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.12227195501327515  to: 0.12225406169891358\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.12225406169891358  to: 0.12223613262176514\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.12223613262176514  to: 0.1222180962562561\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.1222180962562561  to: 0.12220007181167603\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.12220007181167603  to: 0.12218201160430908\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.12218201160430908  to: 0.12216389179229736\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.12216389179229736  to: 0.12214572429656982\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.12214572429656982  to: 0.12212750911712647\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.12212750911712647  to: 0.12210918664932251\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.12210918664932251  to: 0.12209075689315796\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.12209075689315796  to: 0.12207230329513549\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.12207230329513549  to: 0.1220537543296814\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.1220537543296814  to: 0.1220350980758667\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.1220350980758667  to: 0.12201637029647827\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.12201637029647827  to: 0.12199758291244507\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.12199758291244507  to: 0.12197868824005127\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.12197868824005127  to: 0.12195966243743897\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.12195966243743897  to: 0.12194054126739502\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.12194054126739502  to: 0.12192131280899048\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.12192131280899048  to: 0.12190203666687012\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.12190203666687012  to: 0.12188265323638917\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.12188265323638917  to: 0.1218631386756897\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.1218631386756897  to: 0.12184354066848754\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.12184354066848754  to: 0.12182384729385376\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.12182384729385376  to: 0.1218041181564331\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.1218041181564331  to: 0.12178434133529663\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.12178434133529663  to: 0.12176449298858642\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.12176449298858642  to: 0.12174453735351562\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.12174453735351562  to: 0.12172446250915528\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.12172446250915528  to: 0.12170430421829223\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.12170430421829223  to: 0.12168419361114502\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.12168419361114502  to: 0.12166411876678467\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.12166411876678467  to: 0.12164403200149536\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.12164403200149536  to: 0.12162392139434815\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.12162392139434815  to: 0.12160378694534302\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.12160378694534302  to: 0.12158359289169311\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.12158359289169311  to: 0.12156339883804321\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.12156339883804321  to: 0.12154285907745362\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.12154285907745362  to: 0.12152211666107178\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.12152211666107178  to: 0.12150106430053711\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.12150106430053711  to: 0.12147974967956543\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.12147974967956543  to: 0.12145812511444092\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.12145812511444092  to: 0.12143619060516357\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.12143619060516357  to: 0.12141402959823608\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.12141402959823608  to: 0.1213916540145874\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.1213916540145874  to: 0.12136907577514648\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.12136907577514648  to: 0.1213463544845581\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.1213463544845581  to: 0.12132350206375123\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.12132350206375123  to: 0.12130051851272583\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.12130051851272583  to: 0.12127742767333985\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.12127742767333985  to: 0.12125504016876221\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.12125504016876221  to: 0.12123275995254516\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.12123275995254516  to: 0.12121038436889649\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.12121038436889649  to: 0.12118793725967407\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.12118793725967407  to: 0.12116541862487792\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.12116541862487792  to: 0.12114284038543702\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.12114284038543702  to: 0.12112022638320923\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.12112022638320923  to: 0.12109744548797607\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.12109744548797607  to: 0.12107458114624023\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.12107458114624023  to: 0.12105166912078857\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.12105166912078857  to: 0.12102869749069214\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.12102869749069214  to: 0.12100576162338257\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.12100576162338257  to: 0.12098276615142822\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.12098276615142822  to: 0.12095977067947387\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.12095977067947387  to: 0.12093673944473267\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.12093673944473267  to: 0.12091373205184937\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.12091373205184937  to: 0.12089070081710815\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.12089070081710815  to: 0.12086759805679322\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.12086759805679322  to: 0.12084449529647827\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.12084449529647827  to: 0.12082135677337646\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.12082135677337646  to: 0.12079823017120361\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.12079823017120361  to: 0.12077512741088867\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.12077512741088867  to: 0.12075202465057373\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.12075202465057373  to: 0.12072893381118774\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.12072893381118774  to: 0.12070585489273071\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.12070585489273071  to: 0.12068138122558594\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.12068138122558594  to: 0.12065670490264893\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.12065670490264893  to: 0.12063196897506714\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.12063196897506714  to: 0.12060723304748536\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.12060723304748536  to: 0.12058244943618775\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.12058244943618775  to: 0.12055761814117431\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.12055761814117431  to: 0.12053275108337402\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.12053275108337402  to: 0.12050809860229492\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.12050809860229492  to: 0.12048370838165283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.12048370838165283  to: 0.1204593300819397\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.1204593300819397  to: 0.12043492794036866\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.12043492794036866  to: 0.1204105019569397\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.1204105019569397  to: 0.12038605213165283\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.12038605213165283  to: 0.1203615665435791\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.1203615665435791  to: 0.12033706903457642\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.12033706903457642  to: 0.12031257152557373\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.12031257152557373  to: 0.12028802633285522\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.12028802633285522  to: 0.12026351690292358\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.12026351690292358  to: 0.12023903131484985\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.12023903131484985  to: 0.12021446228027344\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.12021446228027344  to: 0.12018980979919433\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.12018980979919433  to: 0.12016490697860718\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.12016490697860718  to: 0.12014011144638062\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.12014011144638062  to: 0.12011505365371704\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.12011505365371704  to: 0.12008979320526122\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.12008979320526122  to: 0.12006431818008423\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.12006431818008423  to: 0.12003864049911499\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.12003864049911499  to: 0.12001274824142456\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.12001274824142456  to: 0.11998666524887085\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.11998666524887085  to: 0.119960618019104\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.119960618019104  to: 0.1199346899986267\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.1199346899986267  to: 0.11990864276885986\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.11990864276885986  to: 0.11988242864608764\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.11988242864608764  to: 0.11985610723495484\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.11985610723495484  to: 0.11982971429824829\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.11982971429824829  to: 0.11980330944061279\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.11980330944061279  to: 0.11977680921554565\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.11977680921554565  to: 0.11975027322769165\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.11975027322769165  to: 0.11972362995147705\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.11972362995147705  to: 0.11969705820083618\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.11969705820083618  to: 0.1196704626083374\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.1196704626083374  to: 0.11964387893676758\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.11964387893676758  to: 0.11961735486984253\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.11961735486984253  to: 0.11959081888198853\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.11959081888198853  to: 0.1195643424987793\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.1195643424987793  to: 0.11953779458999633\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.11953779458999633  to: 0.11951117515563965\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.11951117515563965  to: 0.11948461532592773\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.11948461532592773  to: 0.11945809125900268\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.11945809125900268  to: 0.11943167448043823\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.11943167448043823  to: 0.11940534114837646\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.11940534114837646  to: 0.11937901973724366\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.11937901973724366  to: 0.11935269832611084\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.11935269832611084  to: 0.11932632923126221\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.11932632923126221  to: 0.11929996013641357\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.11929996013641357  to: 0.11927359104156494\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.11927359104156494  to: 0.11924726963043213\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.11924726963043213  to: 0.1192209005355835\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.1192209005355835  to: 0.11919453144073486\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.11919453144073486  to: 0.11916811466217041\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.11916811466217041  to: 0.11914165019989013\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.11914165019989013  to: 0.11911485195159913\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.11911485195159913  to: 0.1190876841545105\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.1190876841545105  to: 0.1190601110458374\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.1190601110458374  to: 0.11903225183486939\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.11903225183486939  to: 0.11900408267974853\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.11900408267974853  to: 0.11897567510604859\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.11897567510604859  to: 0.11894702911376953\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.11894702911376953  to: 0.11891820430755615\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.11891820430755615  to: 0.11888926029205323\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.11888926029205323  to: 0.11886012554168701\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.11886012554168701  to: 0.11883077621459961\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.11883077621459961  to: 0.11880128383636475\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.11880128383636475  to: 0.11877166032791138\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.11877166032791138  to: 0.11874195337295532\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.11874195337295532  to: 0.11871217489242554\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.11871217489242554  to: 0.11868233680725097\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.11868233680725097  to: 0.11865243911743165\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.11865243911743165  to: 0.11862255334854126\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.11862255334854126  to: 0.1185926079750061\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.1185926079750061  to: 0.11856262683868408\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.11856262683868408  to: 0.11853259801864624\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.11853259801864624  to: 0.11850254535675049\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.11850254535675049  to: 0.11847245693206787\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.11847245693206787  to: 0.11844236850738525\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.11844236850738525  to: 0.11841223239898682\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.11841223239898682  to: 0.11838206052780151\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.11838206052780151  to: 0.11835181713104248\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.11835181713104248  to: 0.11832153797149658\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.11832153797149658  to: 0.11829129457473755\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.11829129457473755  to: 0.11826108694076538\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.11826108694076538  to: 0.11823083162307739\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.11823083162307739  to: 0.11820052862167359\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.11820052862167359  to: 0.11816976070404053\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.11816976070404053  to: 0.11813852787017823\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.11813852787017823  to: 0.1181069016456604\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.1181069016456604  to: 0.11807482242584229\n",
      "Training iteration: 1363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11807482242584229  to: 0.11804239749908448\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.11804239749908448  to: 0.11800973415374756\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.11800973415374756  to: 0.11797692775726318\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.11797692775726318  to: 0.11794388294219971\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.11794388294219971  to: 0.11791064739227294\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.11791064739227294  to: 0.11787728071212769\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.11787728071212769  to: 0.11784369945526123\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.11784369945526123  to: 0.11781008243560791\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.11781008243560791  to: 0.11777638196945191\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.11777638196945191  to: 0.11774259805679321\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.11774259805679321  to: 0.11770880222320557\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.11770880222320557  to: 0.11767500638961792\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.11767500638961792  to: 0.1176412582397461\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.1176412582397461  to: 0.11760752201080323\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.11760752201080323  to: 0.11757385730743408\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.11757385730743408  to: 0.11754024028778076\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.11754024028778076  to: 0.11750675439834594\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.11750675439834594  to: 0.11747337579727173\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.11747337579727173  to: 0.11744017601013183\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.11744017601013183  to: 0.11740731000900269\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.11740731000900269  to: 0.11737496852874756\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.11737496852874756  to: 0.11734281778335572\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.11734281778335572  to: 0.11731082201004028\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.11731082201004028  to: 0.1172784447669983\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.1172784447669983  to: 0.1172457218170166\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.1172457218170166  to: 0.11721271276473999\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.11721271276473999  to: 0.11717945337295532\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.11717945337295532  to: 0.11714595556259155\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.11714595556259155  to: 0.11711227893829346\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.11711227893829346  to: 0.11707842350006104\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.11707842350006104  to: 0.11704441308975219\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.11704441308975219  to: 0.11701034307479859\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.11701034307479859  to: 0.11697614192962646\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.11697614192962646  to: 0.11694177389144897\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.11694177389144897  to: 0.11690723896026611\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.11690723896026611  to: 0.11687262058258056\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.11687262058258056  to: 0.11683794260025024\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.11683794260025024  to: 0.11680320501327515\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.11680320501327515  to: 0.11676843166351318\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.11676843166351318  to: 0.1167330026626587\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.1167330026626587  to: 0.11669709682464599\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.11669709682464599  to: 0.11666072607040405\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.11666072607040405  to: 0.11662391424179078\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.11662391424179078  to: 0.11658668518066406\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.11658668518066406  to: 0.1165490984916687\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.1165490984916687  to: 0.11651109457015991\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.11651109457015991  to: 0.11647275686264039\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.11647275686264039  to: 0.11643419265747071\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.11643419265747071  to: 0.11639547348022461\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.11639547348022461  to: 0.11635664701461793\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.11635664701461793  to: 0.1163177728652954\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.1163177728652954  to: 0.11627895832061767\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.11627895832061767  to: 0.11624019145965576\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.11624019145965576  to: 0.11620149612426758\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.11620149612426758  to: 0.11616284847259521\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.11616284847259521  to: 0.11612430810928345\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.11612430810928345  to: 0.11608585119247436\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.11608585119247436  to: 0.11604750156402588\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.11604750156402588  to: 0.11600925922393798\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.11600925922393798  to: 0.11597108840942383\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.11597108840942383  to: 0.115933096408844\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.115933096408844  to: 0.11589523553848266\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.11589523553848266  to: 0.11585757732391358\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.11585757732391358  to: 0.11582009792327881\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.11582009792327881  to: 0.11578274965286255\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.11578274965286255  to: 0.1157455325126648\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.1157455325126648  to: 0.11570862531661988\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.11570862531661988  to: 0.11567224264144897\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.11567224264144897  to: 0.11563595533370971\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.11563595533370971  to: 0.11559972763061524\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.11559972763061524  to: 0.11556355953216553\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.11556355953216553  to: 0.11552741527557372\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.11552741527557372  to: 0.11549124717712403\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.11549124717712403  to: 0.11545507907867432\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.11545507907867432  to: 0.11541887521743774\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.11541887521743774  to: 0.11538264751434327\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.11538264751434327  to: 0.11534630060195923\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.11534630060195923  to: 0.11530981063842774\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.11530981063842774  to: 0.11527329683303833\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.11527329683303833  to: 0.11523679494857789\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.11523679494857789  to: 0.11520030498504638\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.11520030498504638  to: 0.11516373157501221\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.11516373157501221  to: 0.11512703895568847\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.11512703895568847  to: 0.11509028673171998\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.11509028673171998  to: 0.11505339145660401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.11505339145660401  to: 0.1150162935256958\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.1150162935256958  to: 0.11497942209243775\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.11497942209243775  to: 0.11494235992431641\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.11494235992431641  to: 0.11490517854690552\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.11490517854690552  to: 0.1148678183555603\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.1148678183555603  to: 0.11483029127120972\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.11483029127120972  to: 0.11479270458221436\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.11479270458221436  to: 0.11475503444671631\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.11475503444671631  to: 0.11471704244613648\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.11471704244613648  to: 0.11467888355255126\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.11467888355255126  to: 0.11464053392410278\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.11464053392410278  to: 0.11460201740264893\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.11460201740264893  to: 0.1145632028579712\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.1145632028579712  to: 0.11452422142028809\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.11452422142028809  to: 0.11448502540588379\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.11448502540588379  to: 0.11444563865661621\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.11444563865661621  to: 0.11440603733062744\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.11440603733062744  to: 0.11436634063720703\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.11436634063720703  to: 0.11432644128799438\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.11432644128799438  to: 0.11428633928298951\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.11428633928298951  to: 0.11424604654312134\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.11424604654312134  to: 0.11420538425445556\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.11420538425445556  to: 0.11416448354721069\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.11416448354721069  to: 0.1141237497329712\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.1141237497329712  to: 0.11408286094665528\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.11408286094665528  to: 0.11404098272323608\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.11404098272323608  to: 0.11399810314178467\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.11399810314178467  to: 0.11395452022552491\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.11395452022552491  to: 0.11391041278839112\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.11391041278839112  to: 0.11386638879776001\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.11386638879776001  to: 0.11382203102111817\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.11382203102111817  to: 0.11377723217010498\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.11377723217010498  to: 0.11373202800750733\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.11373202800750733  to: 0.11368649005889893\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.11368649005889893  to: 0.1136406660079956\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.1136406660079956  to: 0.11359411478042603\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.11359411478042603  to: 0.11354678869247437\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.11354678869247437  to: 0.11349879503250122\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.11349879503250122  to: 0.11345037221908569\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.11345037221908569  to: 0.11340157985687256\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.11340157985687256  to: 0.11335252523422241\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.11335252523422241  to: 0.11330339908599854\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.11330339908599854  to: 0.1132542371749878\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.1132542371749878  to: 0.11320499181747437\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.11320499181747437  to: 0.1131557583808899\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.1131557583808899  to: 0.11310657262802123\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.11310657262802123  to: 0.11305834054946899\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.11305834054946899  to: 0.11301100254058838\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.11301100254058838  to: 0.11296452283859253\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.11296452283859253  to: 0.11291911602020263\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.11291911602020263  to: 0.112874436378479\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.112874436378479  to: 0.11283037662506104\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.11283037662506104  to: 0.11278594732284546\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.11278594732284546  to: 0.11274116039276123\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.11274116039276123  to: 0.11269603967666626\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.11269603967666626  to: 0.11265058517456054\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.11265058517456054  to: 0.11260483264923096\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.11260483264923096  to: 0.1125588059425354\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.1125588059425354  to: 0.11251235008239746\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.11251235008239746  to: 0.11246639490127563\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.11246639490127563  to: 0.11242084503173828\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.11242084503173828  to: 0.11237566471099854\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.11237566471099854  to: 0.11233069896697997\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.11233069896697997  to: 0.11228519678115845\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.11228519678115845  to: 0.11223918199539185\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.11223918199539185  to: 0.112192702293396\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.112192702293396  to: 0.11214578151702881\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.11214578151702881  to: 0.11209942102432251\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.11209942102432251  to: 0.11205356121063233\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.11205356121063233  to: 0.11200807094573975\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.11200807094573975  to: 0.1119615077972412\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.1119615077972412  to: 0.11191418170928955\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.11191418170928955  to: 0.11186611652374268\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.11186611652374268  to: 0.1118175745010376\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.1118175745010376  to: 0.11176859140396118\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.11176859140396118  to: 0.11171907186508179\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.11171907186508179  to: 0.11167004108428955\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.11167004108428955  to: 0.11162145137786865\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.11162145137786865  to: 0.111572527885437\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.111572527885437  to: 0.1115237832069397\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.1115237832069397  to: 0.11147468090057373\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.11147468090057373  to: 0.11142526865005493\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.11142526865005493  to: 0.11137558221817016\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.11137558221817016  to: 0.11132568120956421\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.11132568120956421  to: 0.11127541065216065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.11127541065216065  to: 0.11122586727142333\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.11122586727142333  to: 0.11117609739303588\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.11117609739303588  to: 0.11112607717514038\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.11112607717514038  to: 0.11107585430145264\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.11107585430145264  to: 0.11102540493011474\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.11102540493011474  to: 0.11097481250762939\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.11097481250762939  to: 0.11092492341995239\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.11092492341995239  to: 0.11087585687637329\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.11087585687637329  to: 0.11082614660263061\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.11082614660263061  to: 0.11077582836151123\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.11077582836151123  to: 0.11072497367858887\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.11072497367858887  to: 0.11067359447479248\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.11067359447479248  to: 0.1106220006942749\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.1106220006942749  to: 0.11057019233703613\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.11057019233703613  to: 0.11051809787750244\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.11051809787750244  to: 0.1104657769203186\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.1104657769203186  to: 0.1104132056236267\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.1104132056236267  to: 0.11036030054092408\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.11036030054092408  to: 0.11030771732330322\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.11030771732330322  to: 0.11025495529174804\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.11025495529174804  to: 0.11020207405090332\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.11020207405090332  to: 0.11014907360076905\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.11014907360076905  to: 0.11009595394134522\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.11009595394134522  to: 0.11004257202148438\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.11004257202148438  to: 0.10998908281326295\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.10998908281326295  to: 0.10993549823760987\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.10993549823760987  to: 0.10988178253173828\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.10988178253173828  to: 0.10982797145843506\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.10982797145843506  to: 0.10977406501770019\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.10977406501770019  to: 0.10972003936767578\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.10972003936767578  to: 0.10966589450836181\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.10966589450836181  to: 0.10961124897003174\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.10961124897003174  to: 0.10955616235733032\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.10955616235733032  to: 0.10950080156326295\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.10950080156326295  to: 0.10944515466690063\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.10944515466690063  to: 0.10938928127288819\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.10938928127288819  to: 0.10933279991149902\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.10933279991149902  to: 0.10927581787109375\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.10927581787109375  to: 0.10921820402145385\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.10921820402145385  to: 0.10916025638580322\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.10916025638580322  to: 0.10910202264785766\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.10910202264785766  to: 0.1090435266494751\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.1090435266494751  to: 0.10898488759994507\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.10898488759994507  to: 0.10892609357833863\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.10892609357833863  to: 0.10886722803115845\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.10886722803115845  to: 0.10880829095840454\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.10880829095840454  to: 0.1087492823600769\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.1087492823600769  to: 0.10869027376174926\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.10869027376174926  to: 0.10863125324249268\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.10863125324249268  to: 0.10857222080230713\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.10857222080230713  to: 0.10851318836212158\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.10851318836212158  to: 0.10845414400100709\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.10845414400100709  to: 0.10839505195617676\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.10839505195617676  to: 0.10833574533462524\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.10833574533462524  to: 0.10827608108520508\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.10827608108520508  to: 0.10821608304977418\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.10821608304977418  to: 0.10815502405166626\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.10815502405166626  to: 0.10809303522109985\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.10809303522109985  to: 0.10803021192550659\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.10803021192550659  to: 0.10796672105789185\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.10796672105789185  to: 0.1079026460647583\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.1079026460647583  to: 0.10783811807632446\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.10783811807632446  to: 0.10777323246002198\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.10777323246002198  to: 0.10770807266235352\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.10770807266235352  to: 0.10764272212982177\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.10764272212982177  to: 0.10757721662521362\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.10757721662521362  to: 0.10751167535781861\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.10751167535781861  to: 0.10744569301605225\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.10744569301605225  to: 0.10737937688827515\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.10737937688827515  to: 0.10731281042098999\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.10731281042098999  to: 0.10724607706069947\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.10724607706069947  to: 0.10717923641204834\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.10717923641204834  to: 0.10711233615875244\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.10711233615875244  to: 0.10704540014266968\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.10704540014266968  to: 0.10697848796844482\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.10697848796844482  to: 0.10691162347793579\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.10691162347793579  to: 0.10684483051300049\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.10684483051300049  to: 0.106778085231781\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.106778085231781  to: 0.10671141147613525\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.10671141147613525  to: 0.10664447546005248\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.10664447546005248  to: 0.10657727718353271\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.10657727718353271  to: 0.10650990009307862\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.10650990009307862  to: 0.10644237995147705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.10644237995147705  to: 0.10637476444244384\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.10637476444244384  to: 0.10630708932876587\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.10630708932876587  to: 0.10623931884765625\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.10623931884765625  to: 0.10617148876190186\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.10617148876190186  to: 0.10610361099243164\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.10610361099243164  to: 0.10603566169738769\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.10603566169738769  to: 0.10596773624420167\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.10596773624420167  to: 0.10589982271194458\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.10589982271194458  to: 0.1058319091796875\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.1058319091796875  to: 0.10576395988464356\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.10576395988464356  to: 0.10569595098495484\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.10569595098495484  to: 0.10562779903411865\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.10562779903411865  to: 0.10555906295776367\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.10555906295776367  to: 0.10548975467681884\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.10548975467681884  to: 0.10541987419128418\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.10541987419128418  to: 0.10534944534301757\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.10534944534301757  to: 0.10527849197387695\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.10527849197387695  to: 0.10520706176757813\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.10520706176757813  to: 0.10513520240783691\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.10513520240783691  to: 0.10506290197372437\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.10506290197372437  to: 0.10499023199081421\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.10499023199081421  to: 0.1049172043800354\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.1049172043800354  to: 0.1048438310623169\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.1048438310623169  to: 0.10477025508880615\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.10477025508880615  to: 0.10469644069671631\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.10469644069671631  to: 0.10462238788604736\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.10462238788604736  to: 0.10454816818237304\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.10454816818237304  to: 0.10447378158569336\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.10447378158569336  to: 0.10439908504486084\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.10439908504486084  to: 0.10432426929473877\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.10432426929473877  to: 0.10424931049346924\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.10424931049346924  to: 0.10417423248291016\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.10417423248291016  to: 0.10409901142120362\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.10409901142120362  to: 0.1040236473083496\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.1040236473083496  to: 0.10394810438156128\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.10394810438156128  to: 0.10387235879898071\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.10387235879898071  to: 0.10379635095596314\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.10379635095596314  to: 0.1037200927734375\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.1037200927734375  to: 0.10364284515380859\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.10364284515380859  to: 0.10356476306915283\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.10356476306915283  to: 0.10348587036132813\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.10348587036132813  to: 0.10340492725372315\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.10340492725372315  to: 0.10332220792770386\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.10332220792770386  to: 0.10323798656463623\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.10323798656463623  to: 0.1031525731086731\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.1031525731086731  to: 0.1030661940574646\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.1030661940574646  to: 0.10297914743423461\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.10297914743423461  to: 0.10289173126220703\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.10289173126220703  to: 0.10280416011810303\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.10280416011810303  to: 0.10271661281585694\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.10271661281585694  to: 0.10262917280197144\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.10262917280197144  to: 0.10254195928573609\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.10254195928573609  to: 0.10245506763458252\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.10245506763458252  to: 0.10236854553222656\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.10236854553222656  to: 0.10228240489959717\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.10228240489959717  to: 0.10219666957855225\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.10219666957855225  to: 0.10211113691329957\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.10211113691329957  to: 0.10202240943908691\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.10202240943908691  to: 0.10193291902542115\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.10193291902542115  to: 0.10184279680252076\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.10184279680252076  to: 0.10175220966339112\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.10175220966339112  to: 0.1016613245010376\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.1016613245010376  to: 0.10157016515731812\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.10157016515731812  to: 0.10147851705551147\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.10147851705551147  to: 0.101386559009552\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.101386559009552  to: 0.10129444599151612\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.10129444599151612  to: 0.10120229721069336\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.10120229721069336  to: 0.10111026763916016\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.10111026763916016  to: 0.10101848840713501\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.10101848840713501  to: 0.10092701911926269\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.10092701911926269  to: 0.10083593130111694\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.10083593130111694  to: 0.10074528455734252\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.10074528455734252  to: 0.10065510272979736\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.10065510272979736  to: 0.10056511163711548\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.10056511163711548  to: 0.10047540664672852\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.10047540664672852  to: 0.10038602352142334\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.10038602352142334  to: 0.10029702186584473\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.10029702186584473  to: 0.10020840167999268\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.10020840167999268  to: 0.1001201868057251\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.1001201868057251  to: 0.10003236532211304\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.10003236532211304  to: 0.0999449372291565\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.0999449372291565  to: 0.0998578429222107\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.0998578429222107  to: 0.09977108240127563\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.09977108240127563  to: 0.09968460202217103\n",
      "Training iteration: 1700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.09968460202217103  to: 0.09959833025932312\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.09959833025932312  to: 0.09951224327087402\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.09951224327087402  to: 0.09942639470100403\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.09942639470100403  to: 0.09934071302413941\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.09934071302413941  to: 0.09925514459609985\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.09925514459609985  to: 0.09916961789131165\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.09916961789131165  to: 0.09908406138420105\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.09908406138420105  to: 0.0989984154701233\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.0989984154701233  to: 0.09891265034675598\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.09891265034675598  to: 0.09882647395133973\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.09882647395133973  to: 0.09873992800712586\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.09873992800712586  to: 0.09865301847457886\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.09865301847457886  to: 0.09856578707695007\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.09856578707695007  to: 0.09847823977470398\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.09847823977470398  to: 0.09839042425155639\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.09839042425155639  to: 0.0983023464679718\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.0983023464679718  to: 0.0982140064239502\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.0982140064239502  to: 0.0981253743171692\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.0981253743171692  to: 0.09803646802902222\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.09803646802902222  to: 0.09794702529907226\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.09794702529907226  to: 0.09785707592964173\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.09785707592964173  to: 0.09776664972305298\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.09776664972305298  to: 0.09767578840255738\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.09767578840255738  to: 0.09758448600769043\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.09758448600769043  to: 0.09749274253845215\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.09749274253845215  to: 0.09740055799484253\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.09740055799484253  to: 0.09730790257453918\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.09730790257453918  to: 0.09721475839614868\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.09721475839614868  to: 0.09712109565734864\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.09712109565734864  to: 0.09702690243721009\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.09702690243721009  to: 0.09693213701248168\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.09693213701248168  to: 0.09683676958084106\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.09683676958084106  to: 0.09674080014228821\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.09674080014228821  to: 0.09664422273635864\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.09664422273635864  to: 0.09654709696769714\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.09654709696769714  to: 0.09644943475723267\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.09644943475723267  to: 0.09635124206542969\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.09635124206542969  to: 0.0962525725364685\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.0962525725364685  to: 0.09615343809127808\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.09615343809127808  to: 0.09605385065078735\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.09605385065078735  to: 0.09595383405685425\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.09595383405685425  to: 0.09585338830947876\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.09585338830947876  to: 0.09575250744819641\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.09575250744819641  to: 0.09565119743347168\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.09565119743347168  to: 0.09554951786994934\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.09554951786994934  to: 0.09544750452041625\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.09544750452041625  to: 0.09534521102905273\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.09534521102905273  to: 0.09524263143539428\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.09524263143539428  to: 0.09513980150222778\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.09513980150222778  to: 0.09503666758537292\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.09503666758537292  to: 0.09493318796157837\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.09493318796157837  to: 0.09482929110527039\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.09482929110527039  to: 0.09472492337226868\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.09472492337226868  to: 0.09461997151374817\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.09461997151374817  to: 0.09451456069946289\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.09451456069946289  to: 0.09440869092941284\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.09440869092941284  to: 0.09430235028266906\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.09430235028266906  to: 0.09419552087783814\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.09419552087783814  to: 0.09408819079399108\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.09408819079399108  to: 0.09398032426834106\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.09398032426834106  to: 0.09387189149856567\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.09387189149856567  to: 0.09376290440559387\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.09376290440559387  to: 0.0936533272266388\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.0936533272266388  to: 0.09354320764541627\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.09354320764541627  to: 0.09343292117118836\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.09343292117118836  to: 0.0933224081993103\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.0933224081993103  to: 0.09321201443672181\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.09321201443672181  to: 0.09310218691825867\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.09310218691825867  to: 0.09299335479736329\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.09299335479736329  to: 0.09288576245307922\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.09288576245307922  to: 0.09277955889701843\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.09277955889701843  to: 0.09267469644546508\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.09267469644546508  to: 0.09257103204727173\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.09257103204727173  to: 0.09246826171875\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.09246826171875  to: 0.09236606359481811\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.09236606359481811  to: 0.09226404428482056\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.09226404428482056  to: 0.0921617865562439\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.0921617865562439  to: 0.09205902218818665\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.09205902218818665  to: 0.09195547103881836\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.09195547103881836  to: 0.09185073971748352\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.09185073971748352  to: 0.09174486398696899\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.09174486398696899  to: 0.09163788557052613\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.09163788557052613  to: 0.09153015017509461\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.09153015017509461  to: 0.09142186045646668\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.09142186045646668  to: 0.09131343960762024\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.09131343960762024  to: 0.09120523333549499\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.09120523333549499  to: 0.09109746813774108\n",
      "Training iteration: 1787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.09109746813774108  to: 0.09099019169807435\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.09099019169807435  to: 0.09088336825370788\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.09088336825370788  to: 0.0907767653465271\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.0907767653465271  to: 0.09067017436027527\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.09067017436027527  to: 0.09056323766708374\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.09056323766708374  to: 0.09045554399490356\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.09045554399490356  to: 0.09034677743911743\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.09034677743911743  to: 0.09023663401603699\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.09023663401603699  to: 0.09012500047683716\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.09012500047683716  to: 0.09001302719116211\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.09001302719116211  to: 0.0899034857749939\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.0899034857749939  to: 0.08979631662368774\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.08979631662368774  to: 0.08969138860702515\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.08969138860702515  to: 0.08958839178085327\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.08958839178085327  to: 0.08948693275451661\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.08948693275451661  to: 0.08938652276992798\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.08938652276992798  to: 0.08928660154342652\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.08928660154342652  to: 0.08918645977973938\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.08918645977973938  to: 0.08908546566963196\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.08908546566963196  to: 0.08898299932479858\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.08898299932479858  to: 0.08887855410575866\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.08887855410575866  to: 0.08877172470092773\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.08877172470092773  to: 0.08866219520568848\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.08866219520568848  to: 0.08854982256889343\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.08854982256889343  to: 0.08843454122543334\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.08843454122543334  to: 0.08831634521484374\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.08831634521484374  to: 0.08819488286972046\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.08819488286972046  to: 0.08807042241096497\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.08807042241096497  to: 0.08794323801994323\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.08794323801994323  to: 0.08781347274780274\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.08781347274780274  to: 0.08768126368522644\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.08768126368522644  to: 0.08754663467407227\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.08754663467407227  to: 0.08740957975387573\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.08740957975387573  to: 0.08727025985717773\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.08727025985717773  to: 0.08712868690490723\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.08712868690490723  to: 0.08698485493659973\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.08698485493659973  to: 0.08683887720108033\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.08683887720108033  to: 0.08669090270996094\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.08669090270996094  to: 0.08654114007949829\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.08654114007949829  to: 0.08638982772827149\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.08638982772827149  to: 0.08623723983764649\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.08623723983764649  to: 0.08608361482620239\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.08608361482620239  to: 0.08592911958694457\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.08592911958694457  to: 0.08577386140823365\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.08577386140823365  to: 0.08561485409736633\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.08561485409736633  to: 0.08545364141464233\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.08545364141464233  to: 0.08529154658317566\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.08529154658317566  to: 0.08512945175170898\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.08512945175170898  to: 0.08496776819229127\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.08496776819229127  to: 0.08480650186538696\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.08480650186538696  to: 0.08464535474777221\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.08464535474777221  to: 0.08448365330696106\n",
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.08448365330696106  to: 0.0843213677406311\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.0843213677406311  to: 0.08415865898132324\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.08415865898132324  to: 0.08399590253829955\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.08399590253829955  to: 0.08383359909057617\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.08383359909057617  to: 0.08367218971252441\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.08367218971252441  to: 0.0835119605064392\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.0835119605064392  to: 0.0833529770374298\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.0833529770374298  to: 0.08319432139396668\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.08319432139396668  to: 0.08303568959236145\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.08303568959236145  to: 0.0828765869140625\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.0828765869140625  to: 0.08271631002426147\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.08271631002426147  to: 0.08255428075790405\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.08255428075790405  to: 0.08239001035690308\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.08239001035690308  to: 0.08222329020500183\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.08222329020500183  to: 0.08205450177192689\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.08205450177192689  to: 0.08188351392745971\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.08188351392745971  to: 0.0817103385925293\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.0817103385925293  to: 0.08153526186943054\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.08153526186943054  to: 0.08135838508605957\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.08135838508605957  to: 0.08117960095405578\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.08117960095405578  to: 0.08099857568740845\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.08099857568740845  to: 0.0808150291442871\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.0808150291442871  to: 0.08062871694564819\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.08062871694564819  to: 0.08043956756591797\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.08043956756591797  to: 0.08024758100509644\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.08024758100509644  to: 0.08005326986312866\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.08005326986312866  to: 0.07985711097717285\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.07985711097717285  to: 0.07965416312217713\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.07965416312217713  to: 0.07945717573165893\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.07945717573165893  to: 0.07926088571548462\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.07926088571548462  to: 0.07906391024589539\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.07906391024589539  to: 0.07886639833450318\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.07886639833450318  to: 0.07866796851158142\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.07866796851158142  to: 0.07846843004226685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.07846843004226685  to: 0.0782679259777069\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.0782679259777069  to: 0.07806733846664429\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.07806733846664429  to: 0.07786802649497986\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.07786802649497986  to: 0.07767146229743957\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.07767146229743957  to: 0.07747907638549804\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.07747907638549804  to: 0.07729088068008423\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.07729088068008423  to: 0.07710577845573426\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.07710577845573426  to: 0.0769222617149353\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.0769222617149353  to: 0.07673876285552979\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.07673876285552979  to: 0.07655407786369324\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.07655407786369324  to: 0.07636783719062805\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.07636783719062805  to: 0.07618460655212403\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.07618460655212403  to: 0.07600609064102173\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.07600609064102173  to: 0.07583763003349304\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.07583763003349304  to: 0.07567136287689209\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.07567136287689209  to: 0.07550668716430664\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.07550668716430664  to: 0.07534203529357911\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.07534203529357911  to: 0.0751756489276886\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.0751756489276886  to: 0.0750066101551056\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.0750066101551056  to: 0.07483419179916381\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.07483419179916381  to: 0.07466642260551452\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.07466642260551452  to: 0.07450382113456726\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.07450382113456726  to: 0.07434527277946472\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.07434527277946472  to: 0.07418829202651978\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.07418829202651978  to: 0.07402945756912231\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.07402945756912231  to: 0.07386626005172729\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.07386626005172729  to: 0.07369783520698547\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.07369783520698547  to: 0.0735247254371643\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.0735247254371643  to: 0.07334780693054199\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.07334780693054199  to: 0.07316731214523316\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.07316731214523316  to: 0.0729859709739685\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.0729859709739685  to: 0.07280157208442688\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.07280157208442688  to: 0.07260934114456177\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.07260934114456177  to: 0.07240984439849854\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.07240984439849854  to: 0.07220384478569031\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.07220384478569031  to: 0.07199344635009766\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.07199344635009766  to: 0.07178035974502564\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.07178035974502564  to: 0.07156686782836914\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.07156686782836914  to: 0.0713534414768219\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.0713534414768219  to: 0.0711390495300293\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.0711390495300293  to: 0.07092363834381103\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.07092363834381103  to: 0.07070804834365844\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.07070804834365844  to: 0.07049355506896973\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.07049355506896973  to: 0.07028157114982606\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.07028157114982606  to: 0.0700719952583313\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.0700719952583313  to: 0.06986393332481385\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.06986393332481385  to: 0.06965606808662414\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.06965606808662414  to: 0.06944718360900878\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.06944718360900878  to: 0.06923271417617798\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.06923271417617798  to: 0.06901297569274903\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.06901297569274903  to: 0.0687882959842682\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.0687882959842682  to: 0.06855805516242981\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.06855805516242981  to: 0.06832102537155152\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.06832102537155152  to: 0.06807639002799988\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.06807639002799988  to: 0.0678243339061737\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.0678243339061737  to: 0.06757184863090515\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.06757184863090515  to: 0.06731932759284973\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.06731932759284973  to: 0.06706673502922059\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.06706673502922059  to: 0.06681437492370605\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.06681437492370605  to: 0.06656290888786316\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.06656290888786316  to: 0.06631330251693726\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.06631330251693726  to: 0.06606458425521851\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.06606458425521851  to: 0.06581671237945556\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.06581671237945556  to: 0.065569669008255\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.065569669008255  to: 0.06532170176506043\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.06532170176506043  to: 0.06507685780525208\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.06507685780525208  to: 0.06483367085456848\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.06483367085456848  to: 0.06457533240318299\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.06457533240318299  to: 0.06430810093879699\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.06430810093879699  to: 0.06403313875198365\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.06403313875198365  to: 0.06374489068984986\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.06374489068984986  to: 0.06344873309135438\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.06344873309135438  to: 0.06315902471542359\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.06315902471542359  to: 0.06288191080093383\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.06288191080093383  to: 0.06261476874351501\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.06261476874351501  to: 0.062354487180709836\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.062354487180709836  to: 0.06210225820541382\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.06210225820541382  to: 0.06186034679412842\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.06186034679412842  to: 0.061625832319259645\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.061625832319259645  to: 0.06138980984687805\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.06138980984687805  to: 0.061141735315322875\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.061141735315322875  to: 0.060883671045303345\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.060883671045303345  to: 0.060617053508758546\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.060617053508758546  to: 0.060347622632980345\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.060347622632980345  to: 0.060072910785675046\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.060072910785675046  to: 0.059797412157058714\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.059797412157058714  to: 0.05952423810958862\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.05952423810958862  to: 0.059253638982772826\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.059253638982772826  to: 0.058984297513961795\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.058984297513961795  to: 0.058713662624359134\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.058713662624359134  to: 0.058471310138702395\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.058471310138702395  to: 0.058240413665771484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.058240413665771484  to: 0.05801917314529419\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.05801917314529419  to: 0.05780572295188904\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.05780572295188904  to: 0.05759233236312866\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.05759233236312866  to: 0.057367068529129026\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.057367068529129026  to: 0.057125532627105714\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.057125532627105714  to: 0.05686942934989929\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.05686942934989929  to: 0.05659127235412598\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.05659127235412598  to: 0.056284093856811525\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.056284093856811525  to: 0.055967080593109134\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.055967080593109134  to: 0.055647236108779904\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.055647236108779904  to: 0.05532779097557068\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.05532779097557068  to: 0.055018442869186404\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.055018442869186404  to: 0.054723703861236574\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.054723703861236574  to: 0.05444028377532959\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.05444028377532959  to: 0.05416942834854126\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.05416942834854126  to: 0.05390950441360474\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.05390950441360474  to: 0.053654181957244876\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.053654181957244876  to: 0.05340510606765747\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.05340510606765747  to: 0.05316098928451538\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.05316098928451538  to: 0.05291646718978882\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.05291646718978882  to: 0.05267515182495117\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.05267515182495117  to: 0.0524346649646759\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.0524346649646759  to: 0.05219559073448181\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.05219559073448181  to: 0.05196186900138855\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.05196186900138855  to: 0.05173026919364929\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.05173026919364929  to: 0.05150616765022278\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.05150616765022278  to: 0.051283806562423706\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.051283806562423706  to: 0.051066672801971434\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.051066672801971434  to: 0.05084647536277771\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.05084647536277771  to: 0.05062640905380249\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.05062640905380249  to: 0.050394093990325926\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.050394093990325926  to: 0.05016035437583923\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.05016035437583923  to: 0.04990058839321136\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.04990058839321136  to: 0.04965752065181732\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.04965752065181732  to: 0.049359488487243655\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.049359488487243655  to: 0.04919012188911438\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.04919012188911438  to: 0.0489007294178009\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.0489007294178009  to: 0.048709988594055176\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.048709988594055176  to: 0.04820315837860108\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.04820315837860108  to: 0.04792811870574951\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.04792811870574951  to: 0.04762307703495026\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.04762307703495026  to: 0.04726826548576355\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.04726826548576355  to: 0.04690058827400208\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.04690058827400208  to: 0.04653660655021667\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.04653660655021667  to: 0.04632076621055603\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.04632076621055603  to: 0.0458590567111969\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.0458590567111969  to: 0.04552427232265473\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.04552427232265473  to: 0.045292901992797854\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.045292901992797854  to: 0.044847887754440305\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.044847887754440305  to: 0.044504395127296446\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.044504395127296446  to: 0.04417600631713867\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.04417600631713867  to: 0.04377903938293457\n",
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.04377903938293457  to: 0.04338144361972809\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.04338144361972809  to: 0.04297330379486084\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.04297330379486084  to: 0.042593011260032655\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.042593011260032655  to: 0.04219319224357605\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.04219319224357605  to: 0.04178731441497803\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.04178731441497803  to: 0.04143235683441162\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.04143235683441162  to: 0.04108419418334961\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.04108419418334961  to: 0.04071706831455231\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.04071706831455231  to: 0.04040185511112213\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.04040185511112213  to: 0.04010234773159027\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.04010234773159027  to: 0.039721506834030154\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.039721506834030154  to: 0.03933398425579071\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.03933398425579071  to: 0.038945955038070676\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.038945955038070676  to: 0.038479724526405336\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.038479724526405336  to: 0.0380554735660553\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.0380554735660553  to: 0.03766766488552094\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.03766766488552094  to: 0.03727537095546722\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.03727537095546722  to: 0.03695659041404724\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.03695659041404724  to: 0.036539822816848755\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.036539822816848755  to: 0.036161547899246214\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.036161547899246214  to: 0.03572090268135071\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.03572090268135071  to: 0.03526086509227753\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.03526086509227753  to: 0.03486896753311157\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.03486896753311157  to: 0.03441003262996674\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.03441003262996674  to: 0.034150370955467226\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.034150370955467226  to: 0.03370632529258728\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.03370632529258728  to: 0.03357348144054413\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.03357348144054413  to: 0.033039221167564393\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.033039221167564393  to: 0.032495340704917906\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.032495340704917906  to: 0.031843441724777224\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.031843441724777224  to: 0.031415581703186035\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.031415581703186035  to: 0.031136125326156616\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.031136125326156616  to: 0.030690151453018188\n",
      "Training iteration: 2050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.030690151453018188  to: 0.030087998509407042\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.030087998509407042  to: 0.02959849238395691\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.02959849238395691  to: 0.029225629568099976\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.029225629568099976  to: 0.028810107707977296\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.028810107707977296  to: 0.028317928314208984\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.028317928314208984  to: 0.027783697843551634\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.027783697843551634  to: 0.027402830123901368\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.027402830123901368  to: 0.027155771851539612\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.027155771851539612  to: 0.026891207695007323\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.026891207695007323  to: 0.02668280303478241\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.02668280303478241  to: 0.02614079713821411\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.02614079713821411  to: 0.02584802508354187\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.02584802508354187  to: 0.025426802039146424\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.025426802039146424  to: 0.02502559721469879\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.02502559721469879  to: 0.024821364879608156\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.024821364879608156  to: 0.02416653633117676\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.02416653633117676  to: 0.02414158880710602\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.02414158880710602  to: 0.023256909847259522\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.02370799481868744\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.023256909847259522  to: 0.02203511744737625\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.02207575589418411\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.02203511744737625  to: 0.02130940854549408\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.02130940854549408  to: 0.02067093551158905\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): 0.02134624421596527\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.02067093551158905  to: 0.019686366617679595\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.019686366617679595  to: 0.019661180675029755\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.019661180675029755  to: 0.019280853867530822\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.019280853867530822  to: 0.018726010620594025\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.019097456336021425\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.018726010620594025  to: 0.01778288334608078\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.01778288334608078  to: 0.017551977932453156\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.017551977932453156  to: 0.01695347875356674\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.01695347875356674  to: 0.016625165939331055\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): 0.016751129925251008\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.016625165939331055  to: 0.01591190993785858\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.015990321338176728\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.01591190993785858  to: 0.015021321177482606\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.015021321177482606  to: 0.01502009928226471\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.01502009928226471  to: 0.014384248852729797\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.014384248852729797  to: 0.014066457748413086\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.014066457748413086  to: 0.013726988434791565\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.013726988434791565  to: 0.013329392671585083\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): 0.013427239656448365\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.013329392671585083  to: 0.013288119435310363\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.014001739025115967\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.013317295908927917\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.013288119435310363  to: 0.013039901852607727\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.013039901852607727  to: 0.011071328073740005\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.011071328073740005  to: 0.010919185727834702\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.011992859840393066\n",
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.010919185727834702  to: 0.010914818942546844\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.010914818942546844  to: 0.009893958270549775\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.009893958270549775  to: 0.009515094012022019\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.0097897507250309\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.009900985658168793\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.009515094012022019  to: 0.008810627460479736\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.008810627460479736  to: 0.00847211480140686\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.008638890087604522\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.00847211480140686  to: 0.008174850791692733\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.008174850791692733  to: 0.007746894657611847\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.007746894657611847  to: 0.007558467239141465\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.007558467239141465  to: 0.007437418401241303\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.007437418401241303  to: 0.007201255112886429\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.007201255112886429  to: 0.006659092009067535\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.006659092009067535  to: 0.006289036571979522\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.006289036571979522  to: 0.006125517562031746\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.006125517562031746  to: 0.005937628075480461\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.005937628075480461  to: 0.005758257582783699\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.005758257582783699  to: 0.005513042211532593\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.005542833730578423\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.005513042211532593  to: 0.005385583639144898\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.0055523581802845\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.005385583639144898  to: 0.004633376374840736\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.004633376374840736  to: 0.004217343404889107\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.004217343404889107  to: 0.0032632943242788316\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.0032632943242788316  to: 0.0030334973707795143\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.0030334973707795143  to: 0.0029832810163497926\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.0029832810163497926  to: 0.002801835536956787\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.002935934439301491\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.002873615548014641\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.0030791252851486207\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.002801835536956787  to: 0.002357666194438934\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.002357666194438934  to: 0.0018951870501041413\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.0018951870501041413  to: 0.0010054395534098148\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.0010054395534098148  to: 0.0007088982500135898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.0008097472600638866\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.0007088982500135898  to: 0.0006643205881118775\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.0008922303095459938\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.0006643205881118775  to: 0.00035526282154023646\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.000474974000826478\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.00035526282154023646  to: -0.0004927058704197407\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: -0.0004927058704197407  to: -0.0006838006433099508\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: -0.0006838006433099508  to: -0.0014831480570137502\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: -0.0014831480570137502  to: -0.001813211664557457\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: -0.001813211664557457  to: -0.0018609460443258286\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: -0.0018609460443258286  to: -0.002054398134350777\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): -0.0015868667513132095\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): -0.0018040720373392106\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): -0.0005899463780224323\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): -0.0012171109206974506\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): -0.0008757493458688259\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: -0.002054398134350777  to: -0.003582802042365074\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: -0.003582802042365074  to: -0.00427652858197689\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): -0.003241364285349846\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): -0.003478105366230011\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): -0.004032328724861145\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: -0.00427652858197689  to: -0.005344542115926743\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): -0.004944238439202308\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): -0.003690742701292038\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): -0.005075182393193245\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: -0.005344542115926743  to: -0.005783624202013016\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): -0.005563299730420112\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): -0.005600220710039139\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): -0.005449125915765763\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: -0.005783624202013016  to: -0.006280171126127243\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: -0.006280171126127243  to: -0.006531958281993866\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): -0.006110827252268791\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: -0.006531958281993866  to: -0.0066114582121372225\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): -0.006427599489688874\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: -0.0066114582121372225  to: -0.00726727768778801\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: -0.00726727768778801  to: -0.0074773386120796205\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: -0.0074773386120796205  to: -0.007716119289398193\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: -0.007716119289398193  to: -0.007973244041204452\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): -0.007368306815624237\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): -0.00774151086807251\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): -0.005306637287139893\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): -0.004981593042612076\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.0004453358240425587\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): -0.00586627908051014\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: -0.007973244041204452  to: -0.010198795795440673\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): -0.005815009027719498\n",
      "Training iteration: 2181\n",
      "Validation loss (no improvement): -0.007361654937267303\n",
      "Training iteration: 2182\n",
      "Validation loss (no improvement): -0.009885288029909133\n",
      "Training iteration: 2183\n",
      "Validation loss (no improvement): -0.005933170393109322\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): -0.009841578453779221\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): -0.008750013262033462\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): -0.007874351739883424\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): -0.007966876029968262\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): -0.007906763255596161\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): -0.00876767486333847\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): -0.007731692492961883\n",
      "Training iteration: 2191\n",
      "Validation loss (no improvement): -0.008888866752386093\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): -0.007906542718410492\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): -0.008507517725229263\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): -0.007432705163955689\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): -0.008350064605474472\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): -0.007863297313451766\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): -0.008884867280721664\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): -0.008732723444700241\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): -0.009022160619497299\n",
      "Training iteration: 2200\n",
      "Validation loss (no improvement): -0.009130515158176422\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): -0.008932329714298248\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): -0.009761707484722137\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): -0.009893582761287689\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: -0.010198795795440673  to: -0.010307377576828003\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: -0.010307377576828003  to: -0.01044497936964035\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: -0.01044497936964035  to: -0.010554883629083633\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: -0.010554883629083633  to: -0.010594943910837174\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: -0.010594943910837174  to: -0.010625903308391572\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: -0.010625903308391572  to: -0.011233413219451904\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): -0.011069629341363907\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): -0.011223308742046356\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): -0.011145975440740585\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): -0.010134994983673096\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): -0.01102377027273178\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): -0.010580986738204956\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): -0.0107693150639534\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): -0.010550963878631591\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): -0.010088112205266953\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): -0.010831932723522186\n",
      "Training iteration: 2220\n",
      "Validation loss (no improvement): -0.009151493012905122\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): -0.009340741485357285\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.0011819529347121716\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.005161925405263901\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.003051388822495937\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: -0.011233413219451904  to: -0.015482042729854584\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): -0.006770339608192444\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): -0.013672387599945069\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): -0.0036623649299144744\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): -0.012930938601493835\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): -0.009265740215778352\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): -0.01175858974456787\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): -0.01027417927980423\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): -0.008079271763563156\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): -0.010187723487615586\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): -0.007374107837677002\n",
      "Training iteration: 2236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.006568054854869843\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): -0.007970170676708221\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): -0.006298289448022842\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): -0.006228410452604294\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): -0.0076098904013633725\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): -0.006526418030261993\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): -0.006113191694021225\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): -0.007644335180521012\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): -0.008085860311985016\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): -0.007988478988409042\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): -0.008079950511455537\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): -0.007155340164899826\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): -0.00700766071677208\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): -0.00845286101102829\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): -0.009352481365203858\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): -0.009930682182312012\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): -0.010360030084848404\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): -0.009999291598796844\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): -0.010603334754705429\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): -0.011738284677267074\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): -0.012033084779977799\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): -0.012486454099416733\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): -0.012614141404628753\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): -0.013121142983436584\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): -0.014251160621643066\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): -0.01487444043159485\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: -0.015482042729854584  to: -0.015568619966506958\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: -0.015568619966506958  to: -0.015847891569137573\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: -0.015847891569137573  to: -0.016415414214134217\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: -0.016415414214134217  to: -0.016687479615211488\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: -0.016687479615211488  to: -0.017210055887699128\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: -0.017210055887699128  to: -0.017779424786567688\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: -0.017779424786567688  to: -0.01838240921497345\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: -0.01838240921497345  to: -0.018763747811317445\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: -0.018763747811317445  to: -0.01923826038837433\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: -0.01923826038837433  to: -0.019532065093517303\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: -0.019532065093517303  to: -0.01969631016254425\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): -0.019679939746856688\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: -0.01969631016254425  to: -0.020308594405651092\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: -0.020308594405651092  to: -0.020701536536216737\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): -0.020683005452156067\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: -0.020701536536216737  to: -0.020995347201824187\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: -0.020995347201824187  to: -0.021119126677513124\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): -0.020928826928138734\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: -0.021119126677513124  to: -0.021416254341602325\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: -0.021416254341602325  to: -0.021781620383262635\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): -0.021562512218952178\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: -0.021781620383262635  to: -0.021896691620349885\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): -0.0218696266412735\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): -0.021818225085735322\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: -0.021896691620349885  to: -0.02235877960920334\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): -0.02209879606962204\n",
      "Training iteration: 2288\n",
      "Improved validation loss from: -0.02235877960920334  to: -0.022411298751831055\n",
      "Training iteration: 2289\n",
      "Validation loss (no improvement): -0.022283558547496796\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): -0.02240414321422577\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: -0.022411298751831055  to: -0.022680261731147768\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): -0.022451944649219513\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: -0.022680261731147768  to: -0.022726063430309296\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): -0.022188177704811095\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: -0.022726063430309296  to: -0.02277071923017502\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): -0.021572121977806093\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): -0.02180562764406204\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): -0.017715828120708467\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): -0.015020719170570374\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): -0.008629151433706284\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): -0.01736121326684952\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: -0.02277071923017502  to: -0.026285344362258913\n",
      "Training iteration: 2303\n",
      "Validation loss (no improvement): -0.01814279556274414\n",
      "Training iteration: 2304\n",
      "Validation loss (no improvement): -0.021414852142333983\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): -0.022303500771522523\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): -0.02219308614730835\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): -0.023640811443328857\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): -0.020511329174041748\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): -0.02073088437318802\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): -0.021976378560066224\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): -0.020992036163806915\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): -0.020307061076164246\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): -0.0174168735742569\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): -0.02048579752445221\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): -0.021015405654907227\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): -0.020999948680400848\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): -0.01839403808116913\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): -0.017657944560050966\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): -0.020176708698272705\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): -0.020474544167518614\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): -0.01998264491558075\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): -0.017881242930889128\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): -0.01914855241775513\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): -0.021273989975452424\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): -0.02157641649246216\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): -0.020480266213417052\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): -0.01945783793926239\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): -0.021391670405864715\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): -0.022374074161052703\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): -0.022341307997703553\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): -0.021494536101818083\n",
      "Training iteration: 2332\n",
      "Validation loss (no improvement): -0.022771844267845155\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): -0.023548424243927002\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): -0.023206834495067597\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): -0.022510702908039092\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): -0.02380003184080124\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): -0.02436717301607132\n",
      "Training iteration: 2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.023864562809467315\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): -0.02397523820400238\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): -0.025035327672958373\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): -0.024897679686546326\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): -0.024174103140830995\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): -0.025086981058120728\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): -0.025357207655906676\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): -0.02485361397266388\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): -0.025446894764900207\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): -0.02519289553165436\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): -0.02455616444349289\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): -0.02525606155395508\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): -0.02486271858215332\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): -0.024601264297962187\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): -0.024717691540718078\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): -0.023847536742687227\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): -0.024388411641120912\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): -0.023539820313453676\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): -0.02404477298259735\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): -0.023363223671913146\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): -0.023550505936145782\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): -0.022850139439105986\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): -0.023526592552661894\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): -0.022120001912117004\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): -0.023216286301612855\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): -0.017265661060810088\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): -0.016724035143852234\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.015873311460018157\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.007090680301189423\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): -0.01833922415971756\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): -0.0063655093312263485\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): -0.02486104965209961\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): -0.01851334273815155\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): -0.015691064298152924\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): -0.0029656637459993364\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): -0.020319743454456328\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): -0.01986442059278488\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): -0.01776876747608185\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): -0.017382866144180296\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): -0.006614012271165847\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): -0.004080447554588318\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): -0.010745074599981308\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): -0.013225337862968445\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): -0.013621644675731659\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): -0.01457372009754181\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): -0.01017373353242874\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): -0.003205857425928116\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): -0.0036681950092315673\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): -0.00869094580411911\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): -0.010623864829540253\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): -0.011194773018360138\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): -0.013291364908218384\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): -0.012937481701374053\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): -0.008265598863363265\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): -0.006241070106625557\n",
      "Training iteration: 2393\n",
      "Validation loss (no improvement): -0.009665977954864503\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): -0.012318763881921768\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): -0.012470968067646027\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): -0.01374698281288147\n",
      "Training iteration: 2397\n",
      "Validation loss (no improvement): -0.014964111149311066\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): -0.012859301269054412\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): -0.011337586492300034\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): -0.014254146814346313\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): -0.017101964354515074\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): -0.017525210976600647\n"
     ]
    }
   ],
   "source": [
    "mixture_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 13.363334655761719\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 13.363334655761719  to: 9.462158203125\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 9.462158203125  to: 6.846082305908203\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 6.846082305908203  to: 5.0579689025878904\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 5.0579689025878904  to: 3.8142738342285156\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 3.8142738342285156  to: 2.934548568725586\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 2.934548568725586  to: 2.3033559799194334\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 2.3033559799194334  to: 1.8461559295654297\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.8461559295654297  to: 1.5068652153015136\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.5068652153015136  to: 1.249727439880371\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.249727439880371  to: 1.0521143913269042\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 1.0521143913269042  to: 0.8981446266174317\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.8981446266174317  to: 0.7768502712249756\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.7768502712249756  to: 0.6800601959228516\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.6800601959228516  to: 0.6018518924713134\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.6018518924713134  to: 0.5380349159240723\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.5380349159240723  to: 0.48551149368286134\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.48551149368286134  to: 0.4419602870941162\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.4419602870941162  to: 0.40560340881347656\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.40560340881347656  to: 0.3750251054763794\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.3750251054763794  to: 0.3491369247436523\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.3491369247436523  to: 0.3271007537841797\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.3271007537841797  to: 0.308255934715271\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.308255934715271  to: 0.2920703887939453\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.2920703887939453  to: 0.27808926105499265\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.27808926105499265  to: 0.2659806728363037\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.2659806728363037  to: 0.2554461002349854\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.2554461002349854  to: 0.24622890949249268\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.24622890949249268  to: 0.23813464641571044\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.23813464641571044  to: 0.23100428581237792\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.23100428581237792  to: 0.22469820976257324\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.22469820976257324  to: 0.21909918785095214\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.21909918785095214  to: 0.21411268711090087\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.21411268711090087  to: 0.20965878963470458\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.20965878963470458  to: 0.2056654930114746\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.2056654930114746  to: 0.20207645893096923\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.20207645893096923  to: 0.1988414168357849\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.1988414168357849  to: 0.19591684341430665\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.19591684341430665  to: 0.19326536655426024\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.19326536655426024  to: 0.1908537268638611\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.1908537268638611  to: 0.1886553168296814\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.1886553168296814  to: 0.1866457939147949\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.1866457939147949  to: 0.18480379581451417\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.18480379581451417  to: 0.18311084508895875\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.18311084508895875  to: 0.18155092000961304\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.18155092000961304  to: 0.18010990619659423\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.18010990619659423  to: 0.17877540588378907\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.17877540588378907  to: 0.17753661870956422\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.17753661870956422  to: 0.17638438940048218\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.17638438940048218  to: 0.17530958652496337\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.17530958652496337  to: 0.17430467605590821\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.17430467605590821  to: 0.17336299419403076\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.17336299419403076  to: 0.17247867584228516\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.17247867584228516  to: 0.17164634466171264\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.17164634466171264  to: 0.17086141109466552\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.17086141109466552  to: 0.17011983394622804\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.17011983394622804  to: 0.169417667388916\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.169417667388916  to: 0.16875146627426146\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.16875146627426146  to: 0.1681182861328125\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.1681182861328125  to: 0.16751537322998047\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.16751537322998047  to: 0.1669403076171875\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.1669403076171875  to: 0.16639093160629273\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.16639093160629273  to: 0.16586542129516602\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.16586542129516602  to: 0.16536176204681396\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.16536176204681396  to: 0.16487839221954345\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.16487839221954345  to: 0.16441234350204467\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.16441234350204467  to: 0.16396316289901733\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.16396316289901733  to: 0.1635304093360901\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.1635304093360901  to: 0.16311289072036744\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.16311289072036744  to: 0.16270952224731444\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.16270952224731444  to: 0.16231915950775147\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.16231915950775147  to: 0.16194082498550416\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.16194082498550416  to: 0.1615741729736328\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.1615741729736328  to: 0.16121848821640014\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.16121848821640014  to: 0.1608731508255005\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.1608731508255005  to: 0.1605375289916992\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.1605375289916992  to: 0.16021114587783813\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.16021114587783813  to: 0.15989348888397217\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.15989348888397217  to: 0.15958408117294312\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.15958408117294312  to: 0.15928251743316652\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.15928251743316652  to: 0.15898842811584474\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.15898842811584474  to: 0.15870144367218017\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.15870144367218017  to: 0.15842123031616212\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.15842123031616212  to: 0.15814745426177979\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.15814745426177979  to: 0.1578797459602356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 85\n",
      "Improved validation loss from: 0.1578797459602356  to: 0.15761642456054686\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.15761642456054686  to: 0.1573588013648987\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1573588013648987  to: 0.157106614112854\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.157106614112854  to: 0.15685962438583373\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.15685962438583373  to: 0.1566174268722534\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1566174268722534  to: 0.15637927055358886\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.15637927055358886  to: 0.1561457872390747\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1561457872390747  to: 0.15591681003570557\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.15591681003570557  to: 0.15569216012954712\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15569216012954712  to: 0.15547173023223876\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.15547173023223876  to: 0.15525537729263306\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15525537729263306  to: 0.15504295825958253\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.15504295825958253  to: 0.15483434200286866\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.15483434200286866  to: 0.1546294569969177\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.1546294569969177  to: 0.15442816019058228\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.15442816019058228  to: 0.15423036813735963\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.15423036813735963  to: 0.15403597354888915\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.15403597354888915  to: 0.15384485721588134\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.15384485721588134  to: 0.1536569595336914\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.1536569595336914  to: 0.1534721612930298\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.1534721612930298  to: 0.15329040288925172\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.15329040288925172  to: 0.1531116008758545\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.1531116008758545  to: 0.15293567180633544\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.15293567180633544  to: 0.1527625560760498\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.1527625560760498  to: 0.15259217023849486\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.15259217023849486  to: 0.15242445468902588\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.15242445468902588  to: 0.15225932598114014\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.15225932598114014  to: 0.152096688747406\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.152096688747406  to: 0.15193643569946289\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.15193643569946289  to: 0.15177853107452394\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.15177853107452394  to: 0.15162293910980223\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.15162293910980223  to: 0.15146958827972412\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.15146958827972412  to: 0.1513184666633606\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1513184666633606  to: 0.151169490814209\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.151169490814209  to: 0.15102264881134034\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.15102264881134034  to: 0.1508778691291809\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.1508778691291809  to: 0.15073511600494385\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.15073511600494385  to: 0.15059434175491332\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.15059434175491332  to: 0.1504555344581604\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.1504555344581604  to: 0.15031859874725342\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.15031859874725342  to: 0.1501835346221924\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.1501835346221924  to: 0.15005030632019042\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.15005030632019042  to: 0.14991886615753175\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14991886615753175  to: 0.1497890830039978\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.1497890830039978  to: 0.14966070652008057\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14966070652008057  to: 0.14953399896621705\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14953399896621705  to: 0.14940893650054932\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14940893650054932  to: 0.14928548336029052\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14928548336029052  to: 0.1491636276245117\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.1491636276245117  to: 0.14904329776763917\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14904329776763917  to: 0.1489244818687439\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.1489244818687439  to: 0.14880716800689697\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14880716800689697  to: 0.1486912488937378\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1486912488937378  to: 0.14857676029205322\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14857676029205322  to: 0.14846365451812743\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14846365451812743  to: 0.14835188388824463\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14835188388824463  to: 0.14824144840240477\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14824144840240477  to: 0.1481323003768921\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.1481323003768921  to: 0.14802442789077758\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14802442789077758  to: 0.1479177951812744\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.1479177951812744  to: 0.1478123903274536\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.1478123903274536  to: 0.1477081894874573\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1477081894874573  to: 0.14760516881942748\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14760516881942748  to: 0.147502863407135\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.147502863407135  to: 0.14740135669708251\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14740135669708251  to: 0.14730091094970704\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14730091094970704  to: 0.14720146656036376\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14720146656036376  to: 0.14710309505462646\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14710309505462646  to: 0.14700578451156615\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14700578451156615  to: 0.14690951108932496\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14690951108932496  to: 0.14681425094604492\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14681425094604492  to: 0.14671996831893921\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14671996831893921  to: 0.14662666320800782\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.14662666320800782  to: 0.14653422832489013\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14653422832489013  to: 0.1464420199394226\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.1464420199394226  to: 0.14635074138641357\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14635074138641357  to: 0.14626038074493408\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14626038074493408  to: 0.14617089033126832\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14617089033126832  to: 0.14608227014541625\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14608227014541625  to: 0.1459944725036621\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1459944725036621  to: 0.1459074854850769\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.1459074854850769  to: 0.1458213210105896\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.1458213210105896  to: 0.1457359552383423\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1457359552383423  to: 0.14565134048461914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 169\n",
      "Improved validation loss from: 0.14565134048461914  to: 0.14556751251220704\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14556751251220704  to: 0.14548442363739014\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14548442363739014  to: 0.14540207386016846\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14540207386016846  to: 0.14532042741775514\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14532042741775514  to: 0.1452394962310791\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.1452394962310791  to: 0.14515924453735352\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14515924453735352  to: 0.14507968425750734\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14507968425750734  to: 0.14500080347061156\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14500080347061156  to: 0.1449225664138794\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1449225664138794  to: 0.14484498500823975\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14484498500823975  to: 0.14476803541183472\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14476803541183472  to: 0.14469171762466432\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14469171762466432  to: 0.14461599588394164\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14461599588394164  to: 0.1445408821105957\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.1445408821105957  to: 0.14446637630462647\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.14446637630462647  to: 0.14439243078231812\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14439243078231812  to: 0.14431908130645751\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.14431908130645751  to: 0.1442462682723999\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.1442462682723999  to: 0.14417403936386108\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.14417403936386108  to: 0.14410237073898316\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.14410237073898316  to: 0.14403122663497925\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.14403122663497925  to: 0.1439605951309204\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.1439605951309204  to: 0.14389050006866455\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.14389050006866455  to: 0.14382089376449586\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.14382089376449586  to: 0.14375180006027222\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.14375180006027222  to: 0.14368317127227784\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.14368317127227784  to: 0.1436150550842285\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.1436150550842285  to: 0.14354727268218995\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14354727268218995  to: 0.14347991943359376\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.14347991943359376  to: 0.1434130311012268\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1434130311012268  to: 0.14334657192230224\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.14334657192230224  to: 0.14328057765960694\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.14328057765960694  to: 0.14321501255035402\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14321501255035402  to: 0.14314987659454345\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.14314987659454345  to: 0.14308515787124634\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.14308515787124634  to: 0.1430208206176758\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.1430208206176758  to: 0.14295692443847657\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.14295692443847657  to: 0.1428934097290039\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.1428934097290039  to: 0.14283028841018677\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.14283028841018677  to: 0.1427675485610962\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.1427675485610962  to: 0.14270517826080323\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.14270517826080323  to: 0.14264320135116576\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.14264320135116576  to: 0.14258159399032594\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.14258159399032594  to: 0.14252034425735474\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.14252034425735474  to: 0.14245944023132323\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.14245944023132323  to: 0.14239890575408937\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.14239890575408937  to: 0.14233872890472413\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.14233872890472413  to: 0.14227885007858276\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.14227885007858276  to: 0.14221906661987305\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.14221906661987305  to: 0.14215959310531617\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.14215959310531617  to: 0.14210046529769899\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.14210046529769899  to: 0.14204165935516358\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.14204165935516358  to: 0.14198315143585205\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.14198315143585205  to: 0.1419249653816223\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.1419249653816223  to: 0.1418670892715454\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1418670892715454  to: 0.14180948734283447\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.14180948734283447  to: 0.14175221920013428\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.14175221920013428  to: 0.14169522523880004\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.14169522523880004  to: 0.1416385293006897\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.1416385293006897  to: 0.14158213138580322\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.14158213138580322  to: 0.1415260076522827\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.1415260076522827  to: 0.14147015810012817\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.14147015810012817  to: 0.1414145827293396\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.1414145827293396  to: 0.14135926961898804\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.14135926961898804  to: 0.14130420684814454\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.14130420684814454  to: 0.1412493944168091\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1412493944168091  to: 0.14119484424591064\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.14119484424591064  to: 0.1411405086517334\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.1411405086517334  to: 0.1410864233970642\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1410864233970642  to: 0.14103260040283203\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.14103260040283203  to: 0.14097899198532104\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.14097899198532104  to: 0.14092562198638917\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.14092562198638917  to: 0.1408724904060364\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.1408724904060364  to: 0.14081958532333375\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.14081958532333375  to: 0.14076690673828124\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.14076690673828124  to: 0.1407144546508789\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.1407144546508789  to: 0.14066218137741088\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.14066218137741088  to: 0.14061011075973512\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.14061011075973512  to: 0.1405583143234253\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.1405583143234253  to: 0.14050674438476562\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.14050674438476562  to: 0.1404553771018982\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.1404553771018982  to: 0.14040423631668092\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.14040423631668092  to: 0.14035327434539796\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.14035327434539796  to: 0.14030252695083617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 253\n",
      "Improved validation loss from: 0.14030252695083617  to: 0.1402519702911377\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.1402519702911377  to: 0.14020164012908937\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.14020164012908937  to: 0.14015140533447265\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.14015140533447265  to: 0.14010136127471923\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.14010136127471923  to: 0.1400514841079712\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.1400514841079712  to: 0.14000182151794432\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.14000182151794432  to: 0.13995234966278075\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13995234966278075  to: 0.13990304470062256\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13990304470062256  to: 0.13985393047332764\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.13985393047332764  to: 0.13980500698089598\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13980500698089598  to: 0.13975626230239868\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13975626230239868  to: 0.13970770835876464\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13970770835876464  to: 0.13965933322906493\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13965933322906493  to: 0.1396111249923706\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.1396111249923706  to: 0.1395631194114685\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.1395631194114685  to: 0.1395152688026428\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.1395152688026428  to: 0.1394675850868225\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.1394675850868225  to: 0.13942006826400757\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.13942006826400757  to: 0.13937273025512695\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13937273025512695  to: 0.13932554721832274\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13932554721832274  to: 0.13927853107452393\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13927853107452393  to: 0.1392316699028015\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.1392316699028015  to: 0.13918497562408447\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13918497562408447  to: 0.13913841247558595\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13913841247558595  to: 0.13909202814102173\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13909202814102173  to: 0.139045786857605\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.139045786857605  to: 0.13899967670440674\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13899967670440674  to: 0.1389537572860718\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.1389537572860718  to: 0.13890795707702636\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13890795707702636  to: 0.1388622999191284\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.1388622999191284  to: 0.13881679773330688\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13881679773330688  to: 0.1387714385986328\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1387714385986328  to: 0.13872623443603516\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13872623443603516  to: 0.138681161403656\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.138681161403656  to: 0.1386362075805664\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.1386362075805664  to: 0.13859140872955322\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13859140872955322  to: 0.1385467529296875\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.1385467529296875  to: 0.13850224018096924\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13850224018096924  to: 0.13845781087875367\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13845781087875367  to: 0.13841354846954346\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13841354846954346  to: 0.13836944103240967\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13836944103240967  to: 0.13832544088363646\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13832544088363646  to: 0.13828157186508178\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13828157186508178  to: 0.1382378339767456\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.1382378339767456  to: 0.1381942391395569\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1381942391395569  to: 0.13815076351165773\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13815076351165773  to: 0.13810740709304808\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13810740709304808  to: 0.13806419372558593\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13806419372558593  to: 0.13802108764648438\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13802108764648438  to: 0.13797810077667236\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13797810077667236  to: 0.13793525695800782\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13793525695800782  to: 0.1378925323486328\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.1378925323486328  to: 0.1378499150276184\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.1378499150276184  to: 0.13780741691589354\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13780741691589354  to: 0.1377650499343872\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.1377650499343872  to: 0.13772280216217042\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13772280216217042  to: 0.13768064975738525\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13768064975738525  to: 0.13763864040374757\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13763864040374757  to: 0.13759673833847047\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13759673833847047  to: 0.13755495548248292\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13755495548248292  to: 0.13751327991485596\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13751327991485596  to: 0.13747172355651854\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13747172355651854  to: 0.13743027448654174\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13743027448654174  to: 0.13738895654678346\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13738895654678346  to: 0.1373477339744568\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.1373477339744568  to: 0.1373066186904907\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.1373066186904907  to: 0.1372656226158142\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1372656226158142  to: 0.1372247338294983\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.1372247338294983  to: 0.13718395233154296\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13718395233154296  to: 0.13714327812194824\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13714327812194824  to: 0.13710272312164307\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13710272312164307  to: 0.1370623826980591\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.1370623826980591  to: 0.13702223300933838\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13702223300933838  to: 0.13698217868804932\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13698217868804932  to: 0.13694225549697875\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13694225549697875  to: 0.13690240383148194\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13690240383148194  to: 0.1368626832962036\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.1368626832962036  to: 0.13682304620742797\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13682304620742797  to: 0.13678352832794188\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.13678352832794188  to: 0.1367440938949585\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.1367440938949585  to: 0.1367047667503357\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.1367047667503357  to: 0.13666555881500245\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13666555881500245  to: 0.1366264581680298\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.1366264581680298  to: 0.13658744096755981\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13658744096755981  to: 0.1365485429763794\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.1365485429763794  to: 0.13650972843170167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13650972843170167  to: 0.13647100925445557\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13647100925445557  to: 0.13643243312835693\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13643243312835693  to: 0.13639392852783203\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13639392852783203  to: 0.13635553121566774\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.13635553121566774  to: 0.13631722927093506\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13631722927093506  to: 0.13627904653549194\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13627904653549194  to: 0.13624088764190673\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13624088764190673  to: 0.13620277643203735\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13620277643203735  to: 0.13616477251052855\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.13616477251052855  to: 0.13612687587738037\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13612687587738037  to: 0.1360890746116638\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.1360890746116638  to: 0.13605138063430786\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13605138063430786  to: 0.1360137939453125\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1360137939453125  to: 0.13597629070281983\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13597629070281983  to: 0.1359389066696167\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.1359389066696167  to: 0.13590164184570314\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13590164184570314  to: 0.1358644485473633\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1358644485473633  to: 0.135827374458313\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.135827374458313  to: 0.13579039573669432\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13579039573669432  to: 0.1357535243034363\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.1357535243034363  to: 0.1357167601585388\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.1357167601585388  to: 0.13568010330200195\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13568010330200195  to: 0.1356435537338257\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.1356435537338257  to: 0.1356070876121521\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.1356070876121521  to: 0.13557074069976807\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13557074069976807  to: 0.13553451299667357\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.13553451299667357  to: 0.13549836874008178\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13549836874008178  to: 0.13546234369277954\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13546234369277954  to: 0.13542640209197998\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13542640209197998  to: 0.13539059162139894\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.13539059162139894  to: 0.1353548765182495\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.1353548765182495  to: 0.13531925678253173\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.13531925678253173  to: 0.13528378009796144\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.13528378009796144  to: 0.13524837493896485\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13524837493896485  to: 0.13521310091018676\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13521310091018676  to: 0.13517792224884034\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.13517792224884034  to: 0.13514283895492554\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13514283895492554  to: 0.1351078987121582\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1351078987121582  to: 0.13507304191589356\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13507304191589356  to: 0.13503830432891845\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13503830432891845  to: 0.1350036859512329\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.1350036859512329  to: 0.134969162940979\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.134969162940979  to: 0.13493475914001465\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13493475914001465  to: 0.13490047454833984\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13490047454833984  to: 0.13486629724502563\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13486629724502563  to: 0.13483222723007202\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.13483222723007202  to: 0.13479827642440795\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13479827642440795  to: 0.13476444482803346\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13476444482803346  to: 0.13473069667816162\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13473069667816162  to: 0.1346971035003662\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1346971035003662  to: 0.13466360569000244\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.13466360569000244  to: 0.13463023900985718\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.13463023900985718  to: 0.13459697961807252\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.13459697961807252  to: 0.13456382751464843\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.13456382751464843  to: 0.13453080654144287\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13453080654144287  to: 0.13449790477752685\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13449790477752685  to: 0.13446512222290039\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13446512222290039  to: 0.13443245887756347\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13443245887756347  to: 0.1343999147415161\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1343999147415161  to: 0.1343674898147583\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1343674898147583  to: 0.134335196018219\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.134335196018219  to: 0.13430302143096923\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13430302143096923  to: 0.13427096605300903\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13427096605300903  to: 0.1342390298843384\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.1342390298843384  to: 0.13420722484588624\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.13420722484588624  to: 0.1341755509376526\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.1341755509376526  to: 0.13414398431777955\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13414398431777955  to: 0.13411266803741456\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13411266803741456  to: 0.13408149480819703\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13408149480819703  to: 0.134050452709198\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.134050452709198  to: 0.13401954174041747\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13401954174041747  to: 0.13398877382278443\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13398877382278443  to: 0.13395814895629882\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.13395814895629882  to: 0.13392765522003175\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13392765522003175  to: 0.1338972806930542\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1338972806930542  to: 0.13386704921722412\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.13386704921722412  to: 0.1338369607925415\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.1338369607925415  to: 0.13380701541900636\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13380701541900636  to: 0.1337771773338318\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.1337771773338318  to: 0.1337473750114441\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.1337473750114441  to: 0.13371772766113282\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13371772766113282  to: 0.13368818759918213\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.13368818759918213  to: 0.1336588144302368\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1336588144302368  to: 0.13362957239151002\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13362957239151002  to: 0.1336004614830017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 424\n",
      "Improved validation loss from: 0.1336004614830017  to: 0.13357149362564086\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13357149362564086  to: 0.13354265689849854\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13354265689849854  to: 0.1335139513015747\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.1335139513015747  to: 0.13348535299301148\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.13348535299301148  to: 0.13345687389373778\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13345687389373778  to: 0.13342853784561157\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.13342853784561157  to: 0.13340033292770387\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.13340033292770387  to: 0.13337228298187256\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.13337228298187256  to: 0.13334436416625978\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.13334436416625978  to: 0.13331658840179444\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.13331658840179444  to: 0.1332889676094055\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.1332889676094055  to: 0.13326146602630615\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13326146602630615  to: 0.13323413133621215\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13323413133621215  to: 0.13320691585540773\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13320691585540773  to: 0.13317973613739015\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13317973613739015  to: 0.13315269947052003\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.13315269947052003  to: 0.13312580585479736\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.13312580585479736  to: 0.1330990433692932\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.1330990433692932  to: 0.13307244777679444\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13307244777679444  to: 0.13304598331451417\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.13304598331451417  to: 0.13301966190338135\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.13301966190338135  to: 0.132993483543396\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.132993483543396  to: 0.13296747207641602\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13296747207641602  to: 0.1329415798187256\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1329415798187256  to: 0.13291585445404053\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13291585445404053  to: 0.13289026021957398\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13289026021957398  to: 0.13286482095718383\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.13286482095718383  to: 0.13283954858779906\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13283954858779906  to: 0.1328144073486328\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.1328144073486328  to: 0.13278943300247192\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13278943300247192  to: 0.1327646017074585\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.1327646017074585  to: 0.13273991346359254\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13273991346359254  to: 0.13271538019180298\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13271538019180298  to: 0.13269100189208985\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.13269100189208985  to: 0.13266676664352417\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13266676664352417  to: 0.13264267444610595\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.13264267444610595  to: 0.13261873722076417\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.13261873722076417  to: 0.13259494304656982\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13259494304656982  to: 0.1325713038444519\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1325713038444519  to: 0.1325478196144104\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.1325478196144104  to: 0.1325244665145874\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.1325244665145874  to: 0.13250128030776978\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13250128030776978  to: 0.1324782371520996\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.1324782371520996  to: 0.13245536088943483\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.13245536088943483  to: 0.13243262767791747\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.13243262767791747  to: 0.13241002559661866\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13241002559661866  to: 0.13238760232925414\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13238760232925414  to: 0.13236532211303711\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.13236532211303711  to: 0.13234317302703857\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.13234317302703857  to: 0.1323211669921875\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1323211669921875  to: 0.1322991967201233\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.1322991967201233  to: 0.13227735757827758\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13227735757827758  to: 0.1322556495666504\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.1322556495666504  to: 0.1322340965270996\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1322340965270996  to: 0.1322127103805542\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.1322127103805542  to: 0.13219144344329833\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13219144344329833  to: 0.13217031955718994\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.13217031955718994  to: 0.132149338722229\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.132149338722229  to: 0.13212850093841552\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.13212850093841552  to: 0.13210780620574952\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.13210780620574952  to: 0.13208725452423095\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13208725452423095  to: 0.13206682205200196\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.13206682205200196  to: 0.13204653263092042\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.13204653263092042  to: 0.13202641010284424\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13202641010284424  to: 0.13200639486312865\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.13200639486312865  to: 0.13198654651641845\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.13198654651641845  to: 0.13196682929992676\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.13196682929992676  to: 0.13194725513458253\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.13194725513458253  to: 0.13192781209945678\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.13192781209945678  to: 0.13190848827362062\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13190848827362062  to: 0.13188931941986085\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13188931941986085  to: 0.1318702816963196\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.1318702816963196  to: 0.13185136318206786\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.13185136318206786  to: 0.1318325877189636\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.1318325877189636  to: 0.13181393146514891\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.13181393146514891  to: 0.13179540634155273\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.13179540634155273  to: 0.13177701234817504\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.13177701234817504  to: 0.13175874948501587\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13175874948501587  to: 0.13174059391021728\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.13174059391021728  to: 0.1317225694656372\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.1317225694656372  to: 0.1317046880722046\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.1317046880722046  to: 0.13168691396713256\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.13168691396713256  to: 0.13166927099227904\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.13166927099227904  to: 0.13165172338485717\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.13165172338485717  to: 0.13163431882858276\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.13163431882858276  to: 0.13161700963974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 510\n",
      "Improved validation loss from: 0.13161700963974  to: 0.1315998315811157\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.1315998315811157  to: 0.13158276081085205\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.13158276081085205  to: 0.13156579732894896\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.13156579732894896  to: 0.1315489649772644\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1315489649772644  to: 0.13153222799301148\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.13153222799301148  to: 0.13151559829711915\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.13151559829711915  to: 0.1314990758895874\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.1314990758895874  to: 0.13148267269134523\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.13148267269134523  to: 0.13146636486053467\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.13146636486053467  to: 0.13145017623901367\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.13145017623901367  to: 0.13143407106399535\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.13143407106399535  to: 0.13141807317733764\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13141807317733764  to: 0.13140218257904052\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.13140218257904052  to: 0.131386399269104\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.131386399269104  to: 0.13137069940567017\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.13137069940567017  to: 0.13135509490966796\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.13135509490966796  to: 0.13133959770202636\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.13133959770202636  to: 0.13132418394088746\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.13132418394088746  to: 0.13130874633789064\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.13130874633789064  to: 0.1312933921813965\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.1312933921813965  to: 0.131278133392334\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.131278133392334  to: 0.13126295804977417\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13126295804977417  to: 0.131247878074646\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.131247878074646  to: 0.13123289346694947\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13123289346694947  to: 0.13121798038482665\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.13121798038482665  to: 0.1312031626701355\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1312031626701355  to: 0.13118842840194703\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.13118842840194703  to: 0.13117377758026122\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.13117377758026122  to: 0.13115919828414918\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.13115919828414918  to: 0.13114465475082399\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.13114465475082399  to: 0.13113008737564086\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13113008737564086  to: 0.1311155915260315\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.1311155915260315  to: 0.13110116720199586\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13110116720199586  to: 0.1310868263244629\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.1310868263244629  to: 0.13107255697250367\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.13107255697250367  to: 0.13105837106704712\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13105837106704712  to: 0.1310442328453064\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.1310442328453064  to: 0.1310301899909973\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.1310301899909973  to: 0.13101619482040405\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.13101619482040405  to: 0.1310023069381714\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.1310023069381714  to: 0.13098846673965453\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13098846673965453  to: 0.13097469806671141\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.13097469806671141  to: 0.13096097707748414\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13096097707748414  to: 0.1309473395347595\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.1309473395347595  to: 0.1309337615966797\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.1309337615966797  to: 0.13092024326324464\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.13092024326324464  to: 0.13090680837631224\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.13090680837631224  to: 0.13089340925216675\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.13089340925216675  to: 0.13088006973266603\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.13088006973266603  to: 0.13086681365966796\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.13086681365966796  to: 0.1308535933494568\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.1308535933494568  to: 0.13084042072296143\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13084042072296143  to: 0.13082733154296874\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.13082733154296874  to: 0.1308142900466919\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.1308142900466919  to: 0.13080129623413086\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.13080129623413086  to: 0.13078835010528564\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.13078835010528564  to: 0.13077545166015625\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.13077545166015625  to: 0.13076261281967164\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.13076261281967164  to: 0.1307498335838318\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1307498335838318  to: 0.13073712587356567\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.13073712587356567  to: 0.13072443008422852\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13072443008422852  to: 0.1307118058204651\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.1307118058204651  to: 0.1306992292404175\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.1306992292404175  to: 0.1306867003440857\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.1306867003440857  to: 0.13067421913146973\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.13067421913146973  to: 0.13066177368164061\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.13066177368164061  to: 0.13064937591552733\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.13064937591552733  to: 0.13063701391220092\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.13063701391220092  to: 0.13062469959259032\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.13062469959259032  to: 0.1306124210357666\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.1306124210357666  to: 0.1306001663208008\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.1306001663208008  to: 0.13058795928955078\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.13058795928955078  to: 0.1305757761001587\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.1305757761001587  to: 0.1305636167526245\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.1305636167526245  to: 0.1305514931678772\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1305514931678772  to: 0.1305393934249878\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.1305393934249878  to: 0.13052732944488527\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13052732944488527  to: 0.13051528930664064\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.13051528930664064  to: 0.1305032730102539\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.1305032730102539  to: 0.1304912805557251\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.1304912805557251  to: 0.1304793119430542\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.1304793119430542  to: 0.13046737909317016\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.13046737909317016  to: 0.13045545816421508\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.13045545816421508  to: 0.13044357299804688\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.13044357299804688  to: 0.13043168783187867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 595\n",
      "Improved validation loss from: 0.13043168783187867  to: 0.1304198384284973\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.1304198384284973  to: 0.13040800094604493\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.13040800094604493  to: 0.13039621114730834\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.13039621114730834  to: 0.13038442134857178\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.13038442134857178  to: 0.13037267923355103\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.13037267923355103  to: 0.13036094903945922\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.13036094903945922  to: 0.1303492307662964\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.1303492307662964  to: 0.13033754825592042\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.13033754825592042  to: 0.13032587766647338\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.13032587766647338  to: 0.13031424283981324\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.13031424283981324  to: 0.13030261993408204\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.13030261993408204  to: 0.13029100894927978\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.13029100894927978  to: 0.13027942180633545\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.13027942180633545  to: 0.13026783466339112\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.13026783466339112  to: 0.1302562713623047\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.1302562713623047  to: 0.13024470806121827\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.13024470806121827  to: 0.13023316860198975\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.13023316860198975  to: 0.1302216410636902\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.1302216410636902  to: 0.13021018505096435\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.13021018505096435  to: 0.13019874095916747\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.13019874095916747  to: 0.1301873207092285\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1301873207092285  to: 0.1301759123802185\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.1301759123802185  to: 0.13016456365585327\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.13016456365585327  to: 0.13015321493148804\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.13015321493148804  to: 0.13014189004898072\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.13014189004898072  to: 0.13013055324554443\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.13013055324554443  to: 0.13011907339096068\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.13011907339096068  to: 0.13010752201080322\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.13010752201080322  to: 0.1300959825515747\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.1300959825515747  to: 0.13008447885513305\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.13008447885513305  to: 0.1300729751586914\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.1300729751586914  to: 0.1300614595413208\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.1300614595413208  to: 0.1300499439239502\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1300499439239502  to: 0.13003842830657958\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.13003842830657958  to: 0.130026912689209\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.130026912689209  to: 0.13001538515090943\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.13001538515090943  to: 0.13000385761260985\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.13000385761260985  to: 0.12999231815338136\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12999231815338136  to: 0.12998077869415284\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12998077869415284  to: 0.12996922731399535\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12996922731399535  to: 0.12995765209197999\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12995765209197999  to: 0.1299461007118225\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.1299461007118225  to: 0.12993451356887817\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12993451356887817  to: 0.12992291450500487\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12992291450500487  to: 0.12991130352020264\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12991130352020264  to: 0.1298996925354004\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.1298996925354004  to: 0.12988802194595336\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12988802194595336  to: 0.12987630367279052\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12987630367279052  to: 0.12986457347869873\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12986457347869873  to: 0.12985283136367798\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12985283136367798  to: 0.12984106540679932\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12984106540679932  to: 0.12982927560806273\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12982927560806273  to: 0.12981748580932617\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12981748580932617  to: 0.12980566024780274\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12980566024780274  to: 0.12979382276535034\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12979382276535034  to: 0.12978198528289794\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12978198528289794  to: 0.1297701120376587\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.1297701120376587  to: 0.12975822687149047\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12975822687149047  to: 0.1297463059425354\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1297463059425354  to: 0.12973434925079347\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12973434925079347  to: 0.12972236871719361\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12972236871719361  to: 0.12971036434173583\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12971036434173583  to: 0.1296983242034912\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.1296983242034912  to: 0.12968626022338867\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12968626022338867  to: 0.12967416048049926\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12967416048049926  to: 0.12966201305389405\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12966201305389405  to: 0.1296498417854309\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.1296498417854309  to: 0.12963764667510985\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12963764667510985  to: 0.12962541580200196\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12962541580200196  to: 0.1296131730079651\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.1296131730079651  to: 0.12960089445114137\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12960089445114137  to: 0.12958858013153077\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12958858013153077  to: 0.12957627773284913\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12957627773284913  to: 0.1295639157295227\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1295639157295227  to: 0.12955156564712525\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.12955156564712525  to: 0.12953916788101197\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12953916788101197  to: 0.1295267701148987\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.1295267701148987  to: 0.1295143485069275\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1295143485069275  to: 0.12950193881988525\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.12950193881988525  to: 0.1294894576072693\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.1294894576072693  to: 0.12947698831558227\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12947698831558227  to: 0.1294645071029663\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.1294645071029663  to: 0.12945201396942138\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12945201396942138  to: 0.12943952083587645\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.12943952083587645  to: 0.12942702770233155\n",
      "Training iteration: 680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12942702770233155  to: 0.12941452264785766\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.12941452264785766  to: 0.12940200567245483\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12940200567245483  to: 0.129389488697052\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.129389488697052  to: 0.12937694787979126\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12937694787979126  to: 0.12936440706253052\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12936440706253052  to: 0.12935183048248292\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12935183048248292  to: 0.1293392777442932\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.1293392777442932  to: 0.1293266773223877\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.1293266773223877  to: 0.1293141007423401\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.1293141007423401  to: 0.12930128574371338\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12930128574371338  to: 0.1292884111404419\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.1292884111404419  to: 0.1292755126953125\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.1292755126953125  to: 0.12926260232925416\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12926260232925416  to: 0.12924968004226683\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.12924968004226683  to: 0.12923672199249267\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12923672199249267  to: 0.1292237639427185\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1292237639427185  to: 0.12921078205108644\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12921078205108644  to: 0.1291977882385254\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.1291977882385254  to: 0.1291847825050354\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.1291847825050354  to: 0.1291717290878296\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.1291717290878296  to: 0.1291586637496948\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.1291586637496948  to: 0.1291455864906311\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.1291455864906311  to: 0.12913246154785157\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12913246154785157  to: 0.12911933660507202\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12911933660507202  to: 0.12910617589950563\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12910617589950563  to: 0.1290929913520813\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1290929913520813  to: 0.12907978296279907\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12907978296279907  to: 0.12906655073165893\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12906655073165893  to: 0.12905330657958985\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12905330657958985  to: 0.129039990901947\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.129039990901947  to: 0.12902672290802003\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.12902672290802003  to: 0.1290135145187378\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.1290135145187378  to: 0.12900028228759766\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12900028228759766  to: 0.12898702621459962\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12898702621459962  to: 0.12897372245788574\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12897372245788574  to: 0.12896038293838502\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12896038293838502  to: 0.12894704341888427\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12894704341888427  to: 0.12893364429473878\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12893364429473878  to: 0.12892024517059325\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.12892024517059325  to: 0.12890679836273194\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12890679836273194  to: 0.1288933515548706\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.1288933515548706  to: 0.12887985706329347\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12887985706329347  to: 0.12886635065078736\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12886635065078736  to: 0.12885282039642335\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12885282039642335  to: 0.12883925437927246\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12883925437927246  to: 0.12882567644119264\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12882567644119264  to: 0.12881206274032592\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12881206274032592  to: 0.12879842519760132\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.12879842519760132  to: 0.12878477573394775\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12878477573394775  to: 0.12877107858657838\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12877107858657838  to: 0.12875738143920898\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12875738143920898  to: 0.12874362468719483\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12874362468719483  to: 0.12872987985610962\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12872987985610962  to: 0.1287160873413086\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.1287160873413086  to: 0.1287022829055786\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.1287022829055786  to: 0.12868844270706176\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12868844270706176  to: 0.12867457866668702\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12867457866668702  to: 0.12866066694259642\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12866066694259642  to: 0.12864673137664795\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12864673137664795  to: 0.1286327600479126\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.1286327600479126  to: 0.12861874103546142\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12861874103546142  to: 0.12860469818115233\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12860469818115233  to: 0.12859060764312744\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12859060764312744  to: 0.12857649326324463\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12857649326324463  to: 0.128562331199646\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.128562331199646  to: 0.12854814529418945\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12854814529418945  to: 0.128533935546875\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.128533935546875  to: 0.12851967811584472\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12851967811584472  to: 0.12850542068481446\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12850542068481446  to: 0.12849111557006837\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12849111557006837  to: 0.1284767985343933\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.1284767985343933  to: 0.12846248149871825\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12846248149871825  to: 0.12844812870025635\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12844812870025635  to: 0.12843376398086548\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12843376398086548  to: 0.1284193754196167\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.1284193754196167  to: 0.12840495109558106\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12840495109558106  to: 0.12839053869247435\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12839053869247435  to: 0.1283760905265808\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1283760905265808  to: 0.1283616304397583\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.1283616304397583  to: 0.12834715843200684\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12834715843200684  to: 0.12833268642425538\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12833268642425538  to: 0.12831833362579345\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12831833362579345  to: 0.12830398082733155\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12830398082733155  to: 0.1282896637916565\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.1282896637916565  to: 0.12827541828155517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12827541828155517  to: 0.12826114892959595\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12826114892959595  to: 0.12824687957763672\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12824687957763672  to: 0.12823257446289063\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12823257446289063  to: 0.12821826934814454\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12821826934814454  to: 0.12820396423339844\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12820396423339844  to: 0.12818963527679444\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12818963527679444  to: 0.12817527055740358\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12817527055740358  to: 0.1281609058380127\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.1281609058380127  to: 0.1281465172767639\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.1281465172767639  to: 0.12813208103179932\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12813208103179932  to: 0.12811763286590577\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12811763286590577  to: 0.12810314893722535\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12810314893722535  to: 0.128088641166687\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.128088641166687  to: 0.12807416915893555\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12807416915893555  to: 0.12805966138839722\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.12805966138839722  to: 0.1280451536178589\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.1280451536178589  to: 0.1280306100845337\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1280306100845337  to: 0.12801605463027954\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12801605463027954  to: 0.12800145149230957\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12800145149230957  to: 0.1279868721961975\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.1279868721961975  to: 0.12797222137451172\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12797222137451172  to: 0.12795759439468385\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12795759439468385  to: 0.12794291973114014\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12794291973114014  to: 0.12792823314666749\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12792823314666749  to: 0.12791342735290528\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12791342735290528  to: 0.12789835929870605\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12789835929870605  to: 0.12788326740264894\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12788326740264894  to: 0.12786813974380493\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12786813974380493  to: 0.12785300016403198\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12785300016403198  to: 0.12783782482147216\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12783782482147216  to: 0.1278226137161255\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.1278226137161255  to: 0.1278073787689209\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.1278073787689209  to: 0.12779210805892943\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12779210805892943  to: 0.1277768135070801\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.1277768135070801  to: 0.12776168584823608\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12776168584823608  to: 0.1277465581893921\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.1277465581893921  to: 0.12773139476776124\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12773139476776124  to: 0.12771621942520142\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12771621942520142  to: 0.1277009963989258\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.1277009963989258  to: 0.12768577337265014\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12768577337265014  to: 0.12767053842544557\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12767053842544557  to: 0.12765527963638307\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12765527963638307  to: 0.12764002084732057\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12764002084732057  to: 0.1276247262954712\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.1276247262954712  to: 0.1276094436645508\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.1276094436645508  to: 0.12759413719177246\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12759413719177246  to: 0.12757881879806518\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12757881879806518  to: 0.12756348848342897\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12756348848342897  to: 0.12754832506179808\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12754832506179808  to: 0.12753312587738036\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12753312587738036  to: 0.1275179147720337\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.1275179147720337  to: 0.1275026798248291\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.1275026798248291  to: 0.12748740911483764\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12748740911483764  to: 0.12747210264205933\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12747210264205933  to: 0.12745679616928102\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12745679616928102  to: 0.12744146585464478\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12744146585464478  to: 0.1274261236190796\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.1274261236190796  to: 0.12741075754165648\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12741075754165648  to: 0.12739537954330443\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12739537954330443  to: 0.12737996578216554\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12737996578216554  to: 0.12736454010009765\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12736454010009765  to: 0.12734909057617189\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12734909057617189  to: 0.12733362913131713\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12733362913131713  to: 0.12731813192367553\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12731813192367553  to: 0.12730261087417602\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12730261087417602  to: 0.1272870421409607\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.1272870421409607  to: 0.12727149724960327\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12727149724960327  to: 0.12725591659545898\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12725591659545898  to: 0.1272403120994568\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1272403120994568  to: 0.12722470760345458\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12722470760345458  to: 0.12720906734466553\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.12720906734466553  to: 0.12719342708587647\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12719342708587647  to: 0.12717775106430054\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12717775106430054  to: 0.12716203927993774\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12716203927993774  to: 0.12714630365371704\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12714630365371704  to: 0.12713054418563843\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12713054418563843  to: 0.127114737033844\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.127114737033844  to: 0.12709888219833373\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12709888219833373  to: 0.12708297967910767\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12708297967910767  to: 0.12706695795059203\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12706695795059203  to: 0.12705090045928955\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12705090045928955  to: 0.12703473567962648\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.12703473567962648  to: 0.12701873779296874\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12701873779296874  to: 0.12700269222259522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12700269222259522  to: 0.1269865870475769\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.1269865870475769  to: 0.12697043418884277\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12697043418884277  to: 0.12695420980453492\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.12695420980453492  to: 0.12693793773651124\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.12693793773651124  to: 0.12692160606384278\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.12692160606384278  to: 0.1269052267074585\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.1269052267074585  to: 0.12688878774642945\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.12688878774642945  to: 0.12687230110168457\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12687230110168457  to: 0.12685577869415282\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12685577869415282  to: 0.12683913707733155\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.12683913707733155  to: 0.12682243585586547\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.12682243585586547  to: 0.12680565118789672\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12680565118789672  to: 0.12678878307342528\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12678878307342528  to: 0.12677185535430907\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.12677185535430907  to: 0.12675484418869018\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12675484418869018  to: 0.12673778533935548\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.12673778533935548  to: 0.12672069072723388\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.12672069072723388  to: 0.12670354843139647\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12670354843139647  to: 0.12668634653091432\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.12668634653091432  to: 0.1266690969467163\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.1266690969467163  to: 0.12665183544158937\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.12665183544158937  to: 0.12663453817367554\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12663453817367554  to: 0.12661722898483277\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.12661722898483277  to: 0.12659991979599\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12659991979599  to: 0.1265825867652893\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.1265825867652893  to: 0.12656524181365966\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12656524181365966  to: 0.12654789686203002\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12654789686203002  to: 0.12653052806854248\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.12653052806854248  to: 0.12651314735412597\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12651314735412597  to: 0.12649592161178588\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12649592161178588  to: 0.12647879123687744\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.12647879123687744  to: 0.12646185159683226\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.12646185159683226  to: 0.1264448881149292\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.1264448881149292  to: 0.12642791271209716\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.12642791271209716  to: 0.12641093730926514\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12641093730926514  to: 0.12639394998550416\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12639394998550416  to: 0.1263769507408142\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.1263769507408142  to: 0.1263599395751953\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.1263599395751953  to: 0.12634295225143433\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12634295225143433  to: 0.1263259768486023\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.1263259768486023  to: 0.12630898952484132\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.12630898952484132  to: 0.12629200220108033\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12629200220108033  to: 0.1262748956680298\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.1262748956680298  to: 0.12625774145126342\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.12625774145126342  to: 0.12624061107635498\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.12624061107635498  to: 0.12622344493865967\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.12622344493865967  to: 0.12620627880096436\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.12620627880096436  to: 0.12618911266326904\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12618911266326904  to: 0.12617192268371583\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.12617192268371583  to: 0.1261547327041626\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.1261547327041626  to: 0.12613751888275146\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12613751888275146  to: 0.12612028121948243\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.12612028121948243  to: 0.1261030316352844\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.1261030316352844  to: 0.12608578205108642\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.12608578205108642  to: 0.12606847286224365\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12606847286224365  to: 0.12605116367340088\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12605116367340088  to: 0.12603381872177125\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12603381872177125  to: 0.1260164499282837\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.1260164499282837  to: 0.1259990453720093\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.1259990453720093  to: 0.12598159313201904\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12598159313201904  to: 0.1259641170501709\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.1259641170501709  to: 0.1259466290473938\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.1259466290473938  to: 0.12592908143997192\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12592908143997192  to: 0.12591149806976318\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.12591149806976318  to: 0.12589387893676757\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12589387893676757  to: 0.12587629556655883\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.12587629556655883  to: 0.12585887908935547\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.12585887908935547  to: 0.12584140300750732\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.12584140300750732  to: 0.12582391500473022\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.12582391500473022  to: 0.12580639123916626\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.12580639123916626  to: 0.12578881978988649\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12578881978988649  to: 0.12577126026153565\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12577126026153565  to: 0.12575366497039794\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.12575366497039794  to: 0.1257360816001892\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.1257360816001892  to: 0.12571847438812256\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12571847438812256  to: 0.1257008671760559\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.1257008671760559  to: 0.12568325996398927\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12568325996398927  to: 0.1256657361984253\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.1256657361984253  to: 0.12564822435379028\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12564822435379028  to: 0.12563066482543944\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.12563066482543944  to: 0.12561309337615967\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.12561309337615967  to: 0.1255955457687378\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.1255955457687378  to: 0.1255779266357422\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.1255779266357422  to: 0.12556030750274658\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12556030750274658  to: 0.12554266452789306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12554266452789306  to: 0.12552499771118164\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12552499771118164  to: 0.12550729513168335\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12550729513168335  to: 0.1254895567893982\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.1254895567893982  to: 0.1254718065261841\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.1254718065261841  to: 0.12545398473739625\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.12545398473739625  to: 0.12543615102767944\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.12543615102767944  to: 0.12541826963424682\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12541826963424682  to: 0.12540035247802733\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12540035247802733  to: 0.12538238763809204\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12538238763809204  to: 0.12536442279815674\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12536442279815674  to: 0.12534639835357667\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.12534639835357667  to: 0.12532833814620972\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12532833814620972  to: 0.1253100872039795\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.1253100872039795  to: 0.12529178857803344\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12529178857803344  to: 0.1252734422683716\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.1252734422683716  to: 0.1252550482749939\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.1252550482749939  to: 0.1252366304397583\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.1252366304397583  to: 0.1252181649208069\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.1252181649208069  to: 0.1251996636390686\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.1251996636390686  to: 0.12518112659454345\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12518112659454345  to: 0.12516252994537352\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12516252994537352  to: 0.1251438856124878\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.1251438856124878  to: 0.1251252293586731\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.1251252293586731  to: 0.12510653734207153\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12510653734207153  to: 0.12508779764175415\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12508779764175415  to: 0.12506915330886842\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12506915330886842  to: 0.1250506043434143\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.1250506043434143  to: 0.12503200769424438\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.12503200769424438  to: 0.1250133752822876\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.1250133752822876  to: 0.1249947190284729\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.1249947190284729  to: 0.12497602701187134\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12497602701187134  to: 0.12495715618133545\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12495715618133545  to: 0.12493785619735717\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12493785619735717  to: 0.124918532371521\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.124918532371521  to: 0.12489917278289794\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12489917278289794  to: 0.12487976551055908\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12487976551055908  to: 0.12486026287078858\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12486026287078858  to: 0.12484076023101806\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.12484076023101806  to: 0.12482120990753173\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12482120990753173  to: 0.1248016357421875\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.1248016357421875  to: 0.1247820258140564\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.1247820258140564  to: 0.12476241588592529\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12476241588592529  to: 0.12474279403686524\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12474279403686524  to: 0.12472314834594726\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12472314834594726  to: 0.12470347881317138\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12470347881317138  to: 0.12468379735946655\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.12468379735946655  to: 0.1246640920639038\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.1246640920639038  to: 0.12464439868927002\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12464439868927002  to: 0.12462469339370727\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12462469339370727  to: 0.12460500001907349\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12460500001907349  to: 0.1245853066444397\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.1245853066444397  to: 0.1245656132698059\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.1245656132698059  to: 0.12454590797424317\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12454590797424317  to: 0.12452622652053832\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12452622652053832  to: 0.12450653314590454\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12450653314590454  to: 0.12448683977127076\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.12448683977127076  to: 0.12446705102920533\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12446705102920533  to: 0.12444677352905273\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.12444677352905273  to: 0.12442647218704224\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12442647218704224  to: 0.12440613508224488\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.12440613508224488  to: 0.12438576221466065\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12438576221466065  to: 0.12436535358428955\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.12436535358428955  to: 0.12434492111206055\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12434492111206055  to: 0.1243244767189026\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.1243244767189026  to: 0.12430400848388672\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12430400848388672  to: 0.12428350448608398\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.12428350448608398  to: 0.1242629885673523\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.1242629885673523  to: 0.12424243688583374\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.12424243688583374  to: 0.12422195672988892\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12422195672988892  to: 0.12420151233673096\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12420151233673096  to: 0.12418100833892823\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12418100833892823  to: 0.12416044473648072\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.12416044473648072  to: 0.12413980960845947\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12413980960845947  to: 0.12411913871765137\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12411913871765137  to: 0.1240984320640564\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.1240984320640564  to: 0.12407766580581665\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12407766580581665  to: 0.12405688762664795\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.12405688762664795  to: 0.12403604984283448\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.12403604984283448  to: 0.12401520013809204\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.12401520013809204  to: 0.1239943265914917\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.1239943265914917  to: 0.1239734172821045\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.1239734172821045  to: 0.12395247220993041\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12395247220993041  to: 0.12393150329589844\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.12393150329589844  to: 0.12391049861907959\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.12391049861907959  to: 0.12388948202133179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.12388948202133179  to: 0.12386842966079711\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12386842966079711  to: 0.1238473653793335\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.1238473653793335  to: 0.12382628917694091\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.12382628917694091  to: 0.1238052248954773\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.1238052248954773  to: 0.12378413677215576\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.12378413677215576  to: 0.12376302480697632\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.12376302480697632  to: 0.12374188899993896\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.12374188899993896  to: 0.1237207293510437\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.1237207293510437  to: 0.12369954586029053\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.12369954586029053  to: 0.12367831468582154\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.12367831468582154  to: 0.1236572265625\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.1236572265625  to: 0.12363613843917846\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.12363613843917846  to: 0.12361503839492798\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.12361503839492798  to: 0.12359391450881958\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.12359391450881958  to: 0.12357275485992432\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.12357275485992432  to: 0.12355155944824218\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12355155944824218  to: 0.1235303521156311\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1235303521156311  to: 0.1235090970993042\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.1235090970993042  to: 0.12348786592483521\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.12348786592483521  to: 0.12346664667129517\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12346664667129517  to: 0.12344540357589721\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.12344540357589721  to: 0.1234242081642151\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.1234242081642151  to: 0.12340303659439086\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12340303659439086  to: 0.1233818769454956\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.1233818769454956  to: 0.12336071729660034\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.12336071729660034  to: 0.12333958148956299\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.12333958148956299  to: 0.12331844568252563\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12331844568252563  to: 0.12329732179641724\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.12329732179641724  to: 0.12327619791030883\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.12327619791030883  to: 0.12325509786605834\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12325509786605834  to: 0.12323403358459473\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12323403358459473  to: 0.12321296930313111\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.12321296930313111  to: 0.12319190502166748\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.12319190502166748  to: 0.12317080497741699\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.12317080497741699  to: 0.1231496810913086\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.1231496810913086  to: 0.12312848567962646\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.12312848567962646  to: 0.12310726642608642\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.12310726642608642  to: 0.12308597564697266\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.12308597564697266  to: 0.12306468486785889\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12306468486785889  to: 0.12304333448410035\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.12304333448410035  to: 0.12302194833755493\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12302194833755493  to: 0.12300050258636475\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.12300050258636475  to: 0.12297903299331665\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12297903299331665  to: 0.12295753955841064\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.12295753955841064  to: 0.12293602228164673\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.12293602228164673  to: 0.1229144811630249\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.1229144811630249  to: 0.12289290428161621\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12289290428161621  to: 0.1228713035583496\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.1228713035583496  to: 0.12284963130950928\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.12284963130950928  to: 0.12282793521881104\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12282793521881104  to: 0.12280622720718384\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.12280622720718384  to: 0.12278449535369873\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.12278449535369873  to: 0.12276270389556884\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12276270389556884  to: 0.1227408766746521\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.1227408766746521  to: 0.12271901369094848\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.12271901369094848  to: 0.12269713878631591\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.12269713878631591  to: 0.12267521619796753\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.12267521619796753  to: 0.12265326976776122\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.12265326976776122  to: 0.12263128757476807\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.12263128757476807  to: 0.12260929346084595\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.12260929346084595  to: 0.12258728742599487\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.12258728742599487  to: 0.12256525754928589\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12256525754928589  to: 0.122543203830719\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.122543203830719  to: 0.12252113819122315\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12252113819122315  to: 0.12249906063079834\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12249906063079834  to: 0.12247698307037354\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.12247698307037354  to: 0.12245495319366455\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12245495319366455  to: 0.12243318557739258\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.12243318557739258  to: 0.12241145372390747\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.12241145372390747  to: 0.12238969802856445\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12238969802856445  to: 0.1223679780960083\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.1223679780960083  to: 0.12234623432159424\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12234623432159424  to: 0.12232450246810914\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12232450246810914  to: 0.12230275869369507\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12230275869369507  to: 0.12228105068206788\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.12228105068206788  to: 0.12225930690765381\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.12225930690765381  to: 0.12223759889602662\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12223759889602662  to: 0.1222158432006836\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.1222158432006836  to: 0.12219408750534058\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.12219408750534058  to: 0.1221722960472107\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.1221722960472107  to: 0.12215043306350708\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12215043306350708  to: 0.1221285343170166\n",
      "Training iteration: 1101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1221285343170166  to: 0.12210661172866821\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12210661172866821  to: 0.12208467721939087\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12208467721939087  to: 0.12206274271011353\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.12206274271011353  to: 0.12204082012176513\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.12204082012176513  to: 0.12201888561248779\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.12201888561248779  to: 0.12199677228927612\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.12199677228927612  to: 0.12197449207305908\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12197449207305908  to: 0.12195205688476562\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.12195205688476562  to: 0.12192950248718262\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.12192950248718262  to: 0.12190682888031006\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.12190682888031006  to: 0.12188403606414795\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.12188403606414795  to: 0.1218611717224121\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.1218611717224121  to: 0.12183822393417358\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12183822393417358  to: 0.12181516885757446\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.12181516885757446  to: 0.12179207801818848\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.12179207801818848  to: 0.12176897525787353\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.12176897525787353  to: 0.12174580097198487\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12174580097198487  to: 0.1217227816581726\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.1217227816581726  to: 0.1216997742652893\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.1216997742652893  to: 0.12167670726776122\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.12167670726776122  to: 0.12165358066558837\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.12165358066558837  to: 0.12163053750991822\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.12163053750991822  to: 0.12160764932632447\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12160764932632447  to: 0.12158477306365967\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.12158477306365967  to: 0.12156192064285279\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.12156192064285279  to: 0.12153908014297485\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.12153908014297485  to: 0.12151622772216797\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.12151622772216797  to: 0.12149337530136109\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.12149337530136109  to: 0.12147051095962524\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.12147051095962524  to: 0.1214476227760315\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.1214476227760315  to: 0.12142475843429565\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.12142475843429565  to: 0.12140188217163086\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.12140188217163086  to: 0.12137899398803711\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.12137899398803711  to: 0.12135610580444336\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.12135610580444336  to: 0.12133318185806274\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.12133318185806274  to: 0.12131021022796631\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.12131021022796631  to: 0.12128722667694092\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.12128722667694092  to: 0.12126423120498657\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.12126423120498657  to: 0.12124119997024536\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.12124119997024536  to: 0.12121814489364624\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.12121814489364624  to: 0.12119506597518921\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.12119506597518921  to: 0.12117197513580322\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.12117197513580322  to: 0.12114889621734619\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.12114889621734619  to: 0.12112580537796021\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.12112580537796021  to: 0.12110271453857421\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.12110271453857421  to: 0.12107961177825928\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.12107961177825928  to: 0.12105647325515748\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.12105647325515748  to: 0.1210332989692688\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.1210332989692688  to: 0.12101008892059326\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.12101008892059326  to: 0.12098684310913085\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.12098684310913085  to: 0.12096357345581055\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.12096357345581055  to: 0.12094029188156127\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.12094029188156127  to: 0.12091701030731201\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.12091701030731201  to: 0.12089371681213379\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.12089371681213379  to: 0.12087039947509766\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.12087039947509766  to: 0.1208470344543457\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.1208470344543457  to: 0.12082364559173583\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.12082364559173583  to: 0.1207999587059021\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.1207999587059021  to: 0.1207760214805603\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.1207760214805603  to: 0.12075186967849731\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.12075186967849731  to: 0.12072750329971313\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.12072750329971313  to: 0.12070293426513672\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.12070293426513672  to: 0.12067819833755493\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.12067819833755493  to: 0.12065328359603882\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.12065328359603882  to: 0.12062822580337525\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.12062822580337525  to: 0.12060304880142211\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.12060304880142211  to: 0.12057777643203735\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.12057777643203735  to: 0.12055245637893677\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.12055245637893677  to: 0.12052707672119141\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.12052707672119141  to: 0.12050163745880127\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.12050163745880127  to: 0.12047616243362427\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.12047616243362427  to: 0.12045061588287354\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.12045061588287354  to: 0.12042504549026489\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.12042504549026489  to: 0.12039945125579835\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.12039945125579835  to: 0.12037385702133178\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.12037385702133178  to: 0.12034833431243896\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.12034833431243896  to: 0.12032289505004883\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.12032289505004883  to: 0.12029743194580078\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.12029743194580078  to: 0.12027199268341064\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.12027199268341064  to: 0.12024650573730469\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.12024650573730469  to: 0.12022103071212768\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.12022103071212768  to: 0.12019555568695069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.12019555568695069  to: 0.1201701045036316\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.1201701045036316  to: 0.12014467716217041\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.12014467716217041  to: 0.12011924982070923\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.12011924982070923  to: 0.12009382247924805\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.12009382247924805  to: 0.12006841897964478\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.12006841897964478  to: 0.12004305124282837\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.12004305124282837  to: 0.12001769542694092\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.12001769542694092  to: 0.11999218463897705\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.11999218463897705  to: 0.11996654272079468\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.11996654272079468  to: 0.11994082927703857\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.11994082927703857  to: 0.11991500854492188\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.11991500854492188  to: 0.11988914012908936\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.11988914012908936  to: 0.11986320018768311\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.11986320018768311  to: 0.11983693838119507\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.11983693838119507  to: 0.11981048583984374\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.11981048583984374  to: 0.11978384256362914\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.11978384256362914  to: 0.11975702047348022\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.11975702047348022  to: 0.11973005533218384\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.11973005533218384  to: 0.11970295906066894\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.11970295906066894  to: 0.11967573165893555\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.11967573165893555  to: 0.11964845657348633\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.11964845657348633  to: 0.1196210503578186\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.1196210503578186  to: 0.11959359645843506\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.11959359645843506  to: 0.11956608295440674\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.11956608295440674  to: 0.11953855752944946\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.11953855752944946  to: 0.11951106786727905\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.11951106786727905  to: 0.11948364973068237\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.11948364973068237  to: 0.1194562554359436\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.1194562554359436  to: 0.11942886114120484\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.11942886114120484  to: 0.11940149068832398\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.11940149068832398  to: 0.11937410831451416\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.11937410831451416  to: 0.11934674978256225\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.11934674978256225  to: 0.11931941509246827\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.11931941509246827  to: 0.11929210424423217\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.11929210424423217  to: 0.11926478147506714\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.11926478147506714  to: 0.1192374587059021\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.1192374587059021  to: 0.11921011209487915\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.11921011209487915  to: 0.11918271780014038\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.11918271780014038  to: 0.11915533542633057\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.11915533542633057  to: 0.11912791728973389\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.11912791728973389  to: 0.11910048723220826\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.11910048723220826  to: 0.1190730094909668\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.1190730094909668  to: 0.11904547214508057\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.11904547214508057  to: 0.11901788711547852\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.11901788711547852  to: 0.11899023056030274\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.11899023056030274  to: 0.11896257400512696\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.11896257400512696  to: 0.1189348816871643\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.1189348816871643  to: 0.11890712976455689\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.11890712976455689  to: 0.11887930631637574\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.11887930631637574  to: 0.11885141134262085\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.11885141134262085  to: 0.1188234567642212\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.1188234567642212  to: 0.11879545450210571\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.11879545450210571  to: 0.11876742839813233\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.11876742839813233  to: 0.11873934268951417\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.11873934268951417  to: 0.11871116161346436\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.11871116161346436  to: 0.11868295669555665\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.11868295669555665  to: 0.11865470409393311\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.11865470409393311  to: 0.11862642765045166\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.11862642765045166  to: 0.11859815120697022\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.11859815120697022  to: 0.11856986284255981\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.11856986284255981  to: 0.11854158639907837\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.11854158639907837  to: 0.11851332187652588\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.11851332187652588  to: 0.11848502159118653\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.11848502159118653  to: 0.11845672130584717\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.11845672130584717  to: 0.1184283971786499\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.1184283971786499  to: 0.11840006113052368\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.11840006113052368  to: 0.11837168931961059\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.11837168931961059  to: 0.11834328174591065\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.11834328174591065  to: 0.11831461191177368\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.11831461191177368  to: 0.11828566789627075\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.11828566789627075  to: 0.11825649738311768\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.11825649738311768  to: 0.11822711229324341\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.11822711229324341  to: 0.11819761991500854\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.11819761991500854  to: 0.11816799640655518\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.11816799640655518  to: 0.11813827753067016\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.11813827753067016  to: 0.11810852289199829\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.11810852289199829  to: 0.11807869672775269\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.11807869672775269  to: 0.11804887056350707\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.11804887056350707  to: 0.11801903247833252\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.11801903247833252  to: 0.117989182472229\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.117989182472229  to: 0.11795938014984131\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.11795938014984131  to: 0.1179296612739563\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.1179296612739563  to: 0.11790000200271607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.11790000200271607  to: 0.11787033081054688\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.11787033081054688  to: 0.11784069538116455\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.11784069538116455  to: 0.11781102418899536\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.11781102418899536  to: 0.11778136491775512\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.11778136491775512  to: 0.11775208711624145\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.11775208711624145  to: 0.11772289276123046\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.11772289276123046  to: 0.11769368648529052\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.11769368648529052  to: 0.11766446828842163\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.11766446828842163  to: 0.11763476133346558\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.11763476133346558  to: 0.11760469675064086\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.11760469675064086  to: 0.11757453680038452\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.11757453680038452  to: 0.11754432916641236\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.11754432916641236  to: 0.11751435995101929\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.11751435995101929  to: 0.11748449802398682\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.11748449802398682  to: 0.11745456457138062\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.11745456457138062  to: 0.11742454767227173\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.11742454767227173  to: 0.11739442348480225\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.11739442348480225  to: 0.11736423969268799\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.11736423969268799  to: 0.11733392477035523\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.11733392477035523  to: 0.11730349063873291\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.11730349063873291  to: 0.11727292537689209\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.11727292537689209  to: 0.11724228858947754\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.11724228858947754  to: 0.1172115683555603\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.1172115683555603  to: 0.11718077659606933\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.11718077659606933  to: 0.11714990139007568\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.11714990139007568  to: 0.11711862087249755\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.11711862087249755  to: 0.11708692312240601\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.11708692312240601  to: 0.11705495119094848\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.11705495119094848  to: 0.11702272891998292\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.11702272891998292  to: 0.1169903039932251\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.1169903039932251  to: 0.1169576644897461\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.1169576644897461  to: 0.1169248342514038\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.1169248342514038  to: 0.11689186096191406\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.11689186096191406  to: 0.11685880422592163\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.11685880422592163  to: 0.1168256402015686\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.1168256402015686  to: 0.11679236888885498\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.11679236888885498  to: 0.11675909757614136\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.11675909757614136  to: 0.11672576665878295\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.11672576665878295  to: 0.11669237613677978\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.11669237613677978  to: 0.11665893793106079\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.11665893793106079  to: 0.11662545204162597\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.11662545204162597  to: 0.1165919542312622\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.1165919542312622  to: 0.11655845642089843\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.11655845642089843  to: 0.1165249228477478\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.1165249228477478  to: 0.11649134159088134\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.11649134159088134  to: 0.11645768880844116\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.11645768880844116  to: 0.11642398834228515\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.11642398834228515  to: 0.11639024019241333\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.11639024019241333  to: 0.11635643243789673\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.11635643243789673  to: 0.1163225531578064\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.1163225531578064  to: 0.1162886381149292\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.1162886381149292  to: 0.11625468730926514\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.11625468730926514  to: 0.11622072458267212\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.11622072458267212  to: 0.11618666648864746\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.11618666648864746  to: 0.11615252494812012\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.11615252494812012  to: 0.11611827611923217\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.11611827611923217  to: 0.1160839319229126\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.1160839319229126  to: 0.11604944467544556\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.11604944467544556  to: 0.11601487398147584\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.11601487398147584  to: 0.11598018407821656\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.11598018407821656  to: 0.11594541072845459\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.11594541072845459  to: 0.1159105658531189\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.1159105658531189  to: 0.11587563753128052\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.11587563753128052  to: 0.11584060192108155\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.11584060192108155  to: 0.11580549478530884\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.11580549478530884  to: 0.11577029228210449\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.11577029228210449  to: 0.11573503017425538\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.11573503017425538  to: 0.11569973230361938\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.11569973230361938  to: 0.11566431522369384\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.11566431522369384  to: 0.11562880277633666\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.11562880277633666  to: 0.1155931830406189\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.1155931830406189  to: 0.11555753946304322\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.11555753946304322  to: 0.11552178859710693\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.11552178859710693  to: 0.11548595428466797\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.11548595428466797  to: 0.11545000076293946\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.11545000076293946  to: 0.11541368961334228\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.11541368961334228  to: 0.11537704467773438\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.11537704467773438  to: 0.11534011363983154\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.11534011363983154  to: 0.11530288457870483\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.11530288457870483  to: 0.11526538133621216\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.11526538133621216  to: 0.11522762775421143\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.11522762775421143  to: 0.1151896595954895\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.1151896595954895  to: 0.11515151262283325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.11515151262283325  to: 0.11511318683624268\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.11511318683624268  to: 0.11507470607757568\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.11507470607757568  to: 0.11503608226776123\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.11503608226776123  to: 0.11499736309051514\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.11499736309051514  to: 0.11495859622955322\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.11495859622955322  to: 0.11491984128952026\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.11491984128952026  to: 0.11488106250762939\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.11488106250762939  to: 0.11484224796295166\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.11484224796295166  to: 0.1148033618927002\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.1148033618927002  to: 0.11476441621780395\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.11476441621780395  to: 0.11472522020339966\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.11472522020339966  to: 0.11468585729598998\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.11468585729598998  to: 0.11464618444442749\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.11464618444442749  to: 0.11460617780685425\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.11460617780685425  to: 0.11456592082977295\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.11456592082977295  to: 0.11452542543411255\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.11452542543411255  to: 0.11448481082916259\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.11448481082916259  to: 0.11444400548934937\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.11444400548934937  to: 0.11440317630767823\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.11440317630767823  to: 0.11436249017715454\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.11436249017715454  to: 0.11432178020477295\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.11432178020477295  to: 0.11428102254867553\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.11428102254867553  to: 0.11424028873443604\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.11424028873443604  to: 0.11419954299926757\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.11419954299926757  to: 0.11415884494781495\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.11415884494781495  to: 0.11411817073822021\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.11411817073822021  to: 0.11407755613327027\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.11407755613327027  to: 0.11403696537017823\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.11403696537017823  to: 0.11399648189544678\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.11399648189544678  to: 0.11395623683929443\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.11395623683929443  to: 0.11391608715057373\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.11391608715057373  to: 0.11387591361999512\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.11387591361999512  to: 0.11383568048477173\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.11383568048477173  to: 0.1137953519821167\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.1137953519821167  to: 0.11375455856323242\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.11375455856323242  to: 0.11371370553970336\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.11371370553970336  to: 0.11367248296737671\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.11367248296737671  to: 0.11363089084625244\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.11363089084625244  to: 0.11358892917633057\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.11358892917633057  to: 0.11354663372039794\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.11354663372039794  to: 0.11350400447845459\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.11350400447845459  to: 0.11346112489700318\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.11346112489700318  to: 0.11341794729232788\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.11341794729232788  to: 0.11337454319000244\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.11337454319000244  to: 0.11333096027374268\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.11333096027374268  to: 0.11328725814819336\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.11328725814819336  to: 0.11324338912963867\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.11324338912963867  to: 0.11319932937622071\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.11319932937622071  to: 0.11315512657165527\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.11315512657165527  to: 0.11311079263687134\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.11311079263687134  to: 0.11306635141372681\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.11306635141372681  to: 0.11302183866500855\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.11302183866500855  to: 0.11297729015350341\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.11297729015350341  to: 0.11293274164199829\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.11293274164199829  to: 0.11288821697235107\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.11288821697235107  to: 0.1128436803817749\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.1128436803817749  to: 0.11279913187026977\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.11279913187026977  to: 0.11275455951690674\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.11275455951690674  to: 0.11270996332168579\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.11270996332168579  to: 0.11266492605209351\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.11266492605209351  to: 0.11261827945709228\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.11261827945709228  to: 0.11256968975067139\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.11256968975067139  to: 0.1125194787979126\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.1125194787979126  to: 0.11246774196624756\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.11246774196624756  to: 0.11241472959518432\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.11241472959518432  to: 0.11236060857772827\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.11236060857772827  to: 0.1123056173324585\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.1123056173324585  to: 0.11224993467330932\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.11224993467330932  to: 0.11219370365142822\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.11219370365142822  to: 0.11213709115982055\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.11213709115982055  to: 0.11208025217056275\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.11208025217056275  to: 0.11202329397201538\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.11202329397201538  to: 0.11196765899658204\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.11196765899658204  to: 0.11191326379776001\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.11191326379776001  to: 0.11186000108718872\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.11186000108718872  to: 0.11180778741836547\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.11180778741836547  to: 0.11175649166107178\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.11175649166107178  to: 0.11170601844787598\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.11170601844787598  to: 0.11165622472763062\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.11165622472763062  to: 0.11160557270050049\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.11160557270050049  to: 0.11155405044555664\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.11155405044555664  to: 0.11150169372558594\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.11150169372558594  to: 0.11144853830337524\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.11144853830337524  to: 0.11139461994171143\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.11139461994171143  to: 0.11134002208709717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.11134002208709717  to: 0.11128485202789307\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.11128485202789307  to: 0.1112291932106018\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.1112291932106018  to: 0.11117452383041382\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.11117452383041382  to: 0.11112082004547119\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.11112082004547119  to: 0.11106752157211304\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.11106752157211304  to: 0.11101465225219727\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.11101465225219727  to: 0.11096071004867554\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.11096071004867554  to: 0.11090571880340576\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.11090571880340576  to: 0.11084979772567749\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.11084979772567749  to: 0.11079314947128296\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.11079314947128296  to: 0.11073580980300904\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.11073580980300904  to: 0.11067793369293213\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.11067793369293213  to: 0.11061958074569703\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.11061958074569703  to: 0.11056084632873535\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.11056084632873535  to: 0.11050344705581665\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.11050344705581665  to: 0.1104472517967224\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.1104472517967224  to: 0.11039053201675415\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.11039053201675415  to: 0.11033331155776978\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.11033331155776978  to: 0.11027557849884033\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.11027557849884033  to: 0.11021734476089477\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.11021734476089477  to: 0.11016032695770264\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.11016032695770264  to: 0.11010264158248902\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.11010264158248902  to: 0.11004436016082764\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.11004436016082764  to: 0.10998549461364746\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.10998549461364746  to: 0.10992609262466431\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.10992609262466431  to: 0.109866201877594\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.109866201877594  to: 0.10980762243270874\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.10980762243270874  to: 0.10974845886230469\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.10974845886230469  to: 0.10968878269195556\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.10968878269195556  to: 0.10962756872177123\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.10962756872177123  to: 0.10956532955169677\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.10956532955169677  to: 0.10950181484222413\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.10950181484222413  to: 0.10943721532821656\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.10943721532821656  to: 0.10937348604202271\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.10937348604202271  to: 0.1093087911605835\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.1093087911605835  to: 0.10924339294433594\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.10924339294433594  to: 0.10917736291885376\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.10917736291885376  to: 0.10911076068878174\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.10911076068878174  to: 0.10904369354248047\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.10904369354248047  to: 0.10897629261016846\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.10897629261016846  to: 0.10890858173370362\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.10890858173370362  to: 0.1088406801223755\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.1088406801223755  to: 0.10877262353897095\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.10877262353897095  to: 0.10870445966720581\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.10870445966720581  to: 0.10863621234893799\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.10863621234893799  to: 0.10856791734695434\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.10856791734695434  to: 0.10849959850311279\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.10849959850311279  to: 0.10843119621276856\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.10843119621276856  to: 0.10836273431777954\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.10836273431777954  to: 0.10829423666000366\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.10829423666000366  to: 0.10822566747665405\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.10822566747665405  to: 0.10815708637237549\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.10815708637237549  to: 0.10808844566345215\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.10808844566345215  to: 0.10801982879638672\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.10801982879638672  to: 0.10795118808746337\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.10795118808746337  to: 0.10788253545761109\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.10788253545761109  to: 0.1078139066696167\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.1078139066696167  to: 0.10774527788162232\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.10774527788162232  to: 0.10767664909362792\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.10767664909362792  to: 0.10760802030563354\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.10760802030563354  to: 0.10753939151763917\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.10753939151763917  to: 0.10747076272964477\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.10747076272964477  to: 0.10740211009979247\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.10740211009979247  to: 0.10733346939086914\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.10733346939086914  to: 0.10726387500762939\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.10726387500762939  to: 0.10719349384307861\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.10719349384307861  to: 0.10712246894836426\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.10712246894836426  to: 0.10705093145370484\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.10705093145370484  to: 0.10697898864746094\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.10697898864746094  to: 0.10690675973892212\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.10690675973892212  to: 0.10683425664901733\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.10683425664901733  to: 0.10676158666610717\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.10676158666610717  to: 0.10668872594833374\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.10668872594833374  to: 0.10661563873291016\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.10661563873291016  to: 0.10654226541519166\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.10654226541519166  to: 0.10646858215332031\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.10646858215332031  to: 0.10639455318450927\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.10639455318450927  to: 0.10632011890411378\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.10632011890411378  to: 0.10624526739120484\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.10624526739120484  to: 0.1061700463294983\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.1061700463294983  to: 0.1060942530632019\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.1060942530632019  to: 0.10601800680160522\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.10601800680160522  to: 0.1059413194656372\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.1059413194656372  to: 0.10586421489715576\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.10586421489715576  to: 0.1057862401008606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.1057862401008606  to: 0.10570733547210694\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.10570733547210694  to: 0.10562781095504761\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.10562781095504761  to: 0.10554792881011962\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.10554792881011962  to: 0.10546791553497314\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.10546791553497314  to: 0.10538785457611084\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.10538785457611084  to: 0.10530780553817749\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.10530780553817749  to: 0.10522781610488892\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.10522781610488892  to: 0.10514785051345825\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.10514785051345825  to: 0.10506783723831177\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.10506783723831177  to: 0.10498769283294677\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.10498769283294677  to: 0.1049073576927185\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.1049073576927185  to: 0.10482676029205322\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.10482676029205322  to: 0.10474573373794556\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.10474573373794556  to: 0.10466420650482178\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.10466420650482178  to: 0.10458220243453979\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.10458220243453979  to: 0.1044997215270996\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.1044997215270996  to: 0.10441677570343018\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.10441677570343018  to: 0.10433354377746581\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.10433354377746581  to: 0.10425008535385132\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.10425008535385132  to: 0.10416644811630249\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.10416644811630249  to: 0.10408254861831664\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.10408254861831664  to: 0.10399831533432007\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.10399831533432007  to: 0.10391397476196289\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.10391397476196289  to: 0.10382735729217529\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.10382735729217529  to: 0.10373910665512084\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.10373910665512084  to: 0.10364972352981568\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.10364972352981568  to: 0.10355950593948364\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.10355950593948364  to: 0.10346875190734864\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.10346875190734864  to: 0.10337756872177124\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.10337756872177124  to: 0.10328598022460937\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.10328598022460937  to: 0.10319408178329467\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.10319408178329467  to: 0.10310156345367431\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.10310156345367431  to: 0.10300840139389038\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.10300840139389038  to: 0.10291469097137451\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.10291469097137451  to: 0.10282056331634522\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.10282056331634522  to: 0.10272619724273682\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.10272619724273682  to: 0.1026317834854126\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.1026317834854126  to: 0.10253756046295166\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.10253756046295166  to: 0.10244373083114625\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.10244373083114625  to: 0.10235049724578857\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.10235049724578857  to: 0.10225800275802613\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.10225800275802613  to: 0.1021653413772583\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.1021653413772583  to: 0.10207284688949585\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.10207284688949585  to: 0.10198074579238892\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.10198074579238892  to: 0.10188926458358764\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.10188926458358764  to: 0.10179847478866577\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.10179847478866577  to: 0.1017082691192627\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.1017082691192627  to: 0.10161863565444947\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.10161863565444947  to: 0.10152937173843384\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.10152937173843384  to: 0.1014404058456421\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.1014404058456421  to: 0.10135177373886109\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.10135177373886109  to: 0.10126340389251709\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.10126340389251709  to: 0.10117518901824951\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.10117518901824951  to: 0.10108731985092163\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.10108731985092163  to: 0.10099985599517822\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.10099985599517822  to: 0.10091276168823242\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.10091276168823242  to: 0.10082582235336304\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.10082582235336304  to: 0.10073925256729126\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.10073925256729126  to: 0.10065300464630127\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.10065300464630127  to: 0.1005670428276062\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.1005670428276062  to: 0.10048136711120606\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.10048136711120606  to: 0.10039575099945068\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.10039575099945068  to: 0.10031003952026367\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.10031003952026367  to: 0.10022417306900025\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.10022417306900025  to: 0.10013809204101562\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.10013809204101562  to: 0.1000518560409546\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.1000518560409546  to: 0.0999653697013855\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.0999653697013855  to: 0.0998785674571991\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.0998785674571991  to: 0.09979122877120972\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.09979122877120972  to: 0.09970306158065796\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.09970306158065796  to: 0.0996144950389862\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.0996144950389862  to: 0.09952576756477356\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.09952576756477356  to: 0.09943695068359375\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.09943695068359375  to: 0.09934787750244141\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.09934787750244141  to: 0.09925841093063355\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.09925841093063355  to: 0.0991683304309845\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.0991683304309845  to: 0.09907739758491516\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.09907739758491516  to: 0.09898548126220703\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.09898548126220703  to: 0.09889253377914428\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.09889253377914428  to: 0.09879856109619141\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.09879856109619141  to: 0.09870370626449584\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.09870370626449584  to: 0.09860807657241821\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.09860807657241821  to: 0.09851177334785462\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.09851177334785462  to: 0.09841502904891967\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.09841502904891967  to: 0.09831795692443848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.09831795692443848  to: 0.09822059869766235\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.09822059869766235  to: 0.09812297821044921\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.09812297821044921  to: 0.09802494049072266\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.09802494049072266  to: 0.09792346954345703\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.09792346954345703  to: 0.09781951904296875\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.09781951904296875  to: 0.09771391153335571\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.09771391153335571  to: 0.09760698080062866\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.09760698080062866  to: 0.09749904870986939\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.09749904870986939  to: 0.09739018678665161\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.09739018678665161  to: 0.09728034734725952\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.09728034734725952  to: 0.09717008471488953\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.09717008471488953  to: 0.09705982208251954\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.09705982208251954  to: 0.09695006608963012\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.09695006608963012  to: 0.09684141278266907\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.09684141278266907  to: 0.09673431515693665\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.09673431515693665  to: 0.09662917256355286\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.09662917256355286  to: 0.09652620553970337\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.09652620553970337  to: 0.09642542600631714\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.09642542600631714  to: 0.09632661938667297\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.09632661938667297  to: 0.0962296485900879\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.0962296485900879  to: 0.09613409042358398\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.09613409042358398  to: 0.09603959321975708\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.09603959321975708  to: 0.09594591856002807\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.09594591856002807  to: 0.09585292935371399\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.09585292935371399  to: 0.09576038122177125\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.09576038122177125  to: 0.0956682801246643\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.0956682801246643  to: 0.09557636976242065\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.09557636976242065  to: 0.09548429250717164\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.09548429250717164  to: 0.09539254903793334\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.09539254903793334  to: 0.09530122876167298\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.09530122876167298  to: 0.09520984888076782\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.09520984888076782  to: 0.09511750936508179\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.09511750936508179  to: 0.09502344131469727\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.09502344131469727  to: 0.0949273943901062\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.0949273943901062  to: 0.09482933878898621\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.09482933878898621  to: 0.09472972750663758\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.09472972750663758  to: 0.09462891817092896\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.09462891817092896  to: 0.09452751874923707\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.09452751874923707  to: 0.094426429271698\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.094426429271698  to: 0.0943254828453064\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.0943254828453064  to: 0.09422427415847778\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.09422427415847778  to: 0.09412164688110351\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.09412164688110351  to: 0.09401692152023315\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.09401692152023315  to: 0.09390989542007447\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.09390989542007447  to: 0.09380587339401245\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.09380587339401245  to: 0.09370109438896179\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.09370109438896179  to: 0.09359566569328308\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.09359566569328308  to: 0.09348953366279603\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.09348953366279603  to: 0.0933826744556427\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.0933826744556427  to: 0.0932751476764679\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.0932751476764679  to: 0.09316658973693848\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.09316658973693848  to: 0.09305675625801087\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.09305675625801087  to: 0.09294595718383789\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.09294595718383789  to: 0.09283445477485656\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.09283445477485656  to: 0.09272220730781555\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.09272220730781555  to: 0.09260934591293335\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.09260934591293335  to: 0.09249593615531922\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.09249593615531922  to: 0.09238271713256836\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.09238271713256836  to: 0.09226917028427124\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.09226917028427124  to: 0.09215434193611145\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.09215434193611145  to: 0.09203779101371765\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.09203779101371765  to: 0.09191927909851075\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.09191927909851075  to: 0.09179888963699341\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.09179888963699341  to: 0.09166926145553589\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.09166926145553589  to: 0.09153369665145875\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.09153369665145875  to: 0.09139426946640014\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.09139426946640014  to: 0.09125136137008667\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.09125136137008667  to: 0.09110527038574219\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.09110527038574219  to: 0.09095619916915894\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.09095619916915894  to: 0.0908056914806366\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.0908056914806366  to: 0.09065664410591126\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.09065664410591126  to: 0.09051230549812317\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.09051230549812317  to: 0.09037405252456665\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.09037405252456665  to: 0.09024213552474976\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.09024213552474976  to: 0.09011504054069519\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.09011504054069519  to: 0.08999136090278625\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.08999136090278625  to: 0.0898703932762146\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.0898703932762146  to: 0.08975293040275574\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.08975293040275574  to: 0.08963991403579712\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.08963991403579712  to: 0.08953152894973755\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.08953152894973755  to: 0.0894267201423645\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.0894267201423645  to: 0.08932334780693055\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.08932334780693055  to: 0.08921855092048644\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.08921855092048644  to: 0.08911110758781433\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.08911110758781433  to: 0.08900105357170104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.08900105357170104  to: 0.0888888955116272\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.0888888955116272  to: 0.08877531290054322\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.08877531290054322  to: 0.08865941762924194\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.08865941762924194  to: 0.08853968381881713\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.08853968381881713  to: 0.08841440081596375\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.08841440081596375  to: 0.08828365206718444\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.08828365206718444  to: 0.08814924359321594\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.08814924359321594  to: 0.08801291584968567\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.08801291584968567  to: 0.08787528872489929\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.08787528872489929  to: 0.08773525357246399\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.08773525357246399  to: 0.08759176135063171\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.08759176135063171  to: 0.08744558095932006\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.08744558095932006  to: 0.08729925155639648\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.08729925155639648  to: 0.0871548056602478\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.0871548056602478  to: 0.0870119571685791\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.0870119571685791  to: 0.08686906099319458\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.08686906099319458  to: 0.08672496676445007\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.08672496676445007  to: 0.08658123016357422\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.08658123016357422  to: 0.08644078969955445\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.08644078969955445  to: 0.08629578351974487\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.08629578351974487  to: 0.08614907264709473\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.08614907264709473  to: 0.08599997758865356\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.08599997758865356  to: 0.08584760427474976\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.08584760427474976  to: 0.08569365739822388\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.08569365739822388  to: 0.08554224967956543\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.08554224967956543  to: 0.08540183305740356\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.08540183305740356  to: 0.08526771664619445\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.08526771664619445  to: 0.0851369023323059\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.0851369023323059  to: 0.08500859141349792\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.08500859141349792  to: 0.08488447070121766\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.08488447070121766  to: 0.08476654291152955\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.08476654291152955  to: 0.08465315699577332\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.08465315699577332  to: 0.0845415472984314\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.0845415472984314  to: 0.08443068265914917\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.08443068265914917  to: 0.0843204140663147\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.0843204140663147  to: 0.08421053886413574\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.08421053886413574  to: 0.08410025835037231\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.08410025835037231  to: 0.08398720622062683\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.08398720622062683  to: 0.08387004733085632\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.08387004733085632  to: 0.08374903798103332\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.08374903798103332  to: 0.0836259663105011\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.0836259663105011  to: 0.08350019454956055\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.08350019454956055  to: 0.08337148427963256\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.08337148427963256  to: 0.08324009776115418\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.08324009776115418  to: 0.0831070899963379\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.0831070899963379  to: 0.08297433853149414\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.08297433853149414  to: 0.082840096950531\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.082840096950531  to: 0.0827052116394043\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.0827052116394043  to: 0.08257096409797668\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.08257096409797668  to: 0.08243783712387084\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.08243783712387084  to: 0.0823100447654724\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.0823100447654724  to: 0.08218711614608765\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.08218711614608765  to: 0.08206688165664673\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.08206688165664673  to: 0.08194921612739563\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.08194921612739563  to: 0.08183506727218628\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.08183506727218628  to: 0.08172162771224975\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.08172162771224975  to: 0.08160576820373536\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.08160576820373536  to: 0.08148679733276368\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.08148679733276368  to: 0.0813651442527771\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.0813651442527771  to: 0.081239253282547\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.081239253282547  to: 0.0811078429222107\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.0811078429222107  to: 0.08097214698791504\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.08097214698791504  to: 0.08083370327949524\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.08083370327949524  to: 0.0806924819946289\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.0806924819946289  to: 0.08054669499397278\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.08054669499397278  to: 0.08039777874946594\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.08039777874946594  to: 0.08024876713752746\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.08024876713752746  to: 0.08010007739067078\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.08010007739067078  to: 0.0799507737159729\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.0799507737159729  to: 0.07980132699012757\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.07980132699012757  to: 0.07965348362922668\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.07965348362922668  to: 0.07950466275215148\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.07950466275215148  to: 0.07935686111450195\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.07935686111450195  to: 0.07921181321144104\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.07921181321144104  to: 0.07906613349914551\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.07906613349914551  to: 0.07891759872436524\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.07891759872436524  to: 0.0787684977054596\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.0787684977054596  to: 0.07861651182174682\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.07861651182174682  to: 0.0784605622291565\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.0784605622291565  to: 0.07830429077148438\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.07830429077148438  to: 0.07814311981201172\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.07814311981201172  to: 0.07797936201095582\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.07797936201095582  to: 0.0778154969215393\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.0778154969215393  to: 0.07764868140220642\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.07764868140220642  to: 0.0774805188179016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.0774805188179016  to: 0.07731435894966125\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.07731435894966125  to: 0.07714632153511047\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.07714632153511047  to: 0.07698318362236023\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.07698318362236023  to: 0.0768206000328064\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.0768206000328064  to: 0.07665725946426391\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.07665725946426391  to: 0.07649433016777038\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.07649433016777038  to: 0.0763283610343933\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.0763283610343933  to: 0.07616394758224487\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.07616394758224487  to: 0.07599034309387206\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.07599034309387206  to: 0.07581588625907898\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.07581588625907898  to: 0.07563210725784301\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.07563210725784301  to: 0.07545228004455566\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.07545228004455566  to: 0.07525752186775207\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.07525752186775207  to: 0.07508499026298524\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.07508499026298524  to: 0.074871027469635\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.074871027469635  to: 0.07474095225334168\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.07474095225334168  to: 0.07442495226860046\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.07456139326095582\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.07442495226860046  to: 0.07385929822921752\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.07465397715568542\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.07385929822921752  to: 0.07363587617874146\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.07363587617874146  to: 0.0734023094177246\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.07394711375236511\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.0734023094177246  to: 0.07327069044113159\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.07327069044113159  to: 0.07286661267280578\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.07319380044937134\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.07293719053268433\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.07286661267280578  to: 0.07232250571250916\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.07239102125167847\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.07247724533081054\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.07232250571250916  to: 0.07181028723716736\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.07181028723716736  to: 0.07167315483093262\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.07190276980400086\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.07167315483093262  to: 0.07139192819595337\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.07139192819595337  to: 0.07112376689910889\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.0713369905948639\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.07112376689910889  to: 0.07100569009780884\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.07100569009780884  to: 0.07064654231071472\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.07077100872993469\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.07064654231071472  to: 0.07052303552627563\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.07052303552627563  to: 0.07013993859291076\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.07022624015808106\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.07013993859291076  to: 0.07004112005233765\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.07004112005233765  to: 0.06973500847816468\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.06987944841384888\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.06973500847816468  to: 0.06969483494758606\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.06969483494758606  to: 0.06949032545089721\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.06966025829315185\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.06949032545089721  to: 0.06937252283096314\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.06937252283096314  to: 0.069278883934021\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.06934200525283814\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.069278883934021  to: 0.0689727246761322\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.0690603256225586\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.0689727246761322  to: 0.06883108019828796\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.06883108019828796  to: 0.06866241693496704\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.06872156858444214\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.06866241693496704  to: 0.06834377646446228\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.06851991415023803\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.06834377646446228  to: 0.06806465983390808\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.06822568774223328\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.06806465983390808  to: 0.06775889396667481\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.06791788935661316\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.06775889396667481  to: 0.06741257905960082\n",
      "Training iteration: 1837\n",
      "Validation loss (no improvement): 0.06769324541091919\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.06741257905960082  to: 0.06700707674026489\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.06766247153282165\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.06700707674026489  to: 0.06656173467636109\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.06756998896598816\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.06656173467636109  to: 0.06618610620498658\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.06646869778633117\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.06630948185920715\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.06618610620498658  to: 0.0655288577079773\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.06632717847824096\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.0655288577079773  to: 0.06512495279312133\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.06530991792678834\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.06522525548934936\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.06512495279312133  to: 0.06447836756706238\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.0651964783668518\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.06447836756706238  to: 0.06408166885375977\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.06439129114151002\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.06408166885375977  to: 0.06405418515205383\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.06405418515205383  to: 0.06359264850616456\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.06413905620574951\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.06359264850616456  to: 0.06306508779525757\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.06394627690315247\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.06306508779525757  to: 0.06262957453727722\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.06331835985183716\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.06262957453727722  to: 0.06230000257492065\n",
      "Training iteration: 1862\n",
      "Validation loss (no improvement): 0.062480247020721434\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.06230000257492065  to: 0.06205421090126038\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.06205421090126038  to: 0.06170831918716431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.06179198622703552\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.06170831918716431  to: 0.060924208164215087\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.061823475360870364\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.060924208164215087  to: 0.06010053753852844\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.06297507286071777\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.06010053753852844  to: 0.059199726581573485\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.06078314185142517\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.059199726581573485  to: 0.05883216857910156\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.05883216857910156  to: 0.05741086006164551\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.05969624519348145\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.05741086006164551  to: 0.05687861442565918\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.05687861442565918  to: 0.056014204025268556\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.05804575681686401\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.056014204025268556  to: 0.05531432628631592\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.05531432628631592  to: 0.054634702205657956\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.05643940567970276\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.054634702205657956  to: 0.05416637659072876\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.05416637659072876  to: 0.05376619100570679\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.055390346050262454\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.05376619100570679  to: 0.053229916095733645\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.053229916095733645  to: 0.053076821565628055\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.05423300862312317\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.053076821565628055  to: 0.05235103368759155\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.05285389423370361\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.05296826362609863\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.05235103368759155  to: 0.05174059271812439\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.052973657846450806\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.05174059271812439  to: 0.05139123797416687\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.05139123797416687  to: 0.05137007832527161\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.05186349153518677\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.05137007832527161  to: 0.05080577731132507\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.0526708722114563\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.05080577731132507  to: 0.05049844980239868\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.05240933299064636\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.05049844980239868  to: 0.050043237209320066\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.05146564841270447\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.050043237209320066  to: 0.04958638548851013\n",
      "Training iteration: 1902\n",
      "Validation loss (no improvement): 0.05047600865364075\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.04958638548851013  to: 0.048583096265792845\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.04963475167751312\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.048583096265792845  to: 0.04705691337585449\n",
      "Training iteration: 1906\n",
      "Validation loss (no improvement): 0.05090861320495606\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.04705691337585449  to: 0.045990782976150515\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.056532788276672366\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.045990782976150515  to: 0.04383471608161926\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.044367247819900514\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.04850471019744873\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.04383471608161926  to: 0.041679033637046815\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.04168331027030945\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.045565643906593324\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.041679033637046815  to: 0.04073387086391449\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.04073387086391449  to: 0.040085071325302125\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.04199331700801849\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.04203745722770691\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.040085071325302125  to: 0.03988485634326935\n",
      "Training iteration: 1920\n",
      "Validation loss (no improvement): 0.040139040350914\n",
      "Training iteration: 1921\n",
      "Validation loss (no improvement): 0.042441195249557494\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.04017365574836731\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.03988485634326935  to: 0.03949741721153259\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.040719643235206604\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.040866774320602414\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.03949741721153259  to: 0.03939429819583893\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.039883735775947574\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.04097324907779694\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.03939429819583893  to: 0.03916512131690979\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.039289793372154234\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.04035028517246246\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.03916512131690979  to: 0.03895050585269928\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.03944104015827179\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.039537304639816286\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.03895050585269928  to: 0.03839558959007263\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.039394798874855044\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.03839558959007263  to: 0.03771062195301056\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.038024508953094484\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.03771062195301056  to: 0.03750782608985901\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.03750782608985901  to: 0.03701511025428772\n",
      "Training iteration: 1941\n",
      "Validation loss (no improvement): 0.03741258978843689\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.03701511025428772  to: 0.036218419671058655\n",
      "Training iteration: 1943\n",
      "Validation loss (no improvement): 0.03905476033687592\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.03815903663635254\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.048215842247009276\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.0382063090801239\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.036218419671058655  to: 0.033468589186668396\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.0391722172498703\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.033468589186668396  to: 0.03308234214782715\n",
      "Training iteration: 1950\n",
      "Validation loss (no improvement): 0.033569493889808656\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.034875863790512086\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.03555658757686615\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.03308234214782715  to: 0.033011901378631595\n",
      "Training iteration: 1954\n",
      "Validation loss (no improvement): 0.03376000225543976\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.03356291353702545\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.0354854553937912\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.033958601951599124\n",
      "Training iteration: 1958\n",
      "Validation loss (no improvement): 0.03375694155693054\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.0336452454328537\n",
      "Training iteration: 1960\n",
      "Validation loss (no improvement): 0.034344977140426634\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.03481137752532959\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.03351392149925232\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.0335772842168808\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.03323451280593872\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.03392432332038879\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.03317176699638367\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.033011901378631595  to: 0.03267767429351807\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.03267767429351807  to: 0.032323017716407776\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.0325102299451828\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.032323017716407776  to: 0.03183702230453491\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.03183702230453491  to: 0.03137446641921997\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.03137446641921997  to: 0.030833008885383605\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.03093108832836151\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.030833008885383605  to: 0.030116355419158934\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.030116355419158934  to: 0.029875659942626955\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.029875659942626955  to: 0.029664096236228944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.029664096236228944  to: 0.029002222418785095\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.029002222418785095  to: 0.02873448431491852\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.02873448431491852  to: 0.02850416898727417\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.02850416898727417  to: 0.02808329463005066\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.02809135913848877\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.028276288509368898\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.02808329463005066  to: 0.02796604037284851\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.02796604037284851  to: 0.027716779708862306\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.027716779708862306  to: 0.027645829319953918\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.027698701620101927\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.02772802412509918\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.027750423550605773\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.027882325649261474\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.030070412158966064\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.03504401743412018\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.04507425725460053\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.03647785782814026\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.027645829319953918  to: 0.024227015674114227\n",
      "Training iteration: 1995\n",
      "Validation loss (no improvement): 0.032780155539512634\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.025231963396072386\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.03227252960205078\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.029963293671607973\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.029314714670181274\n",
      "Training iteration: 2000\n",
      "Validation loss (no improvement): 0.0323068380355835\n",
      "Training iteration: 2001\n",
      "Validation loss (no improvement): 0.03170313835144043\n",
      "Training iteration: 2002\n",
      "Validation loss (no improvement): 0.03335216045379639\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.03576759099960327\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.03548644483089447\n",
      "Training iteration: 2005\n",
      "Validation loss (no improvement): 0.03499421179294586\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.0358672171831131\n",
      "Training iteration: 2007\n",
      "Validation loss (no improvement): 0.036274504661560056\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.03595418930053711\n",
      "Training iteration: 2009\n",
      "Validation loss (no improvement): 0.03640795350074768\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.037220141291618346\n",
      "Training iteration: 2011\n",
      "Validation loss (no improvement): 0.036933353543281554\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.03595897257328033\n",
      "Training iteration: 2013\n",
      "Validation loss (no improvement): 0.035531502962112424\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.03537661731243134\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.03480296134948731\n",
      "Training iteration: 2016\n",
      "Validation loss (no improvement): 0.03433441519737244\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.03419848680496216\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.033548733592033385\n",
      "Training iteration: 2019\n",
      "Validation loss (no improvement): 0.032391303777694704\n",
      "Training iteration: 2020\n",
      "Validation loss (no improvement): 0.03160930275917053\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.03094194531440735\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.029966896772384642\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.029300761222839356\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.028636765480041505\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.027588626742362975\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.026925700902938842\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.026215550303459168\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.02532200813293457\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.024794280529022217\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.024227015674114227  to: 0.024039748311042785\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.024039748311042785  to: 0.02363709658384323\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.02363709658384323  to: 0.023121769726276397\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.023121769726276397  to: 0.022676301002502442\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.022676301002502442  to: 0.02231907397508621\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.02231907397508621  to: 0.02212315797805786\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.02212315797805786  to: 0.02192401885986328\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.02192401885986328  to: 0.02168166935443878\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.02168166935443878  to: 0.021652853488922118\n",
      "Training iteration: 2039\n",
      "Validation loss (no improvement): 0.02198406755924225\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.02185279577970505\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.021962559223175047\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.022261235117912292\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.022159175574779512\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.022367842495441437\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.022835114598274232\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.022885195910930634\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.023015765845775603\n",
      "Training iteration: 2048\n",
      "Validation loss (no improvement): 0.02291864901781082\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.022921974956989288\n",
      "Training iteration: 2050\n",
      "Validation loss (no improvement): 0.02305820882320404\n",
      "Training iteration: 2051\n",
      "Validation loss (no improvement): 0.022938692569732667\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.02315385788679123\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.022958648204803467\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.024068665504455567\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.024720168113708495\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.028176704049110414\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.028275448083877563\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.023333144187927247\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.021652853488922118  to: 0.01878470927476883\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.02224438637495041\n",
      "Training iteration: 2061\n",
      "Validation loss (no improvement): 0.019595505297183992\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): 0.019854480028152467\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): 0.02033582627773285\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.019525277614593505\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.020607432723045348\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.019922712445259096\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.02050844132900238\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.020251986384391785\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.02070484161376953\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.02055066078901291\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.020489969849586488\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.02041039913892746\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): 0.02028089463710785\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): 0.02033587247133255\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.019655883312225342\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.019525060057640077\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): 0.0193521112203598\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.019274190068244934\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.01878470927476883  to: 0.01848534345626831\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.01848534345626831  to: 0.01815645694732666\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.018340393900871277\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.01815645694732666  to: 0.017844542860984802\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.017844542860984802  to: 0.01744387745857239\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.01744387745857239  to: 0.01736425459384918\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.017678537964820863\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.01736425459384918  to: 0.016931448876857758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.016931448876857758  to: 0.016864652931690215\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): 0.017426128685474395\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.016864652931690215  to: 0.016674415767192842\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.016914553940296173\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): 0.017188796401023866\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): 0.016750982403755187\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.01762167066335678\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.01679605096578598\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.017491987347602843\n",
      "Training iteration: 2096\n",
      "Validation loss (no improvement): 0.016994336247444154\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): 0.01760430783033371\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.017092315852642058\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.018551717698574065\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.01854482591152191\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.02862016558647156\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.043449440598487855\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.05005616545677185\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.016674415767192842  to: 0.0153170645236969\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.03145774900913238\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.01954435557126999\n",
      "Training iteration: 2107\n",
      "Validation loss (no improvement): 0.02615303099155426\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.027041137218475342\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.026397645473480225\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.03289724588394165\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.03632572293281555\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.03516236245632172\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.03474309742450714\n",
      "Training iteration: 2114\n",
      "Validation loss (no improvement): 0.03669736683368683\n",
      "Training iteration: 2115\n",
      "Validation loss (no improvement): 0.03877947926521301\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.03949308693408966\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.0398715078830719\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): 0.040989190340042114\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.042536354064941405\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.04351908564567566\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.04346635341644287\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.04277864992618561\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.04218788743019104\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.0420513927936554\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.04213877320289612\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.041999617218971254\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.04147545397281647\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.04081913530826568\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.04035299718379974\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.04010959267616272\n",
      "Training iteration: 2131\n",
      "Validation loss (no improvement): 0.03981955647468567\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.039214900135993956\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.03830450177192688\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.037340694665908815\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.03652693629264832\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.035817956924438475\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.035043495893478396\n",
      "Training iteration: 2138\n",
      "Validation loss (no improvement): 0.034173622727394104\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.03334868550300598\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.032631775736808775\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.03188183903694153\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.030953145027160643\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.02993071973323822\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.02900887131690979\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.028186240792274476\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.02730114459991455\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.026362329721450806\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.025507894158363343\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.024704989790916444\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): 0.02383381873369217\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): 0.022971972823143005\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.02222413122653961\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.02146120071411133\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): 0.020634040236473083\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.019889500737190247\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): 0.019192366302013396\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.018510186672210695\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.01795579195022583\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): 0.017374691367149354\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.016704978048801424\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.016124531626701355\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.015628881752490997\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.0153170645236969  to: 0.015276019275188447\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.015276019275188447  to: 0.014877042174339295\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.014877042174339295  to: 0.01437731683254242\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.01437731683254242  to: 0.014009255170822143\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.014009255170822143  to: 0.013793794810771942\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.013793794810771942  to: 0.013646715879440307\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.013646715879440307  to: 0.013306862115859986\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.013306862115859986  to: 0.013085165619850158\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.013092005252838134\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.013101676106452942\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.013085165619850158  to: 0.012878036499023438\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.012878036499023438  to: 0.01281488686800003\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.012911957502365113\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.01281488686800003  to: 0.01274181604385376\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.01274181604385376  to: 0.012576094269752503\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.012630350887775421\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.012576094269752503  to: 0.012525777518749236\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.012525777518749236  to: 0.012316219508647919\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.012316219508647919  to: 0.012291587889194489\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.012291587889194489  to: 0.012158515304327011\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.012158515304327011  to: 0.011937991529703141\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.011937991529703141  to: 0.011912508308887482\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.011912508308887482  to: 0.011742520332336425\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.011742520332336425  to: 0.011549489200115204\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.011549489200115204  to: 0.011477451026439666\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.011477451026439666  to: 0.011181815713644027\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.011181815713644027  to: 0.010968343913555145\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.010968343913555145  to: 0.01076665148139\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.01076665148139  to: 0.010430403053760529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.010430403053760529  to: 0.010267762094736099\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.010267762094736099  to: 0.009932777285575867\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.009932777285575867  to: 0.009699903428554535\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.009699903428554535  to: 0.009459590911865235\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.009459590911865235  to: 0.009185408055782319\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.009185408055782319  to: 0.009066511690616608\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.009066511690616608  to: 0.008764020353555679\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.008764020353555679  to: 0.008653794974088668\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.008653794974088668  to: 0.008349771797657012\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.008349771797657012  to: 0.008237519860267639\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.008237519860267639  to: 0.007975620031356812\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.007975620031356812  to: 0.007858626544475555\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.007858626544475555  to: 0.007627509534358978\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.007627509534358978  to: 0.007523982226848603\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.007523982226848603  to: 0.007350064069032669\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.007350064069032669  to: 0.007269413769245147\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.007269413769245147  to: 0.007100589573383331\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.007100589573383331  to: 0.007020237296819687\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.007020237296819687  to: 0.006790067255496979\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.006794022023677826\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.006790067255496979  to: 0.006458650529384613\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.006772366166114807\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.006458650529384613  to: 0.006075643375515938\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.007986144721508026\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.007504455000162125\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): 0.021011194586753844\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.032768040895462036\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.0418029248714447\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.006075643375515938  to: 0.0059004068374633786\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): 0.02374263107776642\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.01148831844329834\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.020063130557537077\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.018491104245185852\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): 0.01986125409603119\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.026930394768714904\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.02720387578010559\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.025116008520126343\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.026467937231063842\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.029039648175239564\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.02950303852558136\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.029733824729919433\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.03164133429527283\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.03401245176792145\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): 0.0349174439907074\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): 0.034261807799339294\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.03344152867794037\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.03347442746162414\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.03408468961715698\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.03441176414489746\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.03414968848228454\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.03376879692077637\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.03373804688453674\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): 0.033937716484069826\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.03388840556144714\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.03335216343402862\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): 0.032573935389518735\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): 0.03193863332271576\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.03151273429393768\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): 0.031030547618865967\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): 0.030304151773452758\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.029480624198913574\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): 0.02879938781261444\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.028227627277374268\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.027511602640151976\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.026556730270385742\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.025564906001091004\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.024726299941539763\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.023958174884319304\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.023106479644775392\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.02223600894212723\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.02145785391330719\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.020695190131664275\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.019840028882026673\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): 0.018983519077301024\n",
      "Training iteration: 2266\n",
      "Validation loss (no improvement): 0.0182411789894104\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): 0.01751006543636322\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.016693031787872313\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): 0.01589894890785217\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.015175506472587585\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.014442333579063415\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): 0.013753734529018402\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): 0.013146045804023742\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): 0.012470413744449616\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): 0.011751203238964081\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): 0.011103447526693344\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): 0.010505211353302003\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.01000993698835373\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.009558474272489547\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.008978167176246643\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.008400102704763412\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.00793926790356636\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): 0.0076178088784217834\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): 0.007308445125818253\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): 0.006838794052600861\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): 0.0064263716340065\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): 0.006190017983317375\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.0060833018273115155\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.0059004068374633786  to: 0.005841064453125\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.005841064453125  to: 0.005557901412248612\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.005557901412248612  to: 0.005451750010251999\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.00546102300286293\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.005451750010251999  to: 0.005250807851552963\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.005250807851552963  to: 0.004971756413578987\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.004971756413578987  to: 0.004873431473970413\n",
      "Training iteration: 2296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.004873431473970413  to: 0.004775645211338997\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.004775645211338997  to: 0.004466339573264122\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.004466339573264122  to: 0.004280798882246017\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.004280798882246017  to: 0.00424119308590889\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.00424119308590889  to: 0.004045157507061958\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.004045157507061958  to: 0.0038842082023620604\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.003913431614637375\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.0038842082023620604  to: 0.003848714381456375\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.003848714381456375  to: 0.0037623949348926542\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.003797838091850281\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.0037623949348926542  to: 0.0036578547209501267\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.0036578547209501267  to: 0.003496462106704712\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.003496462106704712  to: 0.00344553180038929\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.00344553180038929  to: 0.00322352834045887\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.00322352834045887  to: 0.003026053309440613\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.003026053309440613  to: 0.0028563832864165308\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.0028563832864165308  to: 0.002568766102194786\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.002568766102194786  to: 0.002449549362063408\n",
      "Training iteration: 2314\n",
      "Improved validation loss from: 0.002449549362063408  to: 0.0022886358201503754\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.0022886358201503754  to: 0.002130676805973053\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.002130676805973053  to: 0.0020437609404325486\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.0020437609404325486  to: 0.001808994635939598\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.001808994635939598  to: 0.0017054017633199691\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.0017054017633199691  to: 0.0015149503946304322\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.0015149503946304322  to: 0.0013693061657249928\n",
      "Training iteration: 2321\n",
      "Improved validation loss from: 0.0013693061657249928  to: 0.0012410825118422508\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.0012410825118422508  to: 0.0010296613909304142\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.0010296613909304142  to: 0.0009247379377484322\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.0009247379377484322  to: 0.000716618588194251\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.000716618588194251  to: 0.0006675314158201218\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.0006675314158201218  to: 0.00044906483963131906\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.00044906483963131906  to: 0.0003918026573956013\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.0003918026573956013  to: 0.00012101500760763884\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.00012101500760763884  to: 9.958535665646195e-05\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 9.958535665646195e-05  to: -0.00021090626250952482\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): -8.268147939816117e-05\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: -0.00021090626250952482  to: -0.0005325421690940857\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 2.2461116896010935e-05\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: -0.0005325421690940857  to: -0.0007142461836338043\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): 0.0017473623156547546\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): 0.001874382421374321\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): 0.013770459592342377\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.019948115944862364\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): 0.019112008810043334\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: -0.0007142461836338043  to: -0.0014360683970153331\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): 0.01081492453813553\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.002050727605819702\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.00852774828672409\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): 0.0040405627340078356\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.011005272716283798\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.009390948712825775\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.007957502454519271\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.010841640084981919\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): 0.010777630656957627\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.012511157989501953\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.01565620005130768\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.015481415390968322\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): 0.014218096435070039\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.014830437302589417\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.015513162314891814\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.015339872241020203\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.016096793115139008\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.017514538764953614\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.017566271126270294\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): 0.016353717446327208\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.015559954941272736\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.015418189764022826\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.015137389302253723\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.01491612195968628\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.015090736746788024\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.01492423564195633\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.013987495005130768\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.013046768307685853\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): 0.012528708577156067\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.011958756297826768\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.011328525841236115\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.010985392332077026\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.010528700053691864\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.00966559275984764\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.008925863355398179\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.008420666307210922\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.007771058380603791\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.007160274684429169\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.006639973074197769\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): 0.005955780670046806\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.005370555073022842\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.004848961904644966\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.004074130952358246\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.0033488769084215166\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.0027192190289497377\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.00219800528138876\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.001840369962155819\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): 0.001197249721735716\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.0005018779542297125\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): -2.894401586672757e-06\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): -0.00028255791403353214\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): -0.0005837856791913509\n",
      "Training iteration: 2393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.001259989757090807\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: -0.0014360683970153331  to: -0.001808212697505951\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: -0.001808212697505951  to: -0.0020587535575032232\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: -0.0020587535575032232  to: -0.0022104797884821893\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: -0.0022104797884821893  to: -0.002799166180193424\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: -0.002799166180193424  to: -0.0033320590853691103\n",
      "Training iteration: 2399\n",
      "Improved validation loss from: -0.0033320590853691103  to: -0.0035400599241256713\n",
      "Training iteration: 2400\n",
      "Improved validation loss from: -0.0035400599241256713  to: -0.0037120789289474486\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: -0.0037120789289474486  to: -0.004263339936733246\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: -0.004263339936733246  to: -0.004612476006150246\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: -0.004612476006150246  to: -0.00465841218829155\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: -0.00465841218829155  to: -0.004960470646619797\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: -0.004960470646619797  to: -0.005374188348650932\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: -0.005374188348650932  to: -0.005457301065325737\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: -0.005457301065325737  to: -0.005580896139144897\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: -0.005580896139144897  to: -0.005914850160479546\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: -0.005914850160479546  to: -0.005942894145846367\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: -0.005942894145846367  to: -0.005965591222047806\n",
      "Training iteration: 2411\n",
      "Improved validation loss from: -0.005965591222047806  to: -0.00622759573161602\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): -0.006206203624606133\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: -0.00622759573161602  to: -0.0062838271260261536\n",
      "Training iteration: 2414\n",
      "Improved validation loss from: -0.0062838271260261536  to: -0.0064964741468429565\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): -0.006375868618488312\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: -0.0064964741468429565  to: -0.006551022082567215\n",
      "Training iteration: 2417\n",
      "Improved validation loss from: -0.006551022082567215  to: -0.006591121852397919\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): -0.006577280908823013\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: -0.006591121852397919  to: -0.0068022400140762326\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): -0.006719772517681122\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: -0.0068022400140762326  to: -0.0069861248135566715\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): -0.006922062486410141\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: -0.0069861248135566715  to: -0.007078977674245835\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): -0.007070758193731308\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: -0.007078977674245835  to: -0.0071760334074497225\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: -0.0071760334074497225  to: -0.007260308414697647\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: -0.007260308414697647  to: -0.007372041046619415\n",
      "Training iteration: 2428\n",
      "Improved validation loss from: -0.007372041046619415  to: -0.007513026893138886\n",
      "Training iteration: 2429\n",
      "Improved validation loss from: -0.007513026893138886  to: -0.007577082514762879\n",
      "Training iteration: 2430\n",
      "Improved validation loss from: -0.007577082514762879  to: -0.007738480716943741\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): -0.007688120007514954\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: -0.007738480716943741  to: -0.007951215654611588\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): -0.007541289925575257\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: -0.007951215654611588  to: -0.008006039261817931\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): -0.005105395987629891\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): -0.0020260933786630632\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.023113247752189637\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.05471553206443787\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.0137888103723526\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.003785176947712898\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.009360618144273757\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.019409048557281493\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.004608435556292534\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.011291652917861938\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.015031468868255616\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): 0.013105721771717071\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.02025282382965088\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.031213408708572386\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.035718169808387754\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.03287826478481293\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.028300341963768006\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): 0.02671181261539459\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.028391480445861816\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): 0.030741900205612183\n",
      "Training iteration: 2455\n",
      "Validation loss (no improvement): 0.0319941520690918\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): 0.03200099468231201\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.031969159841537476\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.03279758095741272\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.03449143767356873\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.03632170557975769\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): 0.037450185418128966\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.037456619739532473\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.036496701836586\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.03510273396968842\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.0338267982006073\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.032958322763442995\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.03246830105781555\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.0321422815322876\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): 0.03172870576381683\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.0312032550573349\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.030736657977104186\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.030488115549087525\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.030424922704696655\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): 0.030374044179916383\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.03009888231754303\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.029444384574890136\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.028428825736045837\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.027223628759384156\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.0260407954454422\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.025012370944023133\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.02414139062166214\n",
      "Training iteration: 2482\n",
      "Validation loss (no improvement): 0.02335793972015381\n",
      "Training iteration: 2483\n",
      "Validation loss (no improvement): 0.022605785727500917\n",
      "Training iteration: 2484\n",
      "Validation loss (no improvement): 0.021889789402484892\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.02124650776386261\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.02067534476518631\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.02010483294725418\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): 0.019432492554187775\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.018603515625\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): 0.01765441745519638\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.016681063175201415\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.0157610684633255\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.014905750751495361\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.014081540703773498\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): 0.013265815377235413\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.012466581165790558\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): 0.011693049967288972\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): 0.010924899578094482\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.010128684341907501\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.009301121532917022\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.008476885408163071\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.007685546576976776\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.00691397562623024\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.0061279721558094025\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.005318502336740494\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.0045073099434375765\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.003710423409938812\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.0029259508475661276\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.002156577631831169\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.0014149734750390052\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.000692439079284668\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): -4.7647059545852244e-05\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): -0.0008239006623625755\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): -0.0016108786687254906\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): -0.0023724859580397605\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): -0.0030932735651731493\n",
      "Training iteration: 2517\n",
      "Validation loss (no improvement): -0.0037695132195949554\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): -0.004403181746602059\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): -0.0050327606499195095\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): -0.005696207284927368\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): -0.006374491751194001\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): -0.007026625424623489\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): -0.00763469859957695\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: -0.008006039261817931  to: -0.008202537149190902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2525\n",
      "Improved validation loss from: -0.008202537149190902  to: -0.008757800608873368\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: -0.008757800608873368  to: -0.009339174628257752\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: -0.009339174628257752  to: -0.009944361448287965\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: -0.009944361448287965  to: -0.010529960691928863\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: -0.010529960691928863  to: -0.011065117269754409\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: -0.011065117269754409  to: -0.011550883948802947\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: -0.011550883948802947  to: -0.012017140537500382\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: -0.012017140537500382  to: -0.012500825524330138\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: -0.012500825524330138  to: -0.012975044548511505\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: -0.012975044548511505  to: -0.013408568501472474\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: -0.013408568501472474  to: -0.013806232810020446\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: -0.013806232810020446  to: -0.014184406399726868\n",
      "Training iteration: 2537\n",
      "Improved validation loss from: -0.014184406399726868  to: -0.014569081366062164\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: -0.014569081366062164  to: -0.01495429128408432\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: -0.01495429128408432  to: -0.015295515954494476\n",
      "Training iteration: 2540\n",
      "Improved validation loss from: -0.015295515954494476  to: -0.015582837164402008\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: -0.015582837164402008  to: -0.015834730863571168\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: -0.015834730863571168  to: -0.016078084707260132\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: -0.016078084707260132  to: -0.01632290780544281\n",
      "Training iteration: 2544\n",
      "Improved validation loss from: -0.01632290780544281  to: -0.016552442312240602\n",
      "Training iteration: 2545\n",
      "Improved validation loss from: -0.016552442312240602  to: -0.016754545271396637\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: -0.016754545271396637  to: -0.016921380162239076\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: -0.016921380162239076  to: -0.017077450454235078\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: -0.017077450454235078  to: -0.017201705276966094\n",
      "Training iteration: 2549\n",
      "Improved validation loss from: -0.017201705276966094  to: -0.017289432883262634\n",
      "Training iteration: 2550\n",
      "Improved validation loss from: -0.017289432883262634  to: -0.017360982298851014\n",
      "Training iteration: 2551\n",
      "Improved validation loss from: -0.017360982298851014  to: -0.017439821362495424\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: -0.017439821362495424  to: -0.017499694228172304\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: -0.017499694228172304  to: -0.017561523616313933\n",
      "Training iteration: 2554\n",
      "Improved validation loss from: -0.017561523616313933  to: -0.017644441127777098\n",
      "Training iteration: 2555\n",
      "Improved validation loss from: -0.017644441127777098  to: -0.01775738298892975\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: -0.01775738298892975  to: -0.017852893471717833\n",
      "Training iteration: 2557\n",
      "Improved validation loss from: -0.017852893471717833  to: -0.0178984135389328\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: -0.0178984135389328  to: -0.017906507849693297\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): -0.017899470031261445\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): -0.017894983291625977\n",
      "Training iteration: 2561\n",
      "Improved validation loss from: -0.017906507849693297  to: -0.01793370842933655\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: -0.01793370842933655  to: -0.01805279552936554\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: -0.01805279552936554  to: -0.018231506645679473\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: -0.018231506645679473  to: -0.01841002404689789\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: -0.01841002404689789  to: -0.01853642165660858\n",
      "Training iteration: 2566\n",
      "Improved validation loss from: -0.01853642165660858  to: -0.018600033223629\n",
      "Training iteration: 2567\n",
      "Improved validation loss from: -0.018600033223629  to: -0.018630370497703552\n",
      "Training iteration: 2568\n",
      "Improved validation loss from: -0.018630370497703552  to: -0.018672484159469604\n",
      "Training iteration: 2569\n",
      "Improved validation loss from: -0.018672484159469604  to: -0.018743376433849334\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: -0.018743376433849334  to: -0.018835519254207612\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: -0.018835519254207612  to: -0.01894206404685974\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: -0.01894206404685974  to: -0.01904795616865158\n",
      "Training iteration: 2573\n",
      "Improved validation loss from: -0.01904795616865158  to: -0.01912231892347336\n",
      "Training iteration: 2574\n",
      "Improved validation loss from: -0.01912231892347336  to: -0.019194309413433076\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: -0.019194309413433076  to: -0.019300328195095064\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: -0.019300328195095064  to: -0.019432707130908965\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: -0.019432707130908965  to: -0.019560468196868897\n",
      "Training iteration: 2578\n",
      "Improved validation loss from: -0.019560468196868897  to: -0.019631053507328033\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): -0.019630366563797\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): -0.019553694128990173\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): -0.019444768130779267\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): -0.019338104128837585\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): -0.019263795018196105\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): -0.019240085780620576\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): -0.01923426389694214\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): -0.019210615754127504\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): -0.01916116923093796\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): -0.019084498286247253\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): -0.019002333283424377\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): -0.018929865956306458\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): -0.018846049904823303\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): -0.018741723895072938\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): -0.018620893359184265\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): -0.018534310162067413\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): -0.018502801656723022\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): -0.018471632897853852\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): -0.01842859387397766\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): -0.018354520201683044\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): -0.01825000047683716\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): -0.018155767023563384\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): -0.01809137165546417\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): -0.01807391196489334\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): -0.018069943785667418\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): -0.018039175868034364\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): -0.017963109910488127\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): -0.017833943665027618\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): -0.017696185410022734\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): -0.017579229176044465\n",
      "Training iteration: 2609\n",
      "Validation loss (no improvement): -0.017517511546611787\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): -0.017440776526927947\n",
      "Training iteration: 2611\n",
      "Validation loss (no improvement): -0.017352189123630523\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): -0.01720210611820221\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): -0.017048147320747376\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): -0.016881075501441956\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): -0.016830143332481385\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): -0.016712619364261626\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): -0.016712906956672668\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): -0.016047534346580506\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): -0.014803870022296906\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): -0.00648246631026268\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.022522687911987305\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.07335609197616577\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.05030885338783264\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.007713298499584198\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): -0.0025704050436615943\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.006013411283493042\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): -0.003779463842511177\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.02308998107910156\n",
      "Training iteration: 2629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.031692928075790404\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.01783490628004074\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.01012430340051651\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.015098999440670013\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.021840929985046387\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.023417940735816954\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.023583297431468964\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.027688834071159362\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.0352919340133667\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.04273970723152161\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.046750664710998535\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.046564731001853946\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.04361884593963623\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.0400231271982193\n",
      "Training iteration: 2643\n",
      "Validation loss (no improvement): 0.03729493618011474\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.0359309196472168\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.03557012677192688\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.03556572794914246\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): 0.03534615635871887\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.03504915535449982\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.03505591750144958\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.035870006680488585\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.037496748566627505\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.039522808790206906\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.041269198060035706\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.0420728862285614\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.04161213934421539\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.04005169868469238\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.03790430426597595\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.03576852679252625\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.03402150273323059\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.032739031314849856\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.03187344670295715\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.031175151467323303\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.03043503165245056\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.029636085033416748\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.028953826427459715\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.028618830442428588\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.028742718696594238\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.02919395864009857\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.02972496151924133\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.029965561628341675\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.02961276173591614\n",
      "Training iteration: 2672\n",
      "Validation loss (no improvement): 0.028598999977111815\n",
      "Training iteration: 2673\n",
      "Validation loss (no improvement): 0.027105003595352173\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.02543439269065857\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.02384033501148224\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.022515459358692168\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.021478724479675294\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.02064129412174225\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.019946198165416717\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.019396594166755675\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): 0.01904875934123993\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.018932023644447328\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.01897672712802887\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): 0.019010360538959502\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.018828079104423523\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.0182915598154068\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.017393958568573\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.01625472754240036\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.015052171051502227\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.013939805328845978\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.012997342646121979\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.012237942218780518\n",
      "Training iteration: 2693\n",
      "Validation loss (no improvement): 0.011649210751056672\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.011222027242183685\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.010941284894943237\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.010756226629018784\n",
      "Training iteration: 2697\n",
      "Validation loss (no improvement): 0.01056719794869423\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.01025407314300537\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.009730418771505355\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.008986303955316544\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.008089933544397354\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.007153655588626862\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.0062711201608181\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.005502918362617492\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.004871125891804695\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.00437360480427742\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.003989086672663689\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.0036732874810695647\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.0033571582287549974\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.0029637956991791724\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.0024399947375059127\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.0017805587500333785\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.0010298395529389382\n",
      "Training iteration: 2714\n",
      "Validation loss (no improvement): 0.0002576053142547607\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): -0.0004698318429291248\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): -0.001108871679753065\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): -0.0016433561220765113\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): -0.002082660235464573\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): -0.0024521928280591965\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): -0.002809831500053406\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): -0.003215845674276352\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): -0.003712186962366104\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): -0.004304166883230209\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): -0.00495913028717041\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): -0.005623137950897217\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): -0.006245900318026543\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): -0.006783682107925415\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): -0.007135628163814545\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): -0.0074214056134223935\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): -0.007677789032459259\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): -0.007949577271938324\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): -0.008273814618587495\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): -0.008644934743642807\n",
      "Training iteration: 2734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.009052439779043197\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): -0.009461238235235214\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): -0.009833337366580963\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): -0.01014426201581955\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): -0.01038622260093689\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): -0.010578310489654541\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): -0.010751950740814208\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): -0.010942654311656952\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): -0.011176307499408723\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): -0.011441028118133545\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): -0.011716566979885101\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): -0.011976997554302215\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): -0.012191797792911529\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): -0.012363366782665253\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): -0.012501578032970428\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): -0.012627263367176057\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): -0.01277085840702057\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): -0.012950196862220764\n",
      "Training iteration: 2752\n",
      "Validation loss (no improvement): -0.013160867989063263\n",
      "Training iteration: 2753\n",
      "Validation loss (no improvement): -0.01339903324842453\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): -0.013630411028862\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): -0.0138349249958992\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): -0.014002318680286407\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): -0.01414424329996109\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): -0.014274267852306366\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): -0.014416766166687012\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): -0.014587143063545227\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): -0.014767877757549286\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): -0.014949429035186767\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): -0.015123024582862854\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): -0.015272809565067292\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): -0.015387172996997833\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): -0.015471366047859193\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): -0.01555039882659912\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): -0.01566608399152756\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): -0.015767641365528107\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): -0.01584787666797638\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): -0.015911854803562164\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): -0.01594495475292206\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): -0.01595017611980438\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): -0.015951913595199586\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): -0.015936505794525147\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): -0.015904513001441956\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): -0.015926796197891235\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): -0.0159594863653183\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): -0.01601777523756027\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): -0.01604570299386978\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): -0.016076448559761047\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): -0.016094154119491576\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): -0.016109374165534974\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): -0.016206619143486024\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): -0.01632744073867798\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): -0.016462768614292144\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): -0.016568604111671447\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): -0.016602958738803863\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): -0.016558630764484404\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): -0.016521963477134704\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): -0.016556446254253388\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): -0.0166590616106987\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): -0.016754399240016937\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): -0.016757401823997497\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): -0.016693338751792908\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): -0.016594424843788147\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): -0.016574588418006898\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): -0.016677549481391905\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): -0.016850166022777557\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): -0.01701425909996033\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): -0.017048561573028566\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): -0.01697314977645874\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): -0.016900274157524108\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): -0.016888824105262757\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): -0.01700163185596466\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): -0.017171505093574523\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): -0.017335911095142365\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): -0.01738848686218262\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): -0.017277438938617707\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): -0.017114503681659697\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): -0.0170183390378952\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): -0.01711413115262985\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): -0.017288082838058473\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): -0.017440178990364076\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): -0.017515191435813905\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): -0.01746058166027069\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): -0.017351134121417998\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): -0.017291954159736632\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): -0.01739937961101532\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): -0.017635579407215118\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): -0.01782507300376892\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): -0.017841601371765138\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): -0.01776135265827179\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): -0.0176146000623703\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): -0.01745683401823044\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): -0.017488154768943786\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): -0.01769428551197052\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): -0.01790515035390854\n",
      "Training iteration: 2829\n",
      "Validation loss (no improvement): -0.018095371127128602\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): -0.018119524419307708\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): -0.01799251288175583\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): -0.01782127171754837\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): -0.017733678221702576\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): -0.01777743101119995\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): -0.017930951714515687\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): -0.018069982528686523\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): -0.01814368963241577\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): -0.018127889931201936\n",
      "Training iteration: 2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.01811186969280243\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): -0.01812065243721008\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): -0.018170395493507387\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): -0.018304529786109924\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): -0.018464095890522003\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): -0.018445658683776855\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): -0.01819026917219162\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): -0.017862579226493834\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): -0.0178763747215271\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): -0.018184740841388703\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): -0.01865253746509552\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): -0.018876519799232484\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): -0.01873912811279297\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): -0.018417172133922577\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): -0.018124721944332123\n",
      "Training iteration: 2854\n",
      "Validation loss (no improvement): -0.018236547708511353\n",
      "Training iteration: 2855\n",
      "Validation loss (no improvement): -0.01862799972295761\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): -0.018878763914108275\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): -0.018859395384788515\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): -0.01863293945789337\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): -0.01849207580089569\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): -0.01855046898126602\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): -0.01870550662279129\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): -0.018839143216609955\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): -0.01888614445924759\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): -0.01881992518901825\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): -0.018683059513568877\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): -0.018641403317451476\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): -0.018769259750843047\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): -0.019040799140930174\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): -0.019174103438854218\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): -0.019022437930107116\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): -0.018894596397876738\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): -0.01880836933851242\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): -0.018826216459274292\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): -0.01894659101963043\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): -0.01910184621810913\n",
      "Training iteration: 2876\n",
      "Validation loss (no improvement): -0.019075211882591248\n",
      "Training iteration: 2877\n",
      "Validation loss (no improvement): -0.01896542012691498\n",
      "Training iteration: 2878\n",
      "Validation loss (no improvement): -0.01895681470632553\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): -0.019032461941242217\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): -0.01909927576780319\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): -0.019094514846801757\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): -0.019107483327388763\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): -0.019048507511615752\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): -0.019185061752796172\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): -0.019304002821445464\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): -0.01938876211643219\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): -0.019403520226478576\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): -0.019310781359672548\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): -0.019215330481529236\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): -0.019280512630939484\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): -0.019318373501300813\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): -0.01931488811969757\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): -0.019348005950450897\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): -0.019457343220710754\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): -0.01939460039138794\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): -0.019345796108245848\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): -0.019303783774375916\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): -0.019470693171024324\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): -0.019592514634132384\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): -0.019556669890880583\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): -0.019512486457824708\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): -0.019458281993865966\n",
      "Training iteration: 2903\n",
      "Validation loss (no improvement): -0.019362577795982362\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): -0.01936747580766678\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): -0.0195665568113327\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: -0.019631053507328033  to: -0.01968260109424591\n",
      "Training iteration: 2907\n",
      "Validation loss (no improvement): -0.01951741725206375\n",
      "Training iteration: 2908\n",
      "Validation loss (no improvement): -0.019375905394554138\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): -0.01962930113077164\n",
      "Training iteration: 2910\n",
      "Improved validation loss from: -0.01968260109424591  to: -0.01995423883199692\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): -0.019827750325202943\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): -0.019369710981845856\n",
      "Training iteration: 2913\n",
      "Validation loss (no improvement): -0.01916322410106659\n",
      "Training iteration: 2914\n",
      "Validation loss (no improvement): -0.019382527470588683\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): -0.019738441705703734\n",
      "Training iteration: 2916\n",
      "Improved validation loss from: -0.01995423883199692  to: -0.019965775310993195\n",
      "Training iteration: 2917\n",
      "Improved validation loss from: -0.019965775310993195  to: -0.020045988261699677\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): -0.019878044724464417\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): -0.019430854916572572\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): -0.01915280520915985\n",
      "Training iteration: 2921\n",
      "Validation loss (no improvement): -0.019370144605636595\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): -0.019682449102401734\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): -0.019723108410835265\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): -0.019627827405929565\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): -0.0198241263628006\n",
      "Training iteration: 2926\n",
      "Improved validation loss from: -0.020045988261699677  to: -0.020057900249958037\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): -0.01982291042804718\n",
      "Training iteration: 2928\n",
      "Validation loss (no improvement): -0.019410240650177\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): -0.019384017586708067\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): -0.01962167173624039\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): -0.019773565232753754\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): -0.019748792052268982\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): -0.019899339973926546\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): -0.019982412457466125\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): -0.019620856642723082\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): -0.019277448952198028\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): -0.019390007853507994\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): -0.019729951024055482\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): -0.01983511745929718\n",
      "Training iteration: 2940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.019751110672950746\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): -0.01999098211526871\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): -0.020041847229003908\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): -0.019584895670413972\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): -0.01913772076368332\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): -0.019367355108261108\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): -0.019860783219337465\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): -0.01985800266265869\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): -0.019665618240833283\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): -0.019925206899642944\n",
      "Training iteration: 2950\n",
      "Improved validation loss from: -0.020057900249958037  to: -0.0201128289103508\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): -0.019674326479434966\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): -0.019420595467090608\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): -0.019617630541324614\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): -0.01986791044473648\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): -0.020016387104988098\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): -0.01990838795900345\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): -0.019521281123161316\n",
      "Training iteration: 2958\n",
      "Validation loss (no improvement): -0.019765469431877136\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): -0.01996678113937378\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): -0.019961372017860413\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): -0.019141563773155214\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): -0.019705577194690703\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): -0.019038644433021546\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): -0.019634740054607393\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): -0.018754604458808898\n",
      "Training iteration: 2966\n",
      "Improved validation loss from: -0.0201128289103508  to: -0.02111755162477493\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): -0.01627955734729767\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): -0.01783733069896698\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): -0.0012016719207167625\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.002381230518221855\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.02564249038696289\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): -0.0017859458923339844\n",
      "Training iteration: 2973\n",
      "Improved validation loss from: -0.02111755162477493  to: -0.022663044929504394\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): -0.0018461454659700395\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): -0.010977815091609954\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): -0.0020510518923401834\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): -0.01502373218536377\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): -0.006468534469604492\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): -0.011119107902050018\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): -0.0002265703631564975\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): -0.008413468301296235\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): -0.00963428094983101\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): -0.0059831272810697556\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): -0.009802309423685074\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): -0.005369561910629273\n",
      "Training iteration: 2986\n",
      "Validation loss (no improvement): -0.00486036017537117\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): -0.009727505594491958\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): -0.008250784873962403\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): -0.006898698955774307\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): -0.007019088417291641\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): -0.0031396962702274324\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): -0.0023987531661987306\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): -0.006500470638275147\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): -0.007509603351354599\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): -0.006326350569725037\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): -0.0062869139015674595\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): -0.004313229769468308\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): -0.0024205338209867477\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): -0.004923430830240249\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): -0.008016891777515411\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): -0.008493196219205856\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): -0.008801379054784775\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): -0.008541349321603775\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): -0.0069812841713428496\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): -0.007439740002155304\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): -0.00987214595079422\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): -0.0111583411693573\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): -0.011674852669239044\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): -0.011730505526065827\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): -0.010498461872339248\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): -0.010047926753759383\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): -0.011344671249389648\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): -0.012606851756572723\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): -0.013490124046802521\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): -0.013882561028003693\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): -0.013467936217784882\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): -0.013419702649116516\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): -0.014035435020923614\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): -0.014852985739707947\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): -0.01580920219421387\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): -0.01632825881242752\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): -0.016514936089515687\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): -0.016696009039878845\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): -0.016880889236927033\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): -0.01739421784877777\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): -0.018039238452911378\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): -0.018672123551368713\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): -0.019042594730854033\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): -0.019018802046775817\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): -0.019116950035095216\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): -0.019402547180652617\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): -0.019953374564647675\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): -0.020462188124656677\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): -0.02087376415729523\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): -0.021151676774024963\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): -0.02120223343372345\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): -0.020936033129692076\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): -0.02112942636013031\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): -0.022173261642456053\n",
      "Training iteration: 3040\n",
      "Improved validation loss from: -0.022663044929504394  to: -0.022668056190013885\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): -0.02261991947889328\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): -0.02263767719268799\n",
      "Training iteration: 3043\n",
      "Improved validation loss from: -0.022668056190013885  to: -0.022867897152900697\n",
      "Training iteration: 3044\n",
      "Improved validation loss from: -0.022867897152900697  to: -0.023157711327075958\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: -0.023157711327075958  to: -0.02348277121782303\n",
      "Training iteration: 3046\n",
      "Improved validation loss from: -0.02348277121782303  to: -0.023674902319908143\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): -0.023153197765350342\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): -0.02259187251329422\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): -0.022897760570049285\n",
      "Training iteration: 3050\n",
      "Improved validation loss from: -0.023674902319908143  to: -0.024330110847949983\n",
      "Training iteration: 3051\n",
      "Improved validation loss from: -0.024330110847949983  to: -0.02512702941894531\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): -0.024552765488624572\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): -0.023571929335594176\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): -0.02369420975446701\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): -0.024216285347938536\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): -0.02471289187669754\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): -0.024937519431114198\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): -0.024177441000938417\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): -0.024032506346702575\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): -0.024691009521484376\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): -0.024454288184642792\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): -0.024305100739002227\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): -0.024518375098705292\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): -0.024901905655860902\n",
      "Training iteration: 3065\n",
      "Improved validation loss from: -0.02512702941894531  to: -0.025127047300338747\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): -0.024376967549324037\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): -0.02431592494249344\n",
      "Training iteration: 3068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.025042030215263366\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): -0.024461646378040314\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): -0.02370522916316986\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): -0.02467910051345825\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): -0.025078099966049195\n",
      "Training iteration: 3073\n",
      "Improved validation loss from: -0.025127047300338747  to: -0.025247979164123534\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): -0.02362775057554245\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): -0.024229796230793\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): -0.02456166744232178\n",
      "Training iteration: 3077\n",
      "Improved validation loss from: -0.025247979164123534  to: -0.025404319167137146\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): -0.02467123568058014\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): -0.024356313049793243\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): -0.02449564039707184\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): -0.024311542510986328\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): -0.025187775492668152\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): -0.024647732079029084\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): -0.0248812198638916\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): -0.02274016886949539\n",
      "Training iteration: 3086\n",
      "Improved validation loss from: -0.025404319167137146  to: -0.02573069930076599\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): -0.022333709895610808\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): -0.02527661621570587\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): -0.017303244769573213\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): -0.022605247795581818\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): -0.007204342633485794\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): -0.011370692402124405\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.014393243193626403\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): -0.003486328199505806\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): -0.024378684163093568\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): -0.01802937686443329\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): -0.01654890775680542\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): -0.02168252468109131\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): -0.010797728598117829\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): -0.019646613299846648\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): -0.014439156651496888\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): -0.02059849202632904\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): -0.01856060475111008\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): -0.02255619317293167\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): -0.0205411434173584\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): -0.020794185996055602\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): -0.01584130823612213\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): -0.017844243347644805\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): -0.015677951276302338\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): -0.017482273280620575\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): -0.01704164445400238\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): -0.016415658593177795\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): -0.01718832403421402\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): -0.016970200836658476\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): -0.019591300189495085\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): -0.01875804513692856\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): -0.01941920816898346\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): -0.018757759034633635\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): -0.01714543253183365\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): -0.017957742512226104\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): -0.018126113712787627\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): -0.019194526970386504\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): -0.018818847835063934\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): -0.01899537891149521\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): -0.0190117284655571\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): -0.019927814602851868\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): -0.021865102648735046\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): -0.022396378219127655\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): -0.022098951041698456\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): -0.02137404978275299\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): -0.02185758650302887\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): -0.022304478287696838\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): -0.02246558666229248\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): -0.022076252102851867\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): -0.02277909219264984\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): -0.0229635089635849\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): -0.02344629019498825\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): -0.023768138885498048\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): -0.024822863936424255\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): -0.02520996630191803\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): -0.024671205878257753\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): -0.024308247864246367\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): -0.02522771954536438\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): -0.025659427046775818\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): -0.02528027296066284\n",
      "Training iteration: 3146\n",
      "Improved validation loss from: -0.02573069930076599  to: -0.025885516405105592\n",
      "Training iteration: 3147\n",
      "Improved validation loss from: -0.025885516405105592  to: -0.026351311802864076\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): -0.026092880964279176\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): -0.026294419169425966\n",
      "Training iteration: 3150\n",
      "Improved validation loss from: -0.026351311802864076  to: -0.02711012363433838\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): -0.026522883772850038\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): -0.02598654627799988\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): -0.026575011014938355\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): -0.026243570446968078\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): -0.02618291974067688\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): -0.027107924222946167\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): -0.026313194632530214\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): -0.02612859904766083\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): -0.026933011412620545\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): -0.026349836587905885\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): -0.026728150248527528\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): -0.02623562216758728\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): -0.025529512763023378\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): -0.02683974802494049\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): -0.025875505805015565\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): -0.02667122781276703\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): -0.025888910889625548\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): -0.025826039910316467\n",
      "Training iteration: 3169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.026052588224411012\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): -0.025528889894485474\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): -0.026101210713386537\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): -0.02444736212491989\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): -0.02658350467681885\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): -0.024358975887298583\n",
      "Training iteration: 3175\n",
      "Improved validation loss from: -0.02711012363433838  to: -0.027263319492340087\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): -0.02063228636980057\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): -0.02715016305446625\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): -0.01342775672674179\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): -0.01810203343629837\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.015268239378929137\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.011358827352523804\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.014064793288707734\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): -0.024156709015369416\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): -0.020441079139709474\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): -0.006173626333475113\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): -0.018383577466011047\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): -0.009581433236598968\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): -0.017922525107860566\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): -0.018527951836586\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): -0.016151154041290285\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): -0.01903693675994873\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): -0.014797724783420563\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): -0.02060176581144333\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): -0.016655799746513367\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): -0.014583109319210053\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): -0.015022270381450653\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): -0.009002868831157685\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): -0.011710363626480102\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): -0.016621401906013487\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): -0.01456376612186432\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): -0.014079698920249939\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): -0.013078895211219788\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): -0.01176709309220314\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): -0.015822096168994902\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): -0.017997850477695466\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): -0.016494302451610564\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): -0.01601467877626419\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): -0.012999232113361358\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): -0.010723116248846054\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): -0.015012894570827485\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): -0.018621769547462464\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): -0.018290242552757262\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): -0.017848052084445953\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): -0.015709976851940154\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): -0.014809140563011169\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): -0.0182876318693161\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): -0.020583292841911315\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): -0.020789170265197755\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): -0.02055603563785553\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): -0.018521155416965484\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): -0.018669764697551727\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): -0.021527929604053496\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): -0.022918084263801576\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): -0.023334726691246033\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): -0.022239696979522706\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): -0.02042430341243744\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): -0.02124413549900055\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): -0.02335605174303055\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): -0.024953024089336397\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): -0.025122076272964478\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): -0.024036702513694764\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): -0.02368929386138916\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): -0.024732908606529234\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): -0.026494163274765014\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): -0.02720765471458435\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): -0.02644496560096741\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): -0.02552069127559662\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): -0.026110237836837767\n",
      "Training iteration: 3239\n",
      "Improved validation loss from: -0.027263319492340087  to: -0.027632588148117067\n",
      "Training iteration: 3240\n",
      "Improved validation loss from: -0.027632588148117067  to: -0.028314965963363647\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): -0.027673012018203734\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): -0.02709757089614868\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): -0.02746398448944092\n",
      "Training iteration: 3244\n",
      "Improved validation loss from: -0.028314965963363647  to: -0.028352105617523195\n",
      "Training iteration: 3245\n",
      "Improved validation loss from: -0.028352105617523195  to: -0.02861957848072052\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): -0.028494268655776978\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): -0.02798706591129303\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): -0.02763122320175171\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): -0.028092288970947267\n",
      "Training iteration: 3250\n",
      "Improved validation loss from: -0.02861957848072052  to: -0.02894349992275238\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): -0.02869947850704193\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): -0.027681562304496764\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): -0.027900820970535277\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): -0.02872283458709717\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): -0.028517842292785645\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): -0.027909383177757263\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): -0.02786073088645935\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): -0.0277832567691803\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): -0.02791779637336731\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): -0.02812543511390686\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): -0.027332276105880737\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): -0.026944231986999512\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): -0.027581727504730223\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): -0.027279886603355407\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): -0.026780766248703004\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): -0.026789513230323792\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): -0.026430431008338928\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): -0.02665623128414154\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): -0.026517266035079957\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): -0.02575337290763855\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): -0.02616870403289795\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): -0.025905027985572815\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): -0.02557929456233978\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): -0.02543181777000427\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): -0.02501649260520935\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): -0.025434166193008423\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): -0.02481071949005127\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): -0.02474890649318695\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): -0.024662783741950987\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): -0.02434225529432297\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): -0.024167509377002717\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): -0.02356775552034378\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): -0.024119058251380922\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): -0.023469336330890656\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): -0.023728039860725404\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): -0.022744813561439516\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): -0.02371920794248581\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): -0.02227821797132492\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): -0.02366596758365631\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): -0.02104657143354416\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): -0.024449391663074492\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): -0.018028596043586732\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): -0.02533131241798401\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): -0.005975501611828804\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): -0.013716092705726624\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.049340429902076724\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.039693400263786316\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.049862003326416014\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): -0.023410284519195558\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): -0.0005513691809028387\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): -0.02093413323163986\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.005498216301202774\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): -0.010648345947265625\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.006530696153640747\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): -0.009256071597337722\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): -0.015151938796043396\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): -0.0030285406857728956\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): -0.005891956016421318\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): -0.0028027141466736793\n",
      "Training iteration: 3310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.00018322349060326815\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): -0.009219973534345626\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): -0.01246940642595291\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): -0.00709904283285141\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): -0.005586541816592216\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): -0.0035427283495664597\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.004671011120080948\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.006783030182123184\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): -0.0015421092510223388\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): -0.008212050050497055\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): -0.008265617489814758\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): -0.007962299883365631\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): -0.008522813022136689\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): -0.004542016983032226\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.0020011639222502708\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.0012600195594131946\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): -0.007106813043355942\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): -0.01344001442193985\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): -0.014070442318916321\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): -0.01407283842563629\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): -0.015222199261188507\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): -0.013460311293601989\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): -0.009331248700618744\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): -0.009374091774225235\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): -0.0144685298204422\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): -0.018529045581817626\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): -0.019411087036132812\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): -0.019958753883838654\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): -0.02048254758119583\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): -0.018742915987968446\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): -0.016614051163196565\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): -0.017733989655971526\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): -0.02109430730342865\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): -0.023346579074859618\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): -0.024313831329345705\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): -0.024979622662067415\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): -0.02451169937849045\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): -0.023449043929576873\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): -0.024120859801769257\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): -0.026519584655761718\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): -0.028605824708938597\n",
      "Training iteration: 3351\n",
      "Improved validation loss from: -0.02894349992275238  to: -0.029828619956970216\n",
      "Training iteration: 3352\n",
      "Improved validation loss from: -0.029828619956970216  to: -0.030492812395095825\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): -0.030284255743026733\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): -0.029883867502212523\n",
      "Training iteration: 3355\n",
      "Improved validation loss from: -0.030492812395095825  to: -0.030577433109283448\n",
      "Training iteration: 3356\n",
      "Improved validation loss from: -0.030577433109283448  to: -0.0320734441280365\n",
      "Training iteration: 3357\n",
      "Improved validation loss from: -0.0320734441280365  to: -0.03330957889556885\n",
      "Training iteration: 3358\n",
      "Improved validation loss from: -0.03330957889556885  to: -0.034016388654708865\n",
      "Training iteration: 3359\n",
      "Improved validation loss from: -0.034016388654708865  to: -0.03408443033695221\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): -0.03373317122459411\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): -0.03391530215740204\n",
      "Training iteration: 3362\n",
      "Improved validation loss from: -0.03408443033695221  to: -0.03499411642551422\n",
      "Training iteration: 3363\n",
      "Improved validation loss from: -0.03499411642551422  to: -0.03622755110263824\n",
      "Training iteration: 3364\n",
      "Improved validation loss from: -0.03622755110263824  to: -0.0370087593793869\n",
      "Training iteration: 3365\n",
      "Improved validation loss from: -0.0370087593793869  to: -0.037214383482933044\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): -0.037123024463653564\n",
      "Training iteration: 3367\n",
      "Improved validation loss from: -0.037214383482933044  to: -0.03749680817127228\n",
      "Training iteration: 3368\n",
      "Improved validation loss from: -0.03749680817127228  to: -0.038559770584106444\n",
      "Training iteration: 3369\n",
      "Improved validation loss from: -0.038559770584106444  to: -0.03958739042282104\n",
      "Training iteration: 3370\n",
      "Improved validation loss from: -0.03958739042282104  to: -0.040017548203468326\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): -0.04000240266323089\n",
      "Training iteration: 3372\n",
      "Improved validation loss from: -0.040017548203468326  to: -0.0400658369064331\n",
      "Training iteration: 3373\n",
      "Improved validation loss from: -0.0400658369064331  to: -0.04073553085327149\n",
      "Training iteration: 3374\n",
      "Improved validation loss from: -0.04073553085327149  to: -0.041657909750938416\n",
      "Training iteration: 3375\n",
      "Improved validation loss from: -0.041657909750938416  to: -0.04205648303031921\n",
      "Training iteration: 3376\n",
      "Improved validation loss from: -0.04205648303031921  to: -0.04211307466030121\n",
      "Training iteration: 3377\n",
      "Improved validation loss from: -0.04211307466030121  to: -0.04233912825584411\n",
      "Training iteration: 3378\n",
      "Improved validation loss from: -0.04233912825584411  to: -0.04292576313018799\n",
      "Training iteration: 3379\n",
      "Improved validation loss from: -0.04292576313018799  to: -0.043550634384155275\n",
      "Training iteration: 3380\n",
      "Improved validation loss from: -0.043550634384155275  to: -0.043737536668777464\n",
      "Training iteration: 3381\n",
      "Improved validation loss from: -0.043737536668777464  to: -0.04378691613674164\n",
      "Training iteration: 3382\n",
      "Improved validation loss from: -0.04378691613674164  to: -0.04404592514038086\n",
      "Training iteration: 3383\n",
      "Improved validation loss from: -0.04404592514038086  to: -0.044336849451065065\n",
      "Training iteration: 3384\n",
      "Improved validation loss from: -0.044336849451065065  to: -0.04441839754581452\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): -0.04440496563911438\n",
      "Training iteration: 3386\n",
      "Improved validation loss from: -0.04441839754581452  to: -0.04464760720729828\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): -0.04459428787231445\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): -0.04451828002929688\n",
      "Training iteration: 3389\n",
      "Improved validation loss from: -0.04464760720729828  to: -0.04466444551944733\n",
      "Training iteration: 3390\n",
      "Improved validation loss from: -0.04466444551944733  to: -0.044862371683120725\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): -0.04478424489498138\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): -0.04445287585258484\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): -0.04434441030025482\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): -0.04439170360565185\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): -0.044182959198951724\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): -0.04381409287452698\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): -0.04364950656890869\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): -0.04357089996337891\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): -0.043387866020202635\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): -0.043303102254867554\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): -0.043169993162155154\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): -0.0425134003162384\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): -0.04270497262477875\n",
      "Training iteration: 3404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.04277283549308777\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): -0.042273059487342834\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): -0.04200628697872162\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): -0.04171737730503082\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): -0.04135262072086334\n",
      "Training iteration: 3409\n",
      "Validation loss (no improvement): -0.04141513705253601\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): -0.04109293520450592\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): -0.04048900604248047\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): -0.04057573676109314\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): -0.040475568175315856\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): -0.0399702787399292\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): -0.03973502516746521\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): -0.039290043711662295\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): -0.039049732685089114\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): -0.038925015926361085\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): -0.03823158144950867\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): -0.037923043966293334\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): -0.03776689171791077\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): -0.03732596635818482\n",
      "Training iteration: 3423\n",
      "Validation loss (no improvement): -0.03717798590660095\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): -0.03676515221595764\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): -0.036588934063911435\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): -0.03633754849433899\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): -0.03563112616539001\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): -0.035332924127578734\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): -0.03487327396869659\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): -0.03462122678756714\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): -0.03428762853145599\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): -0.033921658992767334\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): -0.03382042050361633\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): -0.033274421095848085\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): -0.033029398322105406\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): -0.03254372477531433\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): -0.0324589729309082\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): -0.03191415667533874\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): -0.03171073496341705\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): -0.031180524826049806\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): -0.03106803596019745\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): -0.03037756085395813\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): -0.030514758825302125\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): -0.029613903164863585\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): -0.03027000427246094\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): -0.028308162093162538\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): -0.030622074007987977\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): -0.02556726336479187\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): -0.03289452493190766\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): -0.01568334102630615\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): -0.03738013803958893\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.028965064883232118\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): -0.0016400540247559548\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.1473500370979309\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.032232779264450076\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): -0.030001696944236756\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.06871898770332337\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): -0.02385498583316803\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.005873942375183105\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): -0.01656998097896576\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.002314027585089207\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.013401851058006287\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): -0.008560793101787567\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.006184716895222664\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.016535285115242004\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): -0.0028009604662656783\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): -0.010023759305477142\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.00417431965470314\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.004588683694601059\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): -0.007073527574539185\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): -0.0039757262915372845\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.011766101419925689\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.017181043326854707\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.0053920019418001175\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): -0.007781751453876495\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): -0.0071123138070106505\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.0020155977457761765\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.0036409389227628707\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): -0.0038889933377504347\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): -0.008661226183176041\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): -0.005069396644830704\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.0004003733396530151\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.00029122172854840757\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): -0.004657994583249092\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): -0.00831514149904251\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): -0.007615305483341217\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): -0.005826598405838013\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): -0.006802986562252045\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): -0.009513089805841446\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): -0.010758216679096221\n",
      "Training iteration: 3491\n",
      "Validation loss (no improvement): -0.010297982394695282\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): -0.010277409851551057\n",
      "Training iteration: 3493\n",
      "Validation loss (no improvement): -0.011304398626089096\n",
      "Training iteration: 3494\n",
      "Validation loss (no improvement): -0.011847813427448273\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): -0.011255047470331191\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): -0.011066315323114395\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): -0.012489473819732666\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): -0.014478498697280883\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): -0.015612475574016571\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): -0.01617366224527359\n",
      "Training iteration: 3501\n",
      "Validation loss (no improvement): -0.016981056332588194\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): -0.01764061748981476\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): -0.017497098445892333\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): -0.017286008596420287\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): -0.018030600249767305\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): -0.019349589943885803\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): -0.020391133427619935\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): -0.021231880784034728\n",
      "Training iteration: 3509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): -0.022137343883514404\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): -0.02262808084487915\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): -0.022575783729553222\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): -0.022906365990638732\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): -0.02404523640871048\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): -0.025224277377128602\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): -0.026010572910308838\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): -0.026704564690589905\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): -0.027178698778152467\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): -0.027166470885276794\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): -0.02728417217731476\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): -0.02810574471950531\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): -0.029113978147506714\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): -0.029703277349472045\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): -0.029887184500694275\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): -0.029760542511940002\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): -0.0296027809381485\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): -0.029997020959854126\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): -0.030954986810684204\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): -0.03183980882167816\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): -0.03226975798606872\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): -0.032234799861907956\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): -0.03204376697540283\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): -0.03228832185268402\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): -0.033065015077590944\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): -0.0338167279958725\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): -0.03408516347408295\n",
      "Training iteration: 3536\n",
      "Validation loss (no improvement): -0.03384538292884827\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): -0.03353636860847473\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): -0.03374102413654327\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): -0.03440594375133514\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): -0.03490971624851227\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): -0.03483489155769348\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): -0.034309443831443784\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): -0.033963745832443236\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): -0.034187689423561096\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): -0.034582167863845825\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): -0.034546712040901185\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): -0.03394518494606018\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): -0.03330212533473968\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): -0.03321635127067566\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): -0.03348456025123596\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): -0.03340718150138855\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): -0.03264693021774292\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): -0.031664738059043886\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): -0.031173700094223024\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): -0.031106361746788026\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): -0.030762463808059692\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): -0.029780790209770203\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): -0.028652435541152953\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): -0.0280762255191803\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): -0.027847951650619505\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): -0.027221646904945374\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): -0.025986596941947937\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): -0.02476874887943268\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): -0.02413966953754425\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): -0.02364092767238617\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): -0.022534194588661193\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): -0.020971469581127167\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): -0.01983688920736313\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): -0.019254159927368165\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): -0.018372096121311188\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): -0.016758282482624055\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): -0.015224888920783997\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): -0.01440070867538452\n",
      "Training iteration: 3574\n",
      "Validation loss (no improvement): -0.013589175045490265\n",
      "Training iteration: 3575\n",
      "Validation loss (no improvement): -0.012052655220031738\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): -0.010342371463775635\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): -0.009288213402032851\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): -0.008394279330968858\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): -0.0068261981010437015\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): -0.005057878419756889\n",
      "Training iteration: 3581\n",
      "Validation loss (no improvement): -0.003868994861841202\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): -0.0027960587292909624\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): -0.0011458491906523704\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.0005924320314079524\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.0018291901797056198\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.0031747173517942427\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.005277693271636963\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.007475626468658447\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.008327615261077882\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.008787450939416885\n"
     ]
    }
   ],
   "source": [
    "swa_model.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.model_inference()\n",
    "dropout_model.model_inference()\n",
    "mixture_model.model_inference()\n",
    "swa_model.model_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline\n",
    "baseline = np.load('Baseline_validation.npy')\n",
    "\n",
    "# Load dropout\n",
    "dropout = np.load('Dropout_validation.npy')\n",
    "dropout_mean = np.mean(dropout,axis=0)\n",
    "dropout_std = np.std(dropout,axis=0)\n",
    "\n",
    "# Load mixture\n",
    "mixture_mean = np.load('mixture_mean_validation.npy')\n",
    "mixture_logvar = np.load('mixture_var_validation.npy')\n",
    "mixture_std = np.sqrt(np.exp(mixture_logvar))\n",
    "\n",
    "# Load swa\n",
    "swa_mean = np.load('SWA_ensembles_mean_multiple.npy')\n",
    "swa_std = np.load('SWA_ensembles_std_multiple.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 19.14300994873047\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 19.14300994873047  to: 14.679765319824218\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 14.679765319824218  to: 11.512687683105469\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 11.512687683105469  to: 9.104600524902343\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 9.104600524902343  to: 7.257030487060547\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 7.257030487060547  to: 5.829792404174805\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 5.829792404174805  to: 4.7219970703125\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 4.7219970703125  to: 3.868897247314453\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 3.868897247314453  to: 3.2013587951660156\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 3.2013587951660156  to: 2.6677997589111326\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 2.6677997589111326  to: 2.2392791748046874\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 2.2392791748046874  to: 1.8935373306274415\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.8935373306274415  to: 1.6132678985595703\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 1.6132678985595703  to: 1.3852022171020508\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 1.3852022171020508  to: 1.1991493225097656\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 1.1991493225097656  to: 1.0463953018188477\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 1.0463953018188477  to: 0.9207537651062012\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.9207537651062012  to: 0.8161540985107422\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.8161540985107422  to: 0.7286032199859619\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.7286032199859619  to: 0.6550589084625245\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.6550589084625245  to: 0.5929640293121338\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.5929640293121338  to: 0.5403290748596191\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.5403290748596191  to: 0.4954944133758545\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.4954944133758545  to: 0.4571669578552246\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.4571669578552246  to: 0.4242719173431396\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.4242719173431396  to: 0.3959171295166016\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.3959171295166016  to: 0.37138183116912843\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.37138183116912843  to: 0.3500772476196289\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.3500772476196289  to: 0.33150744438171387\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.33150744438171387  to: 0.31525726318359376\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.31525726318359376  to: 0.3009856939315796\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.3009856939315796  to: 0.28840758800506594\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.28840758800506594  to: 0.2772838115692139\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.2772838115692139  to: 0.2674129009246826\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.2674129009246826  to: 0.2586242198944092\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.2586242198944092  to: 0.2507741451263428\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.2507741451263428  to: 0.24374036788940429\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.24374036788940429  to: 0.23741860389709474\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.23741860389709474  to: 0.23171966075897216\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.23171966075897216  to: 0.22656693458557128\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.22656693458557128  to: 0.2218949556350708\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.2218949556350708  to: 0.21764674186706542\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.21764674186706542  to: 0.21377308368682862\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.21377308368682862  to: 0.21023135185241698\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.21023135185241698  to: 0.20698463916778564\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.20698463916778564  to: 0.2040005922317505\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.2040005922317505  to: 0.20125107765197753\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.20125107765197753  to: 0.19871139526367188\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.19871139526367188  to: 0.1963598608970642\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.1963598608970642  to: 0.19417743682861327\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.19417743682861327  to: 0.19214727878570556\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.19214727878570556  to: 0.1902545213699341\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.1902545213699341  to: 0.18848602771759032\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.18848602771759032  to: 0.18683007955551148\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.18683007955551148  to: 0.1852763533592224\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.1852763533592224  to: 0.1838156461715698\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.1838156461715698  to: 0.1824396848678589\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1824396848678589  to: 0.18114111423492432\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.18114111423492432  to: 0.1799132704734802\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.1799132704734802  to: 0.17875030040740966\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.17875030040740966  to: 0.17764686346054076\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.17764686346054076  to: 0.1765982389450073\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.1765982389450073  to: 0.17560014724731446\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.17560014724731446  to: 0.1746487259864807\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.1746487259864807  to: 0.1737404465675354\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.1737404465675354  to: 0.1728708863258362\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.1728708863258362  to: 0.17203805446624756\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.17203805446624756  to: 0.1712397813796997\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.1712397813796997  to: 0.17047367095947266\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.17047367095947266  to: 0.16973766088485717\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.16973766088485717  to: 0.16902977228164673\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.16902977228164673  to: 0.16834821701049804\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.16834821701049804  to: 0.16769132614135743\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.16769132614135743  to: 0.16705760955810547\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.16705760955810547  to: 0.16644570827484131\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.16644570827484131  to: 0.16585439443588257\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.16585439443588257  to: 0.1652825117111206\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.1652825117111206  to: 0.16472898721694945\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.16472898721694945  to: 0.16419289112091065\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.16419289112091065  to: 0.16367290019989014\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.16367290019989014  to: 0.16316856145858766\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.16316856145858766  to: 0.16267915964126586\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.16267915964126586  to: 0.1622039794921875\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1622039794921875  to: 0.16174232959747314\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.16174232959747314  to: 0.16129366159439087\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.16129366159439087  to: 0.16085735559463502\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.16085735559463502  to: 0.16043291091918946\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.16043291091918946  to: 0.16001981496810913\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.16001981496810913  to: 0.15961761474609376\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.15961761474609376  to: 0.15922586917877196\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.15922586917877196  to: 0.1588441848754883\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.1588441848754883  to: 0.1584721803665161\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1584721803665161  to: 0.15810948610305786\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.15810948610305786  to: 0.15775574445724488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15775574445724488  to: 0.15741066932678222\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.15741066932678222  to: 0.15707393884658813\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15707393884658813  to: 0.15674530267715453\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.15674530267715453  to: 0.15642346143722535\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.15642346143722535  to: 0.15610896348953246\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.15610896348953246  to: 0.15580174922943116\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.15580174922943116  to: 0.155501651763916\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.155501651763916  to: 0.15520838499069214\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.15520838499069214  to: 0.1549217700958252\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.1549217700958252  to: 0.15464155673980712\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.15464155673980712  to: 0.15436761379241942\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.15436761379241942  to: 0.1540996789932251\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.1540996789932251  to: 0.15383760929107665\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.15383760929107665  to: 0.15358123779296876\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.15358123779296876  to: 0.153330397605896\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.153330397605896  to: 0.15308493375778198\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.15308493375778198  to: 0.1528446912765503\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.1528446912765503  to: 0.15260957479476928\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.15260957479476928  to: 0.15237939357757568\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.15237939357757568  to: 0.15215402841567993\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.15215402841567993  to: 0.15193350315093995\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.15193350315093995  to: 0.1517176866531372\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.1517176866531372  to: 0.1515064001083374\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.1515064001083374  to: 0.1512998342514038\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1512998342514038  to: 0.15109783411026\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.15109783411026  to: 0.15090020895004272\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.15090020895004272  to: 0.15070685148239135\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.15070685148239135  to: 0.15051774978637694\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.15051774978637694  to: 0.1503328800201416\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.1503328800201416  to: 0.15015223026275634\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.15015223026275634  to: 0.14997563362121583\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14997563362121583  to: 0.14980293512344361\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14980293512344361  to: 0.14963396787643432\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14963396787643432  to: 0.14946863651275635\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14946863651275635  to: 0.1493067741394043\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.1493067741394043  to: 0.14914828538894653\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14914828538894653  to: 0.1489930510520935\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.1489930510520935  to: 0.1488410711288452\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.1488410711288452  to: 0.14869225025177002\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14869225025177002  to: 0.14854645729064941\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14854645729064941  to: 0.14840359687805177\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14840359687805177  to: 0.14826358556747438\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14826358556747438  to: 0.14812630414962769\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14812630414962769  to: 0.14799169301986695\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14799169301986695  to: 0.147859787940979\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.147859787940979  to: 0.14773048162460328\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14773048162460328  to: 0.14760382175445558\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14760382175445558  to: 0.14747968912124634\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14747968912124634  to: 0.14735798835754393\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14735798835754393  to: 0.1472386598587036\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.1472386598587036  to: 0.14712172746658325\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14712172746658325  to: 0.14700711965560914\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14700711965560914  to: 0.14689478874206544\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14689478874206544  to: 0.1467846989631653\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.1467846989631653  to: 0.14667675495147706\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.14667675495147706  to: 0.14657087326049806\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14657087326049806  to: 0.14646694660186768\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14646694660186768  to: 0.14636492729187012\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14636492729187012  to: 0.14626474380493165\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14626474380493165  to: 0.14616634845733642\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14616634845733642  to: 0.14606964588165283\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14606964588165283  to: 0.14597461223602295\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14597461223602295  to: 0.145881187915802\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.145881187915802  to: 0.1457893133163452\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.1457893133163452  to: 0.14569896459579468\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14569896459579468  to: 0.14561007022857667\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14561007022857667  to: 0.1455225944519043\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.1455225944519043  to: 0.14543651342391967\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.14543651342391967  to: 0.14535175561904906\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14535175561904906  to: 0.14526805877685547\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14526805877685547  to: 0.14518561363220214\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.14518561363220214  to: 0.14510433673858641\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14510433673858641  to: 0.14502426385879516\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.14502426385879516  to: 0.1449453592300415\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1449453592300415  to: 0.1448676347732544\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1448676347732544  to: 0.1447909951210022\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.1447909951210022  to: 0.14471545219421386\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14471545219421386  to: 0.14464099407196046\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14464099407196046  to: 0.1445675492286682\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.1445675492286682  to: 0.1444952368736267\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.1444952368736267  to: 0.14442402124404907\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14442402124404907  to: 0.14435384273529053\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.14435384273529053  to: 0.14428470134735108\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14428470134735108  to: 0.1442165493965149\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1442165493965149  to: 0.14414933919906617\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14414933919906617  to: 0.14408305883407593\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14408305883407593  to: 0.14401768445968627\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14401768445968627  to: 0.1439531922340393\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.1439531922340393  to: 0.14388953447341918\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14388953447341918  to: 0.14382669925689698\n",
      "Training iteration: 184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14382669925689698  to: 0.14376466274261473\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14376466274261473  to: 0.1437034249305725\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.1437034249305725  to: 0.14364293813705445\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.14364293813705445  to: 0.14358317852020264\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.14358317852020264  to: 0.1435241460800171\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.1435241460800171  to: 0.14346582889556886\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.14346582889556886  to: 0.14340829849243164\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.14340829849243164  to: 0.1433515191078186\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.1433515191078186  to: 0.1432955026626587\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.1432955026626587  to: 0.1432401418685913\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.1432401418685913  to: 0.14318549633026123\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.14318549633026123  to: 0.14313135147094727\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.14313135147094727  to: 0.14307786226272584\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14307786226272584  to: 0.1430249810218811\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.1430249810218811  to: 0.14297268390655518\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.14297268390655518  to: 0.14292099475860595\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.14292099475860595  to: 0.14286983013153076\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.14286983013153076  to: 0.14281923770904542\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14281923770904542  to: 0.14276915788650513\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.14276915788650513  to: 0.1427195906639099\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.1427195906639099  to: 0.14267053604125976\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.14267053604125976  to: 0.14262197017669678\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.14262197017669678  to: 0.14257386922836304\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.14257386922836304  to: 0.14252625703811644\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.14252625703811644  to: 0.1424790620803833\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.1424790620803833  to: 0.14243240356445314\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.14243240356445314  to: 0.14238622188568115\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.14238622188568115  to: 0.14234052896499633\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.14234052896499633  to: 0.14229527711868287\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.14229527711868287  to: 0.1422504663467407\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.1422504663467407  to: 0.142206072807312\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.142206072807312  to: 0.14216210842132568\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.14216210842132568  to: 0.14211853742599487\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.14211853742599487  to: 0.14207534790039061\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.14207534790039061  to: 0.14203253984451295\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.14203253984451295  to: 0.14199007749557496\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.14199007749557496  to: 0.1419479727745056\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.1419479727745056  to: 0.14190620183944702\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.14190620183944702  to: 0.14186477661132812\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.14186477661132812  to: 0.1418236494064331\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1418236494064331  to: 0.14178284406661987\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.14178284406661987  to: 0.14174234867095947\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.14174234867095947  to: 0.14170215129852295\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.14170215129852295  to: 0.14166224002838135\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.14166224002838135  to: 0.14162259101867675\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.14162259101867675  to: 0.14158322811126708\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.14158322811126708  to: 0.14154412746429443\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.14154412746429443  to: 0.14150530099868774\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.14150530099868774  to: 0.14146671295166016\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.14146671295166016  to: 0.14142837524414062\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.14142837524414062  to: 0.1413902759552002\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1413902759552002  to: 0.14135241508483887\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.14135241508483887  to: 0.14131476879119872\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.14131476879119872  to: 0.14127736091613768\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.14127736091613768  to: 0.1412401795387268\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.1412401795387268  to: 0.1412031888961792\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.1412031888961792  to: 0.14116642475128174\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.14116642475128174  to: 0.14112987518310546\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.14112987518310546  to: 0.14109350442886354\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.14109350442886354  to: 0.1410573363304138\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.1410573363304138  to: 0.14102137088775635\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.14102137088775635  to: 0.1409855604171753\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.1409855604171753  to: 0.14094996452331543\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.14094996452331543  to: 0.14091453552246094\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.14091453552246094  to: 0.14087928533554078\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.14087928533554078  to: 0.14084421396255492\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.14084421396255492  to: 0.14080928564071654\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.14080928564071654  to: 0.1407745361328125\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.1407745361328125  to: 0.14073994159698486\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.14073994159698486  to: 0.1407055139541626\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.1407055139541626  to: 0.14067124128341674\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.14067124128341674  to: 0.14063711166381837\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.14063711166381837  to: 0.14060307741165162\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.14060307741165162  to: 0.1405691385269165\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.1405691385269165  to: 0.14053529500961304\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.14053529500961304  to: 0.14050155878067017\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.14050155878067017  to: 0.14046792984008788\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.14046792984008788  to: 0.1404343605041504\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1404343605041504  to: 0.14040093421936034\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.14040093421936034  to: 0.140367591381073\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.140367591381073  to: 0.1403343439102173\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.1403343439102173  to: 0.14030120372772217\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.14030120372772217  to: 0.14026817083358764\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.14026817083358764  to: 0.14023522138595582\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.14023522138595582  to: 0.14020239114761351\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.14020239114761351  to: 0.14016964435577392\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.14016964435577392  to: 0.14013699293136597\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.14013699293136597  to: 0.1401044487953186\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1401044487953186  to: 0.14007198810577393\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.14007198810577393  to: 0.14003961086273192\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.14003961086273192  to: 0.1400073528289795\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.1400073528289795  to: 0.1399751901626587\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.1399751901626587  to: 0.13994309902191163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13994309902191163  to: 0.13991111516952515\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13991111516952515  to: 0.1398792028427124\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1398792028427124  to: 0.1398473858833313\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.1398473858833313  to: 0.13981565237045288\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13981565237045288  to: 0.1397840142250061\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.1397840142250061  to: 0.13975245952606202\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13975245952606202  to: 0.13972097635269165\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13972097635269165  to: 0.1396896004676819\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1396896004676819  to: 0.1396582841873169\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.1396582841873169  to: 0.1396270513534546\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1396270513534546  to: 0.13959590196609498\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13959590196609498  to: 0.1395648241043091\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.1395648241043091  to: 0.1395338296890259\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.1395338296890259  to: 0.13950289487838746\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13950289487838746  to: 0.13947205543518065\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13947205543518065  to: 0.13944127559661865\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13944127559661865  to: 0.13941030502319335\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13941030502319335  to: 0.13937939405441285\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13937939405441285  to: 0.139348566532135\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.139348566532135  to: 0.13931779861450194\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13931779861450194  to: 0.13928711414337158\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13928711414337158  to: 0.139256489276886\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.139256489276886  to: 0.13922592401504516\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13922592401504516  to: 0.13919541835784913\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13919541835784913  to: 0.1391649842262268\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.1391649842262268  to: 0.13913462162017823\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13913462162017823  to: 0.13910433053970336\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13910433053970336  to: 0.13907406330108643\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13907406330108643  to: 0.13904387950897218\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13904387950897218  to: 0.13901375532150267\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13901375532150267  to: 0.13898369073867797\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13898369073867797  to: 0.13895368576049805\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13895368576049805  to: 0.13892372846603393\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13892372846603393  to: 0.1388938307762146\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.1388938307762146  to: 0.13886398077011108\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13886398077011108  to: 0.13883421421051026\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13883421421051026  to: 0.13880445957183837\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13880445957183837  to: 0.13877480030059813\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13877480030059813  to: 0.13874517679214476\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13874517679214476  to: 0.13871567249298095\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13871567249298095  to: 0.13868629932403564\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13868629932403564  to: 0.13865697383880615\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13865697383880615  to: 0.13862770795822144\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13862770795822144  to: 0.13859851360321046\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13859851360321046  to: 0.13856933116912842\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13856933116912842  to: 0.13854020833969116\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13854020833969116  to: 0.13851115703582764\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13851115703582764  to: 0.13848211765289306\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13848211765289306  to: 0.13845313787460328\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13845313787460328  to: 0.13842419385910035\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13842419385910035  to: 0.13839532136917115\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13839532136917115  to: 0.13836647272109986\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13836647272109986  to: 0.13833767175674438\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13833767175674438  to: 0.13830891847610474\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13830891847610474  to: 0.138280189037323\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.138280189037323  to: 0.13825153112411498\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13825153112411498  to: 0.1382228970527649\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.1382228970527649  to: 0.13819432258605957\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13819432258605957  to: 0.13816577196121216\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13816577196121216  to: 0.13813725709915162\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13813725709915162  to: 0.13810880184173585\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13810880184173585  to: 0.138080370426178\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.138080370426178  to: 0.13805198669433594\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13805198669433594  to: 0.13802363872528076\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13802363872528076  to: 0.1379953384399414\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.1379953384399414  to: 0.1379670739173889\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.1379670739173889  to: 0.13793884515762328\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13793884515762328  to: 0.13791064023971558\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13791064023971558  to: 0.13788249492645263\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13788249492645263  to: 0.13785438537597655\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13785438537597655  to: 0.1378263235092163\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.1378263235092163  to: 0.13779828548431397\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13779828548431397  to: 0.13777027130126954\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13777027130126954  to: 0.1377423048019409\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.1377423048019409  to: 0.13771438598632812\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.13771438598632812  to: 0.1376865029335022\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.1376865029335022  to: 0.13765864372253417\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13765864372253417  to: 0.13763082027435303\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13763082027435303  to: 0.13760303258895873\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.13760303258895873  to: 0.13757529258728027\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13757529258728027  to: 0.13754758834838868\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13754758834838868  to: 0.13751989603042603\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13751989603042603  to: 0.1374922513961792\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.1374922513961792  to: 0.13746464252471924\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13746464252471924  to: 0.13743706941604614\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13743706941604614  to: 0.13740952014923097\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13740952014923097  to: 0.13738203048706055\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13738203048706055  to: 0.1373545289039612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 365\n",
      "Improved validation loss from: 0.1373545289039612  to: 0.13732709884643554\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13732709884643554  to: 0.13729968070983886\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13729968070983886  to: 0.13727231025695802\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.13727231025695802  to: 0.13724497556686402\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.13724497556686402  to: 0.13721765279769899\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.13721765279769899  to: 0.1371903657913208\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1371903657913208  to: 0.1371631383895874\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.1371631383895874  to: 0.137135910987854\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.137135910987854  to: 0.13710873126983641\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13710873126983641  to: 0.13708157539367677\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.13708157539367677  to: 0.13705445528030397\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13705445528030397  to: 0.137027370929718\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.137027370929718  to: 0.13700029850006104\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.13700029850006104  to: 0.13697328567504882\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.13697328567504882  to: 0.13694628477096557\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13694628477096557  to: 0.13691933155059816\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.13691933155059816  to: 0.13689239025115968\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.13689239025115968  to: 0.13686552047729492\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.13686552047729492  to: 0.13683865070343018\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13683865070343018  to: 0.1368118405342102\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.1368118405342102  to: 0.13678500652313233\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13678500652313233  to: 0.13675819635391234\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13675819635391234  to: 0.1367314100265503\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.1367314100265503  to: 0.1367046594619751\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1367046594619751  to: 0.13667794466018676\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.13667794466018676  to: 0.13665125370025635\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.13665125370025635  to: 0.1366245985031128\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.1366245985031128  to: 0.1365979790687561\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.1365979790687561  to: 0.13657138347625733\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13657138347625733  to: 0.1365448236465454\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.1365448236465454  to: 0.1365182876586914\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.1365182876586914  to: 0.13649178743362428\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.13649178743362428  to: 0.13646531105041504\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.13646531105041504  to: 0.13643888235092164\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.13643888235092164  to: 0.13641246557235717\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.13641246557235717  to: 0.13638608455657958\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13638608455657958  to: 0.13635975122451782\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13635975122451782  to: 0.136333429813385\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.136333429813385  to: 0.13630714416503906\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.13630714416503906  to: 0.13628089427947998\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13628089427947998  to: 0.13625465631484984\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13625465631484984  to: 0.13622846603393554\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13622846603393554  to: 0.13620229959487914\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13620229959487914  to: 0.13617618083953859\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.13617618083953859  to: 0.13615007400512696\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13615007400512696  to: 0.1361240029335022\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.1361240029335022  to: 0.13609795570373534\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.13609795570373534  to: 0.13607195615768433\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13607195615768433  to: 0.1360459804534912\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1360459804534912  to: 0.13602006435394287\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.13602006435394287  to: 0.13599421977996826\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13599421977996826  to: 0.1359684109687805\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.1359684109687805  to: 0.13594262599945067\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.13594262599945067  to: 0.1359168767929077\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.1359168767929077  to: 0.1358911633491516\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.1358911633491516  to: 0.1358654737472534\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.1358654737472534  to: 0.1358398199081421\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1358398199081421  to: 0.13581421375274658\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.13581421375274658  to: 0.1357886552810669\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.1357886552810669  to: 0.13576314449310303\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13576314449310303  to: 0.1357376456260681\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.1357376456260681  to: 0.13571221828460694\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.13571221828460694  to: 0.1356868028640747\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1356868028640747  to: 0.13566144704818725\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.13566144704818725  to: 0.1356361150741577\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1356361150741577  to: 0.135610830783844\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.135610830783844  to: 0.13558558225631714\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.13558558225631714  to: 0.13556036949157715\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.13556036949157715  to: 0.13553518056869507\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.13553518056869507  to: 0.13551005125045776\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13551005125045776  to: 0.13548494577407838\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13548494577407838  to: 0.13545987606048585\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13545987606048585  to: 0.13543484210968018\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13543484210968018  to: 0.13540985584259033\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13540985584259033  to: 0.13538492918014527\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.13538492918014527  to: 0.135360050201416\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.135360050201416  to: 0.13533519506454467\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.13533519506454467  to: 0.1353103756904602\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.1353103756904602  to: 0.1352855920791626\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.1352855920791626  to: 0.13526084423065185\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.13526084423065185  to: 0.13523613214492797\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13523613214492797  to: 0.13521144390106202\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13521144390106202  to: 0.1351867914199829\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1351867914199829  to: 0.13516217470169067\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13516217470169067  to: 0.13513758182525634\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13513758182525634  to: 0.13511304855346679\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.13511304855346679  to: 0.13508851528167726\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13508851528167726  to: 0.13506405353546141\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.13506405353546141  to: 0.1350395917892456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 454\n",
      "Improved validation loss from: 0.1350395917892456  to: 0.1350151777267456\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.1350151777267456  to: 0.13499081134796143\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13499081134796143  to: 0.13496644496917726\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13496644496917726  to: 0.1349421262741089\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.1349421262741089  to: 0.13491784334182738\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.13491784334182738  to: 0.1348935842514038\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1348935842514038  to: 0.134869384765625\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.134869384765625  to: 0.13484519720077515\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.13484519720077515  to: 0.1348210573196411\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1348210573196411  to: 0.13479692935943605\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13479692935943605  to: 0.13477286100387573\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13477286100387573  to: 0.13474880456924437\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13474880456924437  to: 0.13472479581832886\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13472479581832886  to: 0.13470082283020018\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.13470082283020018  to: 0.13467687368392944\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.13467687368392944  to: 0.13465296030044555\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.13465296030044555  to: 0.13462908267974855\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13462908267974855  to: 0.1346052408218384\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.1346052408218384  to: 0.13458149433135985\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.13458149433135985  to: 0.13455779552459718\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.13455779552459718  to: 0.13453412055969238\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.13453412055969238  to: 0.13451049327850342\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13451049327850342  to: 0.13448688983917237\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.13448688983917237  to: 0.13446333408355712\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.13446333408355712  to: 0.13443979024887084\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13443979024887084  to: 0.1344162940979004\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.1344162940979004  to: 0.1343928337097168\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.1343928337097168  to: 0.13436940908432007\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.13436940908432007  to: 0.13434602022171022\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.13434602022171022  to: 0.1343226671218872\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.1343226671218872  to: 0.13429932594299315\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13429932594299315  to: 0.1342760443687439\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.1342760443687439  to: 0.1342527985572815\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.1342527985572815  to: 0.13422958850860595\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13422958850860595  to: 0.13420639038085938\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.13420639038085938  to: 0.13418325185775756\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.13418325185775756  to: 0.13416014909744262\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.13416014909744262  to: 0.1341370701789856\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.1341370701789856  to: 0.13411402702331543\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.13411402702331543  to: 0.13409101963043213\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13409101963043213  to: 0.13406803607940673\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13406803607940673  to: 0.1340451121330261\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.1340451121330261  to: 0.13402221202850342\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.13402221202850342  to: 0.13399937152862548\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.13399937152862548  to: 0.1339766263961792\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.1339766263961792  to: 0.13395402431488038\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.13395402431488038  to: 0.1339314818382263\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1339314818382263  to: 0.13390896320343018\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13390896320343018  to: 0.1338864803314209\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.1338864803314209  to: 0.13386402130126954\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.13386402130126954  to: 0.133841609954834\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.133841609954834  to: 0.1338192343711853\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.1338192343711853  to: 0.13379690647125245\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.13379690647125245  to: 0.13377459049224855\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.13377459049224855  to: 0.13375232219696045\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.13375232219696045  to: 0.13373010158538817\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.13373010158538817  to: 0.13370790481567382\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.13370790481567382  to: 0.13368575572967528\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.13368575572967528  to: 0.13366372585296632\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.13366372585296632  to: 0.13364183902740479\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.13364183902740479  to: 0.1336200475692749\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.1336200475692749  to: 0.13359830379486085\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.13359830379486085  to: 0.13357667922973632\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.13357667922973632  to: 0.1335551381111145\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.1335551381111145  to: 0.13353369235992432\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.13353369235992432  to: 0.13351231813430786\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.13351231813430786  to: 0.13349103927612305\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.13349103927612305  to: 0.13346980810165404\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13346980810165404  to: 0.13344866037368774\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.13344866037368774  to: 0.13342759609222413\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.13342759609222413  to: 0.13340657949447632\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.13340657949447632  to: 0.13338563442230225\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.13338563442230225  to: 0.13336474895477296\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.13336474895477296  to: 0.13334392309188842\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.13334392309188842  to: 0.13332313299179077\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.13332313299179077  to: 0.1333024263381958\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.1333024263381958  to: 0.13328173160552978\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.13328173160552978  to: 0.13326109647750856\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13326109647750856  to: 0.13324050903320311\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.13324050903320311  to: 0.13321996927261354\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13321996927261354  to: 0.13319945335388184\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.13319945335388184  to: 0.13317893743515014\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.13317893743515014  to: 0.1331583857536316\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.1331583857536316  to: 0.13313788175582886\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.13313788175582886  to: 0.13311740159988403\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.13311740159988403  to: 0.13309695720672607\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.13309695720672607  to: 0.13307654857635498\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13307654857635498  to: 0.13305617570877076\n",
      "Training iteration: 542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13305617570877076  to: 0.13303601741790771\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13303601741790771  to: 0.13301610946655273\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.13301610946655273  to: 0.13299635648727418\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.13299635648727418  to: 0.13297681808471679\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13297681808471679  to: 0.13295742273330688\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.13295742273330688  to: 0.13293817043304443\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.13293817043304443  to: 0.13291906118392943\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.13291906118392943  to: 0.13290008306503295\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.13290008306503295  to: 0.13288121223449706\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13288121223449706  to: 0.13286247253417968\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.13286247253417968  to: 0.13284380435943605\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13284380435943605  to: 0.13282524347305297\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.13282524347305297  to: 0.13280675411224366\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.13280675411224366  to: 0.132788348197937\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.132788348197937  to: 0.13277002573013305\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.13277002573013305  to: 0.1327517509460449\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.1327517509460449  to: 0.13273355960845948\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.13273355960845948  to: 0.1327154278755188\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.1327154278755188  to: 0.13269731998443604\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.13269731998443604  to: 0.13267929553985597\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13267929553985597  to: 0.13266130685806274\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.13266130685806274  to: 0.13264336585998535\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.13264336585998535  to: 0.1326254963874817\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.1326254963874817  to: 0.13260767459869385\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.13260767459869385  to: 0.13258990049362182\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.13258990049362182  to: 0.13257217407226562\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.13257217407226562  to: 0.1325545072555542\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1325545072555542  to: 0.1325368642807007\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.1325368642807007  to: 0.13251926898956298\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13251926898956298  to: 0.1325016975402832\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.1325016975402832  to: 0.13248417377471924\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.13248417377471924  to: 0.13246667385101318\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.13246667385101318  to: 0.132449209690094\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.132449209690094  to: 0.13243179321289061\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.13243179321289061  to: 0.13241440057754517\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.13241440057754517  to: 0.13239701986312866\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.13239701986312866  to: 0.13237968683242798\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.13237968683242798  to: 0.13236238956451415\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.13236238956451415  to: 0.1323450803756714\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.1323450803756714  to: 0.13232784271240233\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.13232784271240233  to: 0.1323106050491333\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.1323106050491333  to: 0.13229339122772216\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.13229339122772216  to: 0.1322762131690979\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1322762131690979  to: 0.1322590470314026\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.1322590470314026  to: 0.13224190473556519\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13224190473556519  to: 0.13222478628158568\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.13222478628158568  to: 0.13220771551132202\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.13220771551132202  to: 0.13219064474105835\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.13219064474105835  to: 0.13217360973358155\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.13217360973358155  to: 0.13215659856796264\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.13215659856796264  to: 0.13213961124420165\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.13213961124420165  to: 0.13212263584136963\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.13212263584136963  to: 0.13210570812225342\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.13210570812225342  to: 0.1320888042449951\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.1320888042449951  to: 0.13207191228866577\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.13207191228866577  to: 0.1320550560951233\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.1320550560951233  to: 0.13203821182250977\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.13203821182250977  to: 0.1320214033126831\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.1320214033126831  to: 0.13200459480285645\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.13200459480285645  to: 0.13198782205581666\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.13198782205581666  to: 0.13197107315063478\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.13197107315063478  to: 0.13195433616638183\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.13195433616638183  to: 0.13193763494491578\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.13193763494491578  to: 0.1319209575653076\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.1319209575653076  to: 0.1319042921066284\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.1319042921066284  to: 0.13188765048980713\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.13188765048980713  to: 0.1318710446357727\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.1318710446357727  to: 0.13185443878173828\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.13185443878173828  to: 0.13183786869049072\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.13183786869049072  to: 0.13182131052017212\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.13182131052017212  to: 0.13180477619171144\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.13180477619171144  to: 0.13178826570510865\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.13178826570510865  to: 0.13177177906036378\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.13177177906036378  to: 0.1317553162574768\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1317553162574768  to: 0.13173887729644776\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.13173887729644776  to: 0.13172245025634766\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.13172245025634766  to: 0.1317060708999634\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.1317060708999634  to: 0.1316896915435791\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.1316896915435791  to: 0.13167333602905273\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.13167333602905273  to: 0.13165700435638428\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.13165700435638428  to: 0.1316407084465027\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.1316407084465027  to: 0.13162446022033691\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.13162446022033691  to: 0.13160823583602904\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.13160823583602904  to: 0.13159207105636597\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.13159207105636597  to: 0.13157594203948975\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.13157594203948975  to: 0.13155983686447142\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.13155983686447142  to: 0.13154375553131104\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.13154375553131104  to: 0.1315277099609375\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1315277099609375  to: 0.13151171207427978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 631\n",
      "Improved validation loss from: 0.13151171207427978  to: 0.1314957857131958\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.1314957857131958  to: 0.13147988319396972\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.13147988319396972  to: 0.13146404027938843\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.13146404027938843  to: 0.13144824504852295\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.13144824504852295  to: 0.13143249750137329\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.13143249750137329  to: 0.13141677379608155\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.13141677379608155  to: 0.13140110969543456\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.13140110969543456  to: 0.13138545751571656\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.13138545751571656  to: 0.13136986494064332\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.13136986494064332  to: 0.13135430812835694\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.13135430812835694  to: 0.1313387632369995\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.1313387632369995  to: 0.1313232660293579\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1313232660293579  to: 0.13130779266357423\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.13130779266357423  to: 0.13129236698150634\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.13129236698150634  to: 0.13127695322036742\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.13127695322036742  to: 0.13126156330108643\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.13126156330108643  to: 0.1312462329864502\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.1312462329864502  to: 0.13123090267181398\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.13123090267181398  to: 0.13121562004089354\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.13121562004089354  to: 0.1312003493309021\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.1312003493309021  to: 0.1311851143836975\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.1311851143836975  to: 0.13116991519927979\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.13116991519927979  to: 0.13115473985671997\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.13115473985671997  to: 0.13113958835601808\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.13113958835601808  to: 0.13112446069717407\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.13112446069717407  to: 0.13110936880111695\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.13110936880111695  to: 0.13109428882598878\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.13109428882598878  to: 0.1310792326927185\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.1310792326927185  to: 0.13106420040130615\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.13106420040130615  to: 0.13104921579360962\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.13104921579360962  to: 0.13103424310684203\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.13103424310684203  to: 0.13101930618286134\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.13101930618286134  to: 0.13100436925888062\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.13100436925888062  to: 0.13098949193954468\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.13098949193954468  to: 0.13097461462020873\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.13097461462020873  to: 0.13095977306365966\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.13095977306365966  to: 0.1309449553489685\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.1309449553489685  to: 0.1309301733970642\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1309301733970642  to: 0.1309153914451599\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.1309153914451599  to: 0.13090064525604247\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.13090064525604247  to: 0.13088593482971192\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.13088593482971192  to: 0.1308712363243103\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1308712363243103  to: 0.13085657358169556\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.13085657358169556  to: 0.13084189891815184\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.13084189891815184  to: 0.13082720041275026\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.13082720041275026  to: 0.13081252574920654\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.13081252574920654  to: 0.13079789876937867\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.13079789876937867  to: 0.1307832956314087\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.1307832956314087  to: 0.1307689905166626\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.1307689905166626  to: 0.13075497150421142\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.13075497150421142  to: 0.13074119091033937\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.13074119091033937  to: 0.1307276487350464\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.1307276487350464  to: 0.1307143449783325\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.1307143449783325  to: 0.13070119619369508\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.13070119619369508  to: 0.13068825006484985\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.13068825006484985  to: 0.13067516088485717\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.13067516088485717  to: 0.130661940574646\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.130661940574646  to: 0.13064863681793212\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.13064863681793212  to: 0.13063520193099976\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.13063520193099976  to: 0.1306217074394226\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.1306217074394226  to: 0.13060814142227173\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.13060814142227173  to: 0.1305944800376892\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.1305944800376892  to: 0.13058078289031982\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.13058078289031982  to: 0.1305670142173767\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.1305670142173767  to: 0.1305535078048706\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1305535078048706  to: 0.1305402159690857\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.1305402159690857  to: 0.13052713871002197\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.13052713871002197  to: 0.1305142641067505\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.1305142641067505  to: 0.13050154447555543\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.13050154447555543  to: 0.1304887056350708\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.1304887056350708  to: 0.13047571182250978\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.13047571182250978  to: 0.1304626226425171\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.1304626226425171  to: 0.13044941425323486\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.13044941425323486  to: 0.13043612241744995\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.13043612241744995  to: 0.1304227590560913\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.1304227590560913  to: 0.13040931224823\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.13040931224823  to: 0.13039610385894776\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.13039610385894776  to: 0.13038312196731566\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.13038312196731566  to: 0.1303701639175415\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.1303701639175415  to: 0.13035738468170166\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.13035738468170166  to: 0.13034477233886718\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.13034477233886718  to: 0.1303320050239563\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.1303320050239563  to: 0.13031909465789795\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.13031909465789795  to: 0.13030605316162108\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.13030605316162108  to: 0.1302928924560547\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.1302928924560547  to: 0.13027962446212768\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.13027962446212768  to: 0.13026659488677977\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.13026659488677977  to: 0.13025373220443726\n",
      "Training iteration: 719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13025373220443726  to: 0.130241060256958\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.130241060256958  to: 0.13022855520248414\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.13022855520248414  to: 0.13021588325500488\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.13021588325500488  to: 0.1302030563354492\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.1302030563354492  to: 0.1301900863647461\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.1301900863647461  to: 0.13017733097076417\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.13017733097076417  to: 0.13016473054885863\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.13016473054885863  to: 0.13015228509902954\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.13015228509902954  to: 0.13013968467712403\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.13013968467712403  to: 0.1301269292831421\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.1301269292831421  to: 0.1301140546798706\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.1301140546798706  to: 0.13010135889053345\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.13010135889053345  to: 0.13008884191513062\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.13008884191513062  to: 0.1300764799118042\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.1300764799118042  to: 0.13006396293640138\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.13006396293640138  to: 0.13005127906799316\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.13005127906799316  to: 0.13003847599029542\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.13003847599029542  to: 0.13002582788467407\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.13002582788467407  to: 0.13001338243484498\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.13001338243484498  to: 0.13000109195709228\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.13000109195709228  to: 0.1299889326095581\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.1299889326095581  to: 0.12997660636901856\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12997660636901856  to: 0.12996408939361573\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12996408939361573  to: 0.1299514055252075\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.1299514055252075  to: 0.1299385905265808\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.1299385905265808  to: 0.12992597818374635\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12992597818374635  to: 0.12991355657577514\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12991355657577514  to: 0.12990131378173828\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12990131378173828  to: 0.12988921403884887\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12988921403884887  to: 0.12987724542617798\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12987724542617798  to: 0.12986507415771484\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12986507415771484  to: 0.12985308170318605\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12985308170318605  to: 0.12984127998352052\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12984127998352052  to: 0.12982962131500245\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12982962131500245  to: 0.12981810569763183\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12981810569763183  to: 0.12980669736862183\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12980669736862183  to: 0.12979543209075928\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12979543209075928  to: 0.12978423833847047\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12978423833847047  to: 0.12977312803268432\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12977312803268432  to: 0.12976211309432983\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12976211309432983  to: 0.12975116968154907\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12975116968154907  to: 0.12974028587341307\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12974028587341307  to: 0.12972947359085082\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12972947359085082  to: 0.12971875667572022\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12971875667572022  to: 0.12970808744430543\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12970808744430543  to: 0.1296975016593933\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.1296975016593933  to: 0.12968696355819703\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12968696355819703  to: 0.1296764850616455\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.1296764850616455  to: 0.12966606616973878\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12966606616973878  to: 0.12965564727783202\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12965564727783202  to: 0.12964528799057007\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12964528799057007  to: 0.12963492870330812\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12963492870330812  to: 0.1296246290206909\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.1296246290206909  to: 0.12961432933807374\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12961432933807374  to: 0.12960402965545653\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12960402965545653  to: 0.12959372997283936\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12959372997283936  to: 0.129583477973938\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.129583477973938  to: 0.12957319021224975\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12957319021224975  to: 0.12956295013427735\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12956295013427735  to: 0.1295527458190918\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.1295527458190918  to: 0.1295425295829773\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1295425295829773  to: 0.12953230142593383\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.12953230142593383  to: 0.12952207326889037\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12952207326889037  to: 0.12951185703277587\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12951185703277587  to: 0.1295016050338745\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.1295016050338745  to: 0.12949135303497314\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12949135303497314  to: 0.12948108911514283\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12948108911514283  to: 0.1294708251953125\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.1294708251953125  to: 0.12946053743362426\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12946053743362426  to: 0.12945024967193602\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12945024967193602  to: 0.12943994998931885\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12943994998931885  to: 0.1294296145439148\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.1294296145439148  to: 0.1294192671775818\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.1294192671775818  to: 0.12940893173217774\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12940893173217774  to: 0.12939856052398682\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12939856052398682  to: 0.1293881893157959\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.1293881893157959  to: 0.12937779426574708\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12937779426574708  to: 0.1293673872947693\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.1293673872947693  to: 0.1293569564819336\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.1293569564819336  to: 0.1293465256690979\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.1293465256690979  to: 0.12933601140975953\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12933601140975953  to: 0.12932533025741577\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.12932533025741577  to: 0.12931463718414307\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12931463718414307  to: 0.1293039321899414\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.1293039321899414  to: 0.12929325103759765\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12929325103759765  to: 0.1292825698852539\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.1292825698852539  to: 0.12927188873291015\n",
      "Training iteration: 806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12927188873291015  to: 0.12926123142242432\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12926123142242432  to: 0.12925056219100953\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12925056219100953  to: 0.12923989295959473\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.12923989295959473  to: 0.12922924757003784\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.12922924757003784  to: 0.12921861410140992\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.12921861410140992  to: 0.12920799255371093\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12920799255371093  to: 0.12919737100601197\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12919737100601197  to: 0.12918676137924195\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12918676137924195  to: 0.12917616367340087\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12917616367340087  to: 0.12916555404663085\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.12916555404663085  to: 0.12915494441986083\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.12915494441986083  to: 0.12914432287216188\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12914432287216188  to: 0.1291337251663208\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.1291337251663208  to: 0.12912310361862184\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12912310361862184  to: 0.12911248207092285\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12911248207092285  to: 0.12910192012786864\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.12910192012786864  to: 0.12909139394760133\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.12909139394760133  to: 0.12908087968826293\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.12908087968826293  to: 0.12907032966613768\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12907032966613768  to: 0.12905977964401244\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12905977964401244  to: 0.1290492296218872\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.1290492296218872  to: 0.12903865575790405\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12903865575790405  to: 0.12902809381484986\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12902809381484986  to: 0.12901750802993775\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.12901750802993775  to: 0.1290069341659546\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.1290069341659546  to: 0.12899634838104249\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.12899634838104249  to: 0.1289857506752014\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.1289857506752014  to: 0.12897514104843139\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.12897514104843139  to: 0.12896453142166137\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.12896453142166137  to: 0.1289539098739624\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.1289539098739624  to: 0.12894328832626342\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12894328832626342  to: 0.12893266677856446\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12893266677856446  to: 0.12892202138900757\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.12892202138900757  to: 0.12891137599945068\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.12891137599945068  to: 0.1289007067680359\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1289007067680359  to: 0.1288900375366211\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.1288900375366211  to: 0.1288793444633484\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.1288793444633484  to: 0.12886866331100463\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12886866331100463  to: 0.12885797023773193\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12885797023773193  to: 0.12884725332260133\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.12884725332260133  to: 0.1288365602493286\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.1288365602493286  to: 0.12882583141326903\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12882583141326903  to: 0.12881510257720946\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12881510257720946  to: 0.1288043737411499\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.1288043737411499  to: 0.12879364490509032\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12879364490509032  to: 0.12878289222717285\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.12878289222717285  to: 0.12877211570739747\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.12877211570739747  to: 0.12876136302948\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.12876136302948  to: 0.12875063419342042\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.12875063419342042  to: 0.12873989343643188\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.12873989343643188  to: 0.12872915267944335\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12872915267944335  to: 0.12871841192245484\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12871841192245484  to: 0.12870768308639527\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.12870768308639527  to: 0.1286969542503357\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.1286969542503357  to: 0.12868621349334716\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12868621349334716  to: 0.1286754846572876\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.1286754846572876  to: 0.12866472005844115\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.12866472005844115  to: 0.12865397930145264\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12865397930145264  to: 0.1286432147026062\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.1286432147026062  to: 0.12863243818283082\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.12863243818283082  to: 0.12862167358398438\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.12862167358398438  to: 0.12861087322235107\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.12861087322235107  to: 0.12860008478164672\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12860008478164672  to: 0.12858927249908447\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.12858927249908447  to: 0.12857846021652222\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.12857846021652222  to: 0.12856767177581788\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.12856767177581788  to: 0.12855687141418456\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12855687141418456  to: 0.12854607105255128\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.12854607105255128  to: 0.12853527069091797\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12853527069091797  to: 0.12852447032928466\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12852447032928466  to: 0.1285136580467224\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.1285136580467224  to: 0.12850284576416016\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12850284576416016  to: 0.12849204540252684\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12849204540252684  to: 0.1284812331199646\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.1284812331199646  to: 0.12847045660018921\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.12847045660018921  to: 0.1284596800804138\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.1284596800804138  to: 0.12844889163970946\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.12844889163970946  to: 0.12843810319900512\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12843810319900512  to: 0.12842731475830077\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12842731475830077  to: 0.12841651439666749\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.12841651439666749  to: 0.12840576171875\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12840576171875  to: 0.12839504480361938\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12839504480361938  to: 0.12838422060012816\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12838422060012816  to: 0.12837331295013427\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.12837331295013427  to: 0.12836236953735353\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12836236953735353  to: 0.1283514380455017\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.1283514380455017  to: 0.128340482711792\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.128340482711792  to: 0.1283295273780823\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.1283295273780823  to: 0.12831853628158568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 895\n",
      "Improved validation loss from: 0.12831853628158568  to: 0.1283075451850891\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.1283075451850891  to: 0.12829654216766356\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12829654216766356  to: 0.1282854914665222\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.1282854914665222  to: 0.12827446460723876\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.12827446460723876  to: 0.12826340198516845\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12826340198516845  to: 0.12825231552124022\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.12825231552124022  to: 0.12824121713638306\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.12824121713638306  to: 0.12823010683059693\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.12823010683059693  to: 0.12821900844573975\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12821900844573975  to: 0.12820794582366943\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12820794582366943  to: 0.12819689512252808\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12819689512252808  to: 0.12818584442138672\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.12818584442138672  to: 0.1281748056411743\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.1281748056411743  to: 0.12816377878189086\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12816377878189086  to: 0.12815275192260742\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.12815275192260742  to: 0.12814171314239503\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12814171314239503  to: 0.12813066244125365\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12813066244125365  to: 0.12811962366104127\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.12811962366104127  to: 0.1281085729598999\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.1281085729598999  to: 0.1280975341796875\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.1280975341796875  to: 0.1280864953994751\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.1280864953994751  to: 0.1280754327774048\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.1280754327774048  to: 0.1280643582344055\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.1280643582344055  to: 0.1280532717704773\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.1280532717704773  to: 0.12804217338562013\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12804217338562013  to: 0.12803103923797607\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12803103923797607  to: 0.12801990509033204\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.12801990509033204  to: 0.12800874710083007\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.12800874710083007  to: 0.12799755334854127\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12799755334854127  to: 0.12798635959625243\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.12798635959625243  to: 0.12797513008117675\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.12797513008117675  to: 0.12796387672424317\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12796387672424317  to: 0.12795263528823853\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12795263528823853  to: 0.12794139385223388\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.12794139385223388  to: 0.12793012857437133\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.12793012857437133  to: 0.12791883945465088\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.12791883945465088  to: 0.1279075026512146\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.1279075026512146  to: 0.12789615392684936\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12789615392684936  to: 0.12788479328155516\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12788479328155516  to: 0.12787340879440307\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12787340879440307  to: 0.12786203622817993\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12786203622817993  to: 0.12785065174102783\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.12785065174102783  to: 0.12783925533294677\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12783925533294677  to: 0.12782785892486573\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.12782785892486573  to: 0.12781646251678466\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.12781646251678466  to: 0.12780503034591675\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12780503034591675  to: 0.12779359817504882\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12779359817504882  to: 0.127782142162323\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.127782142162323  to: 0.12777068614959716\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12777068614959716  to: 0.12775920629501342\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.12775920629501342  to: 0.12774771451950073\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12774771451950073  to: 0.12773621082305908\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12773621082305908  to: 0.12772468328475953\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12772468328475953  to: 0.12771313190460204\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.12771313190460204  to: 0.12770158052444458\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.12770158052444458  to: 0.12768999338150025\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.12768999338150025  to: 0.127678382396698\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.127678382396698  to: 0.12766673564910888\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.12766673564910888  to: 0.12765510082244874\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12765510082244874  to: 0.12764341831207277\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12764341831207277  to: 0.1276317596435547\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.1276317596435547  to: 0.12762006521224975\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.12762006521224975  to: 0.12760835886001587\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12760835886001587  to: 0.12759664058685302\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12759664058685302  to: 0.12758489847183227\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12758489847183227  to: 0.1275731325149536\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.1275731325149536  to: 0.127561354637146\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.127561354637146  to: 0.12754955291748046\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.12754955291748046  to: 0.12753775119781494\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.12753775119781494  to: 0.12752593755722047\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12752593755722047  to: 0.12751410007476807\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12751410007476807  to: 0.12750227451324464\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12750227451324464  to: 0.12749041318893434\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.12749041318893434  to: 0.12747855186462403\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12747855186462403  to: 0.1274666666984558\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.1274666666984558  to: 0.1274547576904297\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.1274547576904297  to: 0.12744284868240358\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.12744284868240358  to: 0.12743090391159057\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12743090391159057  to: 0.12741895914077758\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12741895914077758  to: 0.1274070143699646\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.1274070143699646  to: 0.12739503383636475\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12739503383636475  to: 0.12738306522369386\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12738306522369386  to: 0.12737109661102294\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12737109661102294  to: 0.12735908031463622\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.12735908031463622  to: 0.12734706401824952\n",
      "Training iteration: 980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12734706401824952  to: 0.12733503580093383\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12733503580093383  to: 0.1273229956626892\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.1273229956626892  to: 0.12731093168258667\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12731093168258667  to: 0.12729886770248414\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12729886770248414  to: 0.12728676795959473\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12728676795959473  to: 0.12727466821670533\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.12727466821670533  to: 0.12726253271102905\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.12726253271102905  to: 0.12725039720535278\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12725039720535278  to: 0.12723822593688966\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12723822593688966  to: 0.12722604274749755\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.12722604274749755  to: 0.12721383571624756\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12721383571624756  to: 0.1272016167640686\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.1272016167640686  to: 0.12718936204910278\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12718936204910278  to: 0.1271770715713501\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.1271770715713501  to: 0.12716476917266845\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12716476917266845  to: 0.12715245485305787\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.12715245485305787  to: 0.12714011669158937\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12714011669158937  to: 0.12712773084640502\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12712773084640502  to: 0.12711533308029174\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12711533308029174  to: 0.12710292339324952\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.12710292339324952  to: 0.1270904779434204\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.1270904779434204  to: 0.1270777702331543\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.1270777702331543  to: 0.12706485986709595\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12706485986709595  to: 0.12705191373825073\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12705191373825073  to: 0.12703893184661866\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12703893184661866  to: 0.12702590227127075\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.12702590227127075  to: 0.12701284885406494\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12701284885406494  to: 0.12699975967407226\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12699975967407226  to: 0.12698665857315064\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12698665857315064  to: 0.126973557472229\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.126973557472229  to: 0.12696043252944947\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.12696043252944947  to: 0.126947283744812\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.126947283744812  to: 0.1269336462020874\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.1269336462020874  to: 0.12691972255706788\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.12691972255706788  to: 0.12690577507019044\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12690577507019044  to: 0.12689179182052612\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12689179182052612  to: 0.1268778085708618\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.1268778085708618  to: 0.1268638253211975\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.1268638253211975  to: 0.12684984207153321\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.12684984207153321  to: 0.12683582305908203\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12683582305908203  to: 0.1268217921257019\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.1268217921257019  to: 0.12680771350860595\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.12680771350860595  to: 0.1267936110496521\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.1267936110496521  to: 0.1267794370651245\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.1267794370651245  to: 0.12676527500152587\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.12676527500152587  to: 0.1267510771751404\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.1267510771751404  to: 0.12673686742782592\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.12673686742782592  to: 0.12672263383865356\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.12672263383865356  to: 0.12670837640762328\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.12670837640762328  to: 0.12669408321380615\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.12669408321380615  to: 0.12667977809906006\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.12667977809906006  to: 0.1266654372215271\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.1266654372215271  to: 0.1266510605812073\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.1266510605812073  to: 0.12663664817810058\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.12663664817810058  to: 0.12662218809127807\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12662218809127807  to: 0.1266077160835266\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1266077160835266  to: 0.1265932321548462\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.1265932321548462  to: 0.1265787124633789\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.1265787124633789  to: 0.12656415700912477\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12656415700912477  to: 0.12654964923858641\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.12654964923858641  to: 0.12653511762619019\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.12653511762619019  to: 0.12652060985565186\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12652060985565186  to: 0.12650610208511354\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.12650610208511354  to: 0.12649157047271728\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.12649157047271728  to: 0.12647703886032105\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.12647703886032105  to: 0.12646249532699586\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12646249532699586  to: 0.1264479637145996\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.1264479637145996  to: 0.12643343210220337\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.12643343210220337  to: 0.12641888856887817\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12641888856887817  to: 0.12640432119369507\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12640432119369507  to: 0.126389741897583\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.126389741897583  to: 0.12637513875961304\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.12637513875961304  to: 0.12636054754257203\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.12636054754257203  to: 0.12634594440460206\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.12634594440460206  to: 0.12633131742477416\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.12633131742477416  to: 0.12631669044494628\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.12631669044494628  to: 0.1263020873069763\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.1263020873069763  to: 0.12628743648529053\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12628743648529053  to: 0.1262727737426758\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.1262727737426758  to: 0.12625809907913207\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12625809907913207  to: 0.12624340057373046\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.12624340057373046  to: 0.12622870206832887\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12622870206832887  to: 0.12621400356292725\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.12621400356292725  to: 0.12619929313659667\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.12619929313659667  to: 0.12618459463119508\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.12618459463119508  to: 0.12616987228393556\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12616987228393556  to: 0.12615516185760497\n",
      "Training iteration: 1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12615516185760497  to: 0.12614041566848755\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.12614041566848755  to: 0.12612564563751222\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12612564563751222  to: 0.12611086368560792\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.12611086368560792  to: 0.12609604597091675\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.12609604597091675  to: 0.12608122825622559\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12608122825622559  to: 0.12606639862060548\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.12606639862060548  to: 0.12605186700820922\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.12605186700820922  to: 0.12603744268417358\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.12603744268417358  to: 0.126023006439209\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.126023006439209  to: 0.12600853443145751\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.12600853443145751  to: 0.1259940266609192\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.1259940266609192  to: 0.1259795069694519\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.1259795069694519  to: 0.1259649634361267\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.1259649634361267  to: 0.12595040798187257\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12595040798187257  to: 0.12593581676483154\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.12593581676483154  to: 0.12592118978500366\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12592118978500366  to: 0.12590655088424682\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12590655088424682  to: 0.12589188814163207\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.12589188814163207  to: 0.12587718963623046\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12587718963623046  to: 0.1258624792098999\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.1258624792098999  to: 0.12584774494171141\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.12584774494171141  to: 0.12583297491073608\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12583297491073608  to: 0.12581814527511598\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.12581814527511598  to: 0.12580331563949584\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12580331563949584  to: 0.12578846216201783\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12578846216201783  to: 0.12577354907989502\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12577354907989502  to: 0.1257586359977722\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.1257586359977722  to: 0.12574367523193358\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.12574367523193358  to: 0.12572867870330812\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12572867870330812  to: 0.12571365833282472\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.12571365833282472  to: 0.1256986141204834\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.1256986141204834  to: 0.12568355798721315\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.12568355798721315  to: 0.12566850185394288\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12566850185394288  to: 0.1256534457206726\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.1256534457206726  to: 0.12563836574554443\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12563836574554443  to: 0.12562323808670045\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12562323808670045  to: 0.12560807466506957\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.12560807466506957  to: 0.12559287548065184\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.12559287548065184  to: 0.12557766437530518\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.12557766437530518  to: 0.12556242942810059\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.12556242942810059  to: 0.12554718255996705\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12554718255996705  to: 0.1255319356918335\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.1255319356918335  to: 0.12551672458648683\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.12551672458648683  to: 0.12550147771835327\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.12550147771835327  to: 0.1254861831665039\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.1254861831665039  to: 0.12547082901000978\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.12547082901000978  to: 0.12545545101165773\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12545545101165773  to: 0.1254400610923767\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.1254400610923767  to: 0.1254246473312378\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.1254246473312378  to: 0.12540919780731202\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.12540919780731202  to: 0.12539370059967042\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12539370059967042  to: 0.12537815570831298\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.12537815570831298  to: 0.1253625750541687\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.1253625750541687  to: 0.12534698247909545\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.12534698247909545  to: 0.1253313660621643\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.1253313660621643  to: 0.12531545162200927\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.12531545162200927  to: 0.12529929876327514\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12529929876327514  to: 0.12528308629989623\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.12528308629989623  to: 0.12526687383651733\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.12526687383651733  to: 0.1252506136894226\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.1252506136894226  to: 0.12523430585861206\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.12523430585861206  to: 0.12521793842315673\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.12521793842315673  to: 0.12520155906677247\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.12520155906677247  to: 0.12518513202667236\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.12518513202667236  to: 0.12516865730285645\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.12516865730285645  to: 0.12515214681625367\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.12515214681625367  to: 0.12513562440872192\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.12513562440872192  to: 0.12511904239654542\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.12511904239654542  to: 0.12510238885879515\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.12510238885879515  to: 0.12508573532104492\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.12508573532104492  to: 0.1250690460205078\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.1250690460205078  to: 0.12505232095718383\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.12505232095718383  to: 0.12503554821014404\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.12503554821014404  to: 0.12501872777938844\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.12501872777938844  to: 0.12500191926956178\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.12500191926956178  to: 0.1249854326248169\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.1249854326248169  to: 0.12496894598007202\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.12496894598007202  to: 0.12495241165161133\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.12495241165161133  to: 0.12493582963943481\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.12493582963943481  to: 0.12491919994354247\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.12491919994354247  to: 0.12490254640579224\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.12490254640579224  to: 0.12488586902618408\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.12488586902618408  to: 0.12486919164657592\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.12486919164657592  to: 0.12485249042510986\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.12485249042510986  to: 0.12483574151992798\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.12483574151992798  to: 0.12481893301010132\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.12481893301010132  to: 0.1248021125793457\n",
      "Training iteration: 1154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1248021125793457  to: 0.12478525638580322\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.12478525638580322  to: 0.12476837635040283\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.12476837635040283  to: 0.12475147247314453\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.12475147247314453  to: 0.12473454475402831\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.12473454475402831  to: 0.12471754550933838\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.12471754550933838  to: 0.12470057010650634\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.12470057010650634  to: 0.12468359470367432\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.12468359470367432  to: 0.12466666698455811\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.12466666698455811  to: 0.12464972734451293\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.12464972734451293  to: 0.12463276386260987\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.12463276386260987  to: 0.12461578845977783\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.12461578845977783  to: 0.1245988130569458\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.1245988130569458  to: 0.12458188533782959\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.12458188533782959  to: 0.1245650053024292\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.1245650053024292  to: 0.12454814910888672\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.12454814910888672  to: 0.12453128099441528\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.12453128099441528  to: 0.12451440095901489\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.12451440095901489  to: 0.12449748516082763\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.12449748516082763  to: 0.12448060512542725\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.12448060512542725  to: 0.12446372509002686\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.12446372509002686  to: 0.1244468331336975\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.1244468331336975  to: 0.12442991733551026\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.12442991733551026  to: 0.12441297769546508\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.12441297769546508  to: 0.12439601421356201\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.12439601421356201  to: 0.12437905073165893\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.12437905073165893  to: 0.124362051486969\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.124362051486969  to: 0.12434499263763428\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.12434499263763428  to: 0.12432788610458374\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.12432788610458374  to: 0.12431075572967529\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.12431075572967529  to: 0.1242936372756958\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.1242936372756958  to: 0.12427647113800049\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.12427647113800049  to: 0.12425928115844727\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.12425928115844727  to: 0.12424204349517823\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.12424204349517823  to: 0.1242247223854065\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.1242247223854065  to: 0.1242073655128479\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.1242073655128479  to: 0.12418994903564454\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.12418994903564454  to: 0.12417255640029908\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.12417255640029908  to: 0.12415512800216674\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.12415512800216674  to: 0.12413769960403442\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.12413769960403442  to: 0.12412023544311523\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.12412023544311523  to: 0.12410275936126709\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.12410275936126709  to: 0.12408519983291626\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.12408519983291626  to: 0.12406758069992066\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.12406758069992066  to: 0.12404990196228027\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.12404990196228027  to: 0.12403213977813721\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.12403213977813721  to: 0.12401434183120727\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.12401434183120727  to: 0.12399652004241943\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.12399652004241943  to: 0.12397865056991578\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.12397865056991578  to: 0.12396076917648316\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.12396076917648316  to: 0.12394287586212158\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.12394287586212158  to: 0.12392491102218628\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.12392491102218628  to: 0.1239068865776062\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.1239068865776062  to: 0.12388880252838134\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.12388880252838134  to: 0.1238706350326538\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.1238706350326538  to: 0.12385239601135253\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.12385239601135253  to: 0.12383407354354858\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.12383407354354858  to: 0.12381572723388672\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.12381572723388672  to: 0.12379734516143799\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.12379734516143799  to: 0.1237789511680603\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.1237789511680603  to: 0.1237605094909668\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.1237605094909668  to: 0.12374203205108643\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.12374203205108643  to: 0.12372345924377441\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.12372345924377441  to: 0.12370481491088867\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.12370481491088867  to: 0.1236860990524292\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.1236860990524292  to: 0.123667311668396\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.123667311668396  to: 0.12364851236343384\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.12364851236343384  to: 0.12362967729568482\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.12362967729568482  to: 0.12361081838607788\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.12361081838607788  to: 0.12359192371368408\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.12359192371368408  to: 0.12357292175292969\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.12357292175292969  to: 0.1235539197921753\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.1235539197921753  to: 0.1235349416732788\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.1235349416732788  to: 0.12351596355438232\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.12351596355438232  to: 0.12349700927734375\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.12349700927734375  to: 0.12347800731658935\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.12347800731658935  to: 0.12345895767211915\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.12345895767211915  to: 0.12343989610671997\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.12343989610671997  to: 0.1234208106994629\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.1234208106994629  to: 0.1234018325805664\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.1234018325805664  to: 0.12338296175003052\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.12338296175003052  to: 0.12336437702178955\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.12336437702178955  to: 0.12334588766098023\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.12334588766098023  to: 0.12332741022109986\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.12332741022109986  to: 0.12330892086029052\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.12330892086029052  to: 0.12329050302505493\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.12329050302505493  to: 0.1232720971107483\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.1232720971107483  to: 0.12325372695922851\n",
      "Training iteration: 1241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12325372695922851  to: 0.12323539257049561\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.12323539257049561  to: 0.12321703433990479\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.12321703433990479  to: 0.12319862842559814\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.12319862842559814  to: 0.12318021059036255\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.12318021059036255  to: 0.12316174507141113\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.12316174507141113  to: 0.12314331531524658\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.12314331531524658  to: 0.12312488555908203\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.12312488555908203  to: 0.12310649156570434\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.12310649156570434  to: 0.12308807373046875\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.12308807373046875  to: 0.1230696201324463\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.1230696201324463  to: 0.12305113077163696\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.12305113077163696  to: 0.12303266525268555\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.12303266525268555  to: 0.12301422357559204\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.12301422357559204  to: 0.12299575805664062\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.12299575805664062  to: 0.12297725677490234\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.12297725677490234  to: 0.12295868396759033\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.12295868396759033  to: 0.12294007539749145\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.12294007539749145  to: 0.12292139530181885\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.12292139530181885  to: 0.12290271520614623\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.12290271520614623  to: 0.12288403511047363\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.12288403511047363  to: 0.12286535501480103\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.12286535501480103  to: 0.12284665107727051\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.12284665107727051  to: 0.12282788753509521\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.12282788753509521  to: 0.12280908823013306\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.12280908823013306  to: 0.12279020547866822\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.12279020547866822  to: 0.1227712869644165\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.1227712869644165  to: 0.12275230884552002\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.12275230884552002  to: 0.12273333072662354\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.12273333072662354  to: 0.12271435260772705\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.12271435260772705  to: 0.12269535064697265\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.12269535064697265  to: 0.12267630100250244\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.12267630100250244  to: 0.12265716791152954\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.12265716791152954  to: 0.12263797521591187\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.12263797521591187  to: 0.12261877059936524\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.12261877059936524  to: 0.1225995421409607\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.1225995421409607  to: 0.12258031368255615\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.12258031368255615  to: 0.12256100177764892\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.12256100177764892  to: 0.12254159450531006\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.12254159450531006  to: 0.12252212762832641\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.12252212762832641  to: 0.12250256538391113\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.12250256538391113  to: 0.12248295545578003\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.12248295545578003  to: 0.1224634289741516\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.1224634289741516  to: 0.12244436740875245\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.12244436740875245  to: 0.12242529392242432\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.12242529392242432  to: 0.12240620851516723\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.12240620851516723  to: 0.12238702774047852\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.12238702774047852  to: 0.12236775159835815\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.12236775159835815  to: 0.12234842777252197\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.12234842777252197  to: 0.12232905626296997\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.12232905626296997  to: 0.1223096251487732\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.1223096251487732  to: 0.12229022979736329\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.12229022979736329  to: 0.12227082252502441\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.12227082252502441  to: 0.12225143909454346\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.12225143909454346  to: 0.12223200798034668\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.12223200798034668  to: 0.12221252918243408\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.12221252918243408  to: 0.12219302654266358\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.12219302654266358  to: 0.12217344045639038\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.12217344045639038  to: 0.122153902053833\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.122153902053833  to: 0.12213433980941772\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.12213433980941772  to: 0.1221147894859314\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.1221147894859314  to: 0.12209517955780029\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.12209517955780029  to: 0.12207546234130859\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.12207546234130859  to: 0.12205569744110108\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.12205569744110108  to: 0.12203586101531982\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.12203586101531982  to: 0.12201597690582275\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.12201597690582275  to: 0.12199609279632569\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.12199609279632569  to: 0.12197622060775756\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.12197622060775756  to: 0.12195632457733155\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.12195632457733155  to: 0.12193644046783447\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.12193644046783447  to: 0.12191646099090576\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.12191646099090576  to: 0.12189639806747436\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.12189639806747436  to: 0.12187625169754028\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.12187625169754028  to: 0.12185602188110352\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.12185602188110352  to: 0.12183569669723511\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.12183569669723511  to: 0.12181541919708253\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.12181541919708253  to: 0.1217951774597168\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.1217951774597168  to: 0.12177501916885376\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.12177501916885376  to: 0.12175495624542236\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.12175495624542236  to: 0.12173497676849365\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.12173497676849365  to: 0.1217150330543518\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.1217150330543518  to: 0.12169508934020996\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.12169508934020996  to: 0.12167508602142334\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.12167508602142334  to: 0.12165507078170776\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.12165507078170776  to: 0.12163501977920532\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.12163501977920532  to: 0.12161490917205811\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.12161490917205811  to: 0.12159485816955566\n",
      "Training iteration: 1327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12159485816955566  to: 0.12157478332519531\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.12157478332519531  to: 0.12155461311340332\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.12155461311340332  to: 0.12153434753417969\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.12153434753417969  to: 0.1215139627456665\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.1215139627456665  to: 0.12149356603622437\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.12149356603622437  to: 0.12147318124771118\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.12147318124771118  to: 0.12145271301269531\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.12145271301269531  to: 0.12143217325210572\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.12143217325210572  to: 0.12141153812408448\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.12141153812408448  to: 0.12139081954956055\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.12139081954956055  to: 0.12137012481689453\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.12137012481689453  to: 0.12134945392608643\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.12134945392608643  to: 0.12132790088653564\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.12132790088653564  to: 0.12130621671676636\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.12130621671676636  to: 0.1212844729423523\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.1212844729423523  to: 0.12126266956329346\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.12126266956329346  to: 0.12124085426330566\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.12124085426330566  to: 0.12121906280517578\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.12121906280517578  to: 0.12119719982147217\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.12119719982147217  to: 0.12117526531219483\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.12117526531219483  to: 0.12115324735641479\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.12115324735641479  to: 0.12113115787506104\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.12113115787506104  to: 0.12110908031463623\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.12110908031463623  to: 0.12108702659606933\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.12108702659606933  to: 0.12106491327285766\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.12106491327285766  to: 0.12104270458221436\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.12104270458221436  to: 0.12102044820785522\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.12102044820785522  to: 0.12099814414978027\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.12099814414978027  to: 0.1209757685661316\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.1209757685661316  to: 0.12095341682434083\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.12095341682434083  to: 0.12093111276626586\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.12093111276626586  to: 0.12090880870819092\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.12090880870819092  to: 0.1208865761756897\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.1208865761756897  to: 0.12086471319198608\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.12086471319198608  to: 0.12084277868270873\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.12084277868270873  to: 0.12082078456878662\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.12082078456878662  to: 0.12079871892929077\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.12079871892929077  to: 0.12077659368515015\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.12077659368515015  to: 0.12075437307357788\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.12075437307357788  to: 0.1207321286201477\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.1207321286201477  to: 0.12070991992950439\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.12070991992950439  to: 0.12068773508071899\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.12068773508071899  to: 0.12066558599472046\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.12066558599472046  to: 0.12064340114593505\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.12064340114593505  to: 0.12062110900878906\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.12062110900878906  to: 0.1205987811088562\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.1205987811088562  to: 0.12057640552520751\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.12057640552520751  to: 0.12055412530899048\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.12055412530899048  to: 0.12053204774856567\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.12053204774856567  to: 0.120509934425354\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.120509934425354  to: 0.12048786878585815\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.12048786878585815  to: 0.12046588659286499\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.12046588659286499  to: 0.12044383287429809\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.12044383287429809  to: 0.12042171955108642\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.12042171955108642  to: 0.12039932012557983\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.12039932012557983  to: 0.1203765869140625\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.1203765869140625  to: 0.1203539252281189\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.1203539252281189  to: 0.12033121585845948\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.12033121585845948  to: 0.12030843496322632\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.12030843496322632  to: 0.12028560638427735\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.12028560638427735  to: 0.12026287317276001\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.12026287317276001  to: 0.12024056911468506\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.12024056911468506  to: 0.12021814584732056\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.12021814584732056  to: 0.12019559144973754\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.12019559144973754  to: 0.1201730489730835\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.1201730489730835  to: 0.12015053033828735\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.12015053033828735  to: 0.12012803554534912\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.12012803554534912  to: 0.12010612487792968\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.12010612487792968  to: 0.12008445262908936\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.12008445262908936  to: 0.12006288766860962\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.12006288766860962  to: 0.12004142999649048\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.12004142999649048  to: 0.12001993656158447\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.12001993656158447  to: 0.1199983835220337\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.1199983835220337  to: 0.11997678279876708\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.11997678279876708  to: 0.11995513439178467\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.11995513439178467  to: 0.11993342638015747\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.11993342638015747  to: 0.1199116826057434\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.1199116826057434  to: 0.11988999843597412\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.11988999843597412  to: 0.1198682427406311\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.1198682427406311  to: 0.11984641551971435\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.11984641551971435  to: 0.1198246955871582\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.1198246955871582  to: 0.11980288028717041\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.11980288028717041  to: 0.1197810411453247\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.1197810411453247  to: 0.11975908279418945\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.11975908279418945  to: 0.11973693370819091\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.11973693370819091  to: 0.11971458196640014\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.11971458196640014  to: 0.11969220638275146\n",
      "Training iteration: 1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11969220638275146  to: 0.11966979503631592\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.11966979503631592  to: 0.11964720487594604\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.11964720487594604  to: 0.11962450742721557\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.11962450742721557  to: 0.1196016550064087\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.1196016550064087  to: 0.1195786714553833\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.1195786714553833  to: 0.11955556869506836\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.11955556869506836  to: 0.11953235864639282\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.11953235864639282  to: 0.11950905323028564\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.11950905323028564  to: 0.11948564052581787\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.11948564052581787  to: 0.11946216821670533\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.11946216821670533  to: 0.1194387435913086\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.1194387435913086  to: 0.11941524744033813\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.11941524744033813  to: 0.11939166784286499\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.11939166784286499  to: 0.11936814785003662\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.11936814785003662  to: 0.11934454441070556\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.11934454441070556  to: 0.11932070255279541\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.11932070255279541  to: 0.11929678916931152\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.11929678916931152  to: 0.11927284002304077\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.11927284002304077  to: 0.1192488431930542\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.1192488431930542  to: 0.1192247986793518\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.1192247986793518  to: 0.1192007064819336\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.1192007064819336  to: 0.11917673349380493\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.11917673349380493  to: 0.11915267705917358\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.11915267705917358  to: 0.11912841796875\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.11912841796875  to: 0.11910413503646851\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.11910413503646851  to: 0.11907981634140015\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.11907981634140015  to: 0.11905546188354492\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.11905546188354492  to: 0.1190312385559082\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.1190312385559082  to: 0.11900699138641357\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.11900699138641357  to: 0.11898270845413209\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.11898270845413209  to: 0.11895837783813476\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.11895837783813476  to: 0.11893395185470582\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.11893395185470582  to: 0.11890947818756104\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.11890947818756104  to: 0.11888495683670045\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.11888495683670045  to: 0.11886019706726074\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.11886019706726074  to: 0.1188355803489685\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.1188355803489685  to: 0.11881091594696044\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.11881091594696044  to: 0.11878621578216553\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.11878621578216553  to: 0.11876144409179687\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.11876144409179687  to: 0.11873661279678345\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.11873661279678345  to: 0.11871172189712524\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.11871172189712524  to: 0.11868680715560913\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.11868680715560913  to: 0.11866183280944824\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.11866183280944824  to: 0.11863664388656617\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.11863664388656617  to: 0.11861163377761841\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.11861163377761841  to: 0.11858670711517334\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.11858670711517334  to: 0.118561851978302\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.118561851978302  to: 0.1185370683670044\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.1185370683670044  to: 0.11851233243942261\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.11851233243942261  to: 0.1184876561164856\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.1184876561164856  to: 0.11846303939819336\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.11846303939819336  to: 0.11843844652175903\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.11843844652175903  to: 0.11841390132904053\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.11841390132904053  to: 0.11838936805725098\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.11838936805725098  to: 0.11836467981338501\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.11836467981338501  to: 0.11834018230438233\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.11834018230438233  to: 0.1183157205581665\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.1183157205581665  to: 0.11829125881195068\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.11829125881195068  to: 0.1182667851448059\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.1182667851448059  to: 0.11824231147766114\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.11824231147766114  to: 0.1182178258895874\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.1182178258895874  to: 0.11819331645965576\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.11819331645965576  to: 0.11816861629486083\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.11816861629486083  to: 0.11814390420913697\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.11814390420913697  to: 0.11811916828155518\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.11811916828155518  to: 0.11809440851211547\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.11809440851211547  to: 0.11806967258453369\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.11806967258453369  to: 0.11804492473602295\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.11804492473602295  to: 0.11802018880844116\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.11802018880844116  to: 0.11799545288085937\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.11799545288085937  to: 0.11797090768814086\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.11797090768814086  to: 0.1179463028907776\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.1179463028907776  to: 0.11792167425155639\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.11792167425155639  to: 0.11789699792861938\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.11789699792861938  to: 0.11787208318710327\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.11787208318710327  to: 0.1178471326828003\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.1178471326828003  to: 0.11782217025756836\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.11782217025756836  to: 0.11779714822769165\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.11779714822769165  to: 0.11777209043502808\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.11777209043502808  to: 0.11774702072143554\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.11774702072143554  to: 0.1177219271659851\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.1177219271659851  to: 0.11769682168960571\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.11769682168960571  to: 0.11767170429229737\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.11767170429229737  to: 0.11764651536941528\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.11764651536941528  to: 0.11762129068374634\n",
      "Training iteration: 1499\n",
      "Improved validation loss from: 0.11762129068374634  to: 0.1175958275794983\n",
      "Training iteration: 1500\n",
      "Improved validation loss from: 0.1175958275794983  to: 0.11757032871246338\n",
      "Training iteration: 1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11757032871246338  to: 0.1175447940826416\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.1175447940826416  to: 0.117519211769104\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.117519211769104  to: 0.11749359369277954\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.11749359369277954  to: 0.11746790409088134\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.11746790409088134  to: 0.11744215488433837\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.11744215488433837  to: 0.11741634607315063\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.11741634607315063  to: 0.11739047765731811\n",
      "Training iteration: 1508\n",
      "Improved validation loss from: 0.11739047765731811  to: 0.11736477613449096\n",
      "Training iteration: 1509\n",
      "Improved validation loss from: 0.11736477613449096  to: 0.11733900308609009\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.11733900308609009  to: 0.11731293201446533\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.11731293201446533  to: 0.11728681325912475\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.11728681325912475  to: 0.11726064682006836\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.11726064682006836  to: 0.11723470687866211\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.11723470687866211  to: 0.11720873117446899\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.11720873117446899  to: 0.11718268394470215\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.11718268394470215  to: 0.11715654134750367\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.11715654134750367  to: 0.11713049411773682\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.11713049411773682  to: 0.11710447072982788\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.11710447072982788  to: 0.11707848310470581\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.11707848310470581  to: 0.11705250740051269\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.11705250740051269  to: 0.1170265793800354\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.1170265793800354  to: 0.117000412940979\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.117000412940979  to: 0.11697428226470948\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.11697428226470948  to: 0.1169481635093689\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.1169481635093689  to: 0.11692204475402831\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.11692204475402831  to: 0.11689592599868774\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.11689592599868774  to: 0.11686979532241822\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.11686979532241822  to: 0.11684364080429077\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.11684364080429077  to: 0.11681747436523438\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.11681747436523438  to: 0.11679127216339111\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.11679127216339111  to: 0.11676504611968994\n",
      "Training iteration: 1532\n",
      "Improved validation loss from: 0.11676504611968994  to: 0.11673877239227295\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.11673877239227295  to: 0.11671220064163208\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.11671220064163208  to: 0.11668559312820434\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.11668559312820434  to: 0.11665893793106079\n",
      "Training iteration: 1536\n",
      "Improved validation loss from: 0.11665893793106079  to: 0.11663202047348023\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.11663202047348023  to: 0.11660505533218384\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.11660505533218384  to: 0.11657805442810058\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.11657805442810058  to: 0.11655102968215943\n",
      "Training iteration: 1540\n",
      "Improved validation loss from: 0.11655102968215943  to: 0.11652395725250245\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.11652395725250245  to: 0.11649682521820068\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.11649682521820068  to: 0.11646970510482788\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.11646970510482788  to: 0.11644256114959717\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.11644256114959717  to: 0.11641538143157959\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.11641538143157959  to: 0.11638820171356201\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.11638820171356201  to: 0.11636097431182861\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.11636097431182861  to: 0.11633368730545043\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.11633368730545043  to: 0.1163063645362854\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.1163063645362854  to: 0.11627901792526245\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.11627901792526245  to: 0.11625163555145264\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.11625163555145264  to: 0.11622425317764282\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.11622425317764282  to: 0.11619682312011718\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.11619682312011718  to: 0.11616939306259155\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.11616939306259155  to: 0.11614171266555787\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.11614171266555787  to: 0.11611402034759521\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.11611402034759521  to: 0.11608635187149048\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.11608635187149048  to: 0.11605868339538575\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.11605868339538575  to: 0.116031014919281\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.116031014919281  to: 0.11600371599197387\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.11600371599197387  to: 0.11597673892974854\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.11597673892974854  to: 0.11595009565353394\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.11595009565353394  to: 0.1159237265586853\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.1159237265586853  to: 0.11589759588241577\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.11589759588241577  to: 0.11587164402008057\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.11587164402008057  to: 0.11584563255310058\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.11584563255310058  to: 0.11581954956054688\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.11581954956054688  to: 0.11579328775405884\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.11579328775405884  to: 0.11576684713363647\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.11576684713363647  to: 0.11574046611785889\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.11574046611785889  to: 0.11571409702301025\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.11571409702301025  to: 0.11568756103515625\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.11568756103515625  to: 0.11566088199615479\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.11566088199615479  to: 0.11563403606414795\n",
      "Training iteration: 1574\n",
      "Improved validation loss from: 0.11563403606414795  to: 0.11560742855072022\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.11560742855072022  to: 0.11558098793029785\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.11558098793029785  to: 0.1155548334121704\n",
      "Training iteration: 1577\n",
      "Improved validation loss from: 0.1155548334121704  to: 0.11552894115447998\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.11552894115447998  to: 0.11550287008285523\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.11550287008285523  to: 0.11547664403915406\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.11547664403915406  to: 0.11545025110244751\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.11545025110244751  to: 0.11542370319366455\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.11542370319366455  to: 0.11539701223373414\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.11539701223373414  to: 0.11537015438079834\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.11537015438079834  to: 0.11534354686737061\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.11534354686737061  to: 0.1153171181678772\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.1153171181678772  to: 0.11529086828231812\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.11529086828231812  to: 0.11526472568511963\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.11526472568511963  to: 0.11523833274841308\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.11523833274841308  to: 0.11521167755126953\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.11521167755126953  to: 0.11518476009368897\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.11518476009368897  to: 0.11515734195709229\n",
      "Training iteration: 1592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11515734195709229  to: 0.11513011455535889\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.11513011455535889  to: 0.11510303020477294\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.11510303020477294  to: 0.11507607698440551\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.11507607698440551  to: 0.11504919528961181\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.11504919528961181  to: 0.11502234935760498\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.11502234935760498  to: 0.11499515771865845\n",
      "Training iteration: 1598\n",
      "Improved validation loss from: 0.11499515771865845  to: 0.11496763229370117\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.11496763229370117  to: 0.11493834257125854\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.11493834257125854  to: 0.11490745544433593\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.11490745544433593  to: 0.11487557888031005\n",
      "Training iteration: 1602\n",
      "Improved validation loss from: 0.11487557888031005  to: 0.11484272480010986\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.11484272480010986  to: 0.11480903625488281\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.11480903625488281  to: 0.11477465629577636\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.11477465629577636  to: 0.11473968029022216\n",
      "Training iteration: 1606\n",
      "Improved validation loss from: 0.11473968029022216  to: 0.11470425128936768\n",
      "Training iteration: 1607\n",
      "Improved validation loss from: 0.11470425128936768  to: 0.11466844081878662\n",
      "Training iteration: 1608\n",
      "Improved validation loss from: 0.11466844081878662  to: 0.11463209390640258\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.11463209390640258  to: 0.11459558010101319\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.11459558010101319  to: 0.114559006690979\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.114559006690979  to: 0.11452243328094483\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.11452243328094483  to: 0.11448590755462647\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.11448590755462647  to: 0.11444950103759766\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.11444950103759766  to: 0.11441329717636109\n",
      "Training iteration: 1615\n",
      "Improved validation loss from: 0.11441329717636109  to: 0.1143773078918457\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.1143773078918457  to: 0.1143415927886963\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.1143415927886963  to: 0.11430617570877075\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.11430617570877075  to: 0.11427090167999268\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.11427090167999268  to: 0.11423625946044921\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.11423625946044921  to: 0.11420198678970336\n",
      "Training iteration: 1621\n",
      "Improved validation loss from: 0.11420198678970336  to: 0.11416810750961304\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.11416810750961304  to: 0.11413463354110717\n",
      "Training iteration: 1623\n",
      "Improved validation loss from: 0.11413463354110717  to: 0.11410154104232788\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.11410154104232788  to: 0.11406886577606201\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.11406886577606201  to: 0.11403658390045165\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.11403658390045165  to: 0.11400469541549682\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.11400469541549682  to: 0.11397316455841064\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.11397316455841064  to: 0.11394201517105103\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.11394201517105103  to: 0.1139111876487732\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.1139111876487732  to: 0.11388070583343506\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.11388070583343506  to: 0.11385053396224976\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.11385053396224976  to: 0.11382068395614624\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.11382068395614624  to: 0.11379108428955079\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.11379108428955079  to: 0.11376174688339233\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.11376174688339233  to: 0.11373234987258911\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.11373234987258911  to: 0.11370317935943604\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.11370317935943604  to: 0.1136742115020752\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.1136742115020752  to: 0.11364542245864868\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.11364542245864868  to: 0.11361677646636963\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.11361677646636963  to: 0.11358826160430908\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.11358826160430908  to: 0.11355981826782227\n",
      "Training iteration: 1642\n",
      "Improved validation loss from: 0.11355981826782227  to: 0.11353145837783814\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.11353145837783814  to: 0.11350313425064087\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.11350313425064087  to: 0.11347482204437256\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.11347482204437256  to: 0.11344650983810425\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.11344650983810425  to: 0.11341819763183594\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.11341819763183594  to: 0.11338983774185181\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.11338983774185181  to: 0.11336143016815185\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.11336143016815185  to: 0.11333295106887817\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.11333295106887817  to: 0.1133043646812439\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.1133043646812439  to: 0.11327569484710694\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.11327569484710694  to: 0.11324690580368042\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.11324690580368042  to: 0.11321797370910644\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.11321797370910644  to: 0.11318889856338502\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.11318889856338502  to: 0.11315966844558716\n",
      "Training iteration: 1656\n",
      "Improved validation loss from: 0.11315966844558716  to: 0.11313028335571289\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.11313028335571289  to: 0.11310070753097534\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.11310070753097534  to: 0.11307064294815064\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.11307064294815064  to: 0.11304007768630982\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.11304007768630982  to: 0.11300904750823974\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.11300904750823974  to: 0.11297760009765626\n",
      "Training iteration: 1662\n",
      "Improved validation loss from: 0.11297760009765626  to: 0.11294572353363037\n",
      "Training iteration: 1663\n",
      "Improved validation loss from: 0.11294572353363037  to: 0.11291348934173584\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.11291348934173584  to: 0.1128808856010437\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.1128808856010437  to: 0.11284795999526978\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.11284795999526978  to: 0.11281473636627197\n",
      "Training iteration: 1667\n",
      "Improved validation loss from: 0.11281473636627197  to: 0.11278120279312134\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.11278120279312134  to: 0.1127474069595337\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.1127474069595337  to: 0.11271337270736695\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.11271337270736695  to: 0.112679123878479\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.112679123878479  to: 0.11264467239379883\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.11264467239379883  to: 0.11261003017425537\n",
      "Training iteration: 1673\n",
      "Improved validation loss from: 0.11261003017425537  to: 0.1125752091407776\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.1125752091407776  to: 0.11254024505615234\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.11254024505615234  to: 0.11250513792037964\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.11250513792037964  to: 0.11246992349624634\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.11246992349624634  to: 0.11243457794189453\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.11243457794189453  to: 0.11239914894104004\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.11239914894104004  to: 0.11236362457275391\n",
      "Training iteration: 1680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.11236362457275391  to: 0.11232801675796508\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.11232801675796508  to: 0.11229232549667359\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.11229232549667359  to: 0.11225655078887939\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.11225655078887939  to: 0.11222074031829835\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.11222074031829835  to: 0.11218483448028564\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.11218483448028564  to: 0.11214889287948608\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.11214889287948608  to: 0.11211289167404175\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.11211289167404175  to: 0.11207687854766846\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.11207687854766846  to: 0.1120408058166504\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.1120408058166504  to: 0.1120046854019165\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.1120046854019165  to: 0.11196855306625367\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.11196855306625367  to: 0.11193239688873291\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.11193239688873291  to: 0.1118962049484253\n",
      "Training iteration: 1693\n",
      "Improved validation loss from: 0.1118962049484253  to: 0.11186000108718872\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.11186000108718872  to: 0.11182374954223633\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.11182374954223633  to: 0.11178747415542603\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.11178747415542603  to: 0.1117511510848999\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.1117511510848999  to: 0.11171480417251586\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.11171480417251586  to: 0.11167840957641602\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.11167840957641602  to: 0.11164195537567138\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.11164195537567138  to: 0.11160546541213989\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.11160546541213989  to: 0.11156892776489258\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.11156892776489258  to: 0.11153230667114258\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.11153230667114258  to: 0.1114957332611084\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.1114957332611084  to: 0.11145918369293213\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.11145918369293213  to: 0.11142261028289795\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.11142261028289795  to: 0.11138622760772705\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.11138622760772705  to: 0.11134952306747437\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.11134952306747437  to: 0.1113126039505005\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.1113126039505005  to: 0.11127605438232421\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.11127605438232421  to: 0.11123917102813721\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.11123917102813721  to: 0.11120198965072632\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.11120198965072632  to: 0.11116454601287842\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.11116454601287842  to: 0.11112686395645141\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.11112686395645141  to: 0.11108894348144531\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.11108894348144531  to: 0.11105082035064698\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.11105082035064698  to: 0.1110125184059143\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.1110125184059143  to: 0.11097407341003418\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.11097407341003418  to: 0.11093549728393555\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.11093549728393555  to: 0.11089591979980469\n",
      "Training iteration: 1720\n",
      "Improved validation loss from: 0.11089591979980469  to: 0.1108554482460022\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.1108554482460022  to: 0.11081416606903076\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.11081416606903076  to: 0.11077216863632203\n",
      "Training iteration: 1723\n",
      "Improved validation loss from: 0.11077216863632203  to: 0.11072957515716553\n",
      "Training iteration: 1724\n",
      "Improved validation loss from: 0.11072957515716553  to: 0.1106863260269165\n",
      "Training iteration: 1725\n",
      "Improved validation loss from: 0.1106863260269165  to: 0.11064256429672241\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.11064256429672241  to: 0.11059924364089965\n",
      "Training iteration: 1727\n",
      "Improved validation loss from: 0.11059924364089965  to: 0.11055639982223511\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.11055639982223511  to: 0.11051403284072876\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.11051403284072876  to: 0.11047207117080689\n",
      "Training iteration: 1730\n",
      "Improved validation loss from: 0.11047207117080689  to: 0.11043052673339844\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.11043052673339844  to: 0.11038939952850342\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.11038939952850342  to: 0.11034867763519288\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.11034867763519288  to: 0.11030830144882202\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.11030830144882202  to: 0.11026824712753296\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.11026824712753296  to: 0.11022851467132569\n",
      "Training iteration: 1736\n",
      "Improved validation loss from: 0.11022851467132569  to: 0.11018908023834229\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.11018908023834229  to: 0.11014988422393798\n",
      "Training iteration: 1738\n",
      "Improved validation loss from: 0.11014988422393798  to: 0.11010991334915161\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.11010991334915161  to: 0.11006922721862793\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.11006922721862793  to: 0.11002788543701172\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.11002788543701172  to: 0.1099859595298767\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.1099859595298767  to: 0.10994306802749634\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.10994306802749634  to: 0.10989929437637329\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.10989929437637329  to: 0.1098557710647583\n",
      "Training iteration: 1745\n",
      "Improved validation loss from: 0.1098557710647583  to: 0.10981251001358032\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.10981251001358032  to: 0.10976946353912354\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.10976946353912354  to: 0.10972665548324585\n",
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.10972665548324585  to: 0.10968438386917115\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.10968438386917115  to: 0.10964256525039673\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.10964256525039673  to: 0.10960088968276978\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.10960088968276978  to: 0.10955936908721924\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.10955936908721924  to: 0.10951688289642333\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.10951688289642333  to: 0.10947351455688477\n",
      "Training iteration: 1754\n",
      "Improved validation loss from: 0.10947351455688477  to: 0.1094293475151062\n",
      "Training iteration: 1755\n",
      "Improved validation loss from: 0.1094293475151062  to: 0.10938448905944824\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.10938448905944824  to: 0.10933899879455566\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.10933899879455566  to: 0.10929406881332397\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.10929406881332397  to: 0.1092496633529663\n",
      "Training iteration: 1759\n",
      "Improved validation loss from: 0.1092496633529663  to: 0.10920572280883789\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.10920572280883789  to: 0.10916221141815186\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.10916221141815186  to: 0.1091191053390503\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.1091191053390503  to: 0.10907518863677979\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.10907518863677979  to: 0.10903050899505615\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.10903050899505615  to: 0.10898517370223999\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.10898517370223999  to: 0.10893923044204712\n",
      "Training iteration: 1766\n",
      "Improved validation loss from: 0.10893923044204712  to: 0.10889394283294677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.10889394283294677  to: 0.10884923934936523\n",
      "Training iteration: 1768\n",
      "Improved validation loss from: 0.10884923934936523  to: 0.10880504846572876\n",
      "Training iteration: 1769\n",
      "Improved validation loss from: 0.10880504846572876  to: 0.10876133441925048\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.10876133441925048  to: 0.10871683359146118\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.10871683359146118  to: 0.10867159366607666\n",
      "Training iteration: 1772\n",
      "Improved validation loss from: 0.10867159366607666  to: 0.10862566232681274\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.10862566232681274  to: 0.10857924222946166\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.10857924222946166  to: 0.10853383541107178\n",
      "Training iteration: 1775\n",
      "Improved validation loss from: 0.10853383541107178  to: 0.1084894061088562\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.1084894061088562  to: 0.10844560861587524\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.10844560861587524  to: 0.10840108394622802\n",
      "Training iteration: 1778\n",
      "Improved validation loss from: 0.10840108394622802  to: 0.1083559274673462\n",
      "Training iteration: 1779\n",
      "Improved validation loss from: 0.1083559274673462  to: 0.10831013917922974\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.10831013917922974  to: 0.10826382637023926\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.10826382637023926  to: 0.10821833610534667\n",
      "Training iteration: 1782\n",
      "Improved validation loss from: 0.10821833610534667  to: 0.10817359685897827\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.10817359685897827  to: 0.10812956094741821\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.10812956094741821  to: 0.10808610916137695\n",
      "Training iteration: 1785\n",
      "Improved validation loss from: 0.10808610916137695  to: 0.10804185867309571\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.10804185867309571  to: 0.10799686908721924\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.10799686908721924  to: 0.10795117616653442\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.10795117616653442  to: 0.10790486335754394\n",
      "Training iteration: 1789\n",
      "Improved validation loss from: 0.10790486335754394  to: 0.10785799026489258\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.10785799026489258  to: 0.10781060457229615\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.10781060457229615  to: 0.10776416063308716\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.10776416063308716  to: 0.10771856307983399\n",
      "Training iteration: 1793\n",
      "Improved validation loss from: 0.10771856307983399  to: 0.10767372846603393\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.10767372846603393  to: 0.10762956142425537\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.10762956142425537  to: 0.10758460760116577\n",
      "Training iteration: 1796\n",
      "Improved validation loss from: 0.10758460760116577  to: 0.10753892660140991\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.10753892660140991  to: 0.10749251842498779\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.10749251842498779  to: 0.10744545459747315\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.10744545459747315  to: 0.10739777088165284\n",
      "Training iteration: 1800\n",
      "Improved validation loss from: 0.10739777088165284  to: 0.10735100507736206\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.10735100507736206  to: 0.10730512142181396\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.10730512142181396  to: 0.10725997686386109\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.10725997686386109  to: 0.10721400976181031\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.10721400976181031  to: 0.10716729164123535\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.10716729164123535  to: 0.10711987018585205\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.10711987018585205  to: 0.10707180500030518\n",
      "Training iteration: 1807\n",
      "Improved validation loss from: 0.10707180500030518  to: 0.10702311992645264\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.10702311992645264  to: 0.1069754958152771\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.1069754958152771  to: 0.1069287896156311\n",
      "Training iteration: 1810\n",
      "Improved validation loss from: 0.1069287896156311  to: 0.10688133239746093\n",
      "Training iteration: 1811\n",
      "Improved validation loss from: 0.10688133239746093  to: 0.10683315992355347\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.10683315992355347  to: 0.1067859411239624\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.1067859411239624  to: 0.10673795938491822\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.10673795938491822  to: 0.10668927431106567\n",
      "Training iteration: 1815\n",
      "Improved validation loss from: 0.10668927431106567  to: 0.1066399335861206\n",
      "Training iteration: 1816\n",
      "Improved validation loss from: 0.1066399335861206  to: 0.10659162998199463\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.10659162998199463  to: 0.10654425621032715\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.10654425621032715  to: 0.10649601221084595\n",
      "Training iteration: 1819\n",
      "Improved validation loss from: 0.10649601221084595  to: 0.10644698143005371\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.10644698143005371  to: 0.10639717578887939\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.10639717578887939  to: 0.10634835958480834\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.10634835958480834  to: 0.10629873275756836\n",
      "Training iteration: 1823\n",
      "Improved validation loss from: 0.10629873275756836  to: 0.10624831914901733\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.10624831914901733  to: 0.10619890689849854\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.10619890689849854  to: 0.10614864826202393\n",
      "Training iteration: 1826\n",
      "Improved validation loss from: 0.10614864826202393  to: 0.10609757900238037\n",
      "Training iteration: 1827\n",
      "Improved validation loss from: 0.10609757900238037  to: 0.106047523021698\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.106047523021698  to: 0.10599659681320191\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.10599659681320191  to: 0.10594484806060792\n",
      "Training iteration: 1830\n",
      "Improved validation loss from: 0.10594484806060792  to: 0.10589411258697509\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.10589411258697509  to: 0.1058424711227417\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.1058424711227417  to: 0.10579006671905518\n",
      "Training iteration: 1833\n",
      "Improved validation loss from: 0.10579006671905518  to: 0.10573692321777343\n",
      "Training iteration: 1834\n",
      "Improved validation loss from: 0.10573692321777343  to: 0.1056849479675293\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.1056849479675293  to: 0.10563400983810425\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.10563400983810425  to: 0.10558211803436279\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.10558211803436279  to: 0.10552934408187867\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.10552934408187867  to: 0.10547572374343872\n",
      "Training iteration: 1839\n",
      "Improved validation loss from: 0.10547572374343872  to: 0.10542143583297729\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.10542143583297729  to: 0.10536653995513916\n",
      "Training iteration: 1841\n",
      "Improved validation loss from: 0.10536653995513916  to: 0.1053109884262085\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.1053109884262085  to: 0.10525674819946289\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.10525674819946289  to: 0.10520371198654174\n",
      "Training iteration: 1844\n",
      "Improved validation loss from: 0.10520371198654174  to: 0.10515172481536865\n",
      "Training iteration: 1845\n",
      "Improved validation loss from: 0.10515172481536865  to: 0.10509867668151855\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.10509867668151855  to: 0.1050422191619873\n",
      "Training iteration: 1847\n",
      "Improved validation loss from: 0.1050422191619873  to: 0.10498267412185669\n",
      "Training iteration: 1848\n",
      "Improved validation loss from: 0.10498267412185669  to: 0.10492035150527954\n",
      "Training iteration: 1849\n",
      "Improved validation loss from: 0.10492035150527954  to: 0.10485557317733765\n",
      "Training iteration: 1850\n",
      "Improved validation loss from: 0.10485557317733765  to: 0.10479071140289306\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.10479071140289306  to: 0.10472584962844848\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.10472584962844848  to: 0.1046610951423645\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.1046610951423645  to: 0.1045965313911438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.1045965313911438  to: 0.10453221797943116\n",
      "Training iteration: 1855\n",
      "Improved validation loss from: 0.10453221797943116  to: 0.10446611642837525\n",
      "Training iteration: 1856\n",
      "Improved validation loss from: 0.10446611642837525  to: 0.10439850091934204\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.10439850091934204  to: 0.10432972908020019\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.10432972908020019  to: 0.10426011085510253\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.10426011085510253  to: 0.10418968200683594\n",
      "Training iteration: 1860\n",
      "Improved validation loss from: 0.10418968200683594  to: 0.10412079095840454\n",
      "Training iteration: 1861\n",
      "Improved validation loss from: 0.10412079095840454  to: 0.10405336618423462\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.10405336618423462  to: 0.10398737192153931\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.10398737192153931  to: 0.10392054319381713\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.10392054319381713  to: 0.10385303497314453\n",
      "Training iteration: 1865\n",
      "Improved validation loss from: 0.10385303497314453  to: 0.10378497838973999\n",
      "Training iteration: 1866\n",
      "Improved validation loss from: 0.10378497838973999  to: 0.10371649265289307\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.10371649265289307  to: 0.10364991426467896\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.10364991426467896  to: 0.10358504056930543\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.10358504056930543  to: 0.10351957082748413\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.10351957082748413  to: 0.10345357656478882\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.10345357656478882  to: 0.10338717699050903\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.10338717699050903  to: 0.10332037210464477\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.10332037210464477  to: 0.10325548648834229\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.10325548648834229  to: 0.103190016746521\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.103190016746521  to: 0.10312399864196778\n",
      "Training iteration: 1876\n",
      "Improved validation loss from: 0.10312399864196778  to: 0.10305974483489991\n",
      "Training iteration: 1877\n",
      "Improved validation loss from: 0.10305974483489991  to: 0.10299477577209473\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.10299477577209473  to: 0.10292907953262329\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.10292907953262329  to: 0.10286270380020142\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.10286270380020142  to: 0.10279569625854493\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.10279569625854493  to: 0.10272976160049438\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.10272976160049438  to: 0.10266478061676025\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.10266478061676025  to: 0.10259830951690674\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.10259830951690674  to: 0.10253044366836547\n",
      "Training iteration: 1885\n",
      "Improved validation loss from: 0.10253044366836547  to: 0.10246129035949707\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.10246129035949707  to: 0.1023909568786621\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.1023909568786621  to: 0.10231955051422119\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.10231955051422119  to: 0.10224717855453491\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.10224717855453491  to: 0.10217397212982178\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.10217397212982178  to: 0.10210007429122925\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.10210007429122925  to: 0.10202553272247314\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.10202553272247314  to: 0.10195049047470092\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.10195049047470092  to: 0.10187501907348633\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.10187501907348633  to: 0.10180169343948364\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.10180169343948364  to: 0.10172779560089111\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.10172779560089111  to: 0.10165345668792725\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.10165345668792725  to: 0.10157867670059204\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.10157867670059204  to: 0.10150353908538819\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.10150353908538819  to: 0.10142810344696045\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.10142810344696045  to: 0.10135239362716675\n",
      "Training iteration: 1901\n",
      "Improved validation loss from: 0.10135239362716675  to: 0.10127649307250977\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.10127649307250977  to: 0.10120041370391845\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.10120041370391845  to: 0.10112416744232178\n",
      "Training iteration: 1904\n",
      "Improved validation loss from: 0.10112416744232178  to: 0.10104758739471435\n",
      "Training iteration: 1905\n",
      "Improved validation loss from: 0.10104758739471435  to: 0.10097070932388305\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.10097070932388305  to: 0.10089356899261474\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.10089356899261474  to: 0.10081717967987061\n",
      "Training iteration: 1908\n",
      "Improved validation loss from: 0.10081717967987061  to: 0.10074135065078735\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.10074135065078735  to: 0.10066522359848022\n",
      "Training iteration: 1910\n",
      "Improved validation loss from: 0.10066522359848022  to: 0.10058883428573609\n",
      "Training iteration: 1911\n",
      "Improved validation loss from: 0.10058883428573609  to: 0.1005122184753418\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.1005122184753418  to: 0.10043537616729736\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.10043537616729736  to: 0.10035830736160278\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.10035830736160278  to: 0.10028104782104492\n",
      "Training iteration: 1915\n",
      "Improved validation loss from: 0.10028104782104492  to: 0.10020358562469482\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.10020358562469482  to: 0.10012592077255249\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.10012592077255249  to: 0.10004806518554688\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.10004806518554688  to: 0.09997000694274902\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.09997000694274902  to: 0.09989174008369446\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.09989174008369446  to: 0.0998113989830017\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.0998113989830017  to: 0.09972920417785644\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.09972920417785644  to: 0.09964543581008911\n",
      "Training iteration: 1923\n",
      "Improved validation loss from: 0.09964543581008911  to: 0.09956035614013672\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.09956035614013672  to: 0.09947425127029419\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.09947425127029419  to: 0.09938737750053406\n",
      "Training iteration: 1926\n",
      "Improved validation loss from: 0.09938737750053406  to: 0.09930002093315124\n",
      "Training iteration: 1927\n",
      "Improved validation loss from: 0.09930002093315124  to: 0.0992124080657959\n",
      "Training iteration: 1928\n",
      "Improved validation loss from: 0.0992124080657959  to: 0.09912478327751159\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.09912478327751159  to: 0.09903732538223267\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.09903732538223267  to: 0.09895021319389344\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.09895021319389344  to: 0.09886358380317688\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.09886358380317688  to: 0.0987775444984436\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.0987775444984436  to: 0.09869199991226196\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.09869199991226196  to: 0.09860703349113464\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.09860703349113464  to: 0.09852269887924195\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.09852269887924195  to: 0.09843870401382446\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.09843870401382446  to: 0.0983551025390625\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.0983551025390625  to: 0.09827185869216919\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.09827185869216919  to: 0.0981889545917511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.0981889545917511  to: 0.09810630679130554\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.09810630679130554  to: 0.09802389144897461\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.09802389144897461  to: 0.09794172048568725\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.09794172048568725  to: 0.0978598415851593\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.0978598415851593  to: 0.09777787923812867\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.09777787923812867  to: 0.09769583940505981\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.09769583940505981  to: 0.09761360883712769\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.09761360883712769  to: 0.09753096699714661\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.09753096699714661  to: 0.09744781255722046\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.09744781255722046  to: 0.0973641574382782\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.0973641574382782  to: 0.0972799003124237\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.0972799003124237  to: 0.09719486236572265\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.09719486236572265  to: 0.0971090018749237\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.0971090018749237  to: 0.09702199697494507\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.09702199697494507  to: 0.0969338595867157\n",
      "Training iteration: 1955\n",
      "Improved validation loss from: 0.0969338595867157  to: 0.0968446135520935\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.0968446135520935  to: 0.0967542827129364\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.0967542827129364  to: 0.09666292071342468\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.09666292071342468  to: 0.0965705692768097\n",
      "Training iteration: 1959\n",
      "Improved validation loss from: 0.0965705692768097  to: 0.09647728204727173\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.09647728204727173  to: 0.09638313055038453\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.09638313055038453  to: 0.0962881863117218\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.0962881863117218  to: 0.09619249105453491\n",
      "Training iteration: 1963\n",
      "Improved validation loss from: 0.09619249105453491  to: 0.09609615206718444\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.09609615206718444  to: 0.09599921107292175\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.09599921107292175  to: 0.09590173959732055\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.09590173959732055  to: 0.09580038189888\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.09580038189888  to: 0.09569565057754517\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.09569565057754517  to: 0.09558804631233216\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.09558804631233216  to: 0.09547807574272156\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.09547807574272156  to: 0.09536622166633606\n",
      "Training iteration: 1971\n",
      "Improved validation loss from: 0.09536622166633606  to: 0.09525299072265625\n",
      "Training iteration: 1972\n",
      "Improved validation loss from: 0.09525299072265625  to: 0.09513909220695496\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.09513909220695496  to: 0.09502469897270202\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.09502469897270202  to: 0.09491011500358582\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.09491011500358582  to: 0.09479562640190124\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.09479562640190124  to: 0.09468148946762085\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.09468148946762085  to: 0.09456791877746581\n",
      "Training iteration: 1978\n",
      "Improved validation loss from: 0.09456791877746581  to: 0.0944550633430481\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.0944550633430481  to: 0.09434288740158081\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.09434288740158081  to: 0.09423149824142456\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.09423149824142456  to: 0.09412091970443726\n",
      "Training iteration: 1982\n",
      "Improved validation loss from: 0.09412091970443726  to: 0.09401118159294128\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.09401118159294128  to: 0.09390226602554322\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.09390226602554322  to: 0.09379736185073853\n",
      "Training iteration: 1985\n",
      "Improved validation loss from: 0.09379736185073853  to: 0.09369276165962219\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.09369276165962219  to: 0.09358836412429809\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.09358836412429809  to: 0.09348403811454772\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.09348403811454772  to: 0.09337965846061706\n",
      "Training iteration: 1989\n",
      "Improved validation loss from: 0.09337965846061706  to: 0.0932751476764679\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.0932751476764679  to: 0.09317039251327515\n",
      "Training iteration: 1991\n",
      "Improved validation loss from: 0.09317039251327515  to: 0.09306532740592957\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.09306532740592957  to: 0.09295988082885742\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.09295988082885742  to: 0.09285400509834289\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.09285400509834289  to: 0.09274762868881226\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.09274762868881226  to: 0.09264079332351685\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.09264079332351685  to: 0.0925334632396698\n",
      "Training iteration: 1997\n",
      "Improved validation loss from: 0.0925334632396698  to: 0.09242566227912903\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.09242566227912903  to: 0.09231739044189453\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.09231739044189453  to: 0.09220846891403198\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.09220846891403198  to: 0.0920989990234375\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.0920989990234375  to: 0.09198901057243347\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.09198901057243347  to: 0.09187860488891601\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.09187860488891601  to: 0.09176772236824035\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.09176772236824035  to: 0.09165643453598023\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.09165643453598023  to: 0.09154481887817383\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.09154481887817383  to: 0.09143295288085937\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.09143295288085937  to: 0.09132086634635925\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.09132086634635925  to: 0.09120861887931823\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.09120861887931823  to: 0.09109619855880738\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.09109619855880738  to: 0.09098361134529113\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.09098361134529113  to: 0.09087084531784058\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.09087084531784058  to: 0.09075786471366883\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.09075786471366883  to: 0.09064462780952454\n",
      "Training iteration: 2014\n",
      "Improved validation loss from: 0.09064462780952454  to: 0.0905311107635498\n",
      "Training iteration: 2015\n",
      "Improved validation loss from: 0.0905311107635498  to: 0.09041725397109986\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.09041725397109986  to: 0.09030299186706543\n",
      "Training iteration: 2017\n",
      "Improved validation loss from: 0.09030299186706543  to: 0.09018823504447937\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.09018823504447937  to: 0.09007290601730347\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.09007290601730347  to: 0.08995693922042847\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.08995693922042847  to: 0.08984028100967408\n",
      "Training iteration: 2021\n",
      "Improved validation loss from: 0.08984028100967408  to: 0.0896986484527588\n",
      "Training iteration: 2022\n",
      "Improved validation loss from: 0.0896986484527588  to: 0.0895356297492981\n",
      "Training iteration: 2023\n",
      "Improved validation loss from: 0.0895356297492981  to: 0.08935548067092895\n",
      "Training iteration: 2024\n",
      "Improved validation loss from: 0.08935548067092895  to: 0.08916292190551758\n",
      "Training iteration: 2025\n",
      "Improved validation loss from: 0.08916292190551758  to: 0.08897021412849426\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.08897021412849426  to: 0.08878070712089539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.08878070712089539  to: 0.08859726786613464\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.08859726786613464  to: 0.08844584226608276\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.08844584226608276  to: 0.08830065727233886\n",
      "Training iteration: 2030\n",
      "Improved validation loss from: 0.08830065727233886  to: 0.088162362575531\n",
      "Training iteration: 2031\n",
      "Improved validation loss from: 0.088162362575531  to: 0.08803127408027649\n",
      "Training iteration: 2032\n",
      "Improved validation loss from: 0.08803127408027649  to: 0.08790740966796876\n",
      "Training iteration: 2033\n",
      "Improved validation loss from: 0.08790740966796876  to: 0.08779042959213257\n",
      "Training iteration: 2034\n",
      "Improved validation loss from: 0.08779042959213257  to: 0.08770427703857422\n",
      "Training iteration: 2035\n",
      "Improved validation loss from: 0.08770427703857422  to: 0.08762035369873047\n",
      "Training iteration: 2036\n",
      "Improved validation loss from: 0.08762035369873047  to: 0.08753703832626343\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.08753703832626343  to: 0.08744342923164368\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.08744342923164368  to: 0.0873393714427948\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.0873393714427948  to: 0.08722547292709351\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.08722547292709351  to: 0.08710298538208008\n",
      "Training iteration: 2041\n",
      "Improved validation loss from: 0.08710298538208008  to: 0.08697354197502136\n",
      "Training iteration: 2042\n",
      "Improved validation loss from: 0.08697354197502136  to: 0.08683916926383972\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.08683916926383972  to: 0.08670199513435364\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.08670199513435364  to: 0.08656412363052368\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.08656412363052368  to: 0.08642934560775757\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.08642934560775757  to: 0.0863086998462677\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.0863086998462677  to: 0.08620158433914185\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.08620158433914185  to: 0.08610679507255554\n",
      "Training iteration: 2049\n",
      "Improved validation loss from: 0.08610679507255554  to: 0.08601251840591431\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.08601251840591431  to: 0.0859178900718689\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.0859178900718689  to: 0.08582269549369811\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.08582269549369811  to: 0.08572586178779602\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.08572586178779602  to: 0.08562496304512024\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.08562496304512024  to: 0.08551994562149048\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.08551994562149048  to: 0.08541091084480286\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.08541091084480286  to: 0.08529815673828126\n",
      "Training iteration: 2057\n",
      "Improved validation loss from: 0.08529815673828126  to: 0.08518210649490357\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.08518210649490357  to: 0.08506525158882142\n",
      "Training iteration: 2059\n",
      "Improved validation loss from: 0.08506525158882142  to: 0.08493701219558716\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.08493701219558716  to: 0.08479908108711243\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.08479908108711243  to: 0.0846644401550293\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.0846644401550293  to: 0.08453365564346313\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.08453365564346313  to: 0.0844067931175232\n",
      "Training iteration: 2064\n",
      "Improved validation loss from: 0.0844067931175232  to: 0.08428375124931335\n",
      "Training iteration: 2065\n",
      "Improved validation loss from: 0.08428375124931335  to: 0.08416433334350586\n",
      "Training iteration: 2066\n",
      "Improved validation loss from: 0.08416433334350586  to: 0.08404814004898072\n",
      "Training iteration: 2067\n",
      "Improved validation loss from: 0.08404814004898072  to: 0.08393497467041015\n",
      "Training iteration: 2068\n",
      "Improved validation loss from: 0.08393497467041015  to: 0.0838121235370636\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.0838121235370636  to: 0.08368003964424134\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.08368003964424134  to: 0.08355209231376648\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.08355209231376648  to: 0.08342960476875305\n",
      "Training iteration: 2072\n",
      "Improved validation loss from: 0.08342960476875305  to: 0.08331300616264344\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.08331300616264344  to: 0.08318822979927062\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.08318822979927062  to: 0.08305503129959106\n",
      "Training iteration: 2075\n",
      "Improved validation loss from: 0.08305503129959106  to: 0.08291419744491577\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.08291419744491577  to: 0.08277932405471802\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.08277932405471802  to: 0.08264979124069213\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.08264979124069213  to: 0.08252478837966919\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.08252478837966919  to: 0.08240335583686828\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.08240335583686828  to: 0.08225821256637574\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.08225821256637574  to: 0.08211904764175415\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.08211904764175415  to: 0.08198592066764832\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.08198592066764832  to: 0.08185860514640808\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.08185860514640808  to: 0.08172274827957153\n",
      "Training iteration: 2085\n",
      "Improved validation loss from: 0.08172274827957153  to: 0.08157928586006165\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.08157928586006165  to: 0.08142935633659362\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.08142935633659362  to: 0.08128824234008789\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.08128824234008789  to: 0.08115520477294921\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.08115520477294921  to: 0.08100064992904663\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.08100064992904663  to: 0.08085650205612183\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.08085650205612183  to: 0.08072223663330078\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.08072223663330078  to: 0.08058195114135742\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.08058195114135742  to: 0.08043600916862488\n",
      "Training iteration: 2094\n",
      "Improved validation loss from: 0.08043600916862488  to: 0.08028484582901001\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.08028484582901001  to: 0.08012933731079101\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.08012933731079101  to: 0.07998573780059814\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.07998573780059814  to: 0.07985296249389648\n",
      "Training iteration: 2098\n",
      "Improved validation loss from: 0.07985296249389648  to: 0.07969846725463867\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.07969846725463867  to: 0.07955631613731384\n",
      "Training iteration: 2100\n",
      "Improved validation loss from: 0.07955631613731384  to: 0.07942523956298828\n",
      "Training iteration: 2101\n",
      "Improved validation loss from: 0.07942523956298828  to: 0.07927268147468566\n",
      "Training iteration: 2102\n",
      "Improved validation loss from: 0.07927268147468566  to: 0.07911671996116638\n",
      "Training iteration: 2103\n",
      "Improved validation loss from: 0.07911671996116638  to: 0.07897415161132812\n",
      "Training iteration: 2104\n",
      "Improved validation loss from: 0.07897415161132812  to: 0.07882663011550903\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.07882663011550903  to: 0.07865859270095825\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.07865859270095825  to: 0.07849479913711548\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.07849479913711548  to: 0.07833558917045594\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.07833558917045594  to: 0.07816382646560668\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.07816382646560668  to: 0.07798267602920532\n",
      "Training iteration: 2110\n",
      "Improved validation loss from: 0.07798267602920532  to: 0.07779491543769837\n",
      "Training iteration: 2111\n",
      "Improved validation loss from: 0.07779491543769837  to: 0.07758101224899291\n",
      "Training iteration: 2112\n",
      "Improved validation loss from: 0.07758101224899291  to: 0.07741263508796692\n",
      "Training iteration: 2113\n",
      "Improved validation loss from: 0.07741263508796692  to: 0.07726055979728699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.07726055979728699  to: 0.07710346579551697\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.07710346579551697  to: 0.07694003582000733\n",
      "Training iteration: 2116\n",
      "Improved validation loss from: 0.07694003582000733  to: 0.07677050828933715\n",
      "Training iteration: 2117\n",
      "Improved validation loss from: 0.07677050828933715  to: 0.07659653425216675\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.07659653425216675  to: 0.07642077207565308\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.07642077207565308  to: 0.07624646425247192\n",
      "Training iteration: 2120\n",
      "Improved validation loss from: 0.07624646425247192  to: 0.07607676386833191\n",
      "Training iteration: 2121\n",
      "Improved validation loss from: 0.07607676386833191  to: 0.07594005465507507\n",
      "Training iteration: 2122\n",
      "Improved validation loss from: 0.07594005465507507  to: 0.07583295702934265\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.07583295702934265  to: 0.07572264075279236\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.07572264075279236  to: 0.07560577392578124\n",
      "Training iteration: 2125\n",
      "Improved validation loss from: 0.07560577392578124  to: 0.07548033595085143\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.07548033595085143  to: 0.07534594535827636\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.07534594535827636  to: 0.0752035915851593\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.0752035915851593  to: 0.07505530118942261\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.07505530118942261  to: 0.0749036967754364\n",
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.0749036967754364  to: 0.07477905154228211\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.07477905154228211  to: 0.07464895248413086\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.07464895248413086  to: 0.07446708679199218\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.07446708679199218  to: 0.07427065372467041\n",
      "Training iteration: 2134\n",
      "Improved validation loss from: 0.07427065372467041  to: 0.07413079142570496\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.07413079142570496  to: 0.07397173643112183\n",
      "Training iteration: 2136\n",
      "Improved validation loss from: 0.07397173643112183  to: 0.07376171946525574\n",
      "Training iteration: 2137\n",
      "Improved validation loss from: 0.07376171946525574  to: 0.07350842356681823\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.07350842356681823  to: 0.07325388193130493\n",
      "Training iteration: 2139\n",
      "Improved validation loss from: 0.07325388193130493  to: 0.07300490140914917\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.07300490140914917  to: 0.0727660596370697\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.0727660596370697  to: 0.07258650660514832\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.07258650660514832  to: 0.07241142392158509\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.07241142392158509  to: 0.07222046852111816\n",
      "Training iteration: 2144\n",
      "Improved validation loss from: 0.07222046852111816  to: 0.07196801304817199\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.07196801304817199  to: 0.0717370092868805\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.0717370092868805  to: 0.07162297368049622\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.07162297368049622  to: 0.07151643633842468\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.07151643633842468  to: 0.07133742570877075\n",
      "Training iteration: 2149\n",
      "Improved validation loss from: 0.07133742570877075  to: 0.07110106348991393\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.07110106348991393  to: 0.07089430689811707\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.07089430689811707  to: 0.07080653309822083\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.07080653309822083  to: 0.07073203921318054\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.07073203921318054  to: 0.07064265012741089\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.07064265012741089  to: 0.07044578790664673\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.07044578790664673  to: 0.07026299238204955\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.07026299238204955  to: 0.07011808156967163\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.07011808156967163  to: 0.07000690102577209\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.07000690102577209  to: 0.06989966034889221\n",
      "Training iteration: 2159\n",
      "Improved validation loss from: 0.06989966034889221  to: 0.06968461871147155\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.06968461871147155  to: 0.06948577761650085\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.06948577761650085  to: 0.0693062961101532\n",
      "Training iteration: 2162\n",
      "Improved validation loss from: 0.0693062961101532  to: 0.069171142578125\n",
      "Training iteration: 2163\n",
      "Improved validation loss from: 0.069171142578125  to: 0.06905196905136109\n",
      "Training iteration: 2164\n",
      "Improved validation loss from: 0.06905196905136109  to: 0.0689026653766632\n",
      "Training iteration: 2165\n",
      "Improved validation loss from: 0.0689026653766632  to: 0.06872501373291015\n",
      "Training iteration: 2166\n",
      "Improved validation loss from: 0.06872501373291015  to: 0.06855028867721558\n",
      "Training iteration: 2167\n",
      "Improved validation loss from: 0.06855028867721558  to: 0.06842988729476929\n",
      "Training iteration: 2168\n",
      "Improved validation loss from: 0.06842988729476929  to: 0.06832629442214966\n",
      "Training iteration: 2169\n",
      "Improved validation loss from: 0.06832629442214966  to: 0.06821178793907165\n",
      "Training iteration: 2170\n",
      "Improved validation loss from: 0.06821178793907165  to: 0.06808229684829711\n",
      "Training iteration: 2171\n",
      "Improved validation loss from: 0.06808229684829711  to: 0.06793758273124695\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.06793758273124695  to: 0.06778097748756409\n",
      "Training iteration: 2173\n",
      "Improved validation loss from: 0.06778097748756409  to: 0.06763544082641601\n",
      "Training iteration: 2174\n",
      "Improved validation loss from: 0.06763544082641601  to: 0.0675000011920929\n",
      "Training iteration: 2175\n",
      "Improved validation loss from: 0.0675000011920929  to: 0.06735233664512634\n",
      "Training iteration: 2176\n",
      "Improved validation loss from: 0.06735233664512634  to: 0.06719298958778382\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.06719298958778382  to: 0.06704427599906922\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.06704427599906922  to: 0.06685410141944885\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.06685410141944885  to: 0.06670977473258972\n",
      "Training iteration: 2180\n",
      "Improved validation loss from: 0.06670977473258972  to: 0.06658013463020325\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.06658013463020325  to: 0.0664033591747284\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.0664033591747284  to: 0.06619252562522888\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.06619252562522888  to: 0.06607604026794434\n",
      "Training iteration: 2184\n",
      "Improved validation loss from: 0.06607604026794434  to: 0.06602056622505188\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.06602056622505188  to: 0.06574655175209046\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.06574655175209046  to: 0.06549133658409119\n",
      "Training iteration: 2187\n",
      "Improved validation loss from: 0.06549133658409119  to: 0.06533588171005249\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.06533588171005249  to: 0.06524578332901002\n",
      "Training iteration: 2189\n",
      "Improved validation loss from: 0.06524578332901002  to: 0.06491428017616271\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.06491428017616271  to: 0.06462308168411254\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.06462308168411254  to: 0.06444560885429382\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.06444560885429382  to: 0.06437680125236511\n",
      "Training iteration: 2193\n",
      "Improved validation loss from: 0.06437680125236511  to: 0.06421927213668824\n",
      "Training iteration: 2194\n",
      "Improved validation loss from: 0.06421927213668824  to: 0.06398805975914001\n",
      "Training iteration: 2195\n",
      "Improved validation loss from: 0.06398805975914001  to: 0.0638695478439331\n",
      "Training iteration: 2196\n",
      "Improved validation loss from: 0.0638695478439331  to: 0.06384645104408264\n",
      "Training iteration: 2197\n",
      "Improved validation loss from: 0.06384645104408264  to: 0.0637109935283661\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.0637109935283661  to: 0.06350196599960327\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.06350196599960327  to: 0.06332935094833374\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.06332935094833374  to: 0.06318613886833191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.06318613886833191  to: 0.06305823922157287\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.06305823922157287  to: 0.06282503008842469\n",
      "Training iteration: 2203\n",
      "Improved validation loss from: 0.06282503008842469  to: 0.06255511045455933\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.06255511045455933  to: 0.06234381794929504\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.06234381794929504  to: 0.06218155026435852\n",
      "Training iteration: 2206\n",
      "Improved validation loss from: 0.06218155026435852  to: 0.06203662753105164\n",
      "Training iteration: 2207\n",
      "Improved validation loss from: 0.06203662753105164  to: 0.06184713244438171\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.06184713244438171  to: 0.06163147687911987\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.06163147687911987  to: 0.061472725868225095\n",
      "Training iteration: 2210\n",
      "Improved validation loss from: 0.061472725868225095  to: 0.0613591194152832\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.0613591194152832  to: 0.06121413111686706\n",
      "Training iteration: 2212\n",
      "Improved validation loss from: 0.06121413111686706  to: 0.06104271411895752\n",
      "Training iteration: 2213\n",
      "Improved validation loss from: 0.06104271411895752  to: 0.060916244983673096\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.060916244983673096  to: 0.06083105802536011\n",
      "Training iteration: 2215\n",
      "Improved validation loss from: 0.06083105802536011  to: 0.060704803466796874\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.060704803466796874  to: 0.06051913499832153\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.06051913499832153  to: 0.06038259267807007\n",
      "Training iteration: 2218\n",
      "Improved validation loss from: 0.06038259267807007  to: 0.060280829668045044\n",
      "Training iteration: 2219\n",
      "Improved validation loss from: 0.060280829668045044  to: 0.060168856382369997\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.060168856382369997  to: 0.05990577936172485\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.05990577936172485  to: 0.059700673818588255\n",
      "Training iteration: 2222\n",
      "Improved validation loss from: 0.059700673818588255  to: 0.05954987406730652\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.05954987406730652  to: 0.05937434434890747\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.05937434434890747  to: 0.05918186902999878\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.05918186902999878  to: 0.05904421806335449\n",
      "Training iteration: 2226\n",
      "Improved validation loss from: 0.05904421806335449  to: 0.05892890691757202\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.05892890691757202  to: 0.058811652660369876\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.058811652660369876  to: 0.058696252107620236\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.058696252107620236  to: 0.058504235744476316\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.058504235744476316  to: 0.05836046934127807\n",
      "Training iteration: 2231\n",
      "Improved validation loss from: 0.05836046934127807  to: 0.05804339647293091\n",
      "Training iteration: 2232\n",
      "Improved validation loss from: 0.05804339647293091  to: 0.05782055854797363\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.05782055854797363  to: 0.05767580270767212\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.05767580270767212  to: 0.0575428307056427\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.0575428307056427  to: 0.05739496946334839\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.05739496946334839  to: 0.05727687478065491\n",
      "Training iteration: 2237\n",
      "Improved validation loss from: 0.05727687478065491  to: 0.057212436199188234\n",
      "Training iteration: 2238\n",
      "Improved validation loss from: 0.057212436199188234  to: 0.05721201300621033\n",
      "Training iteration: 2239\n",
      "Improved validation loss from: 0.05721201300621033  to: 0.057104289531707764\n",
      "Training iteration: 2240\n",
      "Improved validation loss from: 0.057104289531707764  to: 0.05697348713874817\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.05697348713874817  to: 0.056846797466278076\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.056846797466278076  to: 0.05671671628952026\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.05671671628952026  to: 0.05658639073371887\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.05658639073371887  to: 0.05639598965644836\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.05639598965644836  to: 0.056204235553741454\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.056204235553741454  to: 0.05603740811347961\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.05603740811347961  to: 0.05592924952507019\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.05592924952507019  to: 0.055822867155075076\n",
      "Training iteration: 2249\n",
      "Improved validation loss from: 0.055822867155075076  to: 0.05567149519920349\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.05567149519920349  to: 0.05554012060165405\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.05554012060165405  to: 0.05541682243347168\n",
      "Training iteration: 2252\n",
      "Improved validation loss from: 0.05541682243347168  to: 0.05529762506484985\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.05529762506484985  to: 0.05518333911895752\n",
      "Training iteration: 2254\n",
      "Improved validation loss from: 0.05518333911895752  to: 0.055052530765533444\n",
      "Training iteration: 2255\n",
      "Improved validation loss from: 0.055052530765533444  to: 0.054931086301803586\n",
      "Training iteration: 2256\n",
      "Improved validation loss from: 0.054931086301803586  to: 0.05480700731277466\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.05480700731277466  to: 0.0546953558921814\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.0546953558921814  to: 0.054563283920288086\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.054563283920288086  to: 0.05444524884223938\n",
      "Training iteration: 2260\n",
      "Improved validation loss from: 0.05444524884223938  to: 0.054333817958831784\n",
      "Training iteration: 2261\n",
      "Improved validation loss from: 0.054333817958831784  to: 0.054220926761627194\n",
      "Training iteration: 2262\n",
      "Improved validation loss from: 0.054220926761627194  to: 0.05414396524429321\n",
      "Training iteration: 2263\n",
      "Improved validation loss from: 0.05414396524429321  to: 0.05402727127075195\n",
      "Training iteration: 2264\n",
      "Improved validation loss from: 0.05402727127075195  to: 0.053895384073257446\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.053895384073257446  to: 0.053789150714874265\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.053789150714874265  to: 0.05367298722267151\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.05367298722267151  to: 0.05357722640037536\n",
      "Training iteration: 2268\n",
      "Improved validation loss from: 0.05357722640037536  to: 0.05346372723579407\n",
      "Training iteration: 2269\n",
      "Improved validation loss from: 0.05346372723579407  to: 0.053337597846984865\n",
      "Training iteration: 2270\n",
      "Improved validation loss from: 0.053337597846984865  to: 0.05322929620742798\n",
      "Training iteration: 2271\n",
      "Improved validation loss from: 0.05322929620742798  to: 0.053121691942214964\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.053121691942214964  to: 0.05303784012794495\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.05303784012794495  to: 0.05292907953262329\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.05292907953262329  to: 0.05282782316207886\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.05282782316207886  to: 0.052733737230300906\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.052733737230300906  to: 0.052629363536834714\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.052629363536834714  to: 0.052573806047439574\n",
      "Training iteration: 2278\n",
      "Improved validation loss from: 0.052573806047439574  to: 0.052476131916046144\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.052476131916046144  to: 0.05234538316726685\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.05234538316726685  to: 0.052247142791748045\n",
      "Training iteration: 2281\n",
      "Improved validation loss from: 0.052247142791748045  to: 0.05214227437973022\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.05214227437973022  to: 0.05204092264175415\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.05204092264175415  to: 0.05193224549293518\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.05193224549293518  to: 0.051815569400787354\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.051815569400787354  to: 0.05169963836669922\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.05169963836669922  to: 0.051587247848510744\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.051587247848510744  to: 0.05149737596511841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2288\n",
      "Improved validation loss from: 0.05149737596511841  to: 0.0513785719871521\n",
      "Training iteration: 2289\n",
      "Improved validation loss from: 0.0513785719871521  to: 0.051276499032974245\n",
      "Training iteration: 2290\n",
      "Improved validation loss from: 0.051276499032974245  to: 0.05117871165275574\n",
      "Training iteration: 2291\n",
      "Improved validation loss from: 0.05117871165275574  to: 0.05108073353767395\n",
      "Training iteration: 2292\n",
      "Improved validation loss from: 0.05108073353767395  to: 0.05100163221359253\n",
      "Training iteration: 2293\n",
      "Improved validation loss from: 0.05100163221359253  to: 0.05089101195335388\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.05089101195335388  to: 0.05079963803291321\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.05079963803291321  to: 0.05069934129714966\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.05069934129714966  to: 0.05059110522270203\n",
      "Training iteration: 2297\n",
      "Improved validation loss from: 0.05059110522270203  to: 0.05048820376396179\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.05048820376396179  to: 0.05036112666130066\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.05036112666130066  to: 0.050247412919998166\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.050247412919998166  to: 0.050131368637084964\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.050131368637084964  to: 0.05003188848495484\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.05003188848495484  to: 0.04991572797298431\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.04991572797298431  to: 0.04979277551174164\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.04979277551174164  to: 0.04968419075012207\n",
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.04968419075012207  to: 0.04957871437072754\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.04957871437072754  to: 0.04951272904872894\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.04951272904872894  to: 0.049403971433639525\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.049403971433639525  to: 0.049285000562667845\n",
      "Training iteration: 2309\n",
      "Improved validation loss from: 0.049285000562667845  to: 0.04919218122959137\n",
      "Training iteration: 2310\n",
      "Improved validation loss from: 0.04919218122959137  to: 0.049091243743896486\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.049091243743896486  to: 0.049012559652328494\n",
      "Training iteration: 2312\n",
      "Improved validation loss from: 0.049012559652328494  to: 0.048811396956443785\n",
      "Training iteration: 2313\n",
      "Improved validation loss from: 0.048811396956443785  to: 0.048650503158569336\n",
      "Training iteration: 2314\n",
      "Improved validation loss from: 0.048650503158569336  to: 0.04852601587772369\n",
      "Training iteration: 2315\n",
      "Improved validation loss from: 0.04852601587772369  to: 0.04840819239616394\n",
      "Training iteration: 2316\n",
      "Improved validation loss from: 0.04840819239616394  to: 0.04835419654846192\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.04835419654846192  to: 0.048275727033615115\n",
      "Training iteration: 2318\n",
      "Improved validation loss from: 0.048275727033615115  to: 0.04814133644104004\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.04814133644104004  to: 0.04803184866905212\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.04803184866905212  to: 0.047928839921951294\n",
      "Training iteration: 2321\n",
      "Improved validation loss from: 0.047928839921951294  to: 0.04781618118286133\n",
      "Training iteration: 2322\n",
      "Improved validation loss from: 0.04781618118286133  to: 0.047629132866859436\n",
      "Training iteration: 2323\n",
      "Improved validation loss from: 0.047629132866859436  to: 0.04751102924346924\n",
      "Training iteration: 2324\n",
      "Improved validation loss from: 0.04751102924346924  to: 0.04742496907711029\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.04742496907711029  to: 0.04733784198760986\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.04733784198760986  to: 0.047327613830566405\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.047327613830566405  to: 0.047219276428222656\n",
      "Training iteration: 2328\n",
      "Improved validation loss from: 0.047219276428222656  to: 0.04709059298038483\n",
      "Training iteration: 2329\n",
      "Improved validation loss from: 0.04709059298038483  to: 0.04699017405509949\n",
      "Training iteration: 2330\n",
      "Improved validation loss from: 0.04699017405509949  to: 0.046864986419677734\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.046864986419677734  to: 0.046826180815696714\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.046826180815696714  to: 0.04671229422092438\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.04671229422092438  to: 0.04658598899841308\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.04658598899841308  to: 0.04649526476860046\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.04649526476860046  to: 0.04638589024543762\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.04638589024543762  to: 0.04629994332790375\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.04629994332790375  to: 0.04614904522895813\n",
      "Training iteration: 2338\n",
      "Improved validation loss from: 0.04614904522895813  to: 0.04601648449897766\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.04601648449897766  to: 0.04593367576599121\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.04593367576599121  to: 0.04581132829189301\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.04581132829189301  to: 0.0457561582326889\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.04578316807746887\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.0457709401845932\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.0457561582326889  to: 0.04570259153842926\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.04570259153842926  to: 0.04546526074409485\n",
      "Training iteration: 2346\n",
      "Improved validation loss from: 0.04546526074409485  to: 0.04521288871765137\n",
      "Training iteration: 2347\n",
      "Improved validation loss from: 0.04521288871765137  to: 0.04505851268768311\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.04505851268768311  to: 0.044956055283546445\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.044956055283546445  to: 0.044926118850708005\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.04495348930358887\n",
      "Training iteration: 2351\n",
      "Improved validation loss from: 0.044926118850708005  to: 0.044913411140441895\n",
      "Training iteration: 2352\n",
      "Improved validation loss from: 0.044913411140441895  to: 0.044760435819625854\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.044760435819625854  to: 0.044558706879615786\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.044558706879615786  to: 0.044354248046875\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.044354248046875  to: 0.0442349910736084\n",
      "Training iteration: 2356\n",
      "Improved validation loss from: 0.0442349910736084  to: 0.044156217575073244\n",
      "Training iteration: 2357\n",
      "Improved validation loss from: 0.044156217575073244  to: 0.0441456139087677\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.044176870584487916\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.044163265824317934\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.0441456139087677  to: 0.0440110594034195\n",
      "Training iteration: 2361\n",
      "Improved validation loss from: 0.0440110594034195  to: 0.04378992915153503\n",
      "Training iteration: 2362\n",
      "Improved validation loss from: 0.04378992915153503  to: 0.04357694089412689\n",
      "Training iteration: 2363\n",
      "Improved validation loss from: 0.04357694089412689  to: 0.04343401789665222\n",
      "Training iteration: 2364\n",
      "Improved validation loss from: 0.04343401789665222  to: 0.043342262506484985\n",
      "Training iteration: 2365\n",
      "Improved validation loss from: 0.043342262506484985  to: 0.043293365836143495\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.0433373361825943\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.04335331916809082\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.043293365836143495  to: 0.04327716827392578\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.04327716827392578  to: 0.04310022294521332\n",
      "Training iteration: 2370\n",
      "Improved validation loss from: 0.04310022294521332  to: 0.04287203848361969\n",
      "Training iteration: 2371\n",
      "Improved validation loss from: 0.04287203848361969  to: 0.04270858764648437\n",
      "Training iteration: 2372\n",
      "Improved validation loss from: 0.04270858764648437  to: 0.04261099696159363\n",
      "Training iteration: 2373\n",
      "Improved validation loss from: 0.04261099696159363  to: 0.0425288051366806\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.0425288051366806  to: 0.042523184418678285\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.04252967834472656\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.042523184418678285  to: 0.04247262477874756\n",
      "Training iteration: 2377\n",
      "Improved validation loss from: 0.04247262477874756  to: 0.04235703945159912\n",
      "Training iteration: 2378\n",
      "Improved validation loss from: 0.04235703945159912  to: 0.0421434223651886\n",
      "Training iteration: 2379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.0421434223651886  to: 0.041952204704284665\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.041952204704284665  to: 0.0418170154094696\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.0418170154094696  to: 0.04172375202178955\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.04172375202178955  to: 0.04167444109916687\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.04168636798858642\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.04171539843082428\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.04175488948822022\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.04167444109916687  to: 0.04167106151580811\n",
      "Training iteration: 2387\n",
      "Improved validation loss from: 0.04167106151580811  to: 0.04144894182682037\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.04144894182682037  to: 0.04125115871429443\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.04125115871429443  to: 0.04109482765197754\n",
      "Training iteration: 2390\n",
      "Improved validation loss from: 0.04109482765197754  to: 0.04102873206138611\n",
      "Training iteration: 2391\n",
      "Improved validation loss from: 0.04102873206138611  to: 0.040959349274635314\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.040959349274635314  to: 0.040924477577209475\n",
      "Training iteration: 2393\n",
      "Validation loss (no improvement): 0.040944534540176394\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): 0.0409454882144928\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.040924477577209475  to: 0.040869927406311034\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.040869927406311034  to: 0.04073391556739807\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.04073391556739807  to: 0.04059272408485413\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.04059272408485413  to: 0.040471434593200684\n",
      "Training iteration: 2399\n",
      "Improved validation loss from: 0.040471434593200684  to: 0.04041902422904968\n",
      "Training iteration: 2400\n",
      "Improved validation loss from: 0.04041902422904968  to: 0.040369391441345215\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: 0.040369391441345215  to: 0.040315094590187076\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.040315094590187076  to: 0.04020842611789703\n",
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.04020842611789703  to: 0.04015337824821472\n",
      "Training iteration: 2404\n",
      "Improved validation loss from: 0.04015337824821472  to: 0.040037012100219725\n",
      "Training iteration: 2405\n",
      "Improved validation loss from: 0.040037012100219725  to: 0.03989459872245789\n",
      "Training iteration: 2406\n",
      "Improved validation loss from: 0.03989459872245789  to: 0.03979218900203705\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.03979218900203705  to: 0.03970899879932403\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.03970899879932403  to: 0.039671093225479126\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.039671093225479126  to: 0.03963684439659119\n",
      "Training iteration: 2410\n",
      "Improved validation loss from: 0.03963684439659119  to: 0.03960219919681549\n",
      "Training iteration: 2411\n",
      "Improved validation loss from: 0.03960219919681549  to: 0.039486974477767944\n",
      "Training iteration: 2412\n",
      "Improved validation loss from: 0.039486974477767944  to: 0.039306095242500304\n",
      "Training iteration: 2413\n",
      "Improved validation loss from: 0.039306095242500304  to: 0.03921790719032288\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.03921810984611511\n",
      "Training iteration: 2415\n",
      "Improved validation loss from: 0.03921790719032288  to: 0.03919643759727478\n",
      "Training iteration: 2416\n",
      "Improved validation loss from: 0.03919643759727478  to: 0.039173784852027896\n",
      "Training iteration: 2417\n",
      "Improved validation loss from: 0.039173784852027896  to: 0.03908537924289703\n",
      "Training iteration: 2418\n",
      "Improved validation loss from: 0.03908537924289703  to: 0.03898048102855682\n",
      "Training iteration: 2419\n",
      "Improved validation loss from: 0.03898048102855682  to: 0.03883813917636871\n",
      "Training iteration: 2420\n",
      "Improved validation loss from: 0.03883813917636871  to: 0.03874042630195618\n",
      "Training iteration: 2421\n",
      "Improved validation loss from: 0.03874042630195618  to: 0.03870757520198822\n",
      "Training iteration: 2422\n",
      "Improved validation loss from: 0.03870757520198822  to: 0.03863682746887207\n",
      "Training iteration: 2423\n",
      "Improved validation loss from: 0.03863682746887207  to: 0.0385452926158905\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.0385452926158905  to: 0.03850006759166717\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.03850006759166717  to: 0.03846201300621033\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: 0.03846201300621033  to: 0.03831959664821625\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.03831959664821625  to: 0.0382068932056427\n",
      "Training iteration: 2428\n",
      "Improved validation loss from: 0.0382068932056427  to: 0.038139048218727115\n",
      "Training iteration: 2429\n",
      "Improved validation loss from: 0.038139048218727115  to: 0.0381146490573883\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.03815816640853882\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.03818327784538269\n",
      "Training iteration: 2432\n",
      "Improved validation loss from: 0.0381146490573883  to: 0.038027364015579226\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.038027364015579226  to: 0.037768617272377014\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: 0.037768617272377014  to: 0.03760554194450379\n",
      "Training iteration: 2435\n",
      "Improved validation loss from: 0.03760554194450379  to: 0.037510961294174194\n",
      "Training iteration: 2436\n",
      "Improved validation loss from: 0.037510961294174194  to: 0.03750570416450501\n",
      "Training iteration: 2437\n",
      "Improved validation loss from: 0.03750570416450501  to: 0.03746370673179626\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.03747831881046295\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.03746370673179626  to: 0.0374483197927475\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.0374483197927475  to: 0.03733397126197815\n",
      "Training iteration: 2441\n",
      "Improved validation loss from: 0.03733397126197815  to: 0.03723909556865692\n",
      "Training iteration: 2442\n",
      "Improved validation loss from: 0.03723909556865692  to: 0.03722098171710968\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.03722098171710968  to: 0.03712217807769776\n",
      "Training iteration: 2444\n",
      "Improved validation loss from: 0.03712217807769776  to: 0.03699585497379303\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.03699585497379303  to: 0.03689814209938049\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.03689814209938049  to: 0.036825919151306154\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.036825919151306154  to: 0.036811068654060364\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.036811068654060364  to: 0.03673838973045349\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.03673838973045349  to: 0.03665814399719238\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.03665814399719238  to: 0.03657824397087097\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.03657824397087097  to: 0.036512070894241334\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.036512070894241334  to: 0.03646177649497986\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.03646177649497986  to: 0.036422109603881835\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.036422109603881835  to: 0.036308574676513675\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.036308574676513675  to: 0.036150962114334106\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.036150962114334106  to: 0.03601146340370178\n",
      "Training iteration: 2457\n",
      "Improved validation loss from: 0.03601146340370178  to: 0.035975536704063414\n",
      "Training iteration: 2458\n",
      "Improved validation loss from: 0.035975536704063414  to: 0.03585720658302307\n",
      "Training iteration: 2459\n",
      "Improved validation loss from: 0.03585720658302307  to: 0.03570388853549957\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.03571593165397644\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): 0.03570567667484283\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.035712847113609315\n",
      "Training iteration: 2463\n",
      "Improved validation loss from: 0.03570388853549957  to: 0.03562367558479309\n",
      "Training iteration: 2464\n",
      "Improved validation loss from: 0.03562367558479309  to: 0.03545881807804108\n",
      "Training iteration: 2465\n",
      "Improved validation loss from: 0.03545881807804108  to: 0.03541122376918793\n",
      "Training iteration: 2466\n",
      "Improved validation loss from: 0.03541122376918793  to: 0.03533588051795959\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.03533588051795959  to: 0.0352655291557312\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.03528672158718109\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.0352655291557312  to: 0.035262465476989746\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.03531085848808289\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.03539285063743591\n",
      "Training iteration: 2472\n",
      "Improved validation loss from: 0.035262465476989746  to: 0.03522750437259674\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.03522750437259674  to: 0.03512313663959503\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.03512313663959503  to: 0.03490208089351654\n",
      "Training iteration: 2475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03490208089351654  to: 0.034844061732292174\n",
      "Training iteration: 2476\n",
      "Improved validation loss from: 0.034844061732292174  to: 0.03482903838157654\n",
      "Training iteration: 2477\n",
      "Improved validation loss from: 0.03482903838157654  to: 0.03475523293018341\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.03486044704914093\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.03488009572029114\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.03481667041778565\n",
      "Training iteration: 2481\n",
      "Improved validation loss from: 0.03475523293018341  to: 0.03472043573856354\n",
      "Training iteration: 2482\n",
      "Improved validation loss from: 0.03472043573856354  to: 0.034618452191352844\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.034618452191352844  to: 0.034573882818222046\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.034573882818222046  to: 0.03450311720371246\n",
      "Training iteration: 2485\n",
      "Improved validation loss from: 0.03450311720371246  to: 0.03449023365974426\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.03449321985244751\n",
      "Training iteration: 2487\n",
      "Improved validation loss from: 0.03449023365974426  to: 0.034432634711265564\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.034432634711265564  to: 0.03426886200904846\n",
      "Training iteration: 2489\n",
      "Improved validation loss from: 0.03426886200904846  to: 0.03418148159980774\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.03418148159980774  to: 0.03410904109477997\n",
      "Training iteration: 2491\n",
      "Improved validation loss from: 0.03410904109477997  to: 0.03407665193080902\n",
      "Training iteration: 2492\n",
      "Improved validation loss from: 0.03407665193080902  to: 0.033995649218559264\n",
      "Training iteration: 2493\n",
      "Improved validation loss from: 0.033995649218559264  to: 0.033855533599853514\n",
      "Training iteration: 2494\n",
      "Improved validation loss from: 0.033855533599853514  to: 0.03372792899608612\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.03372792899608612  to: 0.03364655077457428\n",
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.03364655077457428  to: 0.03356907665729523\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.03356907665729523  to: 0.03347947597503662\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.03347947597503662  to: 0.03341212868690491\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.03341212868690491  to: 0.033303076028823854\n",
      "Training iteration: 2500\n",
      "Improved validation loss from: 0.033303076028823854  to: 0.03317731022834778\n",
      "Training iteration: 2501\n",
      "Improved validation loss from: 0.03317731022834778  to: 0.0330172449350357\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.0330172449350357  to: 0.032893434166908264\n",
      "Training iteration: 2503\n",
      "Improved validation loss from: 0.032893434166908264  to: 0.03276990652084351\n",
      "Training iteration: 2504\n",
      "Improved validation loss from: 0.03276990652084351  to: 0.032666489481925964\n",
      "Training iteration: 2505\n",
      "Improved validation loss from: 0.032666489481925964  to: 0.03261602520942688\n",
      "Training iteration: 2506\n",
      "Improved validation loss from: 0.03261602520942688  to: 0.03256737589836121\n",
      "Training iteration: 2507\n",
      "Improved validation loss from: 0.03256737589836121  to: 0.032488292455673216\n",
      "Training iteration: 2508\n",
      "Improved validation loss from: 0.032488292455673216  to: 0.03235190212726593\n",
      "Training iteration: 2509\n",
      "Improved validation loss from: 0.03235190212726593  to: 0.032181525230407716\n",
      "Training iteration: 2510\n",
      "Improved validation loss from: 0.032181525230407716  to: 0.032003939151763916\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.032003939151763916  to: 0.031917798519134524\n",
      "Training iteration: 2512\n",
      "Improved validation loss from: 0.031917798519134524  to: 0.031794360280036925\n",
      "Training iteration: 2513\n",
      "Improved validation loss from: 0.031794360280036925  to: 0.03167000114917755\n",
      "Training iteration: 2514\n",
      "Improved validation loss from: 0.03167000114917755  to: 0.031622156500816345\n",
      "Training iteration: 2515\n",
      "Improved validation loss from: 0.031622156500816345  to: 0.031621402502059935\n",
      "Training iteration: 2516\n",
      "Improved validation loss from: 0.031621402502059935  to: 0.03156565725803375\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.03156565725803375  to: 0.03140885829925537\n",
      "Training iteration: 2518\n",
      "Improved validation loss from: 0.03140885829925537  to: 0.03125692307949066\n",
      "Training iteration: 2519\n",
      "Improved validation loss from: 0.03125692307949066  to: 0.031140002608299255\n",
      "Training iteration: 2520\n",
      "Improved validation loss from: 0.031140002608299255  to: 0.03113127648830414\n",
      "Training iteration: 2521\n",
      "Improved validation loss from: 0.03113127648830414  to: 0.031071051955223083\n",
      "Training iteration: 2522\n",
      "Improved validation loss from: 0.031071051955223083  to: 0.030933716893196107\n",
      "Training iteration: 2523\n",
      "Improved validation loss from: 0.030933716893196107  to: 0.030911201238632204\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: 0.030911201238632204  to: 0.03087540566921234\n",
      "Training iteration: 2525\n",
      "Improved validation loss from: 0.03087540566921234  to: 0.030811113119125367\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.030811113119125367  to: 0.03072988986968994\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.03072988986968994  to: 0.030593046545982362\n",
      "Training iteration: 2528\n",
      "Improved validation loss from: 0.030593046545982362  to: 0.03052328824996948\n",
      "Training iteration: 2529\n",
      "Improved validation loss from: 0.03052328824996948  to: 0.03048650622367859\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.030498629808425902\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.03048650622367859  to: 0.030435147881507873\n",
      "Training iteration: 2532\n",
      "Improved validation loss from: 0.030435147881507873  to: 0.030307632684707642\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.030307632684707642  to: 0.030243274569511414\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.030243274569511414  to: 0.03021174967288971\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.03021174967288971  to: 0.03021029829978943\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.03021029829978943  to: 0.030167606472969056\n",
      "Training iteration: 2537\n",
      "Improved validation loss from: 0.030167606472969056  to: 0.030100911855697632\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.030100911855697632  to: 0.029975700378417968\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.030080127716064452\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.030085307359695435\n",
      "Training iteration: 2541\n",
      "Improved validation loss from: 0.029975700378417968  to: 0.02989683747291565\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.02989683747291565  to: 0.0298003613948822\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.0298003613948822  to: 0.029788699746131898\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.0298402339220047\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.02988908290863037\n",
      "Training iteration: 2546\n",
      "Improved validation loss from: 0.029788699746131898  to: 0.02976105809211731\n",
      "Training iteration: 2547\n",
      "Improved validation loss from: 0.02976105809211731  to: 0.0296470046043396\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: 0.0296470046043396  to: 0.0296123743057251\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.02966112196445465\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.029774108529090883\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.02968338131904602\n",
      "Training iteration: 2552\n",
      "Improved validation loss from: 0.0296123743057251  to: 0.029476508498191833\n",
      "Training iteration: 2553\n",
      "Improved validation loss from: 0.029476508498191833  to: 0.029437446594238283\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.029453161358833312\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.029653188586235047\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.029704931378364562\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.029459136724472045\n",
      "Training iteration: 2558\n",
      "Improved validation loss from: 0.029437446594238283  to: 0.029367929697036742\n",
      "Training iteration: 2559\n",
      "Improved validation loss from: 0.029367929697036742  to: 0.02933611273765564\n",
      "Training iteration: 2560\n",
      "Improved validation loss from: 0.02933611273765564  to: 0.02929612100124359\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.029461377859115602\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.02944341003894806\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.02929612100124359  to: 0.029226690530776978\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.029226690530776978  to: 0.02909911572933197\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.02909911572933197  to: 0.02909334599971771\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.02920624613761902\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.0293815940618515\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.029388073086738586\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.02916691601276398\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.02909334599971771  to: 0.02902660667896271\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.02902660667896271  to: 0.028949046134948732\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.028996437788009644\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.028975200653076173\n",
      "Training iteration: 2574\n",
      "Improved validation loss from: 0.028949046134948732  to: 0.028873801231384277\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.028873801231384277  to: 0.02879455089569092\n",
      "Training iteration: 2576\n",
      "Improved validation loss from: 0.02879455089569092  to: 0.028790578246116638\n",
      "Training iteration: 2577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.028790578246116638  to: 0.028767722845077514\n",
      "Training iteration: 2578\n",
      "Improved validation loss from: 0.028767722845077514  to: 0.02866876721382141\n",
      "Training iteration: 2579\n",
      "Improved validation loss from: 0.02866876721382141  to: 0.02846207022666931\n",
      "Training iteration: 2580\n",
      "Improved validation loss from: 0.02846207022666931  to: 0.028332453966140748\n",
      "Training iteration: 2581\n",
      "Improved validation loss from: 0.028332453966140748  to: 0.028289920091629027\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.02831982672214508\n",
      "Training iteration: 2583\n",
      "Improved validation loss from: 0.028289920091629027  to: 0.02826092839241028\n",
      "Training iteration: 2584\n",
      "Improved validation loss from: 0.02826092839241028  to: 0.028084486722946167\n",
      "Training iteration: 2585\n",
      "Improved validation loss from: 0.028084486722946167  to: 0.02802192568778992\n",
      "Training iteration: 2586\n",
      "Improved validation loss from: 0.02802192568778992  to: 0.02799944281578064\n",
      "Training iteration: 2587\n",
      "Improved validation loss from: 0.02799944281578064  to: 0.027932342886924744\n",
      "Training iteration: 2588\n",
      "Improved validation loss from: 0.027932342886924744  to: 0.027746528387069702\n",
      "Training iteration: 2589\n",
      "Improved validation loss from: 0.027746528387069702  to: 0.027630075812339783\n",
      "Training iteration: 2590\n",
      "Improved validation loss from: 0.027630075812339783  to: 0.02762104868888855\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.0276652991771698\n",
      "Training iteration: 2592\n",
      "Improved validation loss from: 0.02762104868888855  to: 0.027521219849586488\n",
      "Training iteration: 2593\n",
      "Improved validation loss from: 0.027521219849586488  to: 0.027418294548988344\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.027418294548988344  to: 0.027398088574409486\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.027398088574409486  to: 0.02737303376197815\n",
      "Training iteration: 2596\n",
      "Improved validation loss from: 0.02737303376197815  to: 0.027297201752662658\n",
      "Training iteration: 2597\n",
      "Improved validation loss from: 0.027297201752662658  to: 0.027223682403564452\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.027261534333229066\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.02724793255329132\n",
      "Training iteration: 2600\n",
      "Improved validation loss from: 0.027223682403564452  to: 0.027139836549758913\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.027139836549758913  to: 0.027074331045150758\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.027074331045150758  to: 0.0270540714263916\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.02708476185798645\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.0270540714263916  to: 0.027001041173934936\n",
      "Training iteration: 2605\n",
      "Improved validation loss from: 0.027001041173934936  to: 0.026876750588417053\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.026876750588417053  to: 0.026848989725112914\n",
      "Training iteration: 2607\n",
      "Improved validation loss from: 0.026848989725112914  to: 0.026848664879798888\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.026893049478530884\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.026848664879798888  to: 0.0267982542514801\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.0267982542514801  to: 0.026675617694854735\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.026675617694854735  to: 0.02666814625263214\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): 0.026718005537986755\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.02670665383338928\n",
      "Training iteration: 2614\n",
      "Improved validation loss from: 0.02666814625263214  to: 0.026660501956939697\n",
      "Training iteration: 2615\n",
      "Improved validation loss from: 0.026660501956939697  to: 0.02663664221763611\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.026720207929611207\n",
      "Training iteration: 2617\n",
      "Improved validation loss from: 0.02663664221763611  to: 0.026613503694534302\n",
      "Training iteration: 2618\n",
      "Improved validation loss from: 0.026613503694534302  to: 0.02650415599346161\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.026510006189346312\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.026602965593338013\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.026575636863708497\n",
      "Training iteration: 2622\n",
      "Improved validation loss from: 0.02650415599346161  to: 0.026410308480262757\n",
      "Training iteration: 2623\n",
      "Improved validation loss from: 0.026410308480262757  to: 0.02632666230201721\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.02635067403316498\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.026524966955184935\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.026426512002944946\n",
      "Training iteration: 2627\n",
      "Improved validation loss from: 0.02632666230201721  to: 0.026261144876480104\n",
      "Training iteration: 2628\n",
      "Improved validation loss from: 0.026261144876480104  to: 0.02618294656276703\n",
      "Training iteration: 2629\n",
      "Improved validation loss from: 0.02618294656276703  to: 0.02616620659828186\n",
      "Training iteration: 2630\n",
      "Improved validation loss from: 0.02616620659828186  to: 0.026162362098693846\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.026238125562667847\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.02623600363731384\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.026172608137130737\n",
      "Training iteration: 2634\n",
      "Improved validation loss from: 0.026162362098693846  to: 0.026149564981460573\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.02626250684261322\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.026259467005729675\n",
      "Training iteration: 2637\n",
      "Improved validation loss from: 0.026149564981460573  to: 0.02613179087638855\n",
      "Training iteration: 2638\n",
      "Improved validation loss from: 0.02613179087638855  to: 0.026062837243080138\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.02608727216720581\n",
      "Training iteration: 2640\n",
      "Improved validation loss from: 0.026062837243080138  to: 0.026014941930770873\n",
      "Training iteration: 2641\n",
      "Improved validation loss from: 0.026014941930770873  to: 0.02597688138484955\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.02597688138484955  to: 0.025975638628005983\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.025975638628005983  to: 0.025957590341567992\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.025979143381118775\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.02595788836479187\n",
      "Training iteration: 2646\n",
      "Improved validation loss from: 0.025957590341567992  to: 0.02583358883857727\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.02583358883857727  to: 0.02573457360267639\n",
      "Training iteration: 2648\n",
      "Improved validation loss from: 0.02573457360267639  to: 0.025709733366966248\n",
      "Training iteration: 2649\n",
      "Improved validation loss from: 0.025709733366966248  to: 0.025701531767845155\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.025781190395355223\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.0257824569940567\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.025707381963729858\n",
      "Training iteration: 2653\n",
      "Improved validation loss from: 0.025701531767845155  to: 0.02567175030708313\n",
      "Training iteration: 2654\n",
      "Improved validation loss from: 0.02567175030708313  to: 0.025657767057418825\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.02572329044342041\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.025674182176589965\n",
      "Training iteration: 2657\n",
      "Improved validation loss from: 0.025657767057418825  to: 0.025511020421981813\n",
      "Training iteration: 2658\n",
      "Improved validation loss from: 0.025511020421981813  to: 0.02550099790096283\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.025561562180519103\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.02550569176673889\n",
      "Training iteration: 2661\n",
      "Improved validation loss from: 0.02550099790096283  to: 0.02548986077308655\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.02551235556602478\n",
      "Training iteration: 2663\n",
      "Improved validation loss from: 0.02548986077308655  to: 0.02539786696434021\n",
      "Training iteration: 2664\n",
      "Improved validation loss from: 0.02539786696434021  to: 0.02528608441352844\n",
      "Training iteration: 2665\n",
      "Improved validation loss from: 0.02528608441352844  to: 0.02524842619895935\n",
      "Training iteration: 2666\n",
      "Improved validation loss from: 0.02524842619895935  to: 0.025241294503211976\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.025242704153060912\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.025339818000793456\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.02534533143043518\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.025289055705070496\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.02528037130832672\n",
      "Training iteration: 2672\n",
      "Improved validation loss from: 0.025241294503211976  to: 0.025213021039962768\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.025213021039962768  to: 0.025197643041610717\n",
      "Training iteration: 2674\n",
      "Improved validation loss from: 0.025197643041610717  to: 0.02514343857765198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2675\n",
      "Improved validation loss from: 0.02514343857765198  to: 0.025041279196739197\n",
      "Training iteration: 2676\n",
      "Improved validation loss from: 0.025041279196739197  to: 0.025028350949287414\n",
      "Training iteration: 2677\n",
      "Validation loss (no improvement): 0.0250403493642807\n",
      "Training iteration: 2678\n",
      "Validation loss (no improvement): 0.025072336196899414\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.025203755497932433\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.02517881989479065\n",
      "Training iteration: 2681\n",
      "Improved validation loss from: 0.025028350949287414  to: 0.025009095668792725\n",
      "Training iteration: 2682\n",
      "Improved validation loss from: 0.025009095668792725  to: 0.024928569793701172\n",
      "Training iteration: 2683\n",
      "Improved validation loss from: 0.024928569793701172  to: 0.02490796148777008\n",
      "Training iteration: 2684\n",
      "Improved validation loss from: 0.02490796148777008  to: 0.024853411316871642\n",
      "Training iteration: 2685\n",
      "Improved validation loss from: 0.024853411316871642  to: 0.024845096468925475\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.024965567886829375\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.02504098415374756\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.024981069564819335\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.024936218559741975\n",
      "Training iteration: 2690\n",
      "Validation loss (no improvement): 0.02490454614162445\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.02490747720003128\n",
      "Training iteration: 2692\n",
      "Improved validation loss from: 0.024845096468925475  to: 0.02479080706834793\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.02479080706834793  to: 0.024705466628074647\n",
      "Training iteration: 2694\n",
      "Validation loss (no improvement): 0.02473544180393219\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.02478749305009842\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.024733157455921174\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.024705466628074647  to: 0.02460213899612427\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.024603167176246644\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.024638065695762636\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.02470533549785614\n",
      "Training iteration: 2701\n",
      "Improved validation loss from: 0.02460213899612427  to: 0.024560633301734924\n",
      "Training iteration: 2702\n",
      "Improved validation loss from: 0.024560633301734924  to: 0.024480465054512023\n",
      "Training iteration: 2703\n",
      "Improved validation loss from: 0.024480465054512023  to: 0.02437926232814789\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.024407276511192323\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.02437926232814789  to: 0.024269577860832215\n",
      "Training iteration: 2706\n",
      "Improved validation loss from: 0.024269577860832215  to: 0.024234811961650848\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.02428704798221588\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.02437816560268402\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.02432641535997391\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.02425740659236908\n",
      "Training iteration: 2711\n",
      "Improved validation loss from: 0.024234811961650848  to: 0.024228620529174804\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.02435555011034012\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.02429446280002594\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.024228620529174804  to: 0.024133047461509703\n",
      "Training iteration: 2715\n",
      "Improved validation loss from: 0.024133047461509703  to: 0.024100878834724428\n",
      "Training iteration: 2716\n",
      "Improved validation loss from: 0.024100878834724428  to: 0.02404446303844452\n",
      "Training iteration: 2717\n",
      "Improved validation loss from: 0.02404446303844452  to: 0.024024894833564757\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.024173951148986815\n",
      "Training iteration: 2719\n",
      "Validation loss (no improvement): 0.02421545684337616\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.024171623587608337\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.02406546175479889\n",
      "Training iteration: 2722\n",
      "Improved validation loss from: 0.024024894833564757  to: 0.02402428686618805\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.02402428686618805  to: 0.02397855818271637\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.024013908207416536\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.024001431465148926\n",
      "Training iteration: 2726\n",
      "Improved validation loss from: 0.02397855818271637  to: 0.023898625373840333\n",
      "Training iteration: 2727\n",
      "Improved validation loss from: 0.023898625373840333  to: 0.023897230625152588\n",
      "Training iteration: 2728\n",
      "Validation loss (no improvement): 0.02397473603487015\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.023998212814331055\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.02395467311143875\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.024020850658416748\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.024010154604911804\n",
      "Training iteration: 2733\n",
      "Improved validation loss from: 0.023897230625152588  to: 0.02389611452817917\n",
      "Training iteration: 2734\n",
      "Improved validation loss from: 0.02389611452817917  to: 0.0237916499376297\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): 0.023826441168785094\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.023825223743915557\n",
      "Training iteration: 2737\n",
      "Improved validation loss from: 0.0237916499376297  to: 0.023712074756622313\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.023838520050048828\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.02400849312543869\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.024096183478832245\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.023928947746753693\n",
      "Training iteration: 2742\n",
      "Improved validation loss from: 0.023712074756622313  to: 0.023659200966358186\n",
      "Training iteration: 2743\n",
      "Improved validation loss from: 0.023659200966358186  to: 0.023602190613746642\n",
      "Training iteration: 2744\n",
      "Improved validation loss from: 0.023602190613746642  to: 0.02360198050737381\n",
      "Training iteration: 2745\n",
      "Improved validation loss from: 0.02360198050737381  to: 0.023515155911445616\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.023537354171276094\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.023683376610279083\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.02370315045118332\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.023626832664012908\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.02355189621448517\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.02352306842803955\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.023515155911445616  to: 0.023405280709266663\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.023405280709266663  to: 0.023358364403247834\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): 0.023517036437988283\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.023549565672874452\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.023386940360069275\n",
      "Training iteration: 2757\n",
      "Improved validation loss from: 0.023358364403247834  to: 0.023327608406543732\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.0234085276722908\n",
      "Training iteration: 2759\n",
      "Improved validation loss from: 0.023327608406543732  to: 0.023274326324462892\n",
      "Training iteration: 2760\n",
      "Improved validation loss from: 0.023274326324462892  to: 0.023208534717559813\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.02327451705932617\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.02327919751405716\n",
      "Training iteration: 2763\n",
      "Improved validation loss from: 0.023208534717559813  to: 0.023175394535064696\n",
      "Training iteration: 2764\n",
      "Improved validation loss from: 0.023175394535064696  to: 0.02312496155500412\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.023168487846851347\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.023147554695606233\n",
      "Training iteration: 2767\n",
      "Improved validation loss from: 0.02312496155500412  to: 0.023084378242492674\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.023233553767204283\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.02316010743379593\n",
      "Training iteration: 2770\n",
      "Improved validation loss from: 0.023084378242492674  to: 0.0229146808385849\n",
      "Training iteration: 2771\n",
      "Improved validation loss from: 0.0229146808385849  to: 0.02288582772016525\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.022934103012084962\n",
      "Training iteration: 2773\n",
      "Improved validation loss from: 0.02288582772016525  to: 0.02287074029445648\n",
      "Training iteration: 2774\n",
      "Improved validation loss from: 0.02287074029445648  to: 0.022778400778770448\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.022887912392616273\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.023045940697193144\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.022950915992259978\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.022785639762878417\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.022815918922424315\n",
      "Training iteration: 2780\n",
      "Improved validation loss from: 0.022778400778770448  to: 0.02274257242679596\n",
      "Training iteration: 2781\n",
      "Improved validation loss from: 0.02274257242679596  to: 0.02264959067106247\n",
      "Training iteration: 2782\n",
      "Improved validation loss from: 0.02264959067106247  to: 0.02262335568666458\n",
      "Training iteration: 2783\n",
      "Improved validation loss from: 0.02262335568666458  to: 0.022592365741729736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2784\n",
      "Improved validation loss from: 0.022592365741729736  to: 0.022523431479930876\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.022532784938812257\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.022556018829345704\n",
      "Training iteration: 2787\n",
      "Improved validation loss from: 0.022523431479930876  to: 0.02250401973724365\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.02257945090532303\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.022621271014213563\n",
      "Training iteration: 2790\n",
      "Improved validation loss from: 0.02250401973724365  to: 0.02250142991542816\n",
      "Training iteration: 2791\n",
      "Improved validation loss from: 0.02250142991542816  to: 0.022477936744689942\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.022586879134178162\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.022538089752197267\n",
      "Training iteration: 2794\n",
      "Improved validation loss from: 0.022477936744689942  to: 0.022384795546531677\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.022437024116516113\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.022438950836658478\n",
      "Training iteration: 2797\n",
      "Improved validation loss from: 0.022384795546531677  to: 0.022225499153137207\n",
      "Training iteration: 2798\n",
      "Improved validation loss from: 0.022225499153137207  to: 0.02213466167449951\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.022189967334270477\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.02221904695034027\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.02216796875\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.022204384207725525\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.02233642339706421\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.022323937714099885\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.02221066951751709\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.022235509753227235\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): 0.022189822793006898\n",
      "Training iteration: 2808\n",
      "Improved validation loss from: 0.02213466167449951  to: 0.022064991295337677\n",
      "Training iteration: 2809\n",
      "Improved validation loss from: 0.022064991295337677  to: 0.02205429822206497\n",
      "Training iteration: 2810\n",
      "Improved validation loss from: 0.02205429822206497  to: 0.022029264271259306\n",
      "Training iteration: 2811\n",
      "Improved validation loss from: 0.022029264271259306  to: 0.022015659511089324\n",
      "Training iteration: 2812\n",
      "Improved validation loss from: 0.022015659511089324  to: 0.02198925018310547\n",
      "Training iteration: 2813\n",
      "Improved validation loss from: 0.02198925018310547  to: 0.021950337290763854\n",
      "Training iteration: 2814\n",
      "Improved validation loss from: 0.021950337290763854  to: 0.02189064770936966\n",
      "Training iteration: 2815\n",
      "Improved validation loss from: 0.02189064770936966  to: 0.021878032386302947\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.021895718574523926\n",
      "Training iteration: 2817\n",
      "Validation loss (no improvement): 0.021951279044151305\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.02197488844394684\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.021967101097106933\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.02199472486972809\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.022113338112831116\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): 0.022088083624839782\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.02209934890270233\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.022183752059936522\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.02238178253173828\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.022333359718322753\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.022203329205513\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.02215692549943924\n",
      "Training iteration: 2829\n",
      "Validation loss (no improvement): 0.022220237553119658\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.022112758457660676\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): 0.022109062969684602\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): 0.022094810009002687\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.022173675894737243\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.022174791991710664\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.022194509208202363\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.022256922721862794\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): 0.022293567657470703\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): 0.02243134081363678\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.02231944501399994\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.02219308912754059\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.02214154303073883\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.02210793048143387\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.02207329273223877\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.0220469206571579\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.022095492482185362\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.022236843407154084\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.02215041369199753\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.022041447460651398\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.022006312012672426\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.02199489772319794\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.022107632458209993\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.02218476086854935\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.02203563004732132\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.021878032386302947  to: 0.021854908764362337\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.021854908764362337  to: 0.02174365222454071\n",
      "Training iteration: 2856\n",
      "Improved validation loss from: 0.02174365222454071  to: 0.02168287932872772\n",
      "Training iteration: 2857\n",
      "Improved validation loss from: 0.02168287932872772  to: 0.021663793921470643\n",
      "Training iteration: 2858\n",
      "Improved validation loss from: 0.021663793921470643  to: 0.021624703705310822\n",
      "Training iteration: 2859\n",
      "Improved validation loss from: 0.021624703705310822  to: 0.02149171382188797\n",
      "Training iteration: 2860\n",
      "Improved validation loss from: 0.02149171382188797  to: 0.02147391587495804\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): 0.02162359207868576\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): 0.021509912610054017\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.021603289246559142\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.02165145128965378\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.021666839718818665\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): 0.021806459128856658\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.02171359062194824\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.021682503819465637\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.021675817668437958\n",
      "Training iteration: 2870\n",
      "Improved validation loss from: 0.02147391587495804  to: 0.021412810683250426\n",
      "Training iteration: 2871\n",
      "Improved validation loss from: 0.021412810683250426  to: 0.021327576041221617\n",
      "Training iteration: 2872\n",
      "Improved validation loss from: 0.021327576041221617  to: 0.021266424655914308\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.021277809143066408\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.021451763808727264\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.02146834135055542\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.021266424655914308  to: 0.021254467964172363\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.021254467964172363  to: 0.02119651734828949\n",
      "Training iteration: 2878\n",
      "Validation loss (no improvement): 0.021240086853504182\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.021305458247661592\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.021312442421913148\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.021298608183860777\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.021302278339862823\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.02128177136182785\n",
      "Training iteration: 2884\n",
      "Improved validation loss from: 0.02119651734828949  to: 0.02117568552494049\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): 0.021180248260498045\n",
      "Training iteration: 2886\n",
      "Improved validation loss from: 0.02117568552494049  to: 0.02108822762966156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2887\n",
      "Improved validation loss from: 0.02108822762966156  to: 0.021049027144908906\n",
      "Training iteration: 2888\n",
      "Improved validation loss from: 0.021049027144908906  to: 0.020990189909934998\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.021023575961589814\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): 0.021210137009620666\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): 0.021356184780597687\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.021172456443309784\n",
      "Training iteration: 2893\n",
      "Improved validation loss from: 0.020990189909934998  to: 0.020877313613891602\n",
      "Training iteration: 2894\n",
      "Improved validation loss from: 0.020877313613891602  to: 0.020729586482048035\n",
      "Training iteration: 2895\n",
      "Improved validation loss from: 0.020729586482048035  to: 0.02069297283887863\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.0207815557718277\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.020789328217506408\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.020727641880512238\n",
      "Training iteration: 2899\n",
      "Improved validation loss from: 0.02069297283887863  to: 0.020655238628387453\n",
      "Training iteration: 2900\n",
      "Improved validation loss from: 0.020655238628387453  to: 0.020647446811199188\n",
      "Training iteration: 2901\n",
      "Improved validation loss from: 0.020647446811199188  to: 0.020598992705345154\n",
      "Training iteration: 2902\n",
      "Improved validation loss from: 0.020598992705345154  to: 0.020571470260620117\n",
      "Training iteration: 2903\n",
      "Validation loss (no improvement): 0.020646190643310545\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.020594485104084015\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.02058652937412262\n",
      "Training iteration: 2906\n",
      "Validation loss (no improvement): 0.02063116729259491\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.020571470260620117  to: 0.02055140435695648\n",
      "Training iteration: 2908\n",
      "Improved validation loss from: 0.02055140435695648  to: 0.020460376143455507\n",
      "Training iteration: 2909\n",
      "Improved validation loss from: 0.020460376143455507  to: 0.020379090309143068\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.020459333062171937\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.02041618376970291\n",
      "Training iteration: 2912\n",
      "Validation loss (no improvement): 0.020487821102142333\n",
      "Training iteration: 2913\n",
      "Improved validation loss from: 0.020379090309143068  to: 0.02032351940870285\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.02032351940870285  to: 0.020299991965293883\n",
      "Training iteration: 2915\n",
      "Improved validation loss from: 0.020299991965293883  to: 0.020171685516834258\n",
      "Training iteration: 2916\n",
      "Improved validation loss from: 0.020171685516834258  to: 0.020025005936622618\n",
      "Training iteration: 2917\n",
      "Improved validation loss from: 0.020025005936622618  to: 0.01997068226337433\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.020016658306121825\n",
      "Training iteration: 2919\n",
      "Improved validation loss from: 0.01997068226337433  to: 0.019958505034446718\n",
      "Training iteration: 2920\n",
      "Improved validation loss from: 0.019958505034446718  to: 0.01992633044719696\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.01992633044719696  to: 0.019861765205860138\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.019865351915359496\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.019946424663066863\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.020000025629997253\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.020054778456687926\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.019871537387371064\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.019919183850288392\n",
      "Training iteration: 2928\n",
      "Improved validation loss from: 0.019861765205860138  to: 0.01984860897064209\n",
      "Training iteration: 2929\n",
      "Improved validation loss from: 0.01984860897064209  to: 0.01968702971935272\n",
      "Training iteration: 2930\n",
      "Improved validation loss from: 0.01968702971935272  to: 0.019649343192577363\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.01967258155345917\n",
      "Training iteration: 2932\n",
      "Improved validation loss from: 0.019649343192577363  to: 0.019579017162323\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.019623856246471404\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.01960403472185135\n",
      "Training iteration: 2935\n",
      "Improved validation loss from: 0.019579017162323  to: 0.01954159289598465\n",
      "Training iteration: 2936\n",
      "Improved validation loss from: 0.01954159289598465  to: 0.019493409991264345\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.019534960389137268\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.019633519649505615\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.01962326467037201\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.01955145001411438\n",
      "Training iteration: 2941\n",
      "Improved validation loss from: 0.019493409991264345  to: 0.019458489120006563\n",
      "Training iteration: 2942\n",
      "Improved validation loss from: 0.019458489120006563  to: 0.01943032294511795\n",
      "Training iteration: 2943\n",
      "Improved validation loss from: 0.01943032294511795  to: 0.01937914192676544\n",
      "Training iteration: 2944\n",
      "Improved validation loss from: 0.01937914192676544  to: 0.01921052634716034\n",
      "Training iteration: 2945\n",
      "Improved validation loss from: 0.01921052634716034  to: 0.019083356857299803\n",
      "Training iteration: 2946\n",
      "Improved validation loss from: 0.019083356857299803  to: 0.01904941499233246\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.01913422644138336\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.01920512616634369\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.019156685471534728\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.019162264466285706\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.019327494502067565\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.01931889057159424\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.01926272511482239\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): 0.019129303097724915\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.019088241457939147\n",
      "Training iteration: 2956\n",
      "Improved validation loss from: 0.01904941499233246  to: 0.01891007721424103\n",
      "Training iteration: 2957\n",
      "Improved validation loss from: 0.01891007721424103  to: 0.018908993899822236\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.018908993899822236  to: 0.018801724910736083\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.01884017139673233\n",
      "Training iteration: 2960\n",
      "Improved validation loss from: 0.018801724910736083  to: 0.018778936564922334\n",
      "Training iteration: 2961\n",
      "Improved validation loss from: 0.018778936564922334  to: 0.01877804845571518\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.018832309544086455\n",
      "Training iteration: 2963\n",
      "Improved validation loss from: 0.01877804845571518  to: 0.018729734420776366\n",
      "Training iteration: 2964\n",
      "Improved validation loss from: 0.018729734420776366  to: 0.018603959679603578\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.018606853485107423\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.01872561275959015\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.01875203549861908\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.018617986142635344\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.018639037013053895\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.0187140017747879\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.018776866793632507\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.018616953492164613\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.018614773452281953\n",
      "Training iteration: 2974\n",
      "Improved validation loss from: 0.018603959679603578  to: 0.018576900660991668\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.01858573704957962\n",
      "Training iteration: 2976\n",
      "Improved validation loss from: 0.018576900660991668  to: 0.01848338097333908\n",
      "Training iteration: 2977\n",
      "Improved validation loss from: 0.01848338097333908  to: 0.018358989059925078\n",
      "Training iteration: 2978\n",
      "Improved validation loss from: 0.018358989059925078  to: 0.018336132168769836\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.01834712326526642\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.01842297613620758\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.01839475631713867\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.01842316836118698\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.018354646861553192\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.01843058466911316\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.018431408703327178\n",
      "Training iteration: 2986\n",
      "Improved validation loss from: 0.018336132168769836  to: 0.018285055458545686\n",
      "Training iteration: 2987\n",
      "Improved validation loss from: 0.018285055458545686  to: 0.018194063007831572\n",
      "Training iteration: 2988\n",
      "Improved validation loss from: 0.018194063007831572  to: 0.018070928752422333\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.018154774606227875\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.01823226660490036\n",
      "Training iteration: 2991\n",
      "Improved validation loss from: 0.018070928752422333  to: 0.018011267483234405\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.018105858564376832\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.018129560351371764\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.018047139048576355\n",
      "Training iteration: 2995\n",
      "Improved validation loss from: 0.018011267483234405  to: 0.01800079643726349\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.01803337037563324\n",
      "Training iteration: 2997\n",
      "Improved validation loss from: 0.01800079643726349  to: 0.017866863310337065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2998\n",
      "Improved validation loss from: 0.017866863310337065  to: 0.017751555144786834\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.017811524868011474\n",
      "Training iteration: 3000\n",
      "Improved validation loss from: 0.017751555144786834  to: 0.01771666556596756\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.017727819085121155\n",
      "Training iteration: 3002\n",
      "Improved validation loss from: 0.01771666556596756  to: 0.017712342739105224\n",
      "Training iteration: 3003\n",
      "Improved validation loss from: 0.017712342739105224  to: 0.01764020174741745\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.017644017934799194\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.017893338203430177\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.017820322513580324\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.017721021175384523\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.01781134158372879\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.01766761839389801\n",
      "Training iteration: 3010\n",
      "Improved validation loss from: 0.01764020174741745  to: 0.017598389089107512\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): 0.01764618158340454\n",
      "Training iteration: 3012\n",
      "Improved validation loss from: 0.017598389089107512  to: 0.017571327090263367\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.017677876353263854\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): 0.017762279510498045\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): 0.01767945885658264\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.017645937204360963\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.017716032266616822\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.017779330909252166\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.018079960346221925\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.018065537512302398\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): 0.017917419970035552\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): 0.017889460921287535\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.017954686284065248\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.017793087661266326\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.01781321465969086\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.017789730429649354\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.01783756911754608\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.017869260907173157\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.01776082068681717\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.017864391207695007\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.01800442934036255\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.017903153598308564\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.01787041425704956\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.01787290573120117\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.017729468643665314\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.017651914060115813\n",
      "Training iteration: 3037\n",
      "Improved validation loss from: 0.017571327090263367  to: 0.01750207245349884\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.017509929835796356\n",
      "Training iteration: 3039\n",
      "Improved validation loss from: 0.01750207245349884  to: 0.017465385794639587\n",
      "Training iteration: 3040\n",
      "Improved validation loss from: 0.017465385794639587  to: 0.017462125420570372\n",
      "Training iteration: 3041\n",
      "Improved validation loss from: 0.017462125420570372  to: 0.01745446175336838\n",
      "Training iteration: 3042\n",
      "Improved validation loss from: 0.01745446175336838  to: 0.017451027035713197\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.017533397674560545\n",
      "Training iteration: 3044\n",
      "Improved validation loss from: 0.017451027035713197  to: 0.01731047034263611\n",
      "Training iteration: 3045\n",
      "Improved validation loss from: 0.01731047034263611  to: 0.017308583855628966\n",
      "Training iteration: 3046\n",
      "Improved validation loss from: 0.017308583855628966  to: 0.017279206216335295\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.01736104190349579\n",
      "Training iteration: 3048\n",
      "Improved validation loss from: 0.017279206216335295  to: 0.017258203029632567\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.017336563766002656\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.017479975521564484\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.01737843155860901\n",
      "Training iteration: 3052\n",
      "Improved validation loss from: 0.017258203029632567  to: 0.01722995787858963\n",
      "Training iteration: 3053\n",
      "Improved validation loss from: 0.01722995787858963  to: 0.017182712256908417\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.017306531965732574\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.017428307235240935\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.01743190586566925\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.017501112818717957\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.01766035854816437\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): 0.017521144449710847\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.01742876470088959\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.017419108748435976\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.017410974204540252\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.017440137267112733\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.0173400953412056\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.017192545533180236\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): 0.017275771498680113\n",
      "Training iteration: 3067\n",
      "Improved validation loss from: 0.017182712256908417  to: 0.017065919935703278\n",
      "Training iteration: 3068\n",
      "Improved validation loss from: 0.017065919935703278  to: 0.017013165354728698\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.01703040301799774\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.017075464129447937\n",
      "Training iteration: 3071\n",
      "Improved validation loss from: 0.017013165354728698  to: 0.017000789940357208\n",
      "Training iteration: 3072\n",
      "Improved validation loss from: 0.017000789940357208  to: 0.016967229545116425\n",
      "Training iteration: 3073\n",
      "Improved validation loss from: 0.016967229545116425  to: 0.016850972175598146\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.016967637836933135\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.016910675168037414\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.016992917656898497\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.01697565019130707\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.016881339251995087\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.016969048976898195\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.017329025268554687\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.01709655374288559\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.016853339970111847\n",
      "Training iteration: 3083\n",
      "Improved validation loss from: 0.016850972175598146  to: 0.016843631863594055\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.016998234391212463\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.01704406291246414\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.017075154185295104\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.017121878266334534\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.017215129733085633\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.01720963716506958\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.017122140526771544\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.017203499376773835\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.017146047949790955\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.017224648594856264\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.017158663272857665\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.017186884582042695\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.01719050109386444\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.01698659360408783\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.016927699744701385\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.01690570414066315\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.017293956875801087\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.017377322912216185\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.017094333469867707\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.017149993777275087\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.01744745671749115\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.01734418123960495\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.017024172842502593\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.0170377716422081\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.017168302834033967\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.01716507375240326\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.016985754668712615\n",
      "Training iteration: 3111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.016926535964012147\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.016965821385383606\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.017175722122192382\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.01721126735210419\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.017140695452690126\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.0172857329249382\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.0175780326128006\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.01763632595539093\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.017431163787841798\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.017329180240631105\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.01732793152332306\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.01750025600194931\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.0176021546125412\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.017403657734394073\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.017225182056427\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.01724235564470291\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.017424753308296202\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.017381484806537627\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.017234566807746887\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.017220966517925262\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.01734718233346939\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.01724759042263031\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.01707460731267929\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.017081494629383086\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.016987180709838866\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.016901752352714537\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.01689734011888504\n",
      "Training iteration: 3138\n",
      "Improved validation loss from: 0.016843631863594055  to: 0.016834506392478944\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.01689632683992386\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.017056165635585784\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.016912375390529633\n",
      "Training iteration: 3142\n",
      "Improved validation loss from: 0.016834506392478944  to: 0.016755664348602296\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.016791912913322448\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.016865268349647522\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.016760244965553284\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.01681923568248749\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.016926981508731842\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.016843430697917938\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.016765764355659483\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.017017611861228944\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.017230844497680663\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.017064252495765687\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.016931870579719545\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.01697523444890976\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.017191100120544433\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.017358919978141783\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.01706503927707672\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.016936562955379486\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.01693199425935745\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.01723155677318573\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.01705711632966995\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.016787365078926086\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.01679717153310776\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.01695619374513626\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.016902069747447967\n",
      "Training iteration: 3166\n",
      "Improved validation loss from: 0.016755664348602296  to: 0.01672176569700241\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.016792510449886323\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.017147588729858398\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.016978487372398376\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.01682591587305069\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.016832903027534485\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.01716780662536621\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.017113041877746583\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.01686245948076248\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.01680687665939331\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.016988086700439452\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.017343246936798097\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.01719334125518799\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): 0.01703338623046875\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.017075446248054505\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.01727340221405029\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.017352239787578584\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.01725289076566696\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.01717856228351593\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.017014124989509584\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.0169271320104599\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.017015287280082704\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.016878476738929747\n",
      "Training iteration: 3189\n",
      "Improved validation loss from: 0.01672176569700241  to: 0.016630421578884124\n",
      "Training iteration: 3190\n",
      "Improved validation loss from: 0.016630421578884124  to: 0.016576105356216432\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.016774562001228333\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.016954222321510316\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.016718800365924835\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.01684858500957489\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.017161524295806883\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.017278771102428436\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.01715523302555084\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.017040428519248963\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.017074200510978698\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.017409777641296385\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.017194673418998718\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.016748708486557008\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.016725951433181764\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.016828015446662903\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.017164584994316102\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.017188350856304168\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.017055468261241914\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.0170115202665329\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.017233529686927797\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.01741833984851837\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.01739351451396942\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.01706080138683319\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.01703302711248398\n",
      "Training iteration: 3214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01720692664384842\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.017175579071044923\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.01698320209980011\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.017022550106048584\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.017133599519729613\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.017230062186717986\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.01726289540529251\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.017199452221393585\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.01722753494977951\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.01730481833219528\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.017329099774360656\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.017279890179634095\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.01707424968481064\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.01702541559934616\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.017197799682617188\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): 0.017234763503074645\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.017181894183158873\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.0171646311879158\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.017143115401268005\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.017500101029872893\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.017650805413722992\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.017238318920135498\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.01694074720144272\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.016966216266155243\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.017132286727428437\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.017624542117118835\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.017885956168174743\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.017597846686840057\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.01739429235458374\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.017221975326538085\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.017122144997119903\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.01707572638988495\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.017317076027393342\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.017127925157547\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.01677643358707428\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.016825948655605317\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.01677631288766861\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.016730093955993654\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.017130884528160095\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.017380484938621522\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.017135393619537354\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.017059215903282167\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.017147283256053924\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.017318956553936005\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.01764938086271286\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.017887774109840392\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.01745880991220474\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.017130127549171446\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.01706787049770355\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.016924460232257844\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.01707751601934433\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.016969402134418488\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.016837295889854432\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.01685396730899811\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.016847458481788636\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.016913512349128725\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.017128050327301025\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.017030972242355346\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.01668546497821808\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.016688701510429383\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.016789597272872925\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.016725221276283266\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.016707280278205873\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.01678404361009598\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.016754159331321718\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.01677817553281784\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.01694105863571167\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.016691383719444276\n",
      "Training iteration: 3282\n",
      "Improved validation loss from: 0.016576105356216432  to: 0.01654595136642456\n",
      "Training iteration: 3283\n",
      "Improved validation loss from: 0.01654595136642456  to: 0.016416318714618683\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.016431358456611634\n",
      "Training iteration: 3285\n",
      "Improved validation loss from: 0.016416318714618683  to: 0.016379618644714357\n",
      "Training iteration: 3286\n",
      "Improved validation loss from: 0.016379618644714357  to: 0.016166910529136658\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.016252242028713226\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.01619807183742523\n",
      "Training iteration: 3289\n",
      "Improved validation loss from: 0.016166910529136658  to: 0.016116078197956085\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.01623983383178711\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.016531257331371306\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.016718609631061553\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.016627731919288635\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.01636902391910553\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.01625392884016037\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.016146624088287355\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.016258656978607178\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.016202130913734437\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.016132450103759764\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.01612115204334259\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.01623043268918991\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.016546502709388733\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.016751961410045625\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.016583284735679625\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.01645936518907547\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.016537055373191833\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.01677836924791336\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.017028811573982238\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.01695309430360794\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.016724978387355805\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.016559307277202607\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.016501709818840027\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.016647693514823914\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.01674601286649704\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.01672278642654419\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.01662861704826355\n",
      "Training iteration: 3317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.016595402359962465\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.016610997915267944\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.01658224016427994\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.016482944786548614\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.016384613513946534\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.016705131530761717\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.016857703030109406\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.0163977712392807\n",
      "Training iteration: 3325\n",
      "Improved validation loss from: 0.016116078197956085  to: 0.01607314944267273\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.0161782830953598\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.01618398427963257\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.01628965586423874\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.016561546921730043\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.016685043275356293\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.016503235697746275\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.016198109090328216\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.01613990366458893\n",
      "Training iteration: 3334\n",
      "Improved validation loss from: 0.01607314944267273  to: 0.01598159521818161\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.015999534726142885\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.016123667359352112\n",
      "Training iteration: 3337\n",
      "Improved validation loss from: 0.01598159521818161  to: 0.01579647362232208\n",
      "Training iteration: 3338\n",
      "Improved validation loss from: 0.01579647362232208  to: 0.015518482029438018\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.015639340877532958\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.01578960716724396\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.01592884063720703\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.015961603820323945\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.015897314250469207\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.015958687663078307\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.01595366895198822\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.015729856491088868\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.015664711594581604\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.015729980170726778\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.01575367897748947\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.015637502074241638\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.015701006352901458\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.015938322246074676\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.015613248944282532\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.015556314587593078\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.015595419704914093\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.015884357690811157\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.01620105504989624\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.016015657782554628\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.01570504605770111\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.01572190076112747\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.01598910689353943\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.016496504843235015\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.01621042788028717\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.015735428035259246\n",
      "Training iteration: 3365\n",
      "Validation loss (no improvement): 0.01575346440076828\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.015980915725231172\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.01632685214281082\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.016284725069999693\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.016060374677181244\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.0159902960062027\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.016057857871055604\n",
      "Training iteration: 3372\n",
      "Validation loss (no improvement): 0.016427192091941833\n",
      "Training iteration: 3373\n",
      "Validation loss (no improvement): 0.016484174132347106\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.016125360131263734\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.016051006317138673\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.01609513461589813\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.015965229272842406\n",
      "Training iteration: 3378\n",
      "Validation loss (no improvement): 0.01567688286304474\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.015559369325637817\n",
      "Training iteration: 3380\n",
      "Validation loss (no improvement): 0.015590614080429077\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.015687640011310577\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.01559361070394516\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.015696796774864196\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.016012859344482423\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): 0.016164059937000274\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.015987853705883025\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.016008734703063965\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): 0.016147787868976592\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.01570405513048172\n",
      "Training iteration: 3390\n",
      "Improved validation loss from: 0.015518482029438018  to: 0.015194159746170045\n",
      "Training iteration: 3391\n",
      "Improved validation loss from: 0.015194159746170045  to: 0.0150636687874794\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.015274079144001007\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.015257076919078827\n",
      "Training iteration: 3394\n",
      "Improved validation loss from: 0.0150636687874794  to: 0.01489623486995697\n",
      "Training iteration: 3395\n",
      "Improved validation loss from: 0.01489623486995697  to: 0.014836180210113525\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.015046973526477814\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.01547304391860962\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.015658675134181975\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.01548634171485901\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.015355560183525085\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): 0.015381225943565368\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.015571916103363037\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.015519282221794129\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.015243130922317504\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.014996150135993957\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.014946968853473663\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): 0.015264809131622314\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.015090428292751312\n",
      "Training iteration: 3409\n",
      "Improved validation loss from: 0.014836180210113525  to: 0.01478971540927887\n",
      "Training iteration: 3410\n",
      "Improved validation loss from: 0.01478971540927887  to: 0.01474544256925583\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.014987070858478547\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.015104936063289642\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.015135398507118225\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.015128146111965179\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.015206319093704224\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.015350921452045441\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): 0.0154371440410614\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.015379545092582703\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.01533462256193161\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.01538931131362915\n",
      "Training iteration: 3421\n",
      "Validation loss (no improvement): 0.015274712443351745\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): 0.014977212250232696\n",
      "Training iteration: 3423\n",
      "Improved validation loss from: 0.01474544256925583  to: 0.014672218263149262\n",
      "Training iteration: 3424\n",
      "Improved validation loss from: 0.014672218263149262  to: 0.014624066650867462\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.014874124526977539\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.01484726369380951\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.014831236004829407\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.014935252070426942\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): 0.015068337321281433\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.015404348075389863\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.015602830052375793\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.015495195984840393\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.015492716431617736\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.01559855192899704\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.015446045994758606\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.015151597559452057\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.014930284023284912\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.01485787183046341\n",
      "Training iteration: 3439\n",
      "Validation loss (no improvement): 0.015035279095172882\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.01505592167377472\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.014804211258888245\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.014733599126338958\n",
      "Training iteration: 3443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.014978811144828796\n",
      "Training iteration: 3444\n",
      "Validation loss (no improvement): 0.015113754570484162\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.015024176239967347\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.014692375063896179\n",
      "Training iteration: 3447\n",
      "Improved validation loss from: 0.014624066650867462  to: 0.014620782434940338\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.014716655015945435\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): 0.014836852252483369\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.014662127196788787\n",
      "Training iteration: 3451\n",
      "Improved validation loss from: 0.014620782434940338  to: 0.014430278539657592\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.014462758600711823\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.014549432694911957\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.014522150158882141\n",
      "Training iteration: 3455\n",
      "Validation loss (no improvement): 0.014466395974159241\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.014463165402412414\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.014636705815792083\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.014606806635856628\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.014682736992836\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.01496119499206543\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.014830070734024047\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.014820103347301484\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.01476801633834839\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.014935891330242156\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.015040585398674011\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.014875841140747071\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.014759664237499238\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.01497979909181595\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.015443919599056244\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.015163847804069519\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): 0.014864866435527802\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.014913205802440644\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.014961358904838563\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.014995110034942628\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.015090921521186828\n",
      "Training iteration: 3476\n",
      "Validation loss (no improvement): 0.014760145545005798\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.01470632255077362\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.014784994721412658\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.014855070412158966\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.014806190133094787\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.014950728416442871\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.015018782019615174\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.0148788183927536\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.014800059795379638\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.014855234324932099\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.014798852801322936\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.0146201953291893\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.0144432932138443\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): 0.014517255127429962\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.014827363193035126\n",
      "Training iteration: 3491\n",
      "Validation loss (no improvement): 0.014810149371623994\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): 0.01450156420469284\n",
      "Training iteration: 3493\n",
      "Improved validation loss from: 0.014430278539657592  to: 0.014400973916053772\n",
      "Training iteration: 3494\n",
      "Improved validation loss from: 0.014400973916053772  to: 0.014259156584739686\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.014381401240825653\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.014264838397502899\n",
      "Training iteration: 3497\n",
      "Improved validation loss from: 0.014259156584739686  to: 0.013964943587779999\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.013996884226799011\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): 0.014212793111801148\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): 0.01413319855928421\n",
      "Training iteration: 3501\n",
      "Improved validation loss from: 0.013964943587779999  to: 0.013874208927154541\n",
      "Training iteration: 3502\n",
      "Validation loss (no improvement): 0.01392240822315216\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.014474873244762421\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.014595189690589904\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.014168910682201385\n",
      "Training iteration: 3506\n",
      "Validation loss (no improvement): 0.013926319777965546\n",
      "Training iteration: 3507\n",
      "Validation loss (no improvement): 0.014075320959091187\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.014449033141136169\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.014491863548755646\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.014047293365001679\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.013953061401844024\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.014403387904167175\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.014354059100151062\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.013975104689598084\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.01389758288860321\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.014206592738628388\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.014369441568851471\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.014070892333984375\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.014025691151618957\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.013916265964508057\n",
      "Training iteration: 3521\n",
      "Improved validation loss from: 0.013874208927154541  to: 0.013565704226493835\n",
      "Training iteration: 3522\n",
      "Improved validation loss from: 0.013565704226493835  to: 0.013539078831672668\n",
      "Training iteration: 3523\n",
      "Validation loss (no improvement): 0.013741636276245117\n",
      "Training iteration: 3524\n",
      "Validation loss (no improvement): 0.01363540142774582\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.013652440905570985\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.013676291704177857\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.013698914647102356\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.013623568415641784\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.013594624400138856\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.013634605705738068\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.01365201324224472\n",
      "Training iteration: 3532\n",
      "Improved validation loss from: 0.013539078831672668  to: 0.013354828953742981\n",
      "Training iteration: 3533\n",
      "Improved validation loss from: 0.013354828953742981  to: 0.013348960876464843\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.01357848048210144\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.013597504794597625\n",
      "Training iteration: 3536\n",
      "Improved validation loss from: 0.013348960876464843  to: 0.013327780365943908\n",
      "Training iteration: 3537\n",
      "Improved validation loss from: 0.013327780365943908  to: 0.013240258395671844\n",
      "Training iteration: 3538\n",
      "Improved validation loss from: 0.013240258395671844  to: 0.013201919198036195\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.01349155604839325\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): 0.013421556353569031\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.013296142220497131\n",
      "Training iteration: 3542\n",
      "Improved validation loss from: 0.013201919198036195  to: 0.01316617727279663\n",
      "Training iteration: 3543\n",
      "Improved validation loss from: 0.01316617727279663  to: 0.013154679536819458\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.013475489616394044\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.014222410321235657\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.014323589205741883\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): 0.0138005331158638\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): 0.013424862921237946\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.01349164843559265\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.013509544730186462\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.013608011603355407\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.013931205868721009\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.013948625326156617\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.013482201099395751\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.013299553096294403\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.013305027782917023\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.013331930339336395\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): 0.014364266395568847\n",
      "Training iteration: 3559\n",
      "Validation loss (no improvement): 0.014612945914268493\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): 0.014020617306232452\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.013645651936531066\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.013842368125915527\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.013789477944374084\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): 0.01402932107448578\n",
      "Training iteration: 3565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.014970719814300537\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): 0.014962111413478852\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): 0.014173807203769683\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.01401032954454422\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.014082488417625428\n",
      "Training iteration: 3570\n",
      "Validation loss (no improvement): 0.014059048891067506\n",
      "Training iteration: 3571\n",
      "Validation loss (no improvement): 0.014380577206611633\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.014565795660018921\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.013992753624916077\n",
      "Training iteration: 3574\n",
      "Validation loss (no improvement): 0.013694608211517334\n",
      "Training iteration: 3575\n",
      "Validation loss (no improvement): 0.013744571805000305\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): 0.013808102905750274\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.014040543138980866\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.014082515239715576\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.0137861967086792\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): 0.013657541573047638\n",
      "Training iteration: 3581\n",
      "Validation loss (no improvement): 0.013444451987743378\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.013375131785869599\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.013725455105304717\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.013633632659912109\n",
      "Training iteration: 3585\n",
      "Improved validation loss from: 0.013154679536819458  to: 0.013141152262687684\n",
      "Training iteration: 3586\n",
      "Improved validation loss from: 0.013141152262687684  to: 0.013110730051994323\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.013175682723522186\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.013487139344215393\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.013418015837669373\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.013123083114624023\n",
      "Training iteration: 3591\n",
      "Improved validation loss from: 0.013110730051994323  to: 0.012975822389125823\n",
      "Training iteration: 3592\n",
      "Improved validation loss from: 0.012975822389125823  to: 0.012865915894508362\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.013236550986766816\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.013675881922245026\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.013345156610012055\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.013000407814979553\n",
      "Training iteration: 3597\n",
      "Validation loss (no improvement): 0.013108310103416444\n",
      "Training iteration: 3598\n",
      "Validation loss (no improvement): 0.01307329386472702\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.013123488426208496\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.01379116326570511\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.014187216758728027\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.01354728639125824\n",
      "Training iteration: 3603\n",
      "Validation loss (no improvement): 0.013014093041419983\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.013057456910610199\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.013138501346111298\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.013829167187213897\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.013970103859901429\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.013467726111412049\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.013329848647117615\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.013515383005142212\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.013391157984733582\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.013134562969207763\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.01323678195476532\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.013148219883441925\n",
      "Training iteration: 3615\n",
      "Improved validation loss from: 0.012865915894508362  to: 0.01274605244398117\n",
      "Training iteration: 3616\n",
      "Improved validation loss from: 0.01274605244398117  to: 0.012730267643928529\n",
      "Training iteration: 3617\n",
      "Improved validation loss from: 0.012730267643928529  to: 0.0126875638961792\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.013157534599304199\n",
      "Training iteration: 3619\n",
      "Validation loss (no improvement): 0.013376693427562713\n",
      "Training iteration: 3620\n",
      "Validation loss (no improvement): 0.012930309772491455\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.01291191130876541\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.012948164343833923\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.013092426955699921\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.013311956822872163\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.013072545826435088\n",
      "Training iteration: 3626\n",
      "Validation loss (no improvement): 0.012957926094532012\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.012815943360328675\n",
      "Training iteration: 3628\n",
      "Improved validation loss from: 0.0126875638961792  to: 0.012633271515369415\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.012786927819252013\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.01265260875225067\n",
      "Training iteration: 3631\n",
      "Improved validation loss from: 0.012633271515369415  to: 0.01245739683508873\n",
      "Training iteration: 3632\n",
      "Improved validation loss from: 0.01245739683508873  to: 0.01245134100317955\n",
      "Training iteration: 3633\n",
      "Improved validation loss from: 0.01245134100317955  to: 0.0124350406229496\n",
      "Training iteration: 3634\n",
      "Improved validation loss from: 0.0124350406229496  to: 0.012330634891986847\n",
      "Training iteration: 3635\n",
      "Validation loss (no improvement): 0.012375279515981674\n",
      "Training iteration: 3636\n",
      "Validation loss (no improvement): 0.012524180114269257\n",
      "Training iteration: 3637\n",
      "Validation loss (no improvement): 0.012394353002309799\n",
      "Training iteration: 3638\n",
      "Improved validation loss from: 0.012330634891986847  to: 0.0120457723736763\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.012075641006231309\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.012475784868001938\n",
      "Training iteration: 3641\n",
      "Validation loss (no improvement): 0.012196505069732666\n",
      "Training iteration: 3642\n",
      "Improved validation loss from: 0.0120457723736763  to: 0.011922250688076019\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.011952288448810577\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.012410926818847656\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.012683744728565215\n",
      "Training iteration: 3646\n",
      "Validation loss (no improvement): 0.01225825995206833\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.012187187373638154\n",
      "Training iteration: 3648\n",
      "Validation loss (no improvement): 0.012423233687877655\n",
      "Training iteration: 3649\n",
      "Validation loss (no improvement): 0.012949347496032715\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.012577979266643525\n",
      "Training iteration: 3651\n",
      "Validation loss (no improvement): 0.012220356613397598\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.01225135698914528\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.012485333532094956\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.012528952956199647\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.012080397456884384\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.012020377814769745\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.01219920888543129\n",
      "Training iteration: 3658\n",
      "Validation loss (no improvement): 0.012355528026819228\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.012134569883346557\n",
      "Training iteration: 3660\n",
      "Validation loss (no improvement): 0.012270955741405487\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.01251937597990036\n",
      "Training iteration: 3662\n",
      "Validation loss (no improvement): 0.012313610315322876\n",
      "Training iteration: 3663\n",
      "Validation loss (no improvement): 0.012056548893451691\n",
      "Training iteration: 3664\n",
      "Validation loss (no improvement): 0.01212441474199295\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.012374949455261231\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.0121015764772892\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.012026619911193848\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.012136484682559966\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.012858149409294129\n",
      "Training iteration: 3670\n",
      "Validation loss (no improvement): 0.01295923739671707\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.01238853707909584\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.012343349307775498\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.012531208992004394\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.012728922069072723\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.012309513241052627\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.012112665176391601\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.012249300628900528\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.012413585186004638\n",
      "Training iteration: 3679\n",
      "Validation loss (no improvement): 0.012246563285589217\n",
      "Training iteration: 3680\n",
      "Validation loss (no improvement): 0.012189897149801255\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.01210419163107872\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.012283571064472198\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.01222849115729332\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.011991968005895614\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.011986418068408966\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.012007482349872589\n",
      "Training iteration: 3687\n",
      "Improved validation loss from: 0.011922250688076019  to: 0.011905399709939956\n",
      "Training iteration: 3688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.011905399709939956  to: 0.011844788491725922\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.01210329756140709\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.012097110599279403\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.011999498307704925\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.012028338015079498\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.011928682029247285\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.011882343143224717\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.012063880264759064\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.011849425733089447\n",
      "Training iteration: 3697\n",
      "Improved validation loss from: 0.011844788491725922  to: 0.011689437925815583\n",
      "Training iteration: 3698\n",
      "Improved validation loss from: 0.011689437925815583  to: 0.011611318588256836\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.011649401485919952\n",
      "Training iteration: 3700\n",
      "Improved validation loss from: 0.011611318588256836  to: 0.011356736719608306\n",
      "Training iteration: 3701\n",
      "Improved validation loss from: 0.011356736719608306  to: 0.011238513141870498\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.011531344801187515\n",
      "Training iteration: 3703\n",
      "Validation loss (no improvement): 0.011383672058582307\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.011245878785848618\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.011322417110204697\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.01152501106262207\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.012444679439067841\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.01252056211233139\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.01205008029937744\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.012106494605541229\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.012360658496618271\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.012424196302890777\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.012304742634296418\n",
      "Training iteration: 3714\n",
      "Validation loss (no improvement): 0.012227010726928712\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.01222718134522438\n",
      "Training iteration: 3716\n",
      "Validation loss (no improvement): 0.012191703170537948\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.012183932214975357\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.01225856989622116\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.012651662528514861\n",
      "Training iteration: 3720\n",
      "Validation loss (no improvement): 0.013002695143222808\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.01279454529285431\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.012716452777385711\n",
      "Training iteration: 3723\n",
      "Validation loss (no improvement): 0.012684038281440735\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.013107812404632569\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.013153031468391418\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.012616889178752899\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.012515190243721008\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.012711855769157409\n",
      "Training iteration: 3729\n",
      "Validation loss (no improvement): 0.012845799326896667\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.012530776858329772\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.012379702180624008\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.012485885620117187\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.01241173967719078\n",
      "Training iteration: 3734\n",
      "Validation loss (no improvement): 0.012393202632665634\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.012186284363269805\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.011906450986862183\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.011654374748468399\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.01153540164232254\n",
      "Training iteration: 3739\n",
      "Validation loss (no improvement): 0.011659321933984756\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.011707746982574463\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.01152063012123108\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.011511919647455215\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.011623644828796386\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.011941912025213242\n",
      "Training iteration: 3745\n",
      "Validation loss (no improvement): 0.011800141632556915\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.011609836667776107\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.01170058399438858\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.011735789477825165\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.011640079319477081\n",
      "Training iteration: 3750\n",
      "Validation loss (no improvement): 0.01160033717751503\n",
      "Training iteration: 3751\n",
      "Validation loss (no improvement): 0.011537916958332062\n",
      "Training iteration: 3752\n",
      "Validation loss (no improvement): 0.01147400587797165\n",
      "Training iteration: 3753\n",
      "Validation loss (no improvement): 0.011571472883224488\n",
      "Training iteration: 3754\n",
      "Validation loss (no improvement): 0.011705279350280762\n",
      "Training iteration: 3755\n",
      "Validation loss (no improvement): 0.01168975830078125\n",
      "Training iteration: 3756\n",
      "Validation loss (no improvement): 0.01183641403913498\n",
      "Training iteration: 3757\n",
      "Validation loss (no improvement): 0.011989102512598038\n",
      "Training iteration: 3758\n",
      "Validation loss (no improvement): 0.011749660968780518\n",
      "Training iteration: 3759\n",
      "Validation loss (no improvement): 0.01171838492155075\n",
      "Training iteration: 3760\n",
      "Validation loss (no improvement): 0.011787209659814835\n",
      "Training iteration: 3761\n",
      "Validation loss (no improvement): 0.011575999110937119\n",
      "Training iteration: 3762\n",
      "Improved validation loss from: 0.011238513141870498  to: 0.011167885363101959\n",
      "Training iteration: 3763\n",
      "Improved validation loss from: 0.011167885363101959  to: 0.011124452203512191\n",
      "Training iteration: 3764\n",
      "Validation loss (no improvement): 0.011236073821783066\n",
      "Training iteration: 3765\n",
      "Improved validation loss from: 0.011124452203512191  to: 0.011019325256347657\n",
      "Training iteration: 3766\n",
      "Improved validation loss from: 0.011019325256347657  to: 0.011013640463352204\n",
      "Training iteration: 3767\n",
      "Validation loss (no improvement): 0.011305968463420867\n",
      "Training iteration: 3768\n",
      "Validation loss (no improvement): 0.0115936242043972\n",
      "Training iteration: 3769\n",
      "Validation loss (no improvement): 0.011494891345500946\n",
      "Training iteration: 3770\n",
      "Validation loss (no improvement): 0.01128784641623497\n",
      "Training iteration: 3771\n",
      "Validation loss (no improvement): 0.01145288497209549\n",
      "Training iteration: 3772\n",
      "Validation loss (no improvement): 0.011549639701843261\n",
      "Training iteration: 3773\n",
      "Validation loss (no improvement): 0.011246721446514129\n",
      "Training iteration: 3774\n",
      "Validation loss (no improvement): 0.01117318868637085\n",
      "Training iteration: 3775\n",
      "Validation loss (no improvement): 0.011343725025653839\n",
      "Training iteration: 3776\n",
      "Validation loss (no improvement): 0.011474980413913727\n",
      "Training iteration: 3777\n",
      "Validation loss (no improvement): 0.011650657653808594\n",
      "Training iteration: 3778\n",
      "Validation loss (no improvement): 0.011798883974552154\n",
      "Training iteration: 3779\n",
      "Validation loss (no improvement): 0.011830923706293106\n",
      "Training iteration: 3780\n",
      "Validation loss (no improvement): 0.011772258579730988\n",
      "Training iteration: 3781\n",
      "Validation loss (no improvement): 0.011423534154891968\n",
      "Training iteration: 3782\n",
      "Validation loss (no improvement): 0.011201806366443634\n",
      "Training iteration: 3783\n",
      "Validation loss (no improvement): 0.011081530898809432\n",
      "Training iteration: 3784\n",
      "Improved validation loss from: 0.011013640463352204  to: 0.010900191962718964\n",
      "Training iteration: 3785\n",
      "Improved validation loss from: 0.010900191962718964  to: 0.010846865177154542\n",
      "Training iteration: 3786\n",
      "Validation loss (no improvement): 0.010851536691188813\n",
      "Training iteration: 3787\n",
      "Validation loss (no improvement): 0.011259341239929199\n",
      "Training iteration: 3788\n",
      "Validation loss (no improvement): 0.01126188263297081\n",
      "Training iteration: 3789\n",
      "Validation loss (no improvement): 0.011002379655838012\n",
      "Training iteration: 3790\n",
      "Validation loss (no improvement): 0.010978007316589355\n",
      "Training iteration: 3791\n",
      "Validation loss (no improvement): 0.011068730056285859\n",
      "Training iteration: 3792\n",
      "Validation loss (no improvement): 0.01145007386803627\n",
      "Training iteration: 3793\n",
      "Validation loss (no improvement): 0.011491721868515015\n",
      "Training iteration: 3794\n",
      "Validation loss (no improvement): 0.011258339881896973\n",
      "Training iteration: 3795\n",
      "Validation loss (no improvement): 0.011264334619045257\n",
      "Training iteration: 3796\n",
      "Validation loss (no improvement): 0.011226125061511993\n",
      "Training iteration: 3797\n",
      "Validation loss (no improvement): 0.011040685325860977\n",
      "Training iteration: 3798\n",
      "Validation loss (no improvement): 0.01106724888086319\n",
      "Training iteration: 3799\n",
      "Validation loss (no improvement): 0.010890449583530425\n",
      "Training iteration: 3800\n",
      "Improved validation loss from: 0.010846865177154542  to: 0.010745122283697128\n",
      "Training iteration: 3801\n",
      "Validation loss (no improvement): 0.010852937400341035\n",
      "Training iteration: 3802\n",
      "Validation loss (no improvement): 0.010925930738449097\n",
      "Training iteration: 3803\n",
      "Validation loss (no improvement): 0.010941405594348908\n",
      "Training iteration: 3804\n",
      "Validation loss (no improvement): 0.010969823598861695\n",
      "Training iteration: 3805\n",
      "Validation loss (no improvement): 0.010942864418029784\n",
      "Training iteration: 3806\n",
      "Validation loss (no improvement): 0.010864223539829253\n",
      "Training iteration: 3807\n",
      "Validation loss (no improvement): 0.010897321999073029\n",
      "Training iteration: 3808\n",
      "Validation loss (no improvement): 0.010991766303777694\n",
      "Training iteration: 3809\n",
      "Validation loss (no improvement): 0.010905230045318603\n",
      "Training iteration: 3810\n",
      "Validation loss (no improvement): 0.011016376316547394\n",
      "Training iteration: 3811\n",
      "Validation loss (no improvement): 0.010912430286407471\n",
      "Training iteration: 3812\n",
      "Improved validation loss from: 0.010745122283697128  to: 0.010718736797571182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3813\n",
      "Improved validation loss from: 0.010718736797571182  to: 0.01063246950507164\n",
      "Training iteration: 3814\n",
      "Validation loss (no improvement): 0.011043670028448105\n",
      "Training iteration: 3815\n",
      "Validation loss (no improvement): 0.011346933990716934\n",
      "Training iteration: 3816\n",
      "Validation loss (no improvement): 0.0108906090259552\n",
      "Training iteration: 3817\n",
      "Validation loss (no improvement): 0.010677294433116912\n",
      "Training iteration: 3818\n",
      "Validation loss (no improvement): 0.010853096097707748\n",
      "Training iteration: 3819\n",
      "Validation loss (no improvement): 0.01097712516784668\n",
      "Training iteration: 3820\n",
      "Validation loss (no improvement): 0.011397898197174072\n",
      "Training iteration: 3821\n",
      "Validation loss (no improvement): 0.011680066585540771\n",
      "Training iteration: 3822\n",
      "Validation loss (no improvement): 0.011327626556158066\n",
      "Training iteration: 3823\n",
      "Validation loss (no improvement): 0.011113779246807098\n",
      "Training iteration: 3824\n",
      "Validation loss (no improvement): 0.010931122303009033\n",
      "Training iteration: 3825\n",
      "Validation loss (no improvement): 0.011022733151912689\n",
      "Training iteration: 3826\n",
      "Validation loss (no improvement): 0.010912962257862091\n",
      "Training iteration: 3827\n",
      "Improved validation loss from: 0.01063246950507164  to: 0.010578244924545288\n",
      "Training iteration: 3828\n",
      "Validation loss (no improvement): 0.010602635145187379\n",
      "Training iteration: 3829\n",
      "Validation loss (no improvement): 0.010791943222284318\n",
      "Training iteration: 3830\n",
      "Validation loss (no improvement): 0.010597078502178192\n",
      "Training iteration: 3831\n",
      "Improved validation loss from: 0.010578244924545288  to: 0.010441567748785019\n",
      "Training iteration: 3832\n",
      "Validation loss (no improvement): 0.010518072545528412\n",
      "Training iteration: 3833\n",
      "Validation loss (no improvement): 0.010536007583141327\n",
      "Training iteration: 3834\n",
      "Validation loss (no improvement): 0.011194133758544922\n",
      "Training iteration: 3835\n",
      "Validation loss (no improvement): 0.011331789195537567\n",
      "Training iteration: 3836\n",
      "Validation loss (no improvement): 0.010636039823293687\n",
      "Training iteration: 3837\n",
      "Validation loss (no improvement): 0.010532726347446442\n",
      "Training iteration: 3838\n",
      "Validation loss (no improvement): 0.010693484544754028\n",
      "Training iteration: 3839\n",
      "Validation loss (no improvement): 0.010632846504449844\n",
      "Training iteration: 3840\n",
      "Validation loss (no improvement): 0.011394443362951279\n",
      "Training iteration: 3841\n",
      "Validation loss (no improvement): 0.012097875028848648\n",
      "Training iteration: 3842\n",
      "Validation loss (no improvement): 0.01174473762512207\n",
      "Training iteration: 3843\n",
      "Validation loss (no improvement): 0.011108735948801041\n",
      "Training iteration: 3844\n",
      "Validation loss (no improvement): 0.011028192937374115\n",
      "Training iteration: 3845\n",
      "Validation loss (no improvement): 0.011072051525115967\n",
      "Training iteration: 3846\n",
      "Validation loss (no improvement): 0.011034391820430756\n",
      "Training iteration: 3847\n",
      "Validation loss (no improvement): 0.011394914239645004\n",
      "Training iteration: 3848\n",
      "Validation loss (no improvement): 0.011352770030498505\n",
      "Training iteration: 3849\n",
      "Validation loss (no improvement): 0.010695059597492219\n",
      "Training iteration: 3850\n",
      "Validation loss (no improvement): 0.010456033051013947\n",
      "Training iteration: 3851\n",
      "Validation loss (no improvement): 0.010445065796375275\n",
      "Training iteration: 3852\n",
      "Validation loss (no improvement): 0.010620181262493134\n",
      "Training iteration: 3853\n",
      "Validation loss (no improvement): 0.011352121829986572\n",
      "Training iteration: 3854\n",
      "Validation loss (no improvement): 0.011585967242717743\n",
      "Training iteration: 3855\n",
      "Validation loss (no improvement): 0.010960538685321809\n",
      "Training iteration: 3856\n",
      "Validation loss (no improvement): 0.010777957737445831\n",
      "Training iteration: 3857\n",
      "Validation loss (no improvement): 0.011001060903072356\n",
      "Training iteration: 3858\n",
      "Validation loss (no improvement): 0.01091352254152298\n",
      "Training iteration: 3859\n",
      "Validation loss (no improvement): 0.011474826186895371\n",
      "Training iteration: 3860\n",
      "Validation loss (no improvement): 0.011721467971801758\n",
      "Training iteration: 3861\n",
      "Validation loss (no improvement): 0.011462829262018203\n",
      "Training iteration: 3862\n",
      "Validation loss (no improvement): 0.011031146347522735\n",
      "Training iteration: 3863\n",
      "Validation loss (no improvement): 0.010909364372491837\n",
      "Training iteration: 3864\n",
      "Validation loss (no improvement): 0.010893674194812774\n",
      "Training iteration: 3865\n",
      "Validation loss (no improvement): 0.01115100160241127\n",
      "Training iteration: 3866\n",
      "Validation loss (no improvement): 0.012051747739315033\n",
      "Training iteration: 3867\n",
      "Validation loss (no improvement): 0.012133343517780304\n",
      "Training iteration: 3868\n",
      "Validation loss (no improvement): 0.011481779813766479\n",
      "Training iteration: 3869\n",
      "Validation loss (no improvement): 0.011266018450260162\n",
      "Training iteration: 3870\n",
      "Validation loss (no improvement): 0.0113322876393795\n",
      "Training iteration: 3871\n",
      "Validation loss (no improvement): 0.011106036603450775\n",
      "Training iteration: 3872\n",
      "Validation loss (no improvement): 0.010969328880310058\n",
      "Training iteration: 3873\n",
      "Validation loss (no improvement): 0.011265718936920166\n",
      "Training iteration: 3874\n",
      "Validation loss (no improvement): 0.011166097968816758\n",
      "Training iteration: 3875\n",
      "Improved validation loss from: 0.010441567748785019  to: 0.010440468788146973\n",
      "Training iteration: 3876\n",
      "Improved validation loss from: 0.010440468788146973  to: 0.010156531631946564\n",
      "Training iteration: 3877\n",
      "Improved validation loss from: 0.010156531631946564  to: 0.010113706439733505\n",
      "Training iteration: 3878\n",
      "Validation loss (no improvement): 0.010507830232381821\n",
      "Training iteration: 3879\n",
      "Validation loss (no improvement): 0.010522085428237914\n",
      "Training iteration: 3880\n",
      "Validation loss (no improvement): 0.010165958106517792\n",
      "Training iteration: 3881\n",
      "Validation loss (no improvement): 0.010195215046405793\n",
      "Training iteration: 3882\n",
      "Validation loss (no improvement): 0.010484127700328827\n",
      "Training iteration: 3883\n",
      "Validation loss (no improvement): 0.010489807277917863\n",
      "Training iteration: 3884\n",
      "Validation loss (no improvement): 0.010460251569747924\n",
      "Training iteration: 3885\n",
      "Validation loss (no improvement): 0.010527010262012481\n",
      "Training iteration: 3886\n",
      "Validation loss (no improvement): 0.0105734683573246\n",
      "Training iteration: 3887\n",
      "Validation loss (no improvement): 0.010495395958423614\n",
      "Training iteration: 3888\n",
      "Validation loss (no improvement): 0.010326504707336426\n",
      "Training iteration: 3889\n",
      "Validation loss (no improvement): 0.010219220817089082\n",
      "Training iteration: 3890\n",
      "Validation loss (no improvement): 0.010660245269536971\n",
      "Training iteration: 3891\n",
      "Validation loss (no improvement): 0.010783068835735321\n",
      "Training iteration: 3892\n",
      "Validation loss (no improvement): 0.010134945064783097\n",
      "Training iteration: 3893\n",
      "Improved validation loss from: 0.010113706439733505  to: 0.00987953096628189\n",
      "Training iteration: 3894\n",
      "Improved validation loss from: 0.00987953096628189  to: 0.00984431654214859\n",
      "Training iteration: 3895\n",
      "Validation loss (no improvement): 0.010143611580133438\n",
      "Training iteration: 3896\n",
      "Validation loss (no improvement): 0.01008460745215416\n",
      "Training iteration: 3897\n",
      "Validation loss (no improvement): 0.009855161607265472\n",
      "Training iteration: 3898\n",
      "Validation loss (no improvement): 0.00994524359703064\n",
      "Training iteration: 3899\n",
      "Validation loss (no improvement): 0.01011502593755722\n",
      "Training iteration: 3900\n",
      "Validation loss (no improvement): 0.010542388260364532\n",
      "Training iteration: 3901\n",
      "Validation loss (no improvement): 0.01039815992116928\n",
      "Training iteration: 3902\n",
      "Validation loss (no improvement): 0.010113687813282013\n",
      "Training iteration: 3903\n",
      "Validation loss (no improvement): 0.010144813358783722\n",
      "Training iteration: 3904\n",
      "Validation loss (no improvement): 0.010104803740978241\n",
      "Training iteration: 3905\n",
      "Validation loss (no improvement): 0.01010611280798912\n",
      "Training iteration: 3906\n",
      "Validation loss (no improvement): 0.009982649236917496\n",
      "Training iteration: 3907\n",
      "Validation loss (no improvement): 0.009844701737165451\n",
      "Training iteration: 3908\n",
      "Validation loss (no improvement): 0.009954623878002167\n",
      "Training iteration: 3909\n",
      "Validation loss (no improvement): 0.010051238536834716\n",
      "Training iteration: 3910\n",
      "Validation loss (no improvement): 0.010187945514917373\n",
      "Training iteration: 3911\n",
      "Validation loss (no improvement): 0.010114379227161407\n",
      "Training iteration: 3912\n",
      "Validation loss (no improvement): 0.010070119053125381\n",
      "Training iteration: 3913\n",
      "Validation loss (no improvement): 0.010128937661647797\n",
      "Training iteration: 3914\n",
      "Validation loss (no improvement): 0.010162271559238434\n",
      "Training iteration: 3915\n",
      "Validation loss (no improvement): 0.010831224918365478\n",
      "Training iteration: 3916\n",
      "Validation loss (no improvement): 0.011227724701166153\n",
      "Training iteration: 3917\n",
      "Validation loss (no improvement): 0.010611510276794434\n",
      "Training iteration: 3918\n",
      "Validation loss (no improvement): 0.010123474895954132\n",
      "Training iteration: 3919\n",
      "Validation loss (no improvement): 0.010342037677764893\n",
      "Training iteration: 3920\n",
      "Validation loss (no improvement): 0.010536179691553117\n",
      "Training iteration: 3921\n",
      "Validation loss (no improvement): 0.01090989112854004\n",
      "Training iteration: 3922\n",
      "Validation loss (no improvement): 0.010799765586853027\n",
      "Training iteration: 3923\n",
      "Validation loss (no improvement): 0.010539140552282333\n",
      "Training iteration: 3924\n",
      "Validation loss (no improvement): 0.010563421249389648\n",
      "Training iteration: 3925\n",
      "Validation loss (no improvement): 0.010757870972156525\n",
      "Training iteration: 3926\n",
      "Validation loss (no improvement): 0.010687979310750962\n",
      "Training iteration: 3927\n",
      "Validation loss (no improvement): 0.010121188312768935\n",
      "Training iteration: 3928\n",
      "Validation loss (no improvement): 0.009976837038993835\n",
      "Training iteration: 3929\n",
      "Validation loss (no improvement): 0.010144517570734025\n",
      "Training iteration: 3930\n",
      "Validation loss (no improvement): 0.010642144829034805\n",
      "Training iteration: 3931\n",
      "Validation loss (no improvement): 0.010372646898031235\n",
      "Training iteration: 3932\n",
      "Validation loss (no improvement): 0.010035650432109832\n",
      "Training iteration: 3933\n",
      "Validation loss (no improvement): 0.01008886843919754\n",
      "Training iteration: 3934\n",
      "Validation loss (no improvement): 0.010258086770772935\n",
      "Training iteration: 3935\n",
      "Validation loss (no improvement): 0.01061537116765976\n",
      "Training iteration: 3936\n",
      "Validation loss (no improvement): 0.010313601791858673\n",
      "Training iteration: 3937\n",
      "Validation loss (no improvement): 0.009925443679094315\n",
      "Training iteration: 3938\n",
      "Improved validation loss from: 0.00984431654214859  to: 0.009834332764148713\n",
      "Training iteration: 3939\n",
      "Validation loss (no improvement): 0.010032212734222412\n",
      "Training iteration: 3940\n",
      "Validation loss (no improvement): 0.010164888948202134\n",
      "Training iteration: 3941\n",
      "Validation loss (no improvement): 0.009904150664806367\n",
      "Training iteration: 3942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.009910732507705688\n",
      "Training iteration: 3943\n",
      "Validation loss (no improvement): 0.010173823684453964\n",
      "Training iteration: 3944\n",
      "Validation loss (no improvement): 0.010181031376123428\n",
      "Training iteration: 3945\n",
      "Validation loss (no improvement): 0.009895002841949463\n",
      "Training iteration: 3946\n",
      "Improved validation loss from: 0.009834332764148713  to: 0.009794497489929199\n",
      "Training iteration: 3947\n",
      "Improved validation loss from: 0.009794497489929199  to: 0.00979357436299324\n",
      "Training iteration: 3948\n",
      "Validation loss (no improvement): 0.009818523377180099\n",
      "Training iteration: 3949\n",
      "Improved validation loss from: 0.00979357436299324  to: 0.009748218208551407\n",
      "Training iteration: 3950\n",
      "Improved validation loss from: 0.009748218208551407  to: 0.009478456526994705\n",
      "Training iteration: 3951\n",
      "Validation loss (no improvement): 0.009493692219257355\n",
      "Training iteration: 3952\n",
      "Validation loss (no improvement): 0.009596429020166396\n",
      "Training iteration: 3953\n",
      "Improved validation loss from: 0.009478456526994705  to: 0.009381867945194244\n",
      "Training iteration: 3954\n",
      "Improved validation loss from: 0.009381867945194244  to: 0.009259366989135742\n",
      "Training iteration: 3955\n",
      "Improved validation loss from: 0.009259366989135742  to: 0.009225411713123322\n",
      "Training iteration: 3956\n",
      "Improved validation loss from: 0.009225411713123322  to: 0.009198355674743652\n",
      "Training iteration: 3957\n",
      "Validation loss (no improvement): 0.009349779784679412\n",
      "Training iteration: 3958\n",
      "Validation loss (no improvement): 0.009347350150346757\n",
      "Training iteration: 3959\n",
      "Validation loss (no improvement): 0.009450682997703552\n",
      "Training iteration: 3960\n",
      "Validation loss (no improvement): 0.009373487532138824\n",
      "Training iteration: 3961\n",
      "Validation loss (no improvement): 0.009235334396362305\n",
      "Training iteration: 3962\n",
      "Validation loss (no improvement): 0.009518425166606902\n",
      "Training iteration: 3963\n",
      "Validation loss (no improvement): 0.009208488464355468\n",
      "Training iteration: 3964\n",
      "Improved validation loss from: 0.009198355674743652  to: 0.008826382458209991\n",
      "Training iteration: 3965\n",
      "Validation loss (no improvement): 0.008834171295166015\n",
      "Training iteration: 3966\n",
      "Validation loss (no improvement): 0.00884796753525734\n",
      "Training iteration: 3967\n",
      "Validation loss (no improvement): 0.0091483972966671\n",
      "Training iteration: 3968\n",
      "Validation loss (no improvement): 0.009344802796840667\n",
      "Training iteration: 3969\n",
      "Validation loss (no improvement): 0.009251825511455536\n",
      "Training iteration: 3970\n",
      "Validation loss (no improvement): 0.009090810269117355\n",
      "Training iteration: 3971\n",
      "Validation loss (no improvement): 0.009277424961328506\n",
      "Training iteration: 3972\n",
      "Validation loss (no improvement): 0.009216085821390153\n",
      "Training iteration: 3973\n",
      "Validation loss (no improvement): 0.009834508597850799\n",
      "Training iteration: 3974\n",
      "Validation loss (no improvement): 0.0104192852973938\n",
      "Training iteration: 3975\n",
      "Validation loss (no improvement): 0.01006547436118126\n",
      "Training iteration: 3976\n",
      "Validation loss (no improvement): 0.009892147779464722\n",
      "Training iteration: 3977\n",
      "Validation loss (no improvement): 0.009915000200271607\n",
      "Training iteration: 3978\n",
      "Validation loss (no improvement): 0.009815500676631927\n",
      "Training iteration: 3979\n",
      "Validation loss (no improvement): 0.010005650669336319\n",
      "Training iteration: 3980\n",
      "Validation loss (no improvement): 0.009818898886442185\n",
      "Training iteration: 3981\n",
      "Validation loss (no improvement): 0.009278525412082673\n",
      "Training iteration: 3982\n",
      "Validation loss (no improvement): 0.009128616750240326\n",
      "Training iteration: 3983\n",
      "Validation loss (no improvement): 0.009182463586330413\n",
      "Training iteration: 3984\n",
      "Validation loss (no improvement): 0.00928608700633049\n",
      "Training iteration: 3985\n",
      "Validation loss (no improvement): 0.009525952488183975\n",
      "Training iteration: 3986\n",
      "Validation loss (no improvement): 0.009316790848970413\n",
      "Training iteration: 3987\n",
      "Validation loss (no improvement): 0.008904935419559478\n",
      "Training iteration: 3988\n",
      "Validation loss (no improvement): 0.008902640640735626\n",
      "Training iteration: 3989\n",
      "Validation loss (no improvement): 0.00888703614473343\n",
      "Training iteration: 3990\n",
      "Validation loss (no improvement): 0.009229277074337006\n",
      "Training iteration: 3991\n",
      "Validation loss (no improvement): 0.009425635635852813\n",
      "Training iteration: 3992\n",
      "Validation loss (no improvement): 0.009141530096530914\n",
      "Training iteration: 3993\n",
      "Validation loss (no improvement): 0.008926299959421157\n",
      "Training iteration: 3994\n",
      "Validation loss (no improvement): 0.008858084678649902\n",
      "Training iteration: 3995\n",
      "Validation loss (no improvement): 0.009160778671503066\n",
      "Training iteration: 3996\n",
      "Validation loss (no improvement): 0.009232473373413087\n",
      "Training iteration: 3997\n",
      "Validation loss (no improvement): 0.009098587930202484\n",
      "Training iteration: 3998\n",
      "Validation loss (no improvement): 0.009190790355205536\n",
      "Training iteration: 3999\n",
      "Validation loss (no improvement): 0.00927172675728798\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_1 = krishnan_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_1.train_model()\n",
    "ensemble_model_1.model_inference()\n",
    "\n",
    "ensemble_mean_1 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_1 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_1 = np.sqrt(np.exp(ensemble_logvar_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 7.025187683105469\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 7.025187683105469  to: 4.959994506835938\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 4.959994506835938  to: 3.5634979248046874\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 3.5634979248046874  to: 2.6053115844726564\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 2.6053115844726564  to: 1.9363204956054687\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 1.9363204956054687  to: 1.46614351272583\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 1.46614351272583  to: 1.1299171447753906\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 1.1299171447753906  to: 0.8874119758605957\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 0.8874119758605957  to: 0.7115871429443359\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 0.7115871429443359  to: 0.5823779106140137\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 0.5823779106140137  to: 0.48589215278625486\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.48589215278625486  to: 0.4131946086883545\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.4131946086883545  to: 0.3578629970550537\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.3578629970550537  to: 0.3153925895690918\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.3153925895690918  to: 0.2825001239776611\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.2825001239776611  to: 0.25681166648864745\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.25681166648864745  to: 0.23658947944641112\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.23658947944641112  to: 0.2205592632293701\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.2205592632293701  to: 0.20776667594909667\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.20776667594909667  to: 0.19749822616577148\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.19749822616577148  to: 0.18920371532440186\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.18920371532440186  to: 0.18246955871582032\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.18246955871582032  to: 0.1769707679748535\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.1769707679748535  to: 0.17245807647705078\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.17245807647705078  to: 0.16873279809951783\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.16873279809951783  to: 0.16564350128173827\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.16564350128173827  to: 0.16307281255722045\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.16307281255722045  to: 0.1609248161315918\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.1609248161315918  to: 0.15912256240844727\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.15912256240844727  to: 0.15760440826416017\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.15760440826416017  to: 0.15632050037384032\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.15632050037384032  to: 0.1552303194999695\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.1552303194999695  to: 0.15430121421813964\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.15430121421813964  to: 0.15350632667541503\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.15350632667541503  to: 0.15282280445098878\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.15282280445098878  to: 0.152232563495636\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.152232563495636  to: 0.15172048807144164\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.15172048807144164  to: 0.15127414464950562\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.15127414464950562  to: 0.15088310241699218\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.15088310241699218  to: 0.15053861141204833\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.15053861141204833  to: 0.15023279190063477\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.15023279190063477  to: 0.14995933771133424\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.14995933771133424  to: 0.14971390962600709\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.14971390962600709  to: 0.1494921088218689\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.1494921088218689  to: 0.14929027557373048\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.14929027557373048  to: 0.14910533428192138\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.14910533428192138  to: 0.14893463850021363\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.14893463850021363  to: 0.14877603054046631\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.14877603054046631  to: 0.14862771034240724\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.14862771034240724  to: 0.1484881043434143\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.1484881043434143  to: 0.1483557105064392\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.1483557105064392  to: 0.14822949171066285\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.14822949171066285  to: 0.1481086492538452\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.1481086492538452  to: 0.14798977375030517\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.14798977375030517  to: 0.14787428379058837\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.14787428379058837  to: 0.1477621912956238\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.1477621912956238  to: 0.147653067111969\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.147653067111969  to: 0.1475464105606079\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.1475464105606079  to: 0.14744032621383668\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.14744032621383668  to: 0.14733607769012452\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.14733607769012452  to: 0.14723266363143922\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.14723266363143922  to: 0.14713070392608643\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.14713070392608643  to: 0.1470300078392029\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.1470300078392029  to: 0.14693039655685425\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.14693039655685425  to: 0.14683176279067994\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.14683176279067994  to: 0.14673395156860353\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.14673395156860353  to: 0.14663689136505126\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.14663689136505126  to: 0.14654052257537842\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.14654052257537842  to: 0.14644473791122437\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.14644473791122437  to: 0.1463495373725891\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.1463495373725891  to: 0.14625483751296997\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14625483751296997  to: 0.14616062641143798\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.14616062641143798  to: 0.1460668683052063\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.1460668683052063  to: 0.1459735155105591\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.1459735155105591  to: 0.14588056802749633\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.14588056802749633  to: 0.14578800201416015\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.14578800201416015  to: 0.14569499492645263\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.14569499492645263  to: 0.14560220241546631\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.14560220241546631  to: 0.14550974369049072\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.14550974369049072  to: 0.14541765451431274\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.14541765451431274  to: 0.1453258752822876\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.1453258752822876  to: 0.1452344298362732\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.1452344298362732  to: 0.14514331817626952\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.14514331817626952  to: 0.14505252838134766\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.14505252838134766  to: 0.14496206045150756\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.14496206045150756  to: 0.14487193822860717\n",
      "Training iteration: 86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.14487193822860717  to: 0.14478212594985962\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.14478212594985962  to: 0.14469263553619385\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14469263553619385  to: 0.14460344314575196\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.14460344314575196  to: 0.1445145606994629\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1445145606994629  to: 0.14442561864852904\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.14442561864852904  to: 0.144336998462677\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.144336998462677  to: 0.1442487955093384\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.1442487955093384  to: 0.14416099786758424\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.14416099786758424  to: 0.14407342672348022\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14407342672348022  to: 0.1439862608909607\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.1439862608909607  to: 0.14389947652816773\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14389947652816773  to: 0.14381306171417235\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.14381306171417235  to: 0.1437269926071167\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.1437269926071167  to: 0.14364120960235596\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14364120960235596  to: 0.14355510473251343\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14355510473251343  to: 0.1434692144393921\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.1434692144393921  to: 0.1433834195137024\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.1433834195137024  to: 0.14329793453216552\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.14329793453216552  to: 0.14321281909942626\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14321281909942626  to: 0.1431281089782715\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.1431281089782715  to: 0.1430436849594116\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.1430436849594116  to: 0.14295963048934937\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.14295963048934937  to: 0.14287589788436889\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14287589788436889  to: 0.14279252290725708\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14279252290725708  to: 0.1427094578742981\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.1427094578742981  to: 0.14262673854827881\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14262673854827881  to: 0.14254438877105713\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14254438877105713  to: 0.14246232509613038\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14246232509613038  to: 0.14238059520721436\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14238059520721436  to: 0.14229919910430908\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14229919910430908  to: 0.14221811294555664\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14221811294555664  to: 0.14213725328445434\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14213725328445434  to: 0.14205644130706788\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14205644130706788  to: 0.14197592735290526\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14197592735290526  to: 0.14189572334289552\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14189572334289552  to: 0.14181588888168334\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14181588888168334  to: 0.14173641204833984\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14173641204833984  to: 0.1416573166847229\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.1416573166847229  to: 0.14157854318618773\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14157854318618773  to: 0.14149962663650512\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14149962663650512  to: 0.14142123460769654\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14142123460769654  to: 0.1413432240486145\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.1413432240486145  to: 0.14126551151275635\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.14126551151275635  to: 0.14118809700012208\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14118809700012208  to: 0.14111096858978273\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14111096858978273  to: 0.14103410243988038\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14103410243988038  to: 0.14095749855041503\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14095749855041503  to: 0.14088118076324463\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.14088118076324463  to: 0.14080510139465333\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14080510139465333  to: 0.14072929620742797\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14072929620742797  to: 0.14065372943878174\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14065372943878174  to: 0.14057817459106445\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14057817459106445  to: 0.14050289392471313\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14050289392471313  to: 0.14042783975601197\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14042783975601197  to: 0.14035303592681886\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14035303592681886  to: 0.14027849435806275\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14027849435806275  to: 0.14020416736602784\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14020416736602784  to: 0.14013012647628784\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14013012647628784  to: 0.1400562882423401\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.1400562882423401  to: 0.1399825096130371\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.1399825096130371  to: 0.1399090051651001\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1399090051651001  to: 0.1398357629776001\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.1398357629776001  to: 0.1397627592086792\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1397627592086792  to: 0.13969002962112426\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.13969002962112426  to: 0.13961751461029054\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.13961751461029054  to: 0.13954527378082277\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.13954527378082277  to: 0.13947365283966065\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.13947365283966065  to: 0.13940225839614867\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.13940225839614867  to: 0.13933112621307372\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.13933112621307372  to: 0.13926022052764891\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.13926022052764891  to: 0.1391894578933716\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.1391894578933716  to: 0.13911887407302856\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.13911887407302856  to: 0.13904823064804078\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.13904823064804078  to: 0.13897762298583985\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.13897762298583985  to: 0.13890730142593383\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.13890730142593383  to: 0.13883720636367797\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.13883720636367797  to: 0.13876737356185914\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.13876737356185914  to: 0.13869779109954833\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.13869779109954833  to: 0.1386284589767456\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1386284589767456  to: 0.13855937719345093\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.13855937719345093  to: 0.13849053382873536\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.13849053382873536  to: 0.1384219765663147\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1384219765663147  to: 0.1383536458015442\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1383536458015442  to: 0.13828517198562623\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.13828517198562623  to: 0.1382167100906372\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.1382167100906372  to: 0.1381485342979431\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.1381485342979431  to: 0.13808066844940187\n",
      "Training iteration: 173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13808066844940187  to: 0.13801295757293702\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.13801295757293702  to: 0.13794540166854857\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.13794540166854857  to: 0.13787801265716554\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.13787801265716554  to: 0.1378108024597168\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.1378108024597168  to: 0.1377437710762024\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1377437710762024  to: 0.1376768946647644\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.1376768946647644  to: 0.13761022090911865\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.13761022090911865  to: 0.13754372596740722\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.13754372596740722  to: 0.13747756481170653\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.13747756481170653  to: 0.13741164207458495\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.13741164207458495  to: 0.13734588623046876\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.13734588623046876  to: 0.13728033304214476\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.13728033304214476  to: 0.13721495866775513\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.13721495866775513  to: 0.13714976310729982\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.13714976310729982  to: 0.13708475828170777\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13708475828170777  to: 0.13701995611190795\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13701995611190795  to: 0.1369553327560425\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.1369553327560425  to: 0.13689091205596923\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13689091205596923  to: 0.13682667016983033\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13682667016983033  to: 0.1367626190185547\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.1367626190185547  to: 0.13669874668121337\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13669874668121337  to: 0.13663507699966432\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13663507699966432  to: 0.1365715742111206\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.1365715742111206  to: 0.13650825023651122\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13650825023651122  to: 0.13644514083862305\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.13644514083862305  to: 0.13638209104537963\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.13638209104537963  to: 0.13631918430328369\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13631918430328369  to: 0.13625646829605104\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.13625646829605104  to: 0.13619396686553956\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13619396686553956  to: 0.13613160848617553\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13613160848617553  to: 0.13606948852539064\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.13606948852539064  to: 0.13600763082504272\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13600763082504272  to: 0.1359459638595581\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.1359459638595581  to: 0.13588451147079467\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13588451147079467  to: 0.13582327365875244\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13582327365875244  to: 0.13576223850250244\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13576223850250244  to: 0.13570120334625244\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.13570120334625244  to: 0.1356402039527893\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.1356402039527893  to: 0.135579252243042\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.135579252243042  to: 0.1355183482170105\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.1355183482170105  to: 0.13545753955841064\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13545753955841064  to: 0.13539670705795287\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.13539670705795287  to: 0.13533591032028197\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13533591032028197  to: 0.13527523279190062\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13527523279190062  to: 0.13521465063095092\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.13521465063095092  to: 0.13515422344207764\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.13515422344207764  to: 0.13509390354156495\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.13509390354156495  to: 0.1350337505340576\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.1350337505340576  to: 0.13497374057769776\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13497374057769776  to: 0.13491389751434327\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13491389751434327  to: 0.1348542094230652\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.1348542094230652  to: 0.1347947120666504\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.1347947120666504  to: 0.134735369682312\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.134735369682312  to: 0.13467620611190795\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13467620611190795  to: 0.13461726903915405\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13461726903915405  to: 0.13455852270126342\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13455852270126342  to: 0.13450000286102295\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13450000286102295  to: 0.13444169759750366\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13444169759750366  to: 0.1343836307525635\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.1343836307525635  to: 0.1343258023262024\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.1343258023262024  to: 0.13426820039749146\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13426820039749146  to: 0.13421084880828857\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.13421084880828857  to: 0.13415374755859374\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.13415374755859374  to: 0.13409690856933593\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13409690856933593  to: 0.13404029607772827\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.13404029607772827  to: 0.13398396968841553\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13398396968841553  to: 0.13392789363861085\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.13392789363861085  to: 0.13387207984924315\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.13387207984924315  to: 0.1338165283203125\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.1338165283203125  to: 0.13376123905181886\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13376123905181886  to: 0.13370641469955444\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13370641469955444  to: 0.13365187644958496\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.13365187644958496  to: 0.1335976243019104\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.1335976243019104  to: 0.13354365825653075\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13354365825653075  to: 0.1334899663925171\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.1334899663925171  to: 0.1334365963935852\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.1334365963935852  to: 0.13338351249694824\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13338351249694824  to: 0.13333072662353515\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.13333072662353515  to: 0.1332782506942749\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.1332782506942749  to: 0.13322608470916747\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13322608470916747  to: 0.13317420482635497\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13317420482635497  to: 0.13312265872955323\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13312265872955323  to: 0.1330713987350464\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.1330713987350464  to: 0.1330204963684082\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1330204963684082  to: 0.132969868183136\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.132969868183136  to: 0.13291956186294557\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.13291956186294557  to: 0.13286956548690795\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13286956548690795  to: 0.13281990289688111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13281990289688111  to: 0.13277020454406738\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.13277020454406738  to: 0.13272048234939576\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13272048234939576  to: 0.13267080783843993\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13267080783843993  to: 0.132621169090271\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.132621169090271  to: 0.13257162570953368\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13257162570953368  to: 0.13252217769622804\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13252217769622804  to: 0.13247287273406982\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.13247287273406982  to: 0.13242372274398803\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13242372274398803  to: 0.13237473964691163\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.13237473964691163  to: 0.1323259472846985\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.1323259472846985  to: 0.13227735757827758\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13227735757827758  to: 0.1322289824485779\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.1322289824485779  to: 0.13218085765838622\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13218085765838622  to: 0.13213299512863158\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13213299512863158  to: 0.13208540678024291\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13208540678024291  to: 0.13203810453414916\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13203810453414916  to: 0.13199111223220825\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13199111223220825  to: 0.1319444179534912\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1319444179534912  to: 0.13189804553985596\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13189804553985596  to: 0.13185198307037355\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13185198307037355  to: 0.13180626630783082\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13180626630783082  to: 0.13176090717315675\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13176090717315675  to: 0.13171590566635133\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13171590566635133  to: 0.1316712737083435\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1316712737083435  to: 0.1316270112991333\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.1316270112991333  to: 0.1315831422805786\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1315831422805786  to: 0.13153963088989257\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13153963088989257  to: 0.13149653673171996\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13149653673171996  to: 0.131453800201416\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.131453800201416  to: 0.13141148090362548\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13141148090362548  to: 0.13136954307556153\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13136954307556153  to: 0.13132801055908203\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.13132801055908203  to: 0.131286883354187\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.131286883354187  to: 0.13124616146087648\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13124616146087648  to: 0.1312058687210083\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.1312058687210083  to: 0.1311659574508667\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.1311659574508667  to: 0.1311264753341675\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1311264753341675  to: 0.13108739852905274\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13108739852905274  to: 0.1310487151145935\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.1310487151145935  to: 0.13101046085357665\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13101046085357665  to: 0.13097257614135743\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13097257614135743  to: 0.1309356451034546\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.1309356451034546  to: 0.13089959621429442\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13089959621429442  to: 0.13086438179016113\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13086438179016113  to: 0.13082996606826783\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13082996606826783  to: 0.13079630136489867\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13079630136489867  to: 0.13076335191726685\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13076335191726685  to: 0.13073108196258545\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13073108196258545  to: 0.13069945573806763\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.13069945573806763  to: 0.13066843748092652\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13066843748092652  to: 0.1306380271911621\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.1306380271911621  to: 0.1306081533432007\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.1306081533432007  to: 0.1305788278579712\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.1305788278579712  to: 0.13054999113082885\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.13054999113082885  to: 0.13052165508270264\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13052165508270264  to: 0.13049377202987672\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13049377202987672  to: 0.13046631813049317\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13046631813049317  to: 0.130439293384552\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.130439293384552  to: 0.1304126501083374\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1304126501083374  to: 0.13038641214370728\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13038641214370728  to: 0.1303605318069458\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1303605318069458  to: 0.13033500909805298\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13033500909805298  to: 0.130309796333313\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.130309796333313  to: 0.13028491735458375\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13028491735458375  to: 0.1302603602409363\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.1302603602409363  to: 0.13023611307144164\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13023611307144164  to: 0.1302121639251709\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.1302121639251709  to: 0.1301884889602661\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1301884889602661  to: 0.13016507625579835\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13016507625579835  to: 0.1301419496536255\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.1301419496536255  to: 0.13011906147003174\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.13011906147003174  to: 0.13009639978408813\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13009639978408813  to: 0.13007397651672364\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13007397651672364  to: 0.13005177974700927\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13005177974700927  to: 0.13002978563308715\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13002978563308715  to: 0.13000795841217042\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.13000795841217042  to: 0.12998640537261963\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.12998640537261963  to: 0.12996505498886107\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.12996505498886107  to: 0.1299438714981079\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.1299438714981079  to: 0.12992289066314697\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.12992289066314697  to: 0.12990207672119142\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.12990207672119142  to: 0.12988144159317017\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.12988144159317017  to: 0.1298609733581543\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.1298609733581543  to: 0.1298406958580017\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.1298406958580017  to: 0.12982057332992553\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.12982057332992553  to: 0.12980061769485474\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.12980061769485474  to: 0.12978070974349976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 348\n",
      "Improved validation loss from: 0.12978070974349976  to: 0.12976096868515014\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.12976096868515014  to: 0.129741370677948\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.129741370677948  to: 0.12972192764282225\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.12972192764282225  to: 0.1297026038169861\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1297026038169861  to: 0.12968343496322632\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.12968343496322632  to: 0.1296643614768982\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.1296643614768982  to: 0.1296454429626465\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.1296454429626465  to: 0.1296267032623291\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1296267032623291  to: 0.1296081066131592\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.1296081066131592  to: 0.12958962917327882\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.12958962917327882  to: 0.1295713186264038\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.1295713186264038  to: 0.12955312728881835\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.12955312728881835  to: 0.1295350432395935\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1295350432395935  to: 0.1295170307159424\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.1295170307159424  to: 0.12949905395507813\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.12949905395507813  to: 0.12948119640350342\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.12948119640350342  to: 0.12946345806121826\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.12946345806121826  to: 0.12944583892822265\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.12944583892822265  to: 0.12942831516265868\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.12942831516265868  to: 0.12941087484359742\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.12941087484359742  to: 0.12939354181289672\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.12939354181289672  to: 0.12937631607055664\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.12937631607055664  to: 0.12935917377471923\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.12935917377471923  to: 0.12934211492538453\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.12934211492538453  to: 0.12932517528533935\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.12932517528533935  to: 0.12930829524993898\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.12930829524993898  to: 0.12929152250289916\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.12929152250289916  to: 0.12927484512329102\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.12927484512329102  to: 0.12925827503204346\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.12925827503204346  to: 0.12924177646636964\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.12924177646636964  to: 0.12922536134719848\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.12922536134719848  to: 0.12920904159545898\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.12920904159545898  to: 0.1291928172111511\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.1291928172111511  to: 0.12917667627334595\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.12917667627334595  to: 0.1291606068611145\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.1291606068611145  to: 0.12914462089538575\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.12914462089538575  to: 0.12912871837615966\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.12912871837615966  to: 0.1291128635406494\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.1291128635406494  to: 0.1290971040725708\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.1290971040725708  to: 0.129081392288208\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.129081392288208  to: 0.12906574010848998\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.12906574010848998  to: 0.1290501832962036\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1290501832962036  to: 0.12903467416763306\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.12903467416763306  to: 0.12901923656463624\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.12901923656463624  to: 0.12900383472442628\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12900383472442628  to: 0.12898849248886107\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12898849248886107  to: 0.12897313833236695\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.12897313833236695  to: 0.12895785570144652\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.12895785570144652  to: 0.12894259691238402\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.12894259691238402  to: 0.1289273500442505\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1289273500442505  to: 0.1289118528366089\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1289118528366089  to: 0.1288964033126831\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.1288964033126831  to: 0.12888100147247314\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12888100147247314  to: 0.1288656234741211\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.1288656234741211  to: 0.12885029315948487\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.12885029315948487  to: 0.1288349747657776\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.1288349747657776  to: 0.12881969213485717\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.12881969213485717  to: 0.12880443334579467\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.12880443334579467  to: 0.12878921031951904\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.12878921031951904  to: 0.1287740111351013\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.1287740111351013  to: 0.1287588357925415\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.1287588357925415  to: 0.12874360084533693\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.12874360084533693  to: 0.12872838973999023\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.12872838973999023  to: 0.1287131905555725\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1287131905555725  to: 0.1286980152130127\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.1286980152130127  to: 0.1286828637123108\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1286828637123108  to: 0.1286677360534668\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.1286677360534668  to: 0.12865267992019652\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.12865267992019652  to: 0.12863763570785522\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.12863763570785522  to: 0.12862261533737182\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.12862261533737182  to: 0.1286076307296753\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.1286076307296753  to: 0.1285926580429077\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.1285926580429077  to: 0.12857768535614014\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.12857768535614014  to: 0.12856268882751465\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.12856268882751465  to: 0.12854771614074706\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.12854771614074706  to: 0.12853275537490844\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.12853275537490844  to: 0.1285177946090698\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.1285177946090698  to: 0.12850285768508912\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.12850285768508912  to: 0.1284879207611084\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.1284879207611084  to: 0.1284729242324829\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1284729242324829  to: 0.1284576654434204\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1284576654434204  to: 0.1284424066543579\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.1284424066543579  to: 0.1284271478652954\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1284271478652954  to: 0.1284118890762329\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.1284118890762329  to: 0.1283965826034546\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1283965826034546  to: 0.12838128805160523\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.12838128805160523  to: 0.12836596965789795\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.12836596965789795  to: 0.12835062742233277\n",
      "Training iteration: 436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12835062742233277  to: 0.12833526134490966\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.12833526134490966  to: 0.12831971645355225\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.12831971645355225  to: 0.128304123878479\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.128304123878479  to: 0.12828849554061889\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.12828849554061889  to: 0.12827279567718505\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.12827279567718505  to: 0.12825709581375122\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12825709581375122  to: 0.12824134826660155\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.12824134826660155  to: 0.128225576877594\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.128225576877594  to: 0.1282097578048706\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.1282097578048706  to: 0.12819392681121827\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12819392681121827  to: 0.1281780481338501\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.1281780481338501  to: 0.12816213369369506\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.12816213369369506  to: 0.12814619541168212\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.12814619541168212  to: 0.12813020944595338\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.12813020944595338  to: 0.1281141757965088\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.1281141757965088  to: 0.12809814214706422\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.12809814214706422  to: 0.12808207273483277\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.12808207273483277  to: 0.12806599140167235\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.12806599140167235  to: 0.12804988622665406\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.12804988622665406  to: 0.12803375720977783\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.12803375720977783  to: 0.12801761627197267\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.12801761627197267  to: 0.12800145149230957\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.12800145149230957  to: 0.1279853105545044\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.1279853105545044  to: 0.12796920537948608\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.12796920537948608  to: 0.12795311212539673\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12795311212539673  to: 0.12793701887130737\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.12793701887130737  to: 0.1279209852218628\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.1279209852218628  to: 0.12790502309799195\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.12790502309799195  to: 0.12788903713226318\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12788903713226318  to: 0.12787307500839235\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12787307500839235  to: 0.12785713672637938\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.12785713672637938  to: 0.12784117460250854\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.12784117460250854  to: 0.1278252363204956\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.1278252363204956  to: 0.1278092622756958\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.1278092622756958  to: 0.1277933120727539\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.1277933120727539  to: 0.12777734994888307\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.12777734994888307  to: 0.12776135206222533\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.12776135206222533  to: 0.12774531841278075\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.12774531841278075  to: 0.12772929668426514\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.12772929668426514  to: 0.12771322727203369\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.12771322727203369  to: 0.12769713401794433\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.12769713401794433  to: 0.1276810050010681\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.1276810050010681  to: 0.12766484022140503\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.12766484022140503  to: 0.12764865159988403\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.12764865159988403  to: 0.12763240337371826\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.12763240337371826  to: 0.12761614322662354\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.12761614322662354  to: 0.1275998592376709\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.1275998592376709  to: 0.12758352756500244\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12758352756500244  to: 0.12756714820861817\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.12756714820861817  to: 0.12755073308944703\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12755073308944703  to: 0.12753428220748902\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.12753428220748902  to: 0.12751779556274415\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.12751779556274415  to: 0.12750126123428346\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.12750126123428346  to: 0.12748467922210693\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.12748467922210693  to: 0.1274680495262146\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.1274680495262146  to: 0.1274513840675354\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.1274513840675354  to: 0.12743467092514038\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.12743467092514038  to: 0.12741791009902953\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12741791009902953  to: 0.1274011254310608\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.1274011254310608  to: 0.12738428115844727\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.12738428115844727  to: 0.12736740112304687\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.12736740112304687  to: 0.12735047340393066\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.12735047340393066  to: 0.12733350992202758\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.12733350992202758  to: 0.12731649875640869\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.12731649875640869  to: 0.12729945182800292\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.12729945182800292  to: 0.12728235721588135\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.12728235721588135  to: 0.1272652268409729\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.1272652268409729  to: 0.12724807262420654\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.12724807262420654  to: 0.12723087072372435\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12723087072372435  to: 0.12721363306045533\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12721363306045533  to: 0.12719634771347046\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12719634771347046  to: 0.12717902660369873\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12717902660369873  to: 0.12716169357299806\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.12716169357299806  to: 0.12714427709579468\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12714427709579468  to: 0.1271268606185913\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.1271268606185913  to: 0.12710940837860107\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12710940837860107  to: 0.12709190845489501\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.12709190845489501  to: 0.1270743727684021\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.1270743727684021  to: 0.12705678939819337\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12705678939819337  to: 0.1270391821861267\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.1270391821861267  to: 0.12702156305313111\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12702156305313111  to: 0.12700389623641967\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.12700389623641967  to: 0.1269862174987793\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.1269862174987793  to: 0.12696850299835205\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.12696850299835205  to: 0.1269507646560669\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.1269507646560669  to: 0.12693297863006592\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.12693297863006592  to: 0.12691516876220704\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.12691516876220704  to: 0.12689732313156127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 524\n",
      "Improved validation loss from: 0.12689732313156127  to: 0.12687947750091552\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.12687947750091552  to: 0.1268615961074829\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.1268615961074829  to: 0.12684367895126342\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12684367895126342  to: 0.12682573795318602\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.12682573795318602  to: 0.12680777311325073\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.12680777311325073  to: 0.12678976058959962\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12678976058959962  to: 0.12677173614501952\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.12677173614501952  to: 0.12675368785858154\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.12675368785858154  to: 0.12673563957214357\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.12673563957214357  to: 0.12671757936477662\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.12671757936477662  to: 0.1266995668411255\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.1266995668411255  to: 0.1266815185546875\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.1266815185546875  to: 0.12666348218917847\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.12666348218917847  to: 0.12664542198181153\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.12664542198181153  to: 0.12662732601165771\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.12662732601165771  to: 0.1266092300415039\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.1266092300415039  to: 0.12659108638763428\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.12659108638763428  to: 0.1265729308128357\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.1265729308128357  to: 0.1265547513961792\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.1265547513961792  to: 0.1265365242958069\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.1265365242958069  to: 0.12651827335357665\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.12651827335357665  to: 0.12650001049041748\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.12650001049041748  to: 0.12648171186447144\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12648171186447144  to: 0.1264634132385254\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.1264634132385254  to: 0.12644509077072144\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12644509077072144  to: 0.1264268398284912\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.1264268398284912  to: 0.1264086127281189\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.1264086127281189  to: 0.12639042139053344\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12639042139053344  to: 0.12637217044830323\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12637217044830323  to: 0.1263538718223572\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.1263538718223572  to: 0.1263355255126953\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.1263355255126953  to: 0.12631714344024658\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12631714344024658  to: 0.126298725605011\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.126298725605011  to: 0.12628021240234374\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.12628021240234374  to: 0.12626166343688966\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12626166343688966  to: 0.12624306678771974\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.12624306678771974  to: 0.12622441053390504\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12622441053390504  to: 0.12620569467544557\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12620569467544557  to: 0.12618694305419922\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12618694305419922  to: 0.12616817951202391\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12616817951202391  to: 0.12614939212799073\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12614939212799073  to: 0.1261305332183838\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.1261305332183838  to: 0.12611165046691894\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.12611165046691894  to: 0.12609277963638305\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.12609277963638305  to: 0.1260738968849182\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.1260738968849182  to: 0.12605504989624022\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.12605504989624022  to: 0.1260361909866333\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.1260361909866333  to: 0.12601720094680785\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.12601720094680785  to: 0.125998055934906\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.125998055934906  to: 0.12597891092300414\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12597891092300414  to: 0.1259597897529602\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.1259597897529602  to: 0.12594066858291625\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12594066858291625  to: 0.12592155933380128\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12592155933380128  to: 0.12590248584747316\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.12590248584747316  to: 0.1258833646774292\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.1258833646774292  to: 0.1258642554283142\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.1258642554283142  to: 0.12584514617919923\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12584514617919923  to: 0.12582600116729736\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.12582600116729736  to: 0.12580684423446656\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12580684423446656  to: 0.1257876753807068\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.1257876753807068  to: 0.12576847076416015\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.12576847076416015  to: 0.12574924230575563\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12574924230575563  to: 0.12572996616363524\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.12572996616363524  to: 0.12571065425872802\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.12571065425872802  to: 0.12569133043289185\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12569133043289185  to: 0.12567195892333985\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.12567195892333985  to: 0.12565256357192994\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.12565256357192994  to: 0.1256331205368042\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.1256331205368042  to: 0.12561361789703368\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12561361789703368  to: 0.12559406757354735\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.12559406757354735  to: 0.12557445764541625\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12557445764541625  to: 0.12555477619171143\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12555477619171143  to: 0.12553504705429078\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12553504705429078  to: 0.12551517486572267\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.12551517486572267  to: 0.12549514770507814\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.12549514770507814  to: 0.1254749894142151\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.1254749894142151  to: 0.12545474767684936\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12545474767684936  to: 0.12543445825576782\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12543445825576782  to: 0.1254141092300415\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.1254141092300415  to: 0.1253936767578125\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.1253936767578125  to: 0.12537320852279663\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.12537320852279663  to: 0.12535264492034912\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12535264492034912  to: 0.12533204555511473\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.12533204555511473  to: 0.12531139850616455\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12531139850616455  to: 0.12529067993164061\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12529067993164061  to: 0.12526991367340087\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.12526991367340087  to: 0.12524909973144532\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12524909973144532  to: 0.12522823810577394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12522823810577394  to: 0.1252073287963867\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.1252073287963867  to: 0.12518635988235474\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12518635988235474  to: 0.12516536712646484\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12516536712646484  to: 0.12514432668685913\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12514432668685913  to: 0.1251232624053955\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.1251232624053955  to: 0.12510216236114502\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12510216236114502  to: 0.12508105039596557\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12508105039596557  to: 0.12505995035171508\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12505995035171508  to: 0.12503881454467775\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12503881454467775  to: 0.12501769065856932\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12501769065856932  to: 0.12499653100967408\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12499653100967408  to: 0.12497537136077881\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12497537136077881  to: 0.12495416402816772\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12495416402816772  to: 0.12493294477462769\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12493294477462769  to: 0.12491168975830078\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12491168975830078  to: 0.12489042282104493\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.12489042282104493  to: 0.12486910820007324\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12486910820007324  to: 0.1248477578163147\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.1248477578163147  to: 0.12482637166976929\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12482637166976929  to: 0.12480493783950805\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12480493783950805  to: 0.12478348016738891\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12478348016738891  to: 0.12476197481155396\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12476197481155396  to: 0.12473976612091064\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12473976612091064  to: 0.12471749782562255\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12471749782562255  to: 0.12469520568847656\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12469520568847656  to: 0.12467288970947266\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12467288970947266  to: 0.12465051412582398\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12465051412582398  to: 0.12462806701660156\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12462806701660156  to: 0.12460552453994751\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12460552453994751  to: 0.12458293437957764\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12458293437957764  to: 0.12456024885177612\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12456024885177612  to: 0.12453750371932984\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12453750371932984  to: 0.12451462745666504\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12451462745666504  to: 0.12449167966842652\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12449167966842652  to: 0.12446863651275634\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12446863651275634  to: 0.12444553375244141\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12444553375244141  to: 0.12442225217819214\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12442225217819214  to: 0.12439882755279541\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12439882755279541  to: 0.12437527179718018\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12437527179718018  to: 0.1243515968322754\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.1243515968322754  to: 0.1243277907371521\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.1243277907371521  to: 0.12430387735366821\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12430387735366821  to: 0.12427985668182373\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12427985668182373  to: 0.12425572872161865\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12425572872161865  to: 0.12423149347305298\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12423149347305298  to: 0.12420716285705566\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12420716285705566  to: 0.12418273687362671\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12418273687362671  to: 0.12415821552276611\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12415821552276611  to: 0.12413361072540283\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12413361072540283  to: 0.12410900592803956\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12410900592803956  to: 0.12408441305160522\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12408441305160522  to: 0.12405980825424194\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12405980825424194  to: 0.12403520345687866\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12403520345687866  to: 0.12401058673858642\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12401058673858642  to: 0.12398593425750733\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12398593425750733  to: 0.12396128177642822\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12396128177642822  to: 0.12393661737442016\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.12393661737442016  to: 0.1239119291305542\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.1239119291305542  to: 0.12388724088668823\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12388724088668823  to: 0.12386242151260377\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12386242151260377  to: 0.12383745908737183\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.12383745908737183  to: 0.12381240129470825\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.12381240129470825  to: 0.12378723621368408\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12378723621368408  to: 0.12376194000244141\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12376194000244141  to: 0.12373656034469604\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12373656034469604  to: 0.12371110916137695\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12371110916137695  to: 0.12368555068969726\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.12368555068969726  to: 0.12366005182266235\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.12366005182266235  to: 0.1236345648765564\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.1236345648765564  to: 0.12360912561416626\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12360912561416626  to: 0.12358369827270507\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.12358369827270507  to: 0.12355830669403076\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12355830669403076  to: 0.12353279590606689\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12353279590606689  to: 0.1235072135925293\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.1235072135925293  to: 0.12348155975341797\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12348155975341797  to: 0.12345582246780396\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12345582246780396  to: 0.12343016862869263\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.12343016862869263  to: 0.12340457439422607\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12340457439422607  to: 0.12337905168533325\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12337905168533325  to: 0.12335348129272461\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12335348129272461  to: 0.12332785129547119\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12332785129547119  to: 0.123302161693573\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.123302161693573  to: 0.12327656745910645\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12327656745910645  to: 0.12325104475021362\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.12325104475021362  to: 0.12322547435760497\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12322547435760497  to: 0.1231998324394226\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.1231998324394226  to: 0.12317426204681396\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12317426204681396  to: 0.12314865589141846\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12314865589141846  to: 0.12312309741973877\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.12312309741973877  to: 0.1230975866317749\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.1230975866317749  to: 0.12307196855545044\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12307196855545044  to: 0.1230462670326233\n",
      "Training iteration: 704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1230462670326233  to: 0.12302048206329345\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12302048206329345  to: 0.12299457788467408\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12299457788467408  to: 0.12296874523162842\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12296874523162842  to: 0.12294294834136962\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12294294834136962  to: 0.12291711568832397\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12291711568832397  to: 0.12289135456085205\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12289135456085205  to: 0.12286550998687744\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.12286550998687744  to: 0.12283933162689209\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.12283933162689209  to: 0.12281259298324584\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12281259298324584  to: 0.12278578281402588\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12278578281402588  to: 0.12275884151458741\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12275884151458741  to: 0.1227319598197937\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.1227319598197937  to: 0.12270510196685791\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12270510196685791  to: 0.12267827987670898\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12267827987670898  to: 0.12265148162841796\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.12265148162841796  to: 0.12262455224990845\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12262455224990845  to: 0.12259747982025146\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.12259747982025146  to: 0.12257025241851807\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12257025241851807  to: 0.122542405128479\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.122542405128479  to: 0.12251464128494263\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12251464128494263  to: 0.12248690128326416\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12248690128326416  to: 0.1224590539932251\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.1224590539932251  to: 0.12243125438690186\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12243125438690186  to: 0.12240333557128906\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.12240333557128906  to: 0.12237532138824463\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12237532138824463  to: 0.12234737873077392\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12234737873077392  to: 0.12231947183609009\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12231947183609009  to: 0.12229145765304565\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12229145765304565  to: 0.12226331233978271\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12226331233978271  to: 0.12223508358001708\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12223508358001708  to: 0.12220691442489624\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12220691442489624  to: 0.12217880487442016\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12217880487442016  to: 0.1221507430076599\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.1221507430076599  to: 0.12212250232696534\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12212250232696534  to: 0.12209409475326538\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12209409475326538  to: 0.12206553220748902\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12206553220748902  to: 0.12203681468963623\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12203681468963623  to: 0.1220081090927124\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.1220081090927124  to: 0.12197940349578858\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12197940349578858  to: 0.12195069789886474\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12195069789886474  to: 0.12192182540893555\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12192182540893555  to: 0.12189280986785889\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12189280986785889  to: 0.12186366319656372\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12186366319656372  to: 0.12183455228805543\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12183455228805543  to: 0.12180575132369995\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12180575132369995  to: 0.12177703380584717\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12177703380584717  to: 0.12174818515777588\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12174818515777588  to: 0.12171919345855713\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12171919345855713  to: 0.12169011831283569\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12169011831283569  to: 0.12166092395782471\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12166092395782471  to: 0.1216318130493164\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.1216318130493164  to: 0.12160277366638184\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12160277366638184  to: 0.121573805809021\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.121573805809021  to: 0.12154470682144165\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12154470682144165  to: 0.12151548862457276\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12151548862457276  to: 0.12148616313934327\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12148616313934327  to: 0.12145689725875855\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12145689725875855  to: 0.12142767906188964\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12142767906188964  to: 0.1213983178138733\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.1213983178138733  to: 0.12136882543563843\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12136882543563843  to: 0.12133942842483521\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12133942842483521  to: 0.12131010293960572\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12131010293960572  to: 0.12128064632415772\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12128064632415772  to: 0.12125107049942016\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12125107049942016  to: 0.12122129201889038\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12122129201889038  to: 0.12119154930114746\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12119154930114746  to: 0.1211618185043335\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.1211618185043335  to: 0.12113209962844848\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12113209962844848  to: 0.12110217809677123\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12110217809677123  to: 0.12107207775115966\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12107207775115966  to: 0.12104175090789795\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12104175090789795  to: 0.12101144790649414\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12101144790649414  to: 0.12098147869110107\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12098147869110107  to: 0.12095158100128174\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12095158100128174  to: 0.12092173099517822\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12092173099517822  to: 0.1208921194076538\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.1208921194076538  to: 0.12086272239685059\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.12086272239685059  to: 0.12083332538604737\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12083332538604737  to: 0.1208038568496704\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.1208038568496704  to: 0.1207742691040039\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.1207742691040039  to: 0.12074474096298218\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12074474096298218  to: 0.12071528434753417\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12071528434753417  to: 0.12068583965301513\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12068583965301513  to: 0.12065616846084595\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12065616846084595  to: 0.12062625885009766\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12062625885009766  to: 0.12059609889984131\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12059609889984131  to: 0.12056591510772705\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12056591510772705  to: 0.12053564786911011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12053564786911011  to: 0.12050530910491944\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12050530910491944  to: 0.12047464847564697\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12047464847564697  to: 0.1204437255859375\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.1204437255859375  to: 0.12041251659393311\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12041251659393311  to: 0.12038102149963378\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12038102149963378  to: 0.12034947872161865\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12034947872161865  to: 0.12031787633895874\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12031787633895874  to: 0.12028616666793823\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12028616666793823  to: 0.12025436162948608\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.12025436162948608  to: 0.12022221088409424\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.12022221088409424  to: 0.12018970251083375\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12018970251083375  to: 0.12015688419342041\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12015688419342041  to: 0.12012374401092529\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12012374401092529  to: 0.1200905680656433\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.1200905680656433  to: 0.12005733251571656\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12005733251571656  to: 0.12002403736114502\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12002403736114502  to: 0.11999039649963379\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.11999039649963379  to: 0.11995699405670165\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.11995699405670165  to: 0.1199234127998352\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.1199234127998352  to: 0.11988983154296876\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.11988983154296876  to: 0.11985630989074707\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.11985630989074707  to: 0.11982284784317017\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.11982284784317017  to: 0.11978909969329835\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.11978909969329835  to: 0.1197550654411316\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.1197550654411316  to: 0.11972078084945678\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.11972078084945678  to: 0.11968650817871093\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.11968650817871093  to: 0.11965227127075195\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.11965227127075195  to: 0.11961791515350342\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.11961791515350342  to: 0.1195834994316101\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.1195834994316101  to: 0.11954911947250366\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.11954911947250366  to: 0.11951513290405273\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.11951513290405273  to: 0.11948173046112061\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.11948173046112061  to: 0.11944811344146729\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.11944811344146729  to: 0.11941431760787964\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.11941431760787964  to: 0.11938064098358155\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.11938064098358155  to: 0.11934705972671508\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.11934705972671508  to: 0.11931358575820923\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.11931358575820923  to: 0.11927988529205322\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.11927988529205322  to: 0.11924600601196289\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.11924600601196289  to: 0.11921176910400391\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.11921176910400391  to: 0.1191774845123291\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.1191774845123291  to: 0.11914317607879639\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.11914317607879639  to: 0.11910854578018189\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.11910854578018189  to: 0.11907356977462769\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.11907356977462769  to: 0.11903860569000244\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.11903860569000244  to: 0.1190036654472351\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.1190036654472351  to: 0.1189684510231018\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.1189684510231018  to: 0.11893298625946044\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.11893298625946044  to: 0.1188973069190979\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1188973069190979  to: 0.11886173486709595\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.11886173486709595  to: 0.11882618665695191\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.11882618665695191  to: 0.11879074573516846\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.11879074573516846  to: 0.11875503063201905\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.11875503063201905  to: 0.11871907711029053\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.11871907711029053  to: 0.11868292093276978\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.11868292093276978  to: 0.11864687204360962\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.11864687204360962  to: 0.11861093044281006\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.11861093044281006  to: 0.11857473850250244\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11857473850250244  to: 0.11853830814361573\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11853830814361573  to: 0.11850197315216064\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11850197315216064  to: 0.1184659719467163\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.1184659719467163  to: 0.11843043565750122\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.11843043565750122  to: 0.1183945894241333\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.1183945894241333  to: 0.11835846900939942\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.11835846900939942  to: 0.11832244396209717\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11832244396209717  to: 0.11828644275665283\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.11828644275665283  to: 0.11825046539306641\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.11825046539306641  to: 0.11821413040161133\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.11821413040161133  to: 0.11817743778228759\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.11817743778228759  to: 0.11814053058624267\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.11814053058624267  to: 0.11810373067855835\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.11810373067855835  to: 0.11806695461273194\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.11806695461273194  to: 0.11802985668182372\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.11802985668182372  to: 0.11799252033233643\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.11799252033233643  to: 0.11795518398284913\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.11795518398284913  to: 0.11791787147521973\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.11791787147521973  to: 0.11788007020950317\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.11788007020950317  to: 0.11784179210662842\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.11784179210662842  to: 0.1178031325340271\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.1178031325340271  to: 0.11776444911956788\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.11776444911956788  to: 0.11772576570510865\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.11772576570510865  to: 0.1176870584487915\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.1176870584487915  to: 0.11764793395996094\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.11764793395996094  to: 0.11760843992233276\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.11760843992233276  to: 0.11756861209869385\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.11756861209869385  to: 0.11752887964248657\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.11752887964248657  to: 0.11748926639556885\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.11748926639556885  to: 0.11744943857192994\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.11744943857192994  to: 0.1174094557762146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 881\n",
      "Improved validation loss from: 0.1174094557762146  to: 0.11736972332000732\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.11736972332000732  to: 0.11732769012451172\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.11732769012451172  to: 0.1172835111618042\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.1172835111618042  to: 0.1172370433807373\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.1172370433807373  to: 0.11718863248825073\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.11718863248825073  to: 0.11713904142379761\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.11713904142379761  to: 0.11708855628967285\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.11708855628967285  to: 0.11703741550445557\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.11703741550445557  to: 0.11698585748672485\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.11698585748672485  to: 0.11693365573883056\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.11693365573883056  to: 0.11688110828399659\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.11688110828399659  to: 0.11682884693145752\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.11682884693145752  to: 0.11677697896957398\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.11677697896957398  to: 0.11672565937042237\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.11672565937042237  to: 0.11667453050613404\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.11667453050613404  to: 0.11662410497665406\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.11662410497665406  to: 0.11657397747039795\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.11657397747039795  to: 0.11652469635009766\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.11652469635009766  to: 0.11647701263427734\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.11647701263427734  to: 0.11643016338348389\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.11643016338348389  to: 0.11638411283493041\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.11638411283493041  to: 0.11633882522583008\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.11633882522583008  to: 0.11629372835159302\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.11629372835159302  to: 0.11624879837036133\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.11624879837036133  to: 0.1162040114402771\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.1162040114402771  to: 0.11615917682647706\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.11615917682647706  to: 0.11611474752426147\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.11611474752426147  to: 0.11607067584991455\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.11607067584991455  to: 0.11602671146392822\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.11602671146392822  to: 0.11598281860351563\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.11598281860351563  to: 0.11593852043151856\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.11593852043151856  to: 0.11589372158050537\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.11589372158050537  to: 0.1158484697341919\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.1158484697341919  to: 0.11580326557159423\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.11580326557159423  to: 0.11575812101364136\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.11575812101364136  to: 0.11571298837661743\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.11571298837661743  to: 0.11566729545593261\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.11566729545593261  to: 0.11562110185623169\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.11562110185623169  to: 0.11557443141937256\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.11557443141937256  to: 0.11552788019180298\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.11552788019180298  to: 0.11548140048980712\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.11548140048980712  to: 0.11543499231338501\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.11543499231338501  to: 0.11538808345794678\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.11538808345794678  to: 0.1153407096862793\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.1153407096862793  to: 0.11529301404953003\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.11529301404953003  to: 0.11524555683135987\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.11524555683135987  to: 0.11519832611083984\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.11519832611083984  to: 0.11515120267868043\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.11515120267868043  to: 0.1151036262512207\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.1151036262512207  to: 0.11505558490753173\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.11505558490753173  to: 0.11500711441040039\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.11500711441040039  to: 0.11495881080627442\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.11495881080627442  to: 0.1149106502532959\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.1149106502532959  to: 0.11486196517944336\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.11486196517944336  to: 0.11481339931488037\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.11481339931488037  to: 0.1147642731666565\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.1147642731666565  to: 0.11471456289291382\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.11471456289291382  to: 0.11466525793075562\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.11466525793075562  to: 0.11461623907089233\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.11461623907089233  to: 0.1145674467086792\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.1145674467086792  to: 0.1145179271697998\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.1145179271697998  to: 0.11446765661239625\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.11446765661239625  to: 0.11441669464111329\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.11441669464111329  to: 0.11436502933502198\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.11436502933502198  to: 0.11431294679641724\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.11431294679641724  to: 0.11426053047180176\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.11426053047180176  to: 0.11420780420303345\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.11420780420303345  to: 0.11415112018585205\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.11415112018585205  to: 0.1140900731086731\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.1140900731086731  to: 0.11402528285980225\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.11402528285980225  to: 0.11395809650421143\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.11395809650421143  to: 0.11388933658599854\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.11388933658599854  to: 0.11381957530975342\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.11381957530975342  to: 0.11374927759170532\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.11374927759170532  to: 0.11367825269699097\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.11367825269699097  to: 0.11360667943954468\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.11360667943954468  to: 0.11353559494018554\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.11353559494018554  to: 0.11346529722213745\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.11346529722213745  to: 0.11339598894119263\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.11339598894119263  to: 0.11332710981369018\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.11332710981369018  to: 0.11325963735580444\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.11325963735580444  to: 0.11319429874420166\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.11319429874420166  to: 0.11313101053237914\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.11313101053237914  to: 0.11306865215301513\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.11306865215301513  to: 0.11300742626190186\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.11300742626190186  to: 0.11294746398925781\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.11294746398925781  to: 0.11288843154907227\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.11288843154907227  to: 0.11282925605773926\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.11282925605773926  to: 0.11276969909667969\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.11276969909667969  to: 0.11271042823791504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 971\n",
      "Improved validation loss from: 0.11271042823791504  to: 0.11265113353729247\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.11265113353729247  to: 0.11259076595306397\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.11259076595306397  to: 0.11252931356430054\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.11252931356430054  to: 0.11246637105941773\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.11246637105941773  to: 0.11240273714065552\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.11240273714065552  to: 0.11233859062194824\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.11233859062194824  to: 0.11227496862411498\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.11227496862411498  to: 0.11221014261245728\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.11221014261245728  to: 0.11214444637298585\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.11214444637298585  to: 0.11207797527313232\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.11207797527313232  to: 0.11201099157333375\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.11201099157333375  to: 0.11194372177124023\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.11194372177124023  to: 0.11187721490859985\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.11187721490859985  to: 0.11181190013885497\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.11181190013885497  to: 0.11174780130386353\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.11174780130386353  to: 0.11168385744094848\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.11168385744094848  to: 0.11162011623382569\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.11162011623382569  to: 0.1115565299987793\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.1115565299987793  to: 0.11149313449859619\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.11149313449859619  to: 0.1114298939704895\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.1114298939704895  to: 0.11136670112609863\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.11136670112609863  to: 0.1113044261932373\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.1113044261932373  to: 0.11124286651611329\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.11124286651611329  to: 0.11118080615997314\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.11118080615997314  to: 0.11111812591552735\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.11111812591552735  to: 0.11105473041534424\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.11105473041534424  to: 0.11099052429199219\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.11099052429199219  to: 0.1109265685081482\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.1109265685081482  to: 0.11086273193359375\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.11086273193359375  to: 0.11079782247543335\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.11079782247543335  to: 0.11073191165924072\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.11073191165924072  to: 0.1106643795967102\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.1106643795967102  to: 0.11059534549713135\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.11059534549713135  to: 0.11052497625350952\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.11052497625350952  to: 0.11045401096343994\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.11045401096343994  to: 0.1103830337524414\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.1103830337524414  to: 0.11031215190887451\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.11031215190887451  to: 0.11024031639099122\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.11024031639099122  to: 0.11016757488250732\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.11016757488250732  to: 0.11009409427642822\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.11009409427642822  to: 0.11001954078674317\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.11001954078674317  to: 0.10994498729705811\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.10994498729705811  to: 0.10987063646316528\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.10987063646316528  to: 0.10979750156402587\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.10979750156402587  to: 0.10972452163696289\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.10972452163696289  to: 0.10965150594711304\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.10965150594711304  to: 0.10957835912704468\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.10957835912704468  to: 0.10950615406036376\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.10950615406036376  to: 0.10943340063095093\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.10943340063095093  to: 0.10936014652252198\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.10936014652252198  to: 0.10928533077239991\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.10928533077239991  to: 0.10920209884643554\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.10920209884643554  to: 0.10911155939102173\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.10911155939102173  to: 0.10901639461517335\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.10901639461517335  to: 0.10891809463500976\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.10891809463500976  to: 0.1088181972503662\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.1088181972503662  to: 0.10871775150299072\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.10871775150299072  to: 0.10861822366714477\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.10861822366714477  to: 0.10852047204971313\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.10852047204971313  to: 0.10842487812042237\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.10842487812042237  to: 0.10833184719085694\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.10833184719085694  to: 0.10824146270751953\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.10824146270751953  to: 0.10815293788909912\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.10815293788909912  to: 0.10806728601455688\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.10806728601455688  to: 0.1079837441444397\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1079837441444397  to: 0.10790001153945923\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.10790001153945923  to: 0.10781453847885132\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.10781453847885132  to: 0.10772664546966552\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.10772664546966552  to: 0.10763592720031738\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.10763592720031738  to: 0.10754215717315674\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.10754215717315674  to: 0.10744552612304688\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.10744552612304688  to: 0.10734654664993286\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.10734654664993286  to: 0.10724575519561767\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.10724575519561767  to: 0.10714391469955445\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.10714391469955445  to: 0.10704162120819091\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.10704162120819091  to: 0.10693958997726441\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.10693958997726441  to: 0.10683943033218384\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.10683943033218384  to: 0.10674242973327637\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.10674242973327637  to: 0.1066486120223999\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.1066486120223999  to: 0.10655775070190429\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.10655775070190429  to: 0.10646774768829345\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.10646774768829345  to: 0.1063773512840271\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.1063773512840271  to: 0.10628585815429688\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.10628585815429688  to: 0.10619319677352905\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.10619319677352905  to: 0.10609915256500244\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.10609915256500244  to: 0.10600354671478271\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.10600354671478271  to: 0.10590656995773315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.10590656995773315  to: 0.1058077096939087\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.1058077096939087  to: 0.10570751428604126\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.10570751428604126  to: 0.1056058168411255\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.1056058168411255  to: 0.10550312995910645\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.10550312995910645  to: 0.10539971590042115\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.10539971590042115  to: 0.10529664754867554\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.10529664754867554  to: 0.10519304275512695\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.10519304275512695  to: 0.10508972406387329\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.10508972406387329  to: 0.10498652458190919\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.10498652458190919  to: 0.10488355159759521\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.10488355159759521  to: 0.10478032827377319\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.10478032827377319  to: 0.10467681884765626\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.10467681884765626  to: 0.10457260608673095\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.10457260608673095  to: 0.104466712474823\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.104466712474823  to: 0.10435917377471923\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.10435917377471923  to: 0.10425008535385132\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.10425008535385132  to: 0.10413963794708252\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.10413963794708252  to: 0.10402822494506836\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.10402822494506836  to: 0.10391591787338257\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.10391591787338257  to: 0.1038029670715332\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.1038029670715332  to: 0.10368955135345459\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.10368955135345459  to: 0.10357930660247802\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.10357930660247802  to: 0.10347189903259277\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.10347189903259277  to: 0.10336554050445557\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.10336554050445557  to: 0.10325939655303955\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.10325939655303955  to: 0.10315132141113281\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.10315132141113281  to: 0.10304048061370849\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.10304048061370849  to: 0.10292675495147705\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.10292675495147705  to: 0.1028102159500122\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.1028102159500122  to: 0.1026909351348877\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.1026909351348877  to: 0.10256965160369873\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.10256965160369873  to: 0.10244721174240112\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.10244721174240112  to: 0.102324378490448\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.102324378490448  to: 0.10220158100128174\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.10220158100128174  to: 0.10207904577255249\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.10207904577255249  to: 0.10195664167404175\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.10195664167404175  to: 0.1018339991569519\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.1018339991569519  to: 0.10171056985855102\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.10171056985855102  to: 0.10158545970916748\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.10158545970916748  to: 0.10145795345306396\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.10145795345306396  to: 0.1013293743133545\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.1013293743133545  to: 0.10119850635528564\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.10119850635528564  to: 0.10106546878814697\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.10106546878814697  to: 0.10092918872833252\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.10092918872833252  to: 0.10079085826873779\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.10079085826873779  to: 0.10065150260925293\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.10065150260925293  to: 0.10051174163818359\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.10051174163818359  to: 0.10037178993225097\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.10037178993225097  to: 0.10023139715194702\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.10023139715194702  to: 0.10009015798568725\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.10009015798568725  to: 0.09994753003120423\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.09994753003120423  to: 0.09980311393737792\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.09980311393737792  to: 0.09965677261352539\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.09965677261352539  to: 0.09950873255729675\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.09950873255729675  to: 0.09935951232910156\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.09935951232910156  to: 0.09920978546142578\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.09920978546142578  to: 0.0989601731300354\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.0989601731300354  to: 0.09865290522575379\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.09865290522575379  to: 0.09843596220016479\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.09843596220016479  to: 0.09831581115722657\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.09831581115722657  to: 0.09817514419555665\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.09817514419555665  to: 0.09801017642021179\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.09801017642021179  to: 0.09782422184944153\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.09782422184944153  to: 0.09765030145645141\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.09765030145645141  to: 0.09749259948730468\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.09749259948730468  to: 0.09735123515129089\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.09735123515129089  to: 0.09722322225570679\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.09722322225570679  to: 0.09710191488265991\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.09710191488265991  to: 0.09697940945625305\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.09697940945625305  to: 0.09685098528861999\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.09685098528861999  to: 0.09671446681022644\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.09671446681022644  to: 0.09650166630744934\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.09650166630744934  to: 0.09632388353347779\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.09632388353347779  to: 0.09619539976119995\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.09619539976119995  to: 0.09611258506774903\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.09611258506774903  to: 0.09605531692504883\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.09605531692504883  to: 0.09588411450386047\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.09588411450386047  to: 0.09562422037124634\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.09562422037124634  to: 0.09543743133544921\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.09543743133544921  to: 0.09532535672187806\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.09532535672187806  to: 0.09519248008728028\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.09519248008728028  to: 0.09503157734870911\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.09503157734870911  to: 0.09484858512878418\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.09484858512878418  to: 0.09465911984443665\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.09465911984443665  to: 0.09448030591011047\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.09448030591011047  to: 0.09432298541069031\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.09432298541069031  to: 0.09418796300888062\n",
      "Training iteration: 1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.09418796300888062  to: 0.09406620860099793\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.09406620860099793  to: 0.09394339323043824\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.09394339323043824  to: 0.09380717277526855\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.09380717277526855  to: 0.0934539020061493\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.0934539020061493  to: 0.09324313998222351\n",
      "Training iteration: 1150\n",
      "Validation loss (no improvement): 0.09326149821281433\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.09324313998222351  to: 0.09309195280075074\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.09309195280075074  to: 0.09275439977645875\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.09275439977645875  to: 0.0924048125743866\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.0924048125743866  to: 0.09237568974494934\n",
      "Training iteration: 1155\n",
      "Validation loss (no improvement): 0.09241595268249511\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.09237568974494934  to: 0.09226991534233094\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.09226991534233094  to: 0.09193570017814637\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.09193570017814637  to: 0.09154915809631348\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.09154915809631348  to: 0.09151995778083802\n",
      "Training iteration: 1160\n",
      "Validation loss (no improvement): 0.09159652590751648\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.09151995778083802  to: 0.0915103554725647\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.0915103554725647  to: 0.09123343229293823\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.09123343229293823  to: 0.09088237881660462\n",
      "Training iteration: 1164\n",
      "Validation loss (no improvement): 0.090892493724823\n",
      "Training iteration: 1165\n",
      "Validation loss (no improvement): 0.09088674783706666\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.09088237881660462  to: 0.09076129794120788\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.09076129794120788  to: 0.09053807258605957\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.09053807258605957  to: 0.0903272032737732\n",
      "Training iteration: 1169\n",
      "Validation loss (no improvement): 0.09047587513923645\n",
      "Training iteration: 1170\n",
      "Validation loss (no improvement): 0.09048951864242553\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.0903272032737732  to: 0.09027889370918274\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.09027889370918274  to: 0.08993291854858398\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.08993291854858398  to: 0.08963934183120728\n",
      "Training iteration: 1174\n",
      "Validation loss (no improvement): 0.08989897966384888\n",
      "Training iteration: 1175\n",
      "Validation loss (no improvement): 0.09001274108886718\n",
      "Training iteration: 1176\n",
      "Validation loss (no improvement): 0.08977434039115906\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.08963934183120728  to: 0.08929300308227539\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.08929300308227539  to: 0.08882820010185241\n",
      "Training iteration: 1179\n",
      "Validation loss (no improvement): 0.08893461227416992\n",
      "Training iteration: 1180\n",
      "Validation loss (no improvement): 0.08923234939575195\n",
      "Training iteration: 1181\n",
      "Validation loss (no improvement): 0.08908459544181824\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.08882820010185241  to: 0.08854684829711915\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.08854684829711915  to: 0.08805997967720032\n",
      "Training iteration: 1184\n",
      "Validation loss (no improvement): 0.08813481330871582\n",
      "Training iteration: 1185\n",
      "Validation loss (no improvement): 0.08842083811759949\n",
      "Training iteration: 1186\n",
      "Validation loss (no improvement): 0.08848407864570618\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.08805997967720032  to: 0.08788951635360717\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.08788951635360717  to: 0.08734936714172363\n",
      "Training iteration: 1189\n",
      "Validation loss (no improvement): 0.08736257553100586\n",
      "Training iteration: 1190\n",
      "Validation loss (no improvement): 0.0875801682472229\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.08742035627365112\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.08734936714172363  to: 0.08688347935676574\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.08688347935676574  to: 0.08651955723762512\n",
      "Training iteration: 1194\n",
      "Validation loss (no improvement): 0.08657696843147278\n",
      "Training iteration: 1195\n",
      "Validation loss (no improvement): 0.08673025965690613\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.08651955723762512  to: 0.0864916205406189\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.0864916205406189  to: 0.08597307205200196\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.08597307205200196  to: 0.08570415377616883\n",
      "Training iteration: 1199\n",
      "Validation loss (no improvement): 0.08587003946304321\n",
      "Training iteration: 1200\n",
      "Validation loss (no improvement): 0.0861403465270996\n",
      "Training iteration: 1201\n",
      "Validation loss (no improvement): 0.08578686714172364\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.08570415377616883  to: 0.08518401384353638\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.08518401384353638  to: 0.08491195440292358\n",
      "Training iteration: 1204\n",
      "Validation loss (no improvement): 0.0852454960346222\n",
      "Training iteration: 1205\n",
      "Validation loss (no improvement): 0.08553842306137086\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.08491195440292358  to: 0.08489967584609985\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.08489967584609985  to: 0.08410900235176086\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.08410900235176086  to: 0.08399226069450379\n",
      "Training iteration: 1209\n",
      "Validation loss (no improvement): 0.08436748385429382\n",
      "Training iteration: 1210\n",
      "Validation loss (no improvement): 0.08431276082992553\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.08399226069450379  to: 0.08374579548835755\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.08374579548835755  to: 0.08341410756111145\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.08341410756111145  to: 0.08334094285964966\n",
      "Training iteration: 1214\n",
      "Validation loss (no improvement): 0.0835008978843689\n",
      "Training iteration: 1215\n",
      "Validation loss (no improvement): 0.08382139205932618\n",
      "Training iteration: 1216\n",
      "Validation loss (no improvement): 0.0833523452281952\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.08334094285964966  to: 0.08288511037826538\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.08288511037826538  to: 0.08272684216499329\n",
      "Training iteration: 1219\n",
      "Validation loss (no improvement): 0.08285220861434936\n",
      "Training iteration: 1220\n",
      "Validation loss (no improvement): 0.08310630917549133\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.08272684216499329  to: 0.08250811696052551\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.08250811696052551  to: 0.0819064736366272\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.0819064736366272  to: 0.08164583444595337\n",
      "Training iteration: 1224\n",
      "Validation loss (no improvement): 0.08171994090080262\n",
      "Training iteration: 1225\n",
      "Validation loss (no improvement): 0.08183286786079406\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.08164583444595337  to: 0.08128203153610229\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.08128203153610229  to: 0.08085999488830567\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.08085999488830567  to: 0.08077598810195923\n",
      "Training iteration: 1229\n",
      "Validation loss (no improvement): 0.081007319688797\n",
      "Training iteration: 1230\n",
      "Validation loss (no improvement): 0.0809690773487091\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.08077598810195923  to: 0.08032825589179993\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.08032825589179993  to: 0.08010051846504211\n",
      "Training iteration: 1233\n",
      "Validation loss (no improvement): 0.08025328516960144\n",
      "Training iteration: 1234\n",
      "Validation loss (no improvement): 0.08052164912223816\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.08010051846504211  to: 0.08009540438652038\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.08009540438652038  to: 0.07975754141807556\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.07975754141807556  to: 0.07971237897872925\n",
      "Training iteration: 1238\n",
      "Validation loss (no improvement): 0.0799191951751709\n",
      "Training iteration: 1239\n",
      "Validation loss (no improvement): 0.079793781042099\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.07971237897872925  to: 0.07909305691719055\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.07909305691719055  to: 0.0788038730621338\n",
      "Training iteration: 1242\n",
      "Validation loss (no improvement): 0.07885540127754212\n",
      "Training iteration: 1243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.07903186678886413\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.0788038730621338  to: 0.07848854660987854\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.07848854660987854  to: 0.07812340259552002\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.07812340259552002  to: 0.07809139490127563\n",
      "Training iteration: 1247\n",
      "Validation loss (no improvement): 0.07819530367851257\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.07809139490127563  to: 0.07778078317642212\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.07778078317642212  to: 0.07730455994606018\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.07730455994606018  to: 0.07727001309394836\n",
      "Training iteration: 1251\n",
      "Validation loss (no improvement): 0.07766718864440918\n",
      "Training iteration: 1252\n",
      "Validation loss (no improvement): 0.07749952673912049\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.07727001309394836  to: 0.07691205143928528\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.07691205143928528  to: 0.07679942846298218\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.07708477973937988\n",
      "Training iteration: 1256\n",
      "Validation loss (no improvement): 0.07721821069717408\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.07679942846298218  to: 0.0764685332775116\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.0764685332775116  to: 0.0761978268623352\n",
      "Training iteration: 1259\n",
      "Validation loss (no improvement): 0.07635183334350586\n",
      "Training iteration: 1260\n",
      "Validation loss (no improvement): 0.07679502964019776\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.0761978268623352  to: 0.07590917348861695\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.07590917348861695  to: 0.07548025846481324\n",
      "Training iteration: 1263\n",
      "Validation loss (no improvement): 0.07553280591964721\n",
      "Training iteration: 1264\n",
      "Validation loss (no improvement): 0.07596308588981629\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.07548025846481324  to: 0.07538262605667115\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.07538262605667115  to: 0.07476400136947632\n",
      "Training iteration: 1267\n",
      "Validation loss (no improvement): 0.0747974693775177\n",
      "Training iteration: 1268\n",
      "Validation loss (no improvement): 0.07546159029006957\n",
      "Training iteration: 1269\n",
      "Validation loss (no improvement): 0.07515743374824524\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.07476400136947632  to: 0.07445018887519836\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.07445018887519836  to: 0.07431644797325135\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.07470083832740784\n",
      "Training iteration: 1273\n",
      "Validation loss (no improvement): 0.07546741366386414\n",
      "Training iteration: 1274\n",
      "Validation loss (no improvement): 0.07472037076950074\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.07431644797325135  to: 0.07416174411773682\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.07416174411773682  to: 0.07406038641929627\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.07435905337333679\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.07446476221084594\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.07406038641929627  to: 0.07372254133224487\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.07372254133224487  to: 0.07308416366577149\n",
      "Training iteration: 1281\n",
      "Validation loss (no improvement): 0.07317095994949341\n",
      "Training iteration: 1282\n",
      "Validation loss (no improvement): 0.07335689663887024\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.0733923614025116\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.07308416366577149  to: 0.07299362421035767\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.07299362421035767  to: 0.07265917658805847\n",
      "Training iteration: 1286\n",
      "Validation loss (no improvement): 0.0728274941444397\n",
      "Training iteration: 1287\n",
      "Validation loss (no improvement): 0.07321292161941528\n",
      "Training iteration: 1288\n",
      "Validation loss (no improvement): 0.07299020290374755\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.07265917658805847  to: 0.07235736846923828\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.07235736846923828  to: 0.07226036787033081\n",
      "Training iteration: 1291\n",
      "Validation loss (no improvement): 0.07259607315063477\n",
      "Training iteration: 1292\n",
      "Validation loss (no improvement): 0.07239722013473511\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.07226036787033081  to: 0.07180666923522949\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.07180666923522949  to: 0.07174938321113586\n",
      "Training iteration: 1295\n",
      "Validation loss (no improvement): 0.07211672067642212\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.07174938321113586  to: 0.07141634821891785\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.07141634821891785  to: 0.07113150358200074\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.07138909697532654\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.07113150358200074  to: 0.07107641100883484\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.07107641100883484  to: 0.0709799885749817\n",
      "Training iteration: 1301\n",
      "Validation loss (no improvement): 0.071273672580719\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.0709799885749817  to: 0.0706152081489563\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.0706152081489563  to: 0.07054216265678406\n",
      "Training iteration: 1304\n",
      "Validation loss (no improvement): 0.07099858522415162\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.07081822156906128\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.07054216265678406  to: 0.07045725584030152\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.07066662907600403\n",
      "Training iteration: 1308\n",
      "Validation loss (no improvement): 0.07139989137649536\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.07045725584030152  to: 0.0704412043094635\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.0704412043094635  to: 0.07010204195976258\n",
      "Training iteration: 1311\n",
      "Validation loss (no improvement): 0.07028877139091491\n",
      "Training iteration: 1312\n",
      "Validation loss (no improvement): 0.07042980790138245\n",
      "Training iteration: 1313\n",
      "Validation loss (no improvement): 0.07016310691833497\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.07010204195976258  to: 0.06984721422195435\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.06984721422195435  to: 0.06981856226921082\n",
      "Training iteration: 1316\n",
      "Validation loss (no improvement): 0.07005201578140259\n",
      "Training iteration: 1317\n",
      "Validation loss (no improvement): 0.06994954943656921\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.06981856226921082  to: 0.06957571506500244\n",
      "Training iteration: 1319\n",
      "Validation loss (no improvement): 0.06969397068023682\n",
      "Training iteration: 1320\n",
      "Validation loss (no improvement): 0.07024719119071961\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.06957571506500244  to: 0.06911904215812684\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.06911904215812684  to: 0.06880921125411987\n",
      "Training iteration: 1323\n",
      "Validation loss (no improvement): 0.06934423446655273\n",
      "Training iteration: 1324\n",
      "Validation loss (no improvement): 0.06975098848342895\n",
      "Training iteration: 1325\n",
      "Validation loss (no improvement): 0.06886873841285705\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.06880921125411987  to: 0.06864229440689087\n",
      "Training iteration: 1327\n",
      "Validation loss (no improvement): 0.068939208984375\n",
      "Training iteration: 1328\n",
      "Validation loss (no improvement): 0.06949456930160522\n",
      "Training iteration: 1329\n",
      "Validation loss (no improvement): 0.06890987753868102\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.06864229440689087  to: 0.06846100091934204\n",
      "Training iteration: 1331\n",
      "Validation loss (no improvement): 0.06857318282127381\n",
      "Training iteration: 1332\n",
      "Validation loss (no improvement): 0.06904246807098388\n",
      "Training iteration: 1333\n",
      "Validation loss (no improvement): 0.06864310503005981\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.06846100091934204  to: 0.06836972236633301\n",
      "Training iteration: 1335\n",
      "Validation loss (no improvement): 0.06860376000404358\n",
      "Training iteration: 1336\n",
      "Validation loss (no improvement): 0.06927279233932496\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.06836972236633301  to: 0.06819247007369995\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.06819247007369995  to: 0.06781713962554932\n",
      "Training iteration: 1339\n",
      "Validation loss (no improvement): 0.06798886656761169\n",
      "Training iteration: 1340\n",
      "Validation loss (no improvement): 0.0680949866771698\n",
      "Training iteration: 1341\n",
      "Validation loss (no improvement): 0.06797798275947571\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.06781713962554932  to: 0.06769292950630187\n",
      "Training iteration: 1343\n",
      "Validation loss (no improvement): 0.06770027875900268\n",
      "Training iteration: 1344\n",
      "Validation loss (no improvement): 0.06795578002929688\n",
      "Training iteration: 1345\n",
      "Validation loss (no improvement): 0.06803223490715027\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.06769292950630187  to: 0.06759151220321655\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.06759151220321655  to: 0.06738125085830689\n",
      "Training iteration: 1348\n",
      "Validation loss (no improvement): 0.06742519736289979\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.06738125085830689  to: 0.06705802083015441\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.06705802083015441  to: 0.06697932481765748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1351\n",
      "Validation loss (no improvement): 0.06732134222984314\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.06697932481765748  to: 0.06647399663925171\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.06647399663925171  to: 0.06622947454452514\n",
      "Training iteration: 1354\n",
      "Validation loss (no improvement): 0.06648102998733521\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.06640125513076782\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.06622947454452514  to: 0.06591339707374573\n",
      "Training iteration: 1357\n",
      "Validation loss (no improvement): 0.0659777581691742\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.06661896705627442\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.06639809608459472\n",
      "Training iteration: 1360\n",
      "Validation loss (no improvement): 0.06593993902206421\n",
      "Training iteration: 1361\n",
      "Validation loss (no improvement): 0.06600154638290405\n",
      "Training iteration: 1362\n",
      "Validation loss (no improvement): 0.06653483510017395\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.0661456823348999\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.06591339707374573  to: 0.06586898565292358\n",
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.06586898565292358  to: 0.06575877666473388\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.06575877666473388  to: 0.06544261574745178\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.06544261574745178  to: 0.06524269580841065\n",
      "Training iteration: 1368\n",
      "Validation loss (no improvement): 0.06530173420906067\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.06524269580841065  to: 0.06517859697341918\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.06517859697341918  to: 0.0650184690952301\n",
      "Training iteration: 1371\n",
      "Validation loss (no improvement): 0.06503671407699585\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.0650184690952301  to: 0.06500691175460815\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.06500691175460815  to: 0.06461967825889588\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.06461967825889588  to: 0.06461578607559204\n",
      "Training iteration: 1375\n",
      "Validation loss (no improvement): 0.06477078199386596\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.06461578607559204  to: 0.06401854753494263\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.06401854753494263  to: 0.06386278867721558\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.06414986848831176\n",
      "Training iteration: 1379\n",
      "Validation loss (no improvement): 0.06425672769546509\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.06386278867721558  to: 0.06362594366073608\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.06362594366073608  to: 0.06355778574943542\n",
      "Training iteration: 1382\n",
      "Validation loss (no improvement): 0.06395761370658874\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.06457735896110535\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.06389614939689636\n",
      "Training iteration: 1385\n",
      "Validation loss (no improvement): 0.06367825269699097\n",
      "Training iteration: 1386\n",
      "Validation loss (no improvement): 0.06384748220443726\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.06355778574943542  to: 0.06351512670516968\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.06363378763198853\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.06381875276565552\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.06351512670516968  to: 0.06337424516677856\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.06337424516677856  to: 0.0633349895477295\n",
      "Training iteration: 1392\n",
      "Validation loss (no improvement): 0.06363648176193237\n",
      "Training iteration: 1393\n",
      "Validation loss (no improvement): 0.0636508047580719\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.0633349895477295  to: 0.06291910409927368\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.06291910409927368  to: 0.0627562165260315\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.06300283670425415\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.06323090195655823\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.0627562165260315  to: 0.06262972354888915\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.06262972354888915  to: 0.06245831251144409\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.06260402202606201\n",
      "Training iteration: 1401\n",
      "Validation loss (no improvement): 0.06291850209236145\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.06245831251144409  to: 0.06227437853813171\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.06227437853813171  to: 0.06197479963302612\n",
      "Training iteration: 1404\n",
      "Validation loss (no improvement): 0.06201770305633545\n",
      "Training iteration: 1405\n",
      "Validation loss (no improvement): 0.06229127049446106\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.06197479963302612  to: 0.06181358098983765\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.06181358098983765  to: 0.06166938543319702\n",
      "Training iteration: 1408\n",
      "Validation loss (no improvement): 0.06180884838104248\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.06166938543319702  to: 0.06145172715187073\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.06145172715187073  to: 0.06121131181716919\n",
      "Training iteration: 1411\n",
      "Validation loss (no improvement): 0.06126826405525208\n",
      "Training iteration: 1412\n",
      "Validation loss (no improvement): 0.06150824427604675\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.06121131181716919  to: 0.060942447185516356\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.060942447185516356  to: 0.06075917482376099\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.06084309816360474\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.06075917482376099  to: 0.06056917309761047\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.06056917309761047  to: 0.06047355532646179\n",
      "Training iteration: 1418\n",
      "Validation loss (no improvement): 0.06053637266159058\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.06059223413467407\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.06047355532646179  to: 0.06015192270278931\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.06015192270278931  to: 0.06003897786140442\n",
      "Training iteration: 1422\n",
      "Validation loss (no improvement): 0.06018185019493103\n",
      "Training iteration: 1423\n",
      "Validation loss (no improvement): 0.060452669858932495\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.06003897786140442  to: 0.059909713268280027\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.059909713268280027  to: 0.059746813774108884\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.059778904914855956\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.0599444568157196\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.059746813774108884  to: 0.0596543550491333\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.0596543550491333  to: 0.05950196385383606\n",
      "Training iteration: 1430\n",
      "Validation loss (no improvement): 0.05955715179443359\n",
      "Training iteration: 1431\n",
      "Validation loss (no improvement): 0.059685122966766355\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.05950196385383606  to: 0.05921222567558289\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.05921222567558289  to: 0.05912560224533081\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.05925561785697937\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.05912560224533081  to: 0.05907186269760132\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.05907186269760132  to: 0.05890699625015259\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.05890699625015259  to: 0.05885534286499024\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.058906495571136475\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.05888407826423645\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.05885534286499024  to: 0.058561313152313235\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.058561313152313235  to: 0.05850056409835815\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.05863121151924133\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.05850056409835815  to: 0.05845788717269897\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.05845788717269897  to: 0.058318310976028444\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.058318310976028444  to: 0.05827768445014954\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.058349120616912845\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.05827768445014954  to: 0.05808312892913818\n",
      "Training iteration: 1448\n",
      "Improved validation loss from: 0.05808312892913818  to: 0.05798273086547852\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.05816243886947632\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.05798273086547852  to: 0.05791941285133362\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.05791941285133362  to: 0.057800662517547605\n",
      "Training iteration: 1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.057800662517547605  to: 0.057773685455322264\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.057773685455322264  to: 0.057743167877197264\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.057743167877197264  to: 0.05754423141479492\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.05754423141479492  to: 0.05744505524635315\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.05744505524635315  to: 0.0574413001537323\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.0574413001537323  to: 0.05722948908805847\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.05722948908805847  to: 0.05715458989143372\n",
      "Training iteration: 1459\n",
      "Validation loss (no improvement): 0.0572248637676239\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.05733171701431274\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.05715458989143372  to: 0.05696948766708374\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.05696948766708374  to: 0.05688621997833252\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.05696576237678528\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.05688621997833252  to: 0.05686922669410706\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.05686922669410706  to: 0.05665651559829712\n",
      "Training iteration: 1466\n",
      "Validation loss (no improvement): 0.056665229797363284\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.05679759979248047\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.05681208968162536\n",
      "Training iteration: 1469\n",
      "Validation loss (no improvement): 0.05679677724838257\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.05685044527053833\n",
      "Training iteration: 1471\n",
      "Validation loss (no improvement): 0.056830871105194095\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.05665651559829712  to: 0.05645111799240112\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.05645111799240112  to: 0.056252747774124146\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.056252747774124146  to: 0.056205260753631595\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.056205260753631595  to: 0.056141126155853274\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.056141126155853274  to: 0.05601574182510376\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.05604814887046814\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.05601574182510376  to: 0.055942094326019286\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.055942094326019286  to: 0.0557513952255249\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.0557513952255249  to: 0.055515986680984494\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.055515986680984494  to: 0.055345988273620604\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.055345988273620604  to: 0.05527203679084778\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.05527203679084778  to: 0.05526624917984009\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.05526624917984009  to: 0.055259484052658084\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.055398333072662356\n",
      "Training iteration: 1486\n",
      "Validation loss (no improvement): 0.05547221899032593\n",
      "Training iteration: 1487\n",
      "Validation loss (no improvement): 0.05531399250030518\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.055259484052658084  to: 0.05504385232925415\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.05504385232925415  to: 0.05489070415496826\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.05489070415496826  to: 0.054523932933807376\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.054523932933807376  to: 0.05442547798156738\n",
      "Training iteration: 1492\n",
      "Validation loss (no improvement): 0.0545045018196106\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.05442547798156738  to: 0.05438200831413269\n",
      "Training iteration: 1494\n",
      "Validation loss (no improvement): 0.05439982414245605\n",
      "Training iteration: 1495\n",
      "Improved validation loss from: 0.05438200831413269  to: 0.05435231924057007\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.05435231924057007  to: 0.05407723188400269\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.05407723188400269  to: 0.053745162487030027\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.053745162487030027  to: 0.053608477115631104\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.053610557317733766\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.05369666814804077\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.053608477115631104  to: 0.053569525480270386\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.05360450148582459\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.053569525480270386  to: 0.053529095649719236\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.053529095649719236  to: 0.053379672765731814\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.053379672765731814  to: 0.053022706508636476\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.053022706508636476  to: 0.05290077924728394\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.05297783613204956\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.05302888751029968\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.0529388964176178\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.05293036699295044\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.05290077924728394  to: 0.05284131169319153\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.05284131169319153  to: 0.05255407094955444\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.05255407094955444  to: 0.05242496132850647\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.05244789123535156\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.05256253480911255\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.05242496132850647  to: 0.05226584076881409\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.05226584076881409  to: 0.05216825008392334\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.0523348867893219\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.05216825008392334  to: 0.051993542909622194\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.051993542909622194  to: 0.051736336946487424\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.051736336946487424  to: 0.05163931846618652\n",
      "Training iteration: 1522\n",
      "Validation loss (no improvement): 0.051665371656417845\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.05163931846618652  to: 0.05146463513374329\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.05146463513374329  to: 0.0514254093170166\n",
      "Training iteration: 1525\n",
      "Validation loss (no improvement): 0.05146571397781372\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.0514254093170166  to: 0.051284033060073855\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.051284033060073855  to: 0.0511833667755127\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.0511833667755127  to: 0.05117605328559875\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.05117605328559875  to: 0.05111544728279114\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.05111544728279114  to: 0.05070320963859558\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.05070320963859558  to: 0.05062404870986938\n",
      "Training iteration: 1532\n",
      "Validation loss (no improvement): 0.050629013776779176\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.05062404870986938  to: 0.05042900443077088\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.05042900443077088  to: 0.0503181517124176\n",
      "Training iteration: 1535\n",
      "Improved validation loss from: 0.0503181517124176  to: 0.05030946135520935\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.05034344792366028\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.05030946135520935  to: 0.05009202361106872\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.05011511445045471\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.05033022165298462\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.05011841058731079\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.05009202361106872  to: 0.0498162567615509\n",
      "Training iteration: 1542\n",
      "Validation loss (no improvement): 0.04988082945346832\n",
      "Training iteration: 1543\n",
      "Validation loss (no improvement): 0.050202572345733644\n",
      "Training iteration: 1544\n",
      "Improved validation loss from: 0.0498162567615509  to: 0.04969212114810943\n",
      "Training iteration: 1545\n",
      "Improved validation loss from: 0.04969212114810943  to: 0.049650144577026364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1546\n",
      "Validation loss (no improvement): 0.049749383330345155\n",
      "Training iteration: 1547\n",
      "Validation loss (no improvement): 0.04983547329902649\n",
      "Training iteration: 1548\n",
      "Improved validation loss from: 0.049650144577026364  to: 0.04946054518222809\n",
      "Training iteration: 1549\n",
      "Improved validation loss from: 0.04946054518222809  to: 0.04927464425563812\n",
      "Training iteration: 1550\n",
      "Validation loss (no improvement): 0.04943023324012756\n",
      "Training iteration: 1551\n",
      "Validation loss (no improvement): 0.049315303564071655\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.04927464425563812  to: 0.04917104840278626\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.04917104840278626  to: 0.04903536736965179\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.04903536736965179  to: 0.048936954140663146\n",
      "Training iteration: 1555\n",
      "Improved validation loss from: 0.048936954140663146  to: 0.04892381727695465\n",
      "Training iteration: 1556\n",
      "Improved validation loss from: 0.04892381727695465  to: 0.0487067312002182\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.0487067312002182  to: 0.048647207021713254\n",
      "Training iteration: 1558\n",
      "Validation loss (no improvement): 0.04878392219543457\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.048647207021713254  to: 0.04855335652828217\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.04855335652828217  to: 0.0485306978225708\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.0485306978225708  to: 0.04852665066719055\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.04852665066719055  to: 0.04822022914886474\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.04822022914886474  to: 0.04793402552604675\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.04793402552604675  to: 0.04777235984802246\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.04777235984802246  to: 0.04758700430393219\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.04758700430393219  to: 0.04751705229282379\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.04751705229282379  to: 0.0473747968673706\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.0473747968673706  to: 0.047372928261756896\n",
      "Training iteration: 1569\n",
      "Improved validation loss from: 0.047372928261756896  to: 0.04736286997795105\n",
      "Training iteration: 1570\n",
      "Improved validation loss from: 0.04736286997795105  to: 0.04717720448970795\n",
      "Training iteration: 1571\n",
      "Validation loss (no improvement): 0.047200098633766174\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.04717720448970795  to: 0.047006306052207944\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.047006306052207944  to: 0.04690485000610352\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.0469109445810318\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.04690485000610352  to: 0.04676145017147064\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.04676145017147064  to: 0.04659274518489838\n",
      "Training iteration: 1577\n",
      "Validation loss (no improvement): 0.04675149023532867\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.04659274518489838  to: 0.046438160538673404\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.04644700884819031\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.04658354222774506\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.046504220366477965\n",
      "Training iteration: 1582\n",
      "Improved validation loss from: 0.046438160538673404  to: 0.04612998366355896\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.04612998366355896  to: 0.04605862498283386\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.04615835547447204\n",
      "Training iteration: 1585\n",
      "Validation loss (no improvement): 0.04627419412136078\n",
      "Training iteration: 1586\n",
      "Validation loss (no improvement): 0.046267947554588316\n",
      "Training iteration: 1587\n",
      "Validation loss (no improvement): 0.04629376530647278\n",
      "Training iteration: 1588\n",
      "Validation loss (no improvement): 0.04641716480255127\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.046069669723510745\n",
      "Training iteration: 1590\n",
      "Improved validation loss from: 0.04605862498283386  to: 0.04589163362979889\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.04591214656829834\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.04589163362979889  to: 0.045847877860069275\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.045847877860069275  to: 0.045706534385681154\n",
      "Training iteration: 1594\n",
      "Improved validation loss from: 0.045706534385681154  to: 0.0455606609582901\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.045607557892799376\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.0455606609582901  to: 0.04531582295894623\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.04531582295894623  to: 0.045231229066848753\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.04547828733921051\n",
      "Training iteration: 1599\n",
      "Improved validation loss from: 0.045231229066848753  to: 0.045081859827041625\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.045081859827041625  to: 0.044811344146728514\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.04483073353767395\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.0449011504650116\n",
      "Training iteration: 1603\n",
      "Improved validation loss from: 0.044811344146728514  to: 0.044776758551597594\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.044776758551597594  to: 0.04477023482322693\n",
      "Training iteration: 1605\n",
      "Validation loss (no improvement): 0.04477511942386627\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.04536001086235046\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.044979387521743776\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.04493817389011383\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.04497403204441071\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.044780999422073364\n",
      "Training iteration: 1611\n",
      "Improved validation loss from: 0.04477023482322693  to: 0.0444720983505249\n",
      "Training iteration: 1612\n",
      "Improved validation loss from: 0.0444720983505249  to: 0.044303297996520996\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.04432660937309265\n",
      "Training iteration: 1614\n",
      "Validation loss (no improvement): 0.04435705542564392\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.044428715109825136\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.04436900019645691\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.044314661622047426\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.044303297996520996  to: 0.04397564828395843\n",
      "Training iteration: 1619\n",
      "Improved validation loss from: 0.04397564828395843  to: 0.04377774298191071\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.04377774298191071  to: 0.04376834034919739\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.04380845427513123\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.043923559784889224\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.04387734830379486\n",
      "Training iteration: 1624\n",
      "Improved validation loss from: 0.04376834034919739  to: 0.043617600202560426\n",
      "Training iteration: 1625\n",
      "Improved validation loss from: 0.043617600202560426  to: 0.04340004026889801\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.04340004026889801  to: 0.04327468872070313\n",
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.04327468872070313  to: 0.04321067333221436\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.04321067333221436  to: 0.04309418201446533\n",
      "Training iteration: 1629\n",
      "Improved validation loss from: 0.04309418201446533  to: 0.04295620322227478\n",
      "Training iteration: 1630\n",
      "Improved validation loss from: 0.04295620322227478  to: 0.04271306097507477\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.04271306097507477  to: 0.04257422983646393\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.04260440766811371\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.0427473247051239\n",
      "Training iteration: 1634\n",
      "Validation loss (no improvement): 0.04264237880706787\n",
      "Training iteration: 1635\n",
      "Validation loss (no improvement): 0.04262961447238922\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.04257422983646393  to: 0.042470616102218625\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.042470616102218625  to: 0.042261943221092224\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.042261943221092224  to: 0.04221510887145996\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.04225679337978363\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.04226887822151184\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.04221510887145996  to: 0.04210058152675629\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.042148590087890625\n",
      "Training iteration: 1643\n",
      "Improved validation loss from: 0.04210058152675629  to: 0.04172200560569763\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.04172200560569763  to: 0.041634494066238405\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.041901415586471556\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.041995519399642946\n",
      "Training iteration: 1647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.042008000612258914\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.04180552363395691\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.04183848798274994\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.04165554642677307\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.04171801209449768\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.04167998433113098\n",
      "Training iteration: 1653\n",
      "Validation loss (no improvement): 0.0416907787322998\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.041634494066238405  to: 0.041571694612503055\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.041571694612503055  to: 0.04129813611507416\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.04131925106048584\n",
      "Training iteration: 1657\n",
      "Improved validation loss from: 0.04129813611507416  to: 0.040971198678016664\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.040971198678016664  to: 0.04081643223762512\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.04088602960109711\n",
      "Training iteration: 1660\n",
      "Validation loss (no improvement): 0.04096965193748474\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.04103821218013763\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.04095030426979065\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.040926876664161685\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.04081643223762512  to: 0.0407193511724472\n",
      "Training iteration: 1665\n",
      "Improved validation loss from: 0.0407193511724472  to: 0.04065782129764557\n",
      "Training iteration: 1666\n",
      "Improved validation loss from: 0.04065782129764557  to: 0.040608066320419314\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.04063468873500824\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.040608066320419314  to: 0.040324443578720094\n",
      "Training iteration: 1669\n",
      "Improved validation loss from: 0.040324443578720094  to: 0.04021710455417633\n",
      "Training iteration: 1670\n",
      "Improved validation loss from: 0.04021710455417633  to: 0.040021958947181704\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.040021958947181704  to: 0.03993708193302155\n",
      "Training iteration: 1672\n",
      "Improved validation loss from: 0.03993708193302155  to: 0.03982841968536377\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.03984673619270325\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.03996372818946838\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.03991610109806061\n",
      "Training iteration: 1676\n",
      "Improved validation loss from: 0.03982841968536377  to: 0.039816954731941225\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.039816954731941225  to: 0.03963134884834289\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.03963134884834289  to: 0.039505988359451294\n",
      "Training iteration: 1679\n",
      "Improved validation loss from: 0.039505988359451294  to: 0.03938918709754944\n",
      "Training iteration: 1680\n",
      "Improved validation loss from: 0.03938918709754944  to: 0.03928891122341156\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.03928891122341156  to: 0.039163249731063846\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.039163249731063846  to: 0.03896215856075287\n",
      "Training iteration: 1683\n",
      "Improved validation loss from: 0.03896215856075287  to: 0.0387035995721817\n",
      "Training iteration: 1684\n",
      "Improved validation loss from: 0.0387035995721817  to: 0.03856938481330872\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.03856938481330872  to: 0.038487178087234494\n",
      "Training iteration: 1686\n",
      "Improved validation loss from: 0.038487178087234494  to: 0.03843266367912292\n",
      "Training iteration: 1687\n",
      "Improved validation loss from: 0.03843266367912292  to: 0.0384058803319931\n",
      "Training iteration: 1688\n",
      "Improved validation loss from: 0.0384058803319931  to: 0.03834573626518249\n",
      "Training iteration: 1689\n",
      "Improved validation loss from: 0.03834573626518249  to: 0.03834262490272522\n",
      "Training iteration: 1690\n",
      "Improved validation loss from: 0.03834262490272522  to: 0.038249817490577695\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.038325586915016176\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.038399165868759154\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.038426297903060916\n",
      "Training iteration: 1694\n",
      "Validation loss (no improvement): 0.03829379975795746\n",
      "Training iteration: 1695\n",
      "Improved validation loss from: 0.038249817490577695  to: 0.03809108138084412\n",
      "Training iteration: 1696\n",
      "Improved validation loss from: 0.03809108138084412  to: 0.0379540741443634\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.0379540741443634  to: 0.037838616967201234\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.037838616967201234  to: 0.03766963481903076\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.03766963481903076  to: 0.0376617819070816\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.0376617819070816  to: 0.037597060203552246\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.0375983327627182\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.037597060203552246  to: 0.03757081925868988\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.03757081925868988  to: 0.037458363175392154\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.037458363175392154  to: 0.037385329604148865\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.03748759329319\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.037483781576156616\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.037475916743278506\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.037385329604148865  to: 0.037281003594398496\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.037281003594398496  to: 0.03707979619503021\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.03707979619503021  to: 0.03694934844970703\n",
      "Training iteration: 1711\n",
      "Improved validation loss from: 0.03694934844970703  to: 0.03679369390010834\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.03679369390010834  to: 0.03659577071666718\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.03659577071666718  to: 0.0364494264125824\n",
      "Training iteration: 1714\n",
      "Improved validation loss from: 0.0364494264125824  to: 0.036281877756118776\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.036281877756118776  to: 0.03623614907264709\n",
      "Training iteration: 1716\n",
      "Improved validation loss from: 0.03623614907264709  to: 0.03611748516559601\n",
      "Training iteration: 1717\n",
      "Validation loss (no improvement): 0.03617194890975952\n",
      "Training iteration: 1718\n",
      "Validation loss (no improvement): 0.03627503514289856\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.03636429905891418\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.03613762557506561\n",
      "Training iteration: 1721\n",
      "Improved validation loss from: 0.03611748516559601  to: 0.036031708121299744\n",
      "Training iteration: 1722\n",
      "Improved validation loss from: 0.036031708121299744  to: 0.035757052898406985\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.035765627026557924\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.0358511745929718\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.03589780330657959\n",
      "Training iteration: 1726\n",
      "Validation loss (no improvement): 0.03600422143936157\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.035793226957321164\n",
      "Training iteration: 1728\n",
      "Validation loss (no improvement): 0.03608608841896057\n",
      "Training iteration: 1729\n",
      "Validation loss (no improvement): 0.03599040508270264\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.03608324825763702\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.035757052898406985  to: 0.03573714792728424\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.03573714792728424  to: 0.03572498857975006\n",
      "Training iteration: 1733\n",
      "Improved validation loss from: 0.03572498857975006  to: 0.035370942950248715\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.035370942950248715  to: 0.03535204529762268\n",
      "Training iteration: 1735\n",
      "Improved validation loss from: 0.03535204529762268  to: 0.03521917760372162\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.035469135642051695\n",
      "Training iteration: 1737\n",
      "Validation loss (no improvement): 0.03543252050876618\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.035977798700332644\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.03570731282234192\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.035908788442611694\n",
      "Training iteration: 1741\n",
      "Validation loss (no improvement): 0.03576135933399201\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.035655802488327025\n",
      "Training iteration: 1743\n",
      "Validation loss (no improvement): 0.03569476008415222\n",
      "Training iteration: 1744\n",
      "Validation loss (no improvement): 0.03558309674263001\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.03529942333698273\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.03521917760372162  to: 0.03515749871730804\n",
      "Training iteration: 1747\n",
      "Improved validation loss from: 0.03515749871730804  to: 0.035079294443130495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1748\n",
      "Improved validation loss from: 0.035079294443130495  to: 0.03496238589286804\n",
      "Training iteration: 1749\n",
      "Improved validation loss from: 0.03496238589286804  to: 0.03489344120025635\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.03489344120025635  to: 0.034657880663871765\n",
      "Training iteration: 1751\n",
      "Improved validation loss from: 0.034657880663871765  to: 0.03433058261871338\n",
      "Training iteration: 1752\n",
      "Improved validation loss from: 0.03433058261871338  to: 0.03432616591453552\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.03432616591453552  to: 0.034116658568382266\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.034262281656265256\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.03424141407012939\n",
      "Training iteration: 1756\n",
      "Improved validation loss from: 0.034116658568382266  to: 0.033867329359054565\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.033867329359054565  to: 0.03365882635116577\n",
      "Training iteration: 1758\n",
      "Improved validation loss from: 0.03365882635116577  to: 0.03361047506332397\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.03365418016910553\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.03361047506332397  to: 0.03357063829898834\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.03357063829898834  to: 0.03323836624622345\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.03323836624622345  to: 0.03287549614906311\n",
      "Training iteration: 1763\n",
      "Improved validation loss from: 0.03287549614906311  to: 0.032673054933547975\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.032673054933547975  to: 0.032602012157440186\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.03271036446094513\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.03289446532726288\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.03298347592353821\n",
      "Training iteration: 1768\n",
      "Validation loss (no improvement): 0.03292080461978912\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.03277405202388763\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.032705357670783995\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.03268764019012451\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.03280386924743652\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.03261931240558624\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.032602012157440186  to: 0.0323494166135788\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.032379674911499026\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.0323494166135788  to: 0.032269245386123656\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.03248966336250305\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.03241907060146332\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.03227849304676056\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.032269245386123656  to: 0.032220843434333804\n",
      "Training iteration: 1781\n",
      "Improved validation loss from: 0.032220843434333804  to: 0.03221790194511413\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.03243704438209534\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.03261691033840179\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.03254517018795013\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.03232313990592957\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.03234753906726837\n",
      "Training iteration: 1787\n",
      "Validation loss (no improvement): 0.032325881719589236\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.032463207840919495\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.03239743113517761\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.032297372817993164\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.03221790194511413  to: 0.032133045792579654\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.032133045792579654  to: 0.03195129334926605\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.03200925886631012\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.032045108079910276\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.03214444816112518\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.032001721858978274\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.03195129334926605  to: 0.031741386651992796\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.031741386651992796  to: 0.03146252930164337\n",
      "Training iteration: 1799\n",
      "Improved validation loss from: 0.03146252930164337  to: 0.03140535354614258\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.03156922161579132\n",
      "Training iteration: 1801\n",
      "Validation loss (no improvement): 0.031607475876808164\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.03140535354614258  to: 0.03132607042789459\n",
      "Training iteration: 1803\n",
      "Improved validation loss from: 0.03132607042789459  to: 0.03108220100402832\n",
      "Training iteration: 1804\n",
      "Improved validation loss from: 0.03108220100402832  to: 0.03101601004600525\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.03101601004600525  to: 0.03096773624420166\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.031017792224884034\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.031094962358474733\n",
      "Training iteration: 1808\n",
      "Improved validation loss from: 0.03096773624420166  to: 0.0308256059885025\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.03087257742881775\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.03087565004825592\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.03096349537372589\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.031070303916931153\n",
      "Training iteration: 1813\n",
      "Validation loss (no improvement): 0.0309864342212677\n",
      "Training iteration: 1814\n",
      "Improved validation loss from: 0.0308256059885025  to: 0.030808016657829285\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.030818453431129454\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.030903333425521852\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.031032896041870116\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.0309889018535614\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.03082800507545471\n",
      "Training iteration: 1820\n",
      "Improved validation loss from: 0.030808016657829285  to: 0.03074362874031067\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.030776733160018922\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.03084532618522644\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.031238803267478944\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.03118646740913391\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.03097643256187439\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.03080633282661438\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.03077126145362854\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.030955514311790465\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.03109537959098816\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.030956333875656127\n",
      "Training iteration: 1831\n",
      "Improved validation loss from: 0.03074362874031067  to: 0.03072754144668579\n",
      "Training iteration: 1832\n",
      "Improved validation loss from: 0.03072754144668579  to: 0.03071090579032898\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.030741378664970398\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.03076041638851166\n",
      "Training iteration: 1835\n",
      "Improved validation loss from: 0.03071090579032898  to: 0.03062915802001953\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.03062915802001953  to: 0.030439203977584837\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.030439203977584837  to: 0.030433303117752074\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.03057388961315155\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.03064294457435608\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.030515623092651368\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.030440717935562134\n",
      "Training iteration: 1842\n",
      "Improved validation loss from: 0.030433303117752074  to: 0.030227139592170715\n",
      "Training iteration: 1843\n",
      "Improved validation loss from: 0.030227139592170715  to: 0.030121701955795287\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.030164992809295653\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.030260252952575683\n",
      "Training iteration: 1846\n",
      "Improved validation loss from: 0.030121701955795287  to: 0.0301118403673172\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.030200067162513732\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.030166596174240112\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.03018977344036102\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.03012838661670685\n",
      "Training iteration: 1851\n",
      "Improved validation loss from: 0.0301118403673172  to: 0.029923009872436523\n",
      "Training iteration: 1852\n",
      "Improved validation loss from: 0.029923009872436523  to: 0.02980523109436035\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.02980523109436035  to: 0.02978843152523041\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.02978843152523041  to: 0.02974207103252411\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.029867792129516603\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.03000275492668152\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.030002593994140625\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.029940450191497804\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.03005947470664978\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.030165570974349975\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.03006935715675354\n",
      "Training iteration: 1862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02986004650592804\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.029883018136024474\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.029764810204505922\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.029956406354904173\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.029999834299087525\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.02985842227935791\n",
      "Training iteration: 1868\n",
      "Validation loss (no improvement): 0.029749545454978942\n",
      "Training iteration: 1869\n",
      "Improved validation loss from: 0.02974207103252411  to: 0.029690489172935486\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.029690489172935486  to: 0.029656615853309632\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.02970011532306671\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.029656615853309632  to: 0.029652738571166994\n",
      "Training iteration: 1873\n",
      "Improved validation loss from: 0.029652738571166994  to: 0.029573199152946473\n",
      "Training iteration: 1874\n",
      "Improved validation loss from: 0.029573199152946473  to: 0.029544913768768312\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.029680699110031128\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.029838243126869203\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.0298578679561615\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.029681974649429323\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.029544913768768312  to: 0.029472243785858155\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.029527658224105836\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.029634428024291993\n",
      "Training iteration: 1882\n",
      "Validation loss (no improvement): 0.029918530583381654\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.029953062534332275\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.02974172532558441\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.02972259819507599\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.02956675589084625\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.02949828505516052\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.02952659726142883\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.029582369327545165\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.029551640152931213\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.029500040411949157\n",
      "Training iteration: 1892\n",
      "Improved validation loss from: 0.029472243785858155  to: 0.029387742280960083\n",
      "Training iteration: 1893\n",
      "Improved validation loss from: 0.029387742280960083  to: 0.029320773482322694\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.029320773482322694  to: 0.0292633980512619\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.02933967411518097\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.02950831949710846\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.029452547430992126\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.0292633980512619  to: 0.029106682538986205\n",
      "Training iteration: 1899\n",
      "Improved validation loss from: 0.029106682538986205  to: 0.029102152585983275\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.029102152585983275  to: 0.02906242311000824\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.02929442226886749\n",
      "Training iteration: 1902\n",
      "Validation loss (no improvement): 0.029480999708175658\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.02945454716682434\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.02917935848236084\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.0290698379278183\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.02906242311000824  to: 0.028897351026535033\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.028897351026535033  to: 0.028851833939552308\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.028944626450538635\n",
      "Training iteration: 1909\n",
      "Improved validation loss from: 0.028851833939552308  to: 0.028834086656570435\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.0289943128824234\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.029035335779190062\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.028911128640174866\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.028834086656570435  to: 0.028831669688224794\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.028831669688224794  to: 0.028730037808418273\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.028792864084243773\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.028777968883514405\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.02887127995491028\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.02891734540462494\n",
      "Training iteration: 1919\n",
      "Validation loss (no improvement): 0.028833037614822386\n",
      "Training iteration: 1920\n",
      "Validation loss (no improvement): 0.028827363252639772\n",
      "Training iteration: 1921\n",
      "Validation loss (no improvement): 0.02900291085243225\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.029275006055831908\n",
      "Training iteration: 1923\n",
      "Validation loss (no improvement): 0.029475846886634828\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.029379135370254515\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.02905765175819397\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.029059815406799316\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.028855755925178528\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.0287733256816864\n",
      "Training iteration: 1929\n",
      "Validation loss (no improvement): 0.02882061302661896\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.028894838690757752\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.028845304250717164\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.028730037808418273  to: 0.02867722511291504\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.02867722511291504  to: 0.028675729036331178\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.028775674104690552\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.02874232828617096\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.028675729036331178  to: 0.028651922941207886\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.028651922941207886  to: 0.028462427854537963\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.028486818075180054\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.028462427854537963  to: 0.028445881605148316\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.028445881605148316  to: 0.02835976183414459\n",
      "Training iteration: 1941\n",
      "Validation loss (no improvement): 0.028449085354804993\n",
      "Training iteration: 1942\n",
      "Validation loss (no improvement): 0.028401556611061095\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.02835976183414459  to: 0.02824949622154236\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.028266137838363646\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.02824949622154236  to: 0.028184574842453004\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.028184574842453004  to: 0.02801014482975006\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.02801014482975006  to: 0.027808311581611633\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.02782999575138092\n",
      "Training iteration: 1949\n",
      "Validation loss (no improvement): 0.028089624643325806\n",
      "Training iteration: 1950\n",
      "Validation loss (no improvement): 0.02816479206085205\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.028075900673866273\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.027896410226821898\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.027808311581611633  to: 0.027776056528091432\n",
      "Training iteration: 1954\n",
      "Improved validation loss from: 0.027776056528091432  to: 0.027737507224082948\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.027904731035232545\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.028003120422363283\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.028003519773483275\n",
      "Training iteration: 1958\n",
      "Validation loss (no improvement): 0.028012824058532716\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.028198808431625366\n",
      "Training iteration: 1960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02863007187843323\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.02884252369403839\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.028721621632575987\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.028695124387741088\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.028671836853027342\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.028562670946121214\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.02850777208805084\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.028682535886764525\n",
      "Training iteration: 1968\n",
      "Validation loss (no improvement): 0.028705701231956482\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.02854542136192322\n",
      "Training iteration: 1970\n",
      "Validation loss (no improvement): 0.028402724862098695\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.028310707211494444\n",
      "Training iteration: 1972\n",
      "Validation loss (no improvement): 0.02837900221347809\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.02826041281223297\n",
      "Training iteration: 1974\n",
      "Validation loss (no improvement): 0.0284546434879303\n",
      "Training iteration: 1975\n",
      "Validation loss (no improvement): 0.028475254774093628\n",
      "Training iteration: 1976\n",
      "Validation loss (no improvement): 0.028514862060546875\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.028254738450050353\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.0280459463596344\n",
      "Training iteration: 1979\n",
      "Validation loss (no improvement): 0.02804986834526062\n",
      "Training iteration: 1980\n",
      "Validation loss (no improvement): 0.028173333406448363\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.028084579110145568\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.02792777419090271\n",
      "Training iteration: 1983\n",
      "Validation loss (no improvement): 0.027836936712265014\n",
      "Training iteration: 1984\n",
      "Validation loss (no improvement): 0.027918940782546996\n",
      "Training iteration: 1985\n",
      "Validation loss (no improvement): 0.027881941199302672\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.027821573615074157\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.02783447504043579\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.027737507224082948  to: 0.027619290351867675\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.027679607272148132\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.027702808380126953\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.02765185236930847\n",
      "Training iteration: 1992\n",
      "Improved validation loss from: 0.027619290351867675  to: 0.02756057381629944\n",
      "Training iteration: 1993\n",
      "Improved validation loss from: 0.02756057381629944  to: 0.027519750595092773\n",
      "Training iteration: 1994\n",
      "Improved validation loss from: 0.027519750595092773  to: 0.02736997902393341\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.02736997902393341  to: 0.027323904633522033\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.027450591325759888\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.027511772513389588\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.02749876081943512\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.027475506067276\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.027323904633522033  to: 0.027282243967056273\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.027282243967056273  to: 0.027176058292388915\n",
      "Training iteration: 2002\n",
      "Validation loss (no improvement): 0.027257078886032106\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.027421793341636656\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.02735368311405182\n",
      "Training iteration: 2005\n",
      "Validation loss (no improvement): 0.027235060930252075\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.027234840393066406\n",
      "Training iteration: 2007\n",
      "Validation loss (no improvement): 0.027398544549942016\n",
      "Training iteration: 2008\n",
      "Validation loss (no improvement): 0.027537688612937927\n",
      "Training iteration: 2009\n",
      "Validation loss (no improvement): 0.027674978971481322\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.027719736099243164\n",
      "Training iteration: 2011\n",
      "Validation loss (no improvement): 0.02754333019256592\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.027407634258270263\n",
      "Training iteration: 2013\n",
      "Validation loss (no improvement): 0.027383202314376832\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.027294877171516418\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.027178603410720825\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.027176058292388915  to: 0.027150389552116395\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.027329757809638977\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.027404341101646423\n",
      "Training iteration: 2019\n",
      "Validation loss (no improvement): 0.027386236190795898\n",
      "Training iteration: 2020\n",
      "Validation loss (no improvement): 0.027329501509666444\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.027456951141357423\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.02751573622226715\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.027424916625022888\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.02720261216163635\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.027193260192871094\n",
      "Training iteration: 2026\n",
      "Improved validation loss from: 0.027150389552116395  to: 0.027045589685440064\n",
      "Training iteration: 2027\n",
      "Improved validation loss from: 0.027045589685440064  to: 0.026956632733345032\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.0269707053899765\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.027029138803482056\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.02714698016643524\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.0271756112575531\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.02717139422893524\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.0274641752243042\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.027161231637001036\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.027167895436286928\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.027114033699035645\n",
      "Training iteration: 2037\n",
      "Improved validation loss from: 0.026956632733345032  to: 0.026906830072402955\n",
      "Training iteration: 2038\n",
      "Improved validation loss from: 0.026906830072402955  to: 0.026755601167678833\n",
      "Training iteration: 2039\n",
      "Improved validation loss from: 0.026755601167678833  to: 0.026693770289421083\n",
      "Training iteration: 2040\n",
      "Improved validation loss from: 0.026693770289421083  to: 0.026559197902679445\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.026587995886802673\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.026764354109764098\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.02692892253398895\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.026948919892311095\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.027070337533950807\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.027175214886665345\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.02709731161594391\n",
      "Training iteration: 2048\n",
      "Validation loss (no improvement): 0.02685926854610443\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.026572006940841674\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.026559197902679445  to: 0.026339605450630188\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.026339605450630188  to: 0.026287269592285157\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.026387399435043334\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.026477298140525816\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.026509231328964232\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.026536387205123902\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.026441311836242674\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.026417118310928345\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.0265237957239151\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.026421380043029786\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.02635961174964905\n",
      "Training iteration: 2061\n",
      "Validation loss (no improvement): 0.02638289928436279\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.026287269592285157  to: 0.026245927810668944\n",
      "Training iteration: 2063\n",
      "Improved validation loss from: 0.026245927810668944  to: 0.026151958107948303\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.02632003128528595\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.026235297322273254\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.026241615414619446\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.026178440451622008\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.0261700302362442\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.026196974515914916\n",
      "Training iteration: 2070\n",
      "Validation loss (no improvement): 0.026337689161300658\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.026569682359695434\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.026601544022560118\n",
      "Training iteration: 2073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.026556903123855592\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): 0.02670988142490387\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.026555296778678895\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.026457014679908752\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): 0.026368269324302675\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.026310107111930846\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): 0.02620806694030762\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): 0.026187011599540712\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.026268595457077028\n",
      "Training iteration: 2082\n",
      "Validation loss (no improvement): 0.02644108235836029\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): 0.026433295011520384\n",
      "Training iteration: 2084\n",
      "Validation loss (no improvement): 0.02639853358268738\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.026295623183250426\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): 0.02652203440666199\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): 0.026295575499534606\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.026151958107948303  to: 0.026149576902389525\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): 0.026276832818984984\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.02631453573703766\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): 0.026206353306770326\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.026149576902389525  to: 0.026099461317062377\n",
      "Training iteration: 2093\n",
      "Improved validation loss from: 0.026099461317062377  to: 0.02607075572013855\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.026102626323699953\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.02607075572013855  to: 0.026052284240722656\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.026052284240722656  to: 0.02592996060848236\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): 0.0259464830160141\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.026036277413368225\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.0260194331407547\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.025991979241371154\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.026007217168807984\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.026117739081382752\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.02606814503669739\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.026011249423027037\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.025960293412208558\n",
      "Training iteration: 2106\n",
      "Improved validation loss from: 0.02592996060848236  to: 0.025784653425216675\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.025784653425216675  to: 0.02564111351966858\n",
      "Training iteration: 2108\n",
      "Improved validation loss from: 0.02564111351966858  to: 0.025568515062332153\n",
      "Training iteration: 2109\n",
      "Improved validation loss from: 0.025568515062332153  to: 0.025544604659080504\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.02558521628379822\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.0256796270608902\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.02566598355770111\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.025605922937393187\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.025544604659080504  to: 0.025520092248916625\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.025520092248916625  to: 0.025467881560325624\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.02555558681488037\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.0255021870136261\n",
      "Training iteration: 2118\n",
      "Improved validation loss from: 0.025467881560325624  to: 0.025343155860900878\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.025436586141586302\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.025368809700012207\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.025353369116783143\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.02538256049156189\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.025456994771957397\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.02546187937259674\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.025406527519226074\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.025554767251014708\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.02549459636211395\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.025381606817245484\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.025436905026435853\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.025523796677589417\n",
      "Training iteration: 2131\n",
      "Validation loss (no improvement): 0.025386905670166014\n",
      "Training iteration: 2132\n",
      "Improved validation loss from: 0.025343155860900878  to: 0.025294461846351625\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.02531578540802002\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.025335359573364257\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.025382620096206666\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.025477057695388793\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.02570233941078186\n",
      "Training iteration: 2138\n",
      "Validation loss (no improvement): 0.02569771409034729\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.025528860092163087\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.025311127305030823\n",
      "Training iteration: 2141\n",
      "Improved validation loss from: 0.025294461846351625  to: 0.025222164392471314\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.025222164392471314  to: 0.025164422392845155\n",
      "Training iteration: 2143\n",
      "Improved validation loss from: 0.025164422392845155  to: 0.025121420621871948\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.02522100806236267\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.025231939554214478\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.02523556351661682\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.025359314680099488\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.02529420256614685\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.025141486525535585\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.025121420621871948  to: 0.025037530064582824\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.025037530064582824  to: 0.024849025905132292\n",
      "Training iteration: 2152\n",
      "Improved validation loss from: 0.024849025905132292  to: 0.024721142649650574\n",
      "Training iteration: 2153\n",
      "Improved validation loss from: 0.024721142649650574  to: 0.024657531082630156\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.024657531082630156  to: 0.02460361421108246\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.024637755751609803\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.02460361421108246  to: 0.024597592651844025\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.02466646134853363\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.02476710081100464\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): 0.024706506729125978\n",
      "Training iteration: 2160\n",
      "Improved validation loss from: 0.024597592651844025  to: 0.024573841691017152\n",
      "Training iteration: 2161\n",
      "Improved validation loss from: 0.024573841691017152  to: 0.024505558609962463\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.024616697430610658\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.024635234475135805\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.02474045306444168\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): 0.0248193234205246\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): 0.024939008057117462\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.024715881049633025\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.024653109908103942\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.024715283513069154\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.024760162830352782\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.024694156646728516\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.024718698859214783\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): 0.024891944229602815\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): 0.0250330775976181\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.025194600224494934\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.02520650327205658\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.025429487228393555\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.02505873441696167\n",
      "Training iteration: 2179\n",
      "Validation loss (no improvement): 0.024769063293933868\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): 0.02466789782047272\n",
      "Training iteration: 2181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.024594028294086457\n",
      "Training iteration: 2182\n",
      "Validation loss (no improvement): 0.024564489722251892\n",
      "Training iteration: 2183\n",
      "Validation loss (no improvement): 0.024653415381908416\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.024711064994335175\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): 0.024934057891368867\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): 0.02491176426410675\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): 0.024868254363536835\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): 0.024761494994163514\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.024645495414733886\n",
      "Training iteration: 2190\n",
      "Improved validation loss from: 0.024505558609962463  to: 0.02440659552812576\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.02440659552812576  to: 0.024374541640281678\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.024374541640281678  to: 0.024307191371917725\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.024413268268108367\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.024385647475719453\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.02437899112701416\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.024370984733104707\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.02431749105453491\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.024307191371917725  to: 0.024210843443870544\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.024210843443870544  to: 0.02411048114299774\n",
      "Training iteration: 2200\n",
      "Improved validation loss from: 0.02411048114299774  to: 0.023979172110557556\n",
      "Training iteration: 2201\n",
      "Improved validation loss from: 0.023979172110557556  to: 0.02391992062330246\n",
      "Training iteration: 2202\n",
      "Improved validation loss from: 0.02391992062330246  to: 0.023861686885356902\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.02392573803663254\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): 0.023896639049053193\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.023861686885356902  to: 0.023842748999595643\n",
      "Training iteration: 2206\n",
      "Validation loss (no improvement): 0.023962447047233583\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.023987562954425813\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.023842748999595643  to: 0.023838531970977784\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.023838531970977784  to: 0.023831693828105925\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.023845383524894716\n",
      "Training iteration: 2211\n",
      "Improved validation loss from: 0.023831693828105925  to: 0.023784151673316954\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.02390890121459961\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.023834674060344695\n",
      "Training iteration: 2214\n",
      "Improved validation loss from: 0.023784151673316954  to: 0.02371538579463959\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.023787911236286163\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.02381834089756012\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): 0.023834159970283507\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.023945650458335875\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.024035337567329406\n",
      "Training iteration: 2220\n",
      "Validation loss (no improvement): 0.0239500567317009\n",
      "Training iteration: 2221\n",
      "Validation loss (no improvement): 0.02384336292743683\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.023807032406330107\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.023778431117534637\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.02373027801513672\n",
      "Training iteration: 2225\n",
      "Improved validation loss from: 0.02371538579463959  to: 0.023713067173957825\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.023761186003684997\n",
      "Training iteration: 2227\n",
      "Improved validation loss from: 0.023713067173957825  to: 0.023707528412342072\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.02371337413787842\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.023770156502723693\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.02378677427768707\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.023804822564125062\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.023767116665840148\n",
      "Training iteration: 2233\n",
      "Improved validation loss from: 0.023707528412342072  to: 0.023639149963855743\n",
      "Training iteration: 2234\n",
      "Improved validation loss from: 0.023639149963855743  to: 0.023506541550159455\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.023506541550159455  to: 0.023490071296691895\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.023490071296691895  to: 0.023479828238487245\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.023565872013568877\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.023575165867805482\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.023655056953430176\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.02372967004776001\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.023838794231414794\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.023579637706279754\n",
      "Training iteration: 2243\n",
      "Improved validation loss from: 0.023479828238487245  to: 0.023425431549549104\n",
      "Training iteration: 2244\n",
      "Improved validation loss from: 0.023425431549549104  to: 0.023318073153495787\n",
      "Training iteration: 2245\n",
      "Improved validation loss from: 0.023318073153495787  to: 0.023277147114276885\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.023302698135375978\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): 0.023339462280273438\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): 0.023506732285022737\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.023732514679431917\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): 0.023604068160057067\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): 0.02347593754529953\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.023353302478790285\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): 0.023292461037635805\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.023334769904613493\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.02339448481798172\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.023321878910064698\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.023467740416526793\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.023337407410144805\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.023357281088829042\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.023507270216941833\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.023527848720550536\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.023433923721313477\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.02332884818315506\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.023350699245929717\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): 0.023308639228343964\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.023277147114276885  to: 0.02314942330121994\n",
      "Training iteration: 2267\n",
      "Improved validation loss from: 0.02314942330121994  to: 0.023066052794456483\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.023089727759361266\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): 0.023126105964183807\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.02317405939102173\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.02310882359743118\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.023066052794456483  to: 0.023030856251716615\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.023030856251716615  to: 0.02295295298099518\n",
      "Training iteration: 2274\n",
      "Improved validation loss from: 0.02295295298099518  to: 0.0228762224316597\n",
      "Training iteration: 2275\n",
      "Improved validation loss from: 0.0228762224316597  to: 0.02279658764600754\n",
      "Training iteration: 2276\n",
      "Improved validation loss from: 0.02279658764600754  to: 0.02277134209871292\n",
      "Training iteration: 2277\n",
      "Improved validation loss from: 0.02277134209871292  to: 0.02275465726852417\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.022790202498435976\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.02282775193452835\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.022835655510425566\n",
      "Training iteration: 2281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02291688472032547\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.02307375371456146\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.02275465726852417  to: 0.022738441824913025\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.022738441824913025  to: 0.02264585942029953\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.02264585942029953  to: 0.022595450282096863\n",
      "Training iteration: 2286\n",
      "Improved validation loss from: 0.022595450282096863  to: 0.022592052817344666\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.022592052817344666  to: 0.022579431533813477\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.02274082899093628\n",
      "Training iteration: 2289\n",
      "Validation loss (no improvement): 0.022746708989143372\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): 0.02292310893535614\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.022922077775001527\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.022901804745197298\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): 0.022898726165294647\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): 0.022936828434467316\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): 0.022817377746105195\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): 0.0227216437458992\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.0226960614323616\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.022684213519096375\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): 0.022598770260810853\n",
      "Training iteration: 2300\n",
      "Improved validation loss from: 0.022579431533813477  to: 0.022558598220348357\n",
      "Training iteration: 2301\n",
      "Improved validation loss from: 0.022558598220348357  to: 0.02254612445831299\n",
      "Training iteration: 2302\n",
      "Improved validation loss from: 0.02254612445831299  to: 0.02244213819503784\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.02244213819503784  to: 0.02236793339252472\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.02236793339252472  to: 0.022333283722400666\n",
      "Training iteration: 2305\n",
      "Improved validation loss from: 0.022333283722400666  to: 0.022224321961402893\n",
      "Training iteration: 2306\n",
      "Improved validation loss from: 0.022224321961402893  to: 0.022086401283740998\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): 0.022090673446655273\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.022086401283740998  to: 0.022078712284564973\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): 0.022186660766601564\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): 0.022244055569171906\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): 0.022367966175079346\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.022628994286060335\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.022444267570972443\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.022417955100536346\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.022337093949317932\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.022363221645355223\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): 0.022305560111999512\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.022392825782299043\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.022377033531665803\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): 0.0224408894777298\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.02238427847623825\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): 0.02233012467622757\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.02228827029466629\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.022139349579811098\n",
      "Training iteration: 2325\n",
      "Improved validation loss from: 0.022078712284564973  to: 0.022062501311302184\n",
      "Training iteration: 2326\n",
      "Improved validation loss from: 0.022062501311302184  to: 0.021995003521442413\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.02201031893491745\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.02205024063587189\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.02211003750562668\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): 0.02202896326780319\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.021995003521442413  to: 0.021881046891212463\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.021881046891212463  to: 0.02174992859363556\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.02174992859363556  to: 0.021653881669044493\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.021653881669044493  to: 0.021600310504436494\n",
      "Training iteration: 2335\n",
      "Improved validation loss from: 0.021600310504436494  to: 0.02154078483581543\n",
      "Training iteration: 2336\n",
      "Improved validation loss from: 0.02154078483581543  to: 0.021514327824115755\n",
      "Training iteration: 2337\n",
      "Improved validation loss from: 0.021514327824115755  to: 0.021506556868553163\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.021517708897590637\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.021506556868553163  to: 0.02150404006242752\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.02150404006242752  to: 0.02149605453014374\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): 0.02168118953704834\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.021585738658905028\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.021517279744148254\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): 0.021618413925170898\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.02192908525466919\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.02206301987171173\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.021885094046592713\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.02179316580295563\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): 0.021942897140979765\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.02210226356983185\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.02212735712528229\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.022274520993232728\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): 0.022516457736492156\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.022598834335803987\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.022492440044879915\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.02241460084915161\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.022426812350749968\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.022409406304359437\n",
      "Training iteration: 2359\n",
      "Validation loss (no improvement): 0.022128506004810332\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): 0.022165712714195252\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.022189469635486604\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.022218570113182068\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.022031307220458984\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.021918758749961853\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.021728745102882384\n",
      "Training iteration: 2366\n",
      "Improved validation loss from: 0.02149605453014374  to: 0.02149333655834198\n",
      "Training iteration: 2367\n",
      "Improved validation loss from: 0.02149333655834198  to: 0.021180137991905212\n",
      "Training iteration: 2368\n",
      "Improved validation loss from: 0.021180137991905212  to: 0.021141545474529268\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.021141545474529268  to: 0.021130008995532988\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.021264772117137908\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.021472716331481935\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.021686749160289766\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.02163316458463669\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.021667322516441344\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.021426823735237122\n",
      "Training iteration: 2376\n",
      "Improved validation loss from: 0.021130008995532988  to: 0.021120329201221467\n",
      "Training iteration: 2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.021120329201221467  to: 0.020933568477630615\n",
      "Training iteration: 2378\n",
      "Improved validation loss from: 0.020933568477630615  to: 0.020860633254051207\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.020948156714439392\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): 0.02104145586490631\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.021128711104393006\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.021206848323345184\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.02128937691450119\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.021320739388465883\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.0213283970952034\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.021177308261394502\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.020977476239204408\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): 0.020934836566448213\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.02094636410474777\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.021037288010120392\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): 0.020961129665374757\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.020860633254051207  to: 0.020806531608104705\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.020806531608104705  to: 0.02067883759737015\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.02067883759737015  to: 0.020675718784332275\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.020675718784332275  to: 0.02063622772693634\n",
      "Training iteration: 2396\n",
      "Improved validation loss from: 0.02063622772693634  to: 0.02058414965867996\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.02058414965867996  to: 0.020584146678447723\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): 0.02065448313951492\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.020607304573059083\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.0206205815076828\n",
      "Training iteration: 2401\n",
      "Improved validation loss from: 0.020584146678447723  to: 0.02051314413547516\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.02051314413547516  to: 0.020449188351631165\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): 0.020461122691631316\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.020531150698661804\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.020655664801597595\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.02070472240447998\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): 0.020502467453479768\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.020449188351631165  to: 0.02035042941570282\n",
      "Training iteration: 2409\n",
      "Improved validation loss from: 0.02035042941570282  to: 0.02032492607831955\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): 0.020432393252849578\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.020515260100364686\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.02064947634935379\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.020918440818786622\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.021067845821380615\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.020759233832359315\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.02058495581150055\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.020554563403129576\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.020602074265480042\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.020510992407798766\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.02048352062702179\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.020580360293388368\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.020696035027503966\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.020426413416862486\n",
      "Training iteration: 2424\n",
      "Improved validation loss from: 0.02032492607831955  to: 0.020320121943950654\n",
      "Training iteration: 2425\n",
      "Improved validation loss from: 0.020320121943950654  to: 0.020291471481323244\n",
      "Training iteration: 2426\n",
      "Improved validation loss from: 0.020291471481323244  to: 0.020215611159801482\n",
      "Training iteration: 2427\n",
      "Improved validation loss from: 0.020215611159801482  to: 0.020110325515270235\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.0201440766453743\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.02026415169239044\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.020232279598712922\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.020192694664001466\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.020252208411693572\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.02067239284515381\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.020597711205482483\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.020351223647594452\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.02022106945514679\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.020150136947631837\n",
      "Training iteration: 2438\n",
      "Improved validation loss from: 0.020110325515270235  to: 0.020030489563941954\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.020030489563941954  to: 0.019973137974739076\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.02003836929798126\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.020125392079353332\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.020179644227027893\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.020233997702598573\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.020022936165332794\n",
      "Training iteration: 2445\n",
      "Improved validation loss from: 0.019973137974739076  to: 0.019846583902835845\n",
      "Training iteration: 2446\n",
      "Improved validation loss from: 0.019846583902835845  to: 0.01976030468940735\n",
      "Training iteration: 2447\n",
      "Improved validation loss from: 0.01976030468940735  to: 0.019659450650215148\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.01966920346021652\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.01978520303964615\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.020029397308826448\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.019892559945583345\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.019659450650215148  to: 0.019624394178390504\n",
      "Training iteration: 2453\n",
      "Improved validation loss from: 0.019624394178390504  to: 0.0195282518863678\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): 0.019541385769844054\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.0195282518863678  to: 0.019489683210849762\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): 0.019567541778087616\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.019754555821418763\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.01998966038227081\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.019853460788726806\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.019705232977867127\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): 0.01960601508617401\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.019520294666290284\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.019507484138011934\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.019611066579818724\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.01976277232170105\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.019714389741420747\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.019731900095939635\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.019804206490516663\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): 0.019654172658920287\n",
      "Training iteration: 2470\n",
      "Improved validation loss from: 0.019489683210849762  to: 0.019407811760902404\n",
      "Training iteration: 2471\n",
      "Improved validation loss from: 0.019407811760902404  to: 0.01935882866382599\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.01940888464450836\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.01935882866382599  to: 0.019327528774738312\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): 0.019390538334846497\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.019645464420318604\n",
      "Training iteration: 2476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.019686380028724672\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.019598802924156188\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.019574347138404845\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.019527228176593782\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.019419607520103455\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.01938276141881943\n",
      "Training iteration: 2482\n",
      "Validation loss (no improvement): 0.01941511482000351\n",
      "Training iteration: 2483\n",
      "Validation loss (no improvement): 0.0194447323679924\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.019327528774738312  to: 0.01929101049900055\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.01930152326822281\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.01942143738269806\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.019304436445236207\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.01929101049900055  to: 0.019288374483585356\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.0193352535367012\n",
      "Training iteration: 2490\n",
      "Improved validation loss from: 0.019288374483585356  to: 0.01924319714307785\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.019265861809253694\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.019353128969669342\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.019362565875053406\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.019401536881923677\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): 0.019503684341907503\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.019535663723945617\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): 0.019286130368709565\n",
      "Training iteration: 2498\n",
      "Validation loss (no improvement): 0.019266432523727416\n",
      "Training iteration: 2499\n",
      "Validation loss (no improvement): 0.019397225975990296\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.019408592581748964\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.019436579942703248\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.01949068307876587\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.019752220809459688\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.019420358538627624\n",
      "Training iteration: 2505\n",
      "Improved validation loss from: 0.01924319714307785  to: 0.01921546012163162\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.019219280779361726\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.0192203551530838\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.019288483262062072\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.019489935040473937\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.019589942693710328\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.019609205424785614\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.01966898739337921\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.019673696160316466\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.019940918684005736\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.01971137672662735\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): 0.019374556839466095\n",
      "Training iteration: 2517\n",
      "Validation loss (no improvement): 0.01929433047771454\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.019305333495140076\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.019309185445308685\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.019383856654167177\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): 0.019555944204330444\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.019534149765968324\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.01936585009098053\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.019290031492710115\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.019287171959877013\n",
      "Training iteration: 2526\n",
      "Improved validation loss from: 0.01921546012163162  to: 0.019117358326911926\n",
      "Training iteration: 2527\n",
      "Improved validation loss from: 0.019117358326911926  to: 0.01904877871274948\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.019179320335388182\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): 0.01920609027147293\n",
      "Training iteration: 2530\n",
      "Validation loss (no improvement): 0.01917198896408081\n",
      "Training iteration: 2531\n",
      "Validation loss (no improvement): 0.01913129389286041\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.01910247802734375\n",
      "Training iteration: 2533\n",
      "Improved validation loss from: 0.01904877871274948  to: 0.019008994102478027\n",
      "Training iteration: 2534\n",
      "Improved validation loss from: 0.019008994102478027  to: 0.01884722411632538\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.018879470229148865\n",
      "Training iteration: 2536\n",
      "Validation loss (no improvement): 0.01914679557085037\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.01913144290447235\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): 0.019213955104351043\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.019342577457427977\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.019179573655128478\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.01887163370847702\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.01884722411632538  to: 0.018846161663532257\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): 0.01900347024202347\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.01911979615688324\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.019237937033176424\n",
      "Training iteration: 2546\n",
      "Validation loss (no improvement): 0.01937088817358017\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.01944253146648407\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.019191578030586243\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.019095268845558167\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.019122579693794252\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.01898060888051987\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.019030657410621644\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.019231633841991426\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.01928054094314575\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.019272848963737488\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.01923724412918091\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.019168683886528017\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.018988339602947234\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.0189641073346138\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.019148218631744384\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.01914585828781128\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.019255587458610536\n",
      "Training iteration: 2563\n",
      "Validation loss (no improvement): 0.019511786103248597\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.01947752684354782\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.01915324926376343\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.019034835696220397\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.019061246514320375\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.01913498193025589\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.019044670462608337\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.01910795271396637\n",
      "Training iteration: 2571\n",
      "Validation loss (no improvement): 0.019335472583770753\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.019391408562660216\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.019161705672740937\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.01898687481880188\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.018908044695854186\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.01903405636548996\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.01906162202358246\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.019255705177783966\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.019611248373985292\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.019538463652133943\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.019209365546703338\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.019147726893424987\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.019331726431846618\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.019411906599998474\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): 0.019233891367912294\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.019304966926574706\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.01956982910633087\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.020087489485740663\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.019609977304935456\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.019402410089969634\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.019394466280937196\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.01937502920627594\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.019275884330272674\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): 0.019334089756011964\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): 0.019414719939231873\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.019712676107883454\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.01982705146074295\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.01982751190662384\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.019869230687618256\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.019667740166187286\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): 0.019062280654907227\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): 0.018905827403068544\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.01903518885374069\n",
      "Training iteration: 2604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01903187483549118\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.01910003274679184\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): 0.019321648776531218\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.019600145518779755\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.019404132664203644\n",
      "Training iteration: 2609\n",
      "Validation loss (no improvement): 0.01919136941432953\n",
      "Training iteration: 2610\n",
      "Validation loss (no improvement): 0.019120745360851288\n",
      "Training iteration: 2611\n",
      "Validation loss (no improvement): 0.019028396904468538\n",
      "Training iteration: 2612\n",
      "Validation loss (no improvement): 0.018875399231910707\n",
      "Training iteration: 2613\n",
      "Validation loss (no improvement): 0.01892066150903702\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): 0.019297417998313905\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.019202667474746703\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.01903403103351593\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.018959814310073854\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.018938666582107543\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.018902525305747986\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.018927666544914245\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.018908050656318665\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.018899090588092804\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.01900412291288376\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.019265280663967134\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.01901576519012451\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.01896245777606964\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.01910577267408371\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.019064077734947206\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.01895151436328888\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.018973349034786223\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.019111981987953185\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.019087585806846618\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.019077612459659575\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.019142834842205046\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.01926378905773163\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.01896880716085434\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.018936887383461\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.018960344791412353\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.019065190851688386\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.01896626502275467\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.019068579375743865\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.019284050166606902\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_2 = krishnan_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_2.train_model()\n",
    "ensemble_model_2.model_inference()\n",
    "\n",
    "ensemble_mean_2 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_2 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_2 = np.sqrt(np.exp(ensemble_logvar_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 20.70026397705078\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 20.70026397705078  to: 14.25919189453125\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 14.25919189453125  to: 10.054856109619141\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 10.054856109619141  to: 7.250257873535157\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 7.250257873535157  to: 5.343587493896484\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 5.343587493896484  to: 4.020647430419922\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 4.020647430419922  to: 3.0858211517333984\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 3.0858211517333984  to: 2.415014457702637\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 2.415014457702637  to: 1.9284622192382812\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.9284622192382812  to: 1.569287395477295\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.569287395477295  to: 1.2968414306640625\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 1.2968414306640625  to: 1.0878540992736816\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.0878540992736816  to: 0.9259660720825196\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.9259660720825196  to: 0.7988139152526855\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.7988139152526855  to: 0.6977200508117676\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.6977200508117676  to: 0.6163029670715332\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.6163029670715332  to: 0.550172233581543\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.550172233581543  to: 0.49603662490844724\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.49603662490844724  to: 0.4513853549957275\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.4513853549957275  to: 0.41429429054260253\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.41429429054260253  to: 0.38325676918029783\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.38325676918029783  to: 0.3571244478225708\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.3571244478225708  to: 0.33498923778533934\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.33498923778533934  to: 0.31613361835479736\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.31613361835479736  to: 0.2999913454055786\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.2999913454055786  to: 0.2860921621322632\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.2860921621322632  to: 0.27405924797058107\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.27405924797058107  to: 0.26359031200408933\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.26359031200408933  to: 0.2544370174407959\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.2544370174407959  to: 0.24639992713928222\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.24639992713928222  to: 0.23930950164794923\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.23930950164794923  to: 0.23303027153015138\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.23303027153015138  to: 0.22745051383972167\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.22745051383972167  to: 0.2224652051925659\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.2224652051925659  to: 0.21799120903015137\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.21799120903015137  to: 0.21396090984344482\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.21396090984344482  to: 0.21031570434570312\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.21031570434570312  to: 0.20700814723968505\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.20700814723968505  to: 0.20399413108825684\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.20399413108825684  to: 0.20123679637908937\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.20123679637908937  to: 0.1987054467201233\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.1987054467201233  to: 0.19637330770492553\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.19637330770492553  to: 0.1942180275917053\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.1942180275917053  to: 0.19222034215927125\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.19222034215927125  to: 0.19036176204681396\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.19036176204681396  to: 0.18862676620483398\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.18862676620483398  to: 0.18700214624404907\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.18700214624404907  to: 0.18547626733779907\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.18547626733779907  to: 0.18403908014297485\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.18403908014297485  to: 0.18268169164657594\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.18268169164657594  to: 0.18139636516571045\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.18139636516571045  to: 0.18017568588256835\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.18017568588256835  to: 0.17901405096054077\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.17901405096054077  to: 0.17790607213974\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.17790607213974  to: 0.17684701681137086\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.17684701681137086  to: 0.17583258152008058\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.17583258152008058  to: 0.17485898733139038\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.17485898733139038  to: 0.17392289638519287\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.17392289638519287  to: 0.17302124500274657\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.17302124500274657  to: 0.17215139865875245\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.17215139865875245  to: 0.17130966186523439\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.17130966186523439  to: 0.1704941749572754\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.1704941749572754  to: 0.16970398426055908\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.16970398426055908  to: 0.16893726587295532\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.16893726587295532  to: 0.16819250583648682\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.16819250583648682  to: 0.16746816635131836\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.16746816635131836  to: 0.16676299571990966\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.16676299571990966  to: 0.16607582569122314\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.16607582569122314  to: 0.16540569067001343\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.16540569067001343  to: 0.1647505521774292\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.1647505521774292  to: 0.16410725116729735\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.16410725116729735  to: 0.16347694396972656\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.16347694396972656  to: 0.16286022663116456\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.16286022663116456  to: 0.16225650310516357\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.16225650310516357  to: 0.1616650938987732\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.1616650938987732  to: 0.16108551025390624\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.16108551025390624  to: 0.16051723957061767\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.16051723957061767  to: 0.15995708703994752\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.15995708703994752  to: 0.15939199924468994\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.15939199924468994  to: 0.1588348150253296\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.1588348150253296  to: 0.15828323364257812\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.15828323364257812  to: 0.1577363967895508\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.1577363967895508  to: 0.1571892499923706\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.1571892499923706  to: 0.15664101839065553\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.15664101839065553  to: 0.15609781742095946\n",
      "Training iteration: 85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.15609781742095946  to: 0.15555295944213868\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.15555295944213868  to: 0.15501129627227783\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.15501129627227783  to: 0.15446102619171143\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.15446102619171143  to: 0.1539148807525635\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.1539148807525635  to: 0.1533764123916626\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.1533764123916626  to: 0.15284746885299683\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.15284746885299683  to: 0.15232447385787964\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.15232447385787964  to: 0.1518113136291504\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.1518113136291504  to: 0.15130898952484131\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15130898952484131  to: 0.15081726312637328\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.15081726312637328  to: 0.15033648014068604\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15033648014068604  to: 0.14986732006072997\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.14986732006072997  to: 0.14940547943115234\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.14940547943115234  to: 0.14895496368408204\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14895496368408204  to: 0.14851796627044678\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14851796627044678  to: 0.1480926275253296\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.1480926275253296  to: 0.14767512083053588\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.14767512083053588  to: 0.14727073907852173\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14727073907852173  to: 0.146878981590271\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.146878981590271  to: 0.14650017023086548\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14650017023086548  to: 0.1461341619491577\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.1461341619491577  to: 0.14578075408935548\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.14578075408935548  to: 0.1454394578933716\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1454394578933716  to: 0.14510973691940307\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14510973691940307  to: 0.1447911500930786\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.1447911500930786  to: 0.14448316097259523\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14448316097259523  to: 0.14418540000915528\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.14418540000915528  to: 0.14389733076095582\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14389733076095582  to: 0.1436189293861389\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.1436189293861389  to: 0.14334967136383056\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.14334967136383056  to: 0.1430894136428833\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.1430894136428833  to: 0.14283770322799683\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14283770322799683  to: 0.14259411096572877\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.14259411096572877  to: 0.14235817193984984\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.14235817193984984  to: 0.14212970733642577\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14212970733642577  to: 0.14190809726715087\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14190809726715087  to: 0.14169228076934814\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.14169228076934814  to: 0.14148268699645997\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14148268699645997  to: 0.14127895832061768\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14127895832061768  to: 0.14108093976974487\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14108093976974487  to: 0.140888249874115\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.140888249874115  to: 0.14070048332214355\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14070048332214355  to: 0.14051655530929566\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14051655530929566  to: 0.14033688306808473\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.14033688306808473  to: 0.1401611566543579\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.1401611566543579  to: 0.1399890661239624\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.1399890661239624  to: 0.13982030153274536\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.13982030153274536  to: 0.13965461254119874\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.13965461254119874  to: 0.13949201107025147\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.13949201107025147  to: 0.13933223485946655\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.13933223485946655  to: 0.13917500972747804\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.13917500972747804  to: 0.1390201210975647\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.1390201210975647  to: 0.1388672709465027\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.1388672709465027  to: 0.13871631622314454\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.13871631622314454  to: 0.13856699466705322\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.13856699466705322  to: 0.13841913938522338\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.13841913938522338  to: 0.1382725477218628\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1382725477218628  to: 0.13812707662582396\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.13812707662582396  to: 0.13798253536224364\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.13798253536224364  to: 0.13783878087997437\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.13783878087997437  to: 0.13769568204879762\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.13769568204879762  to: 0.13755309581756592\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.13755309581756592  to: 0.13741090297698974\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.13741090297698974  to: 0.1372689723968506\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1372689723968506  to: 0.13712723255157472\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.13712723255157472  to: 0.1369856595993042\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.1369856595993042  to: 0.13684409856796265\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.13684409856796265  to: 0.13670244216918945\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.13670244216918945  to: 0.13656086921691896\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.13656086921691896  to: 0.13641948699951173\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.13641948699951173  to: 0.13627818822860718\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.13627818822860718  to: 0.1361369252204895\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.1361369252204895  to: 0.13599560260772706\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.13599560260772706  to: 0.13585418462753296\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.13585418462753296  to: 0.13571263551712037\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.13571263551712037  to: 0.13557093143463134\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.13557093143463134  to: 0.13542898893356323\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.13542898893356323  to: 0.13528680801391602\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.13528680801391602  to: 0.13514440059661864\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.13514440059661864  to: 0.13500181436538697\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.13500181436538697  to: 0.13485877513885497\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.13485877513885497  to: 0.13471536636352538\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.13471536636352538  to: 0.13457162380218507\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.13457162380218507  to: 0.1344277024269104\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1344277024269104  to: 0.13428361415863038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 170\n",
      "Improved validation loss from: 0.13428361415863038  to: 0.13413939476013184\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.13413939476013184  to: 0.1339951276779175\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.1339951276779175  to: 0.13385093212127686\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.13385093212127686  to: 0.1337069272994995\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.1337069272994995  to: 0.13356325626373292\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.13356325626373292  to: 0.1334200382232666\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.1334200382232666  to: 0.13327744007110595\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.13327744007110595  to: 0.13313562870025636\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.13313562870025636  to: 0.13299477100372314\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.13299477100372314  to: 0.13285505771636963\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.13285505771636963  to: 0.13271670341491698\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.13271670341491698  to: 0.1325799345970154\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.1325799345970154  to: 0.13244497776031494\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.13244497776031494  to: 0.13231208324432372\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.13231208324432372  to: 0.1321815013885498\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.1321815013885498  to: 0.13205363750457763\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.13205363750457763  to: 0.1319286346435547\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.1319286346435547  to: 0.13180677890777587\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13180677890777587  to: 0.13168834447860717\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13168834447860717  to: 0.13157360553741454\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.13157360553741454  to: 0.13146283626556396\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13146283626556396  to: 0.13135660886764527\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13135660886764527  to: 0.1312551498413086\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.1312551498413086  to: 0.1311586856842041\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.1311586856842041  to: 0.13106740713119508\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13106740713119508  to: 0.13098150491714478\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13098150491714478  to: 0.1309019446372986\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.1309019446372986  to: 0.1308288335800171\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.1308288335800171  to: 0.1307621955871582\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.1307621955871582  to: 0.13070204257965087\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13070204257965087  to: 0.1306483507156372\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.1306483507156372  to: 0.13060073852539061\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13060073852539061  to: 0.13055999279022218\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.13055999279022218  to: 0.13052675724029542\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.13052675724029542  to: 0.13050066232681273\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13050066232681273  to: 0.13048117160797118\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13048117160797118  to: 0.13046756982803345\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13046756982803345  to: 0.13045916557312012\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13045916557312012  to: 0.13045520782470704\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13045520782470704  to: 0.13045495748519897\n",
      "Training iteration: 210\n",
      "Validation loss (no improvement): 0.13045759201049806\n",
      "Training iteration: 211\n",
      "Validation loss (no improvement): 0.13046224117279054\n",
      "Training iteration: 212\n",
      "Validation loss (no improvement): 0.13046808242797853\n",
      "Training iteration: 213\n",
      "Validation loss (no improvement): 0.13047428131103517\n",
      "Training iteration: 214\n",
      "Validation loss (no improvement): 0.1304800271987915\n",
      "Training iteration: 215\n",
      "Validation loss (no improvement): 0.13048416376113892\n",
      "Training iteration: 216\n",
      "Validation loss (no improvement): 0.13048597574234008\n",
      "Training iteration: 217\n",
      "Validation loss (no improvement): 0.13048479557037354\n",
      "Training iteration: 218\n",
      "Validation loss (no improvement): 0.13048005104064941\n",
      "Training iteration: 219\n",
      "Validation loss (no improvement): 0.13047125339508056\n",
      "Training iteration: 220\n",
      "Validation loss (no improvement): 0.13045806884765626\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13045495748519897  to: 0.13044028282165526\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.13044028282165526  to: 0.1304176926612854\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.1304176926612854  to: 0.13039021492004393\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.13039021492004393  to: 0.13035788536071777\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13035788536071777  to: 0.13032083511352538\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13032083511352538  to: 0.13027924299240112\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13027924299240112  to: 0.13023334741592407\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13023334741592407  to: 0.13018343448638917\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.13018343448638917  to: 0.130129873752594\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.130129873752594  to: 0.13007301092147827\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.13007301092147827  to: 0.13001325130462646\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.13001325130462646  to: 0.12995097637176514\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.12995097637176514  to: 0.12988584041595458\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.12988584041595458  to: 0.1298187255859375\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1298187255859375  to: 0.12975019216537476\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.12975019216537476  to: 0.129680597782135\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.129680597782135  to: 0.1296102523803711\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1296102523803711  to: 0.12953945398330688\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.12953945398330688  to: 0.12946850061416626\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.12946850061416626  to: 0.1293976426124573\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.1293976426124573  to: 0.12932711839675903\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.12932711839675903  to: 0.12925707101821898\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.12925707101821898  to: 0.12918763160705565\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.12918763160705565  to: 0.12911895513534546\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.12911895513534546  to: 0.12905119657516478\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.12905119657516478  to: 0.1289844274520874\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.1289844274520874  to: 0.1289189100265503\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.1289189100265503  to: 0.1288547158241272\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.1288547158241272  to: 0.1287926435470581\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.1287926435470581  to: 0.12873265743255616\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.12873265743255616  to: 0.12867472171783448\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.12867472171783448  to: 0.1286187529563904\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.1286187529563904  to: 0.1285646915435791\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.1285646915435791  to: 0.12851245403289796\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.12851245403289796  to: 0.12846195697784424\n",
      "Training iteration: 256\n",
      "Improved validation loss from: 0.12846195697784424  to: 0.12841306924819945\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.12841306924819945  to: 0.1283657431602478\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.1283657431602478  to: 0.1283198356628418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 259\n",
      "Improved validation loss from: 0.1283198356628418  to: 0.12827528715133668\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.12827528715133668  to: 0.12823197841644288\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.12823197841644288  to: 0.1281898260116577\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1281898260116577  to: 0.1281487226486206\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.1281487226486206  to: 0.1281085729598999\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.1281085729598999  to: 0.12806932926177977\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.12806932926177977  to: 0.1280308485031128\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.1280308485031128  to: 0.12799398899078368\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.12799398899078368  to: 0.12795861959457397\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.12795861959457397  to: 0.12792456150054932\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.12792456150054932  to: 0.12789167165756227\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.12789167165756227  to: 0.12785978317260743\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.12785978317260743  to: 0.1278287649154663\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1278287649154663  to: 0.12779542207717895\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.12779542207717895  to: 0.12776219844818115\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.12776219844818115  to: 0.12772934436798095\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.12772934436798095  to: 0.12769672870635987\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.12769672870635987  to: 0.12766423225402831\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.12766423225402831  to: 0.12763174772262573\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.12763174772262573  to: 0.12759917974472046\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.12759917974472046  to: 0.1275664448738098\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.1275664448738098  to: 0.12753453254699706\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.12753453254699706  to: 0.12750328779220582\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.12750328779220582  to: 0.1274707794189453\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.1274707794189453  to: 0.12743628025054932\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.12743628025054932  to: 0.1274019241333008\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.1274019241333008  to: 0.12736759185791016\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.12736759185791016  to: 0.12733314037322999\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.12733314037322999  to: 0.12729848623275758\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.12729848623275758  to: 0.12726348638534546\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.12726348638534546  to: 0.12722809314727784\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.12722809314727784  to: 0.12719218730926513\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.12719218730926513  to: 0.12715573310852052\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.12715573310852052  to: 0.12711869478225707\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.12711869478225707  to: 0.12708101272583008\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.12708101272583008  to: 0.12704265117645264\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.12704265117645264  to: 0.12700364589691163\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.12700364589691163  to: 0.12696391344070435\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.12696391344070435  to: 0.12692443132400513\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.12692443132400513  to: 0.1268850564956665\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.1268850564956665  to: 0.12684580087661743\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.12684580087661743  to: 0.12680654525756835\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.12680654525756835  to: 0.12676723003387452\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.12676723003387452  to: 0.12672780752182006\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.12672780752182006  to: 0.1266884446144104\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.1266884446144104  to: 0.12664899826049805\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.12664899826049805  to: 0.12660930156707764\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.12660930156707764  to: 0.12656935453414916\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.12656935453414916  to: 0.12652890682220458\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.12652890682220458  to: 0.1264880895614624\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.1264880895614624  to: 0.12644697427749635\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.12644697427749635  to: 0.1264055609703064\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.1264055609703064  to: 0.12636382579803468\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.12636382579803468  to: 0.12632184028625487\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.12632184028625487  to: 0.12627956867218018\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.12627956867218018  to: 0.12623707056045533\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.12623707056045533  to: 0.1261943459510803\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.1261943459510803  to: 0.1261514186859131\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.1261514186859131  to: 0.12610832452774048\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.12610832452774048  to: 0.12606513500213623\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.12606513500213623  to: 0.1260218381881714\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1260218381881714  to: 0.12597848176956178\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.12597848176956178  to: 0.1259351134300232\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1259351134300232  to: 0.12589173316955565\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.12589173316955565  to: 0.125848388671875\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.125848388671875  to: 0.125805127620697\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.125805127620697  to: 0.12576193809509278\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.12576193809509278  to: 0.12571885585784912\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.12571885585784912  to: 0.12567589282989503\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.12567589282989503  to: 0.1256330728530884\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1256330728530884  to: 0.1255903959274292\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.1255903959274292  to: 0.12554785013198852\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.12554785013198852  to: 0.12550649642944336\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.12550649642944336  to: 0.1254661798477173\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.1254661798477173  to: 0.12542687654495238\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.12542687654495238  to: 0.12538846731185913\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.12538846731185913  to: 0.12535082101821898\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.12535082101821898  to: 0.12531379461288453\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.12531379461288453  to: 0.12527720928192138\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.12527720928192138  to: 0.12524086236953735\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.12524086236953735  to: 0.12520458698272705\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.12520458698272705  to: 0.12516958713531495\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.12516958713531495  to: 0.1251356601715088\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.1251356601715088  to: 0.12510251998901367\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.12510251998901367  to: 0.12506994009017944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 344\n",
      "Improved validation loss from: 0.12506994009017944  to: 0.12503767013549805\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.12503767013549805  to: 0.12500550746917724\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.12500550746917724  to: 0.12497316598892212\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.12497316598892212  to: 0.12494043111801148\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.12494043111801148  to: 0.12490711212158204\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.12490711212158204  to: 0.1248729944229126\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.1248729944229126  to: 0.12483791112899781\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.12483791112899781  to: 0.12480175495147705\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.12480175495147705  to: 0.12476439476013183\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.12476439476013183  to: 0.12472573518753052\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.12472573518753052  to: 0.1246833086013794\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.1246833086013794  to: 0.12461817264556885\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.12461817264556885  to: 0.12455102205276489\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.12455102205276489  to: 0.1244818925857544\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.1244818925857544  to: 0.12441084384918213\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.12441084384918213  to: 0.12433792352676391\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.12433792352676391  to: 0.1242632269859314\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.1242632269859314  to: 0.12418683767318725\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.12418683767318725  to: 0.12410879135131836\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.12410879135131836  to: 0.12402915954589844\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.12402915954589844  to: 0.12394821643829346\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.12394821643829346  to: 0.12386608123779297\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.12386608123779297  to: 0.12378286123275757\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.12378286123275757  to: 0.1236986517906189\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.1236986517906189  to: 0.12361357212066651\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.12361357212066651  to: 0.12352769374847412\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.12352769374847412  to: 0.1234411358833313\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1234411358833313  to: 0.12335408926010132\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.12335408926010132  to: 0.12326661348342896\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.12326661348342896  to: 0.12317923307418824\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.12317923307418824  to: 0.12309186458587647\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.12309186458587647  to: 0.12300492525100708\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.12300492525100708  to: 0.12291854619979858\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.12291854619979858  to: 0.1228327989578247\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.1228327989578247  to: 0.12274768352508544\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.12274768352508544  to: 0.12266321182250976\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.12266321182250976  to: 0.12257928848266601\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.12257928848266601  to: 0.12249586582183838\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.12249586582183838  to: 0.12241281270980835\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.12241281270980835  to: 0.1223299741744995\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.1223299741744995  to: 0.12224723100662231\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.12224723100662231  to: 0.12216436862945557\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.12216436862945557  to: 0.12208125591278077\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.12208125591278077  to: 0.12199770212173462\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.12199770212173462  to: 0.12191346883773804\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.12191346883773804  to: 0.12182853221893311\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.12182853221893311  to: 0.12174257040023803\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.12174257040023803  to: 0.1216541886329651\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.1216541886329651  to: 0.12156391143798828\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12156391143798828  to: 0.12147396802902222\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12147396802902222  to: 0.1213846206665039\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.1213846206665039  to: 0.12129608392715455\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.12129608392715455  to: 0.12120739221572877\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.12120739221572877  to: 0.12111895084381104\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.12111895084381104  to: 0.12103112936019897\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.12103112936019897  to: 0.12094423770904542\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.12094423770904542  to: 0.12085847854614258\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12085847854614258  to: 0.1207740068435669\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.1207740068435669  to: 0.12069085836410523\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.12069085836410523  to: 0.1206089735031128\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.1206089735031128  to: 0.1205282211303711\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.1205282211303711  to: 0.12044847011566162\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.12044847011566162  to: 0.12036956548690796\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.12036956548690796  to: 0.12029367685317993\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.12029367685317993  to: 0.12022784948349\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.12022784948349  to: 0.1201621413230896\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.1201621413230896  to: 0.12009627819061279\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.12009627819061279  to: 0.12003004550933838\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.12003004550933838  to: 0.11996076107025147\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.11996076107025147  to: 0.11988874673843383\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.11988874673843383  to: 0.11981422901153564\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.11981422901153564  to: 0.11973755359649658\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.11973755359649658  to: 0.11965901851654052\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.11965901851654052  to: 0.11957896947860717\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.11957896947860717  to: 0.11949783563613892\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.11949783563613892  to: 0.11941592693328858\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.11941592693328858  to: 0.11933357715606689\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.11933357715606689  to: 0.11925103664398193\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.11925103664398193  to: 0.11916860342025756\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.11916860342025756  to: 0.11908649206161499\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.11908649206161499  to: 0.11900491714477539\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.11900491714477539  to: 0.11892402172088623\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.11892402172088623  to: 0.11884393692016601\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.11884393692016601  to: 0.11876480579376221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 428\n",
      "Improved validation loss from: 0.11876480579376221  to: 0.11868666410446167\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.11868666410446167  to: 0.11860942840576172\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.11860942840576172  to: 0.11853306293487549\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.11853306293487549  to: 0.1184574842453003\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.1184574842453003  to: 0.11838258504867553\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.11838258504867553  to: 0.11830823421478272\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.11830823421478272  to: 0.11823437213897706\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.11823437213897706  to: 0.1181607723236084\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.1181607723236084  to: 0.11808723211288452\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.11808723211288452  to: 0.11801363229751587\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.11801363229751587  to: 0.11793982982635498\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.11793982982635498  to: 0.11786569356918335\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.11786569356918335  to: 0.11778613328933715\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.11778613328933715  to: 0.11770167350769042\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.11770167350769042  to: 0.11761289834976196\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.11761289834976196  to: 0.11752053499221801\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.11752053499221801  to: 0.11742532253265381\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.11742532253265381  to: 0.11732810735702515\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.11732810735702515  to: 0.11722962856292725\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.11722962856292725  to: 0.11713066101074218\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.11713066101074218  to: 0.11703184843063355\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.11703184843063355  to: 0.11693370342254639\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.11693370342254639  to: 0.11683673858642578\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.11683673858642578  to: 0.11674137115478515\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.11674137115478515  to: 0.11664783954620361\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.11664783954620361  to: 0.11655628681182861\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.11655628681182861  to: 0.11646666526794433\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.11646666526794433  to: 0.11637895107269287\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.11637895107269287  to: 0.11629068851470947\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.11629068851470947  to: 0.11620199680328369\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.11620199680328369  to: 0.11611297130584716\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.11611297130584716  to: 0.11602375507354737\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.11602375507354737  to: 0.11593799591064453\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.11593799591064453  to: 0.1158552885055542\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.1158552885055542  to: 0.11577503681182862\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.11577503681182862  to: 0.1156965970993042\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.1156965970993042  to: 0.11561927795410157\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.11561927795410157  to: 0.11554234027862549\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.11554234027862549  to: 0.11546493768692016\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.11546493768692016  to: 0.11538645029067993\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.11538645029067993  to: 0.11530632972717285\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.11530632972717285  to: 0.1152236819267273\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.1152236819267273  to: 0.11513850688934327\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.11513850688934327  to: 0.11505109071731567\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.11505109071731567  to: 0.11495412588119507\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.11495412588119507  to: 0.1148486852645874\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.1148486852645874  to: 0.11473621129989624\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.11473621129989624  to: 0.11461837291717529\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.11461837291717529  to: 0.11449697017669677\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.11449697017669677  to: 0.11437370777130126\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.11437370777130126  to: 0.11424272060394287\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.11424272060394287  to: 0.11410645246505738\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.11410645246505738  to: 0.11396715641021729\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.11396715641021729  to: 0.1138242244720459\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.1138242244720459  to: 0.11367965936660766\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.11367965936660766  to: 0.11353510618209839\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.11353510618209839  to: 0.11339178085327148\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.11339178085327148  to: 0.11325041055679322\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.11325041055679322  to: 0.11311140060424804\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.11311140060424804  to: 0.11297481060028076\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.11297481060028076  to: 0.11284043788909912\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.11284043788909912  to: 0.11270794868469239\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.11270794868469239  to: 0.11257685422897339\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.11257685422897339  to: 0.11244678497314453\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.11244678497314453  to: 0.11231738328933716\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.11231738328933716  to: 0.11218845844268799\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.11218845844268799  to: 0.1120599389076233\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.1120599389076233  to: 0.11193184852600098\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.11193184852600098  to: 0.11180431842803955\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.11180431842803955  to: 0.11167750358581544\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.11167750358581544  to: 0.11154276132583618\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.11154276132583618  to: 0.11140134334564208\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.11140134334564208  to: 0.1112546443939209\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1112546443939209  to: 0.11110396385192871\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.11110396385192871  to: 0.11095044612884522\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.11095044612884522  to: 0.11079494953155518\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.11079494953155518  to: 0.1106379747390747\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.1106379747390747  to: 0.110479736328125\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.110479736328125  to: 0.11032013893127442\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.11032013893127442  to: 0.11015887260437011\n",
      "Training iteration: 508\n",
      "Improved validation loss from: 0.11015887260437011  to: 0.10999555587768554\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.10999555587768554  to: 0.1097974181175232\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.1097974181175232  to: 0.10957272052764892\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.10957272052764892  to: 0.10936650037765502\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.10936650037765502  to: 0.109183669090271\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.109183669090271  to: 0.10902494192123413\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.10902494192123413  to: 0.10888679027557373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 515\n",
      "Improved validation loss from: 0.10888679027557373  to: 0.10871362686157227\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.10871362686157227  to: 0.10850783586502075\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.10850783586502075  to: 0.10827794075012206\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.10827794075012206  to: 0.10803616046905518\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.10803616046905518  to: 0.10783082246780396\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.10783082246780396  to: 0.10765063762664795\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.10765063762664795  to: 0.10745450258255004\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.10745450258255004  to: 0.10724239349365235\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.10724239349365235  to: 0.10701649188995362\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.10701649188995362  to: 0.1067440390586853\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1067440390586853  to: 0.10647780895233154\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.10647780895233154  to: 0.10622835159301758\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.10622835159301758  to: 0.10600149631500244\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.10600149631500244  to: 0.10581214427947998\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.10581214427947998  to: 0.10559895038604736\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.10559895038604736  to: 0.10536476373672485\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.10536476373672485  to: 0.10511856079101563\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.10511856079101563  to: 0.10491030216217041\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.10491030216217041  to: 0.10469909906387329\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.10469909906387329  to: 0.10448906421661378\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.10448906421661378  to: 0.10428742170333863\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.10428742170333863  to: 0.10409200191497803\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.10409200191497803  to: 0.10390201807022095\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.10390201807022095  to: 0.10371615886688232\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.10371615886688232  to: 0.10353139638900757\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.10353139638900757  to: 0.10334947109222412\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.10334947109222412  to: 0.10316197872161866\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.10316197872161866  to: 0.1030076026916504\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.1030076026916504  to: 0.10286858081817626\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.10286858081817626  to: 0.10267465114593506\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.10267465114593506  to: 0.10242351293563842\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.10242351293563842  to: 0.10213730335235596\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.10213730335235596  to: 0.1018896222114563\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.1018896222114563  to: 0.10169470310211182\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.10169470310211182  to: 0.1015350341796875\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.1015350341796875  to: 0.10133281946182252\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.10133281946182252  to: 0.10107237100601196\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.10107237100601196  to: 0.10076730251312256\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.10076730251312256  to: 0.10045113563537597\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.10045113563537597  to: 0.10019614696502685\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.10019614696502685  to: 0.09999675750732422\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.09999675750732422  to: 0.09977941513061524\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.09977941513061524  to: 0.09952187538146973\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.09952187538146973  to: 0.0992258369922638\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.0992258369922638  to: 0.09878253936767578\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.09878253936767578  to: 0.09851519465446472\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.09851519465446472  to: 0.09825992584228516\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.09825992584228516  to: 0.09794570803642273\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.09794570803642273  to: 0.09752777814865113\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.09752777814865113  to: 0.0971453070640564\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.0971453070640564  to: 0.09681755304336548\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.09681755304336548  to: 0.09652969241142273\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.09652969241142273  to: 0.09605989456176758\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.09605989456176758  to: 0.0955493152141571\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.0955493152141571  to: 0.09510067105293274\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.09510067105293274  to: 0.09479420781135559\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.09479420781135559  to: 0.0945996642112732\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.0945996642112732  to: 0.09435842633247375\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.09435842633247375  to: 0.09401124715805054\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.09401124715805054  to: 0.09368842840194702\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.09368842840194702  to: 0.09345224499702454\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.09345224499702454  to: 0.09330698251724243\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.09330698251724243  to: 0.09314531087875366\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.09314531087875366  to: 0.09291839599609375\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.09291839599609375  to: 0.09263979792594909\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.09263979792594909  to: 0.09236475229263305\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.09236475229263305  to: 0.09212749600410461\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.09212749600410461  to: 0.09180415868759155\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.09180415868759155  to: 0.09158770442008972\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.09158770442008972  to: 0.09132021069526672\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.09132021069526672  to: 0.09113612174987792\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.09113612174987792  to: 0.0904508113861084\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.0904508113861084  to: 0.09016273617744446\n",
      "Training iteration: 588\n",
      "Validation loss (no improvement): 0.09020620584487915\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.09016273617744446  to: 0.08979871869087219\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.08979871869087219  to: 0.08908864855766296\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.08908864855766296  to: 0.0887912631034851\n",
      "Training iteration: 592\n",
      "Validation loss (no improvement): 0.08887488245964051\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.0887912631034851  to: 0.08872064352035522\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.08872064352035522  to: 0.08806091547012329\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.08806091547012329  to: 0.08776763081550598\n",
      "Training iteration: 596\n",
      "Validation loss (no improvement): 0.08783785104751587\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.08776763081550598  to: 0.08762714266777039\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.08762714266777039  to: 0.08712788820266723\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.08712788820266723  to: 0.08658258318901062\n",
      "Training iteration: 600\n",
      "Validation loss (no improvement): 0.08659302592277526\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.08658258318901062  to: 0.0865220844745636\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.0865220844745636  to: 0.08620740175247192\n",
      "Training iteration: 603\n",
      "Validation loss (no improvement): 0.08621975183486938\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.08620740175247192  to: 0.08576860427856445\n",
      "Training iteration: 605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.08576860427856445  to: 0.0852705955505371\n",
      "Training iteration: 606\n",
      "Validation loss (no improvement): 0.0853077232837677\n",
      "Training iteration: 607\n",
      "Validation loss (no improvement): 0.08529396057128906\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.0852705955505371  to: 0.0849877953529358\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.0849877953529358  to: 0.0846460998058319\n",
      "Training iteration: 610\n",
      "Validation loss (no improvement): 0.08477676510810853\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.0846460998058319  to: 0.08463018536567687\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.08463018536567687  to: 0.08424094915390015\n",
      "Training iteration: 613\n",
      "Validation loss (no improvement): 0.08430298566818237\n",
      "Training iteration: 614\n",
      "Validation loss (no improvement): 0.08426606059074401\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.08424094915390015  to: 0.084051114320755\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.084051114320755  to: 0.0837698757648468\n",
      "Training iteration: 617\n",
      "Validation loss (no improvement): 0.08391440510749817\n",
      "Training iteration: 618\n",
      "Validation loss (no improvement): 0.08379659652709961\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.0837698757648468  to: 0.08345529437065125\n",
      "Training iteration: 620\n",
      "Validation loss (no improvement): 0.08354830741882324\n",
      "Training iteration: 621\n",
      "Validation loss (no improvement): 0.0835118293762207\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.08345529437065125  to: 0.08330472707748413\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.08330472707748413  to: 0.0830218493938446\n",
      "Training iteration: 624\n",
      "Validation loss (no improvement): 0.08318964838981628\n",
      "Training iteration: 625\n",
      "Validation loss (no improvement): 0.08318320512771607\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.0830218493938446  to: 0.08282629251480103\n",
      "Training iteration: 627\n",
      "Validation loss (no improvement): 0.08286316990852356\n",
      "Training iteration: 628\n",
      "Validation loss (no improvement): 0.08320146799087524\n",
      "Training iteration: 629\n",
      "Validation loss (no improvement): 0.08290444612503052\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.08282629251480103  to: 0.08240124583244324\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.08240124583244324  to: 0.08212080001831054\n",
      "Training iteration: 632\n",
      "Validation loss (no improvement): 0.08237150311470032\n",
      "Training iteration: 633\n",
      "Validation loss (no improvement): 0.08273684382438659\n",
      "Training iteration: 634\n",
      "Validation loss (no improvement): 0.08262068033218384\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.08212080001831054  to: 0.0820564866065979\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.0820564866065979  to: 0.08194998502731324\n",
      "Training iteration: 637\n",
      "Validation loss (no improvement): 0.08225749135017395\n",
      "Training iteration: 638\n",
      "Validation loss (no improvement): 0.08233627080917358\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.08194998502731324  to: 0.08191467523574829\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.08191467523574829  to: 0.08148987889289856\n",
      "Training iteration: 641\n",
      "Validation loss (no improvement): 0.08157075643539428\n",
      "Training iteration: 642\n",
      "Validation loss (no improvement): 0.08211167454719544\n",
      "Training iteration: 643\n",
      "Validation loss (no improvement): 0.08208383321762085\n",
      "Training iteration: 644\n",
      "Validation loss (no improvement): 0.08149291872978211\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.08148987889289856  to: 0.08103144764900208\n",
      "Training iteration: 646\n",
      "Validation loss (no improvement): 0.08111084699630737\n",
      "Training iteration: 647\n",
      "Validation loss (no improvement): 0.081663578748703\n",
      "Training iteration: 648\n",
      "Validation loss (no improvement): 0.08180667757987976\n",
      "Training iteration: 649\n",
      "Validation loss (no improvement): 0.08134698867797852\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.08103144764900208  to: 0.08081430196762085\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.08081430196762085  to: 0.08073664903640747\n",
      "Training iteration: 652\n",
      "Validation loss (no improvement): 0.08105809092521668\n",
      "Training iteration: 653\n",
      "Validation loss (no improvement): 0.08121590614318848\n",
      "Training iteration: 654\n",
      "Validation loss (no improvement): 0.08091607093811035\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.08073664903640747  to: 0.08042192459106445\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.08042192459106445  to: 0.08039263486862183\n",
      "Training iteration: 657\n",
      "Validation loss (no improvement): 0.0807924747467041\n",
      "Training iteration: 658\n",
      "Validation loss (no improvement): 0.0809062361717224\n",
      "Training iteration: 659\n",
      "Validation loss (no improvement): 0.08058538436889648\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.08039263486862183  to: 0.08023807406425476\n",
      "Training iteration: 661\n",
      "Validation loss (no improvement): 0.08035968542098999\n",
      "Training iteration: 662\n",
      "Validation loss (no improvement): 0.0809055209159851\n",
      "Training iteration: 663\n",
      "Validation loss (no improvement): 0.0809284806251526\n",
      "Training iteration: 664\n",
      "Validation loss (no improvement): 0.08038454055786133\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.08023807406425476  to: 0.08017936944961548\n",
      "Training iteration: 666\n",
      "Validation loss (no improvement): 0.08030349612236024\n",
      "Training iteration: 667\n",
      "Validation loss (no improvement): 0.08026425242424011\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.08017936944961548  to: 0.07995423078536987\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.07995423078536987  to: 0.07985419034957886\n",
      "Training iteration: 670\n",
      "Validation loss (no improvement): 0.08035280108451844\n",
      "Training iteration: 671\n",
      "Validation loss (no improvement): 0.08089335560798645\n",
      "Training iteration: 672\n",
      "Validation loss (no improvement): 0.08040125966072083\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.07985419034957886  to: 0.0796690583229065\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.0796690583229065  to: 0.07942472696304322\n",
      "Training iteration: 675\n",
      "Validation loss (no improvement): 0.07972154021263123\n",
      "Training iteration: 676\n",
      "Validation loss (no improvement): 0.07980512380599976\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.07942472696304322  to: 0.07937371134757995\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.07937371134757995  to: 0.07879420518875122\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.07879420518875122  to: 0.07870357632637023\n",
      "Training iteration: 680\n",
      "Validation loss (no improvement): 0.07902507781982422\n",
      "Training iteration: 681\n",
      "Validation loss (no improvement): 0.0788844108581543\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.07870357632637023  to: 0.07832353711128234\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.07832353711128234  to: 0.078133225440979\n",
      "Training iteration: 684\n",
      "Validation loss (no improvement): 0.07827736139297485\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.078133225440979  to: 0.07810365557670593\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.07810365557670593  to: 0.0776356041431427\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.0776356041431427  to: 0.07719588279724121\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.07719588279724121  to: 0.07717138528823853\n",
      "Training iteration: 689\n",
      "Validation loss (no improvement): 0.07751785516738892\n",
      "Training iteration: 690\n",
      "Validation loss (no improvement): 0.07730168700218201\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.07717138528823853  to: 0.07661994099617005\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.07661994099617005  to: 0.07635070681571961\n",
      "Training iteration: 693\n",
      "Validation loss (no improvement): 0.07642834186553955\n",
      "Training iteration: 694\n",
      "Validation loss (no improvement): 0.07640121579170227\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.07635070681571961  to: 0.07610426545143127\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.07610426545143127  to: 0.07574660181999207\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.07574660181999207  to: 0.07571808099746705\n",
      "Training iteration: 698\n",
      "Validation loss (no improvement): 0.07587097287178039\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.07571808099746705  to: 0.07558835148811341\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.07558835148811341  to: 0.07538433074951172\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.07538433074951172  to: 0.07498881220817566\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.07498881220817566  to: 0.07489197254180908\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.07489197254180908  to: 0.07476446032524109\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.07476446032524109  to: 0.07461047768592835\n",
      "Training iteration: 705\n",
      "Validation loss (no improvement): 0.07471219301223755\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.07461047768592835  to: 0.07453998327255248\n",
      "Training iteration: 707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.07453998327255248  to: 0.07443944215774537\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.07443944215774537  to: 0.0740705132484436\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.0740705132484436  to: 0.07393530011177063\n",
      "Training iteration: 710\n",
      "Validation loss (no improvement): 0.07397825717926025\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.07393530011177063  to: 0.07381639480590821\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.07381639480590821  to: 0.07353495955467224\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.07353495955467224  to: 0.0732842206954956\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.0732842206954956  to: 0.0732259213924408\n",
      "Training iteration: 715\n",
      "Validation loss (no improvement): 0.07351667881011963\n",
      "Training iteration: 716\n",
      "Validation loss (no improvement): 0.07344229817390442\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.0732259213924408  to: 0.07296932339668274\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.07296932339668274  to: 0.07278505563735962\n",
      "Training iteration: 719\n",
      "Validation loss (no improvement): 0.07279614806175232\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.07278505563735962  to: 0.07274199724197387\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.07274199724197387  to: 0.07260522246360779\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.07260522246360779  to: 0.07240819334983825\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.07240819334983825  to: 0.07234505414962769\n",
      "Training iteration: 724\n",
      "Validation loss (no improvement): 0.07243321537971496\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.07234505414962769  to: 0.07225669622421264\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.07225669622421264  to: 0.07201511263847352\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.07201511263847352  to: 0.07193219661712646\n",
      "Training iteration: 728\n",
      "Validation loss (no improvement): 0.07193821668624878\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.07193219661712646  to: 0.07186592817306518\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.07186592817306518  to: 0.07164920568466186\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.07164920568466186  to: 0.07156317830085754\n",
      "Training iteration: 732\n",
      "Validation loss (no improvement): 0.0715725600719452\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.07156317830085754  to: 0.07149114608764648\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.07149114608764648  to: 0.07133919596672059\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.07133919596672059  to: 0.07118054628372192\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.07118054628372192  to: 0.07104607820510864\n",
      "Training iteration: 737\n",
      "Validation loss (no improvement): 0.07109360694885254\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.07104607820510864  to: 0.0710091769695282\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.0710091769695282  to: 0.07099724411964417\n",
      "Training iteration: 740\n",
      "Validation loss (no improvement): 0.07106413841247558\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.07099724411964417  to: 0.07094107866287232\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.07094107866287232  to: 0.07082294225692749\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.07082294225692749  to: 0.07073687314987183\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.07073687314987183  to: 0.07051874995231629\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.07051874995231629  to: 0.07042271494865418\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.07042271494865418  to: 0.07029912471771241\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.07029912471771241  to: 0.07027076482772827\n",
      "Training iteration: 748\n",
      "Validation loss (no improvement): 0.07029531002044678\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.07027076482772827  to: 0.07023457288742066\n",
      "Training iteration: 750\n",
      "Validation loss (no improvement): 0.07034958600997925\n",
      "Training iteration: 751\n",
      "Validation loss (no improvement): 0.0704680323600769\n",
      "Training iteration: 752\n",
      "Validation loss (no improvement): 0.07043830752372741\n",
      "Training iteration: 753\n",
      "Validation loss (no improvement): 0.07028024792671203\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.07023457288742066  to: 0.0700110912322998\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.0700110912322998  to: 0.06982409954071045\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.06982409954071045  to: 0.06963765025138854\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.06963765025138854  to: 0.06950427293777466\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.06950427293777466  to: 0.06935587525367737\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.06935587525367737  to: 0.06920826435089111\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.06920826435089111  to: 0.06912847757339477\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.06912847757339477  to: 0.06903876066207885\n",
      "Training iteration: 762\n",
      "Validation loss (no improvement): 0.06910747289657593\n",
      "Training iteration: 763\n",
      "Validation loss (no improvement): 0.06926763653755189\n",
      "Training iteration: 764\n",
      "Validation loss (no improvement): 0.06940943598747254\n",
      "Training iteration: 765\n",
      "Validation loss (no improvement): 0.0691254734992981\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.06903876066207885  to: 0.0685509443283081\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.0685509443283081  to: 0.06825022697448731\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.06825022697448731  to: 0.06819460391998292\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.06819460391998292  to: 0.06818529367446899\n",
      "Training iteration: 770\n",
      "Validation loss (no improvement): 0.06822332739830017\n",
      "Training iteration: 771\n",
      "Validation loss (no improvement): 0.06850416660308838\n",
      "Training iteration: 772\n",
      "Validation loss (no improvement): 0.06845408082008361\n",
      "Training iteration: 773\n",
      "Validation loss (no improvement): 0.06819335222244263\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.06818529367446899  to: 0.06806994676589966\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.06806994676589966  to: 0.0679401695728302\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.0679401695728302  to: 0.06792504787445068\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.06792504787445068  to: 0.06751381158828736\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.06751381158828736  to: 0.0672550618648529\n",
      "Training iteration: 779\n",
      "Validation loss (no improvement): 0.0673147201538086\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.0672550618648529  to: 0.06725419759750366\n",
      "Training iteration: 781\n",
      "Validation loss (no improvement): 0.06743683815002441\n",
      "Training iteration: 782\n",
      "Validation loss (no improvement): 0.06779693961143493\n",
      "Training iteration: 783\n",
      "Validation loss (no improvement): 0.06744781732559205\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.06725419759750366  to: 0.06709010004997254\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.06709010004997254  to: 0.06694362163543702\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.06694362163543702  to: 0.06678759455680847\n",
      "Training iteration: 787\n",
      "Validation loss (no improvement): 0.06683319807052612\n",
      "Training iteration: 788\n",
      "Validation loss (no improvement): 0.06688255071640015\n",
      "Training iteration: 789\n",
      "Validation loss (no improvement): 0.06698517799377442\n",
      "Training iteration: 790\n",
      "Validation loss (no improvement): 0.06717661619186402\n",
      "Training iteration: 791\n",
      "Validation loss (no improvement): 0.06707621812820434\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.06678759455680847  to: 0.06670591235160828\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.06670591235160828  to: 0.06644167304039002\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.06644167304039002  to: 0.06631509661674499\n",
      "Training iteration: 795\n",
      "Validation loss (no improvement): 0.06634615659713745\n",
      "Training iteration: 796\n",
      "Validation loss (no improvement): 0.06638034582138061\n",
      "Training iteration: 797\n",
      "Validation loss (no improvement): 0.06673827171325683\n",
      "Training iteration: 798\n",
      "Validation loss (no improvement): 0.06650634407997132\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.06631509661674499  to: 0.0660476565361023\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.0660476565361023  to: 0.06587467193603516\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.06587467193603516  to: 0.06583609580993652\n",
      "Training iteration: 802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.06587312817573547\n",
      "Training iteration: 803\n",
      "Validation loss (no improvement): 0.06609740257263183\n",
      "Training iteration: 804\n",
      "Validation loss (no improvement): 0.06619746088981629\n",
      "Training iteration: 805\n",
      "Validation loss (no improvement): 0.065944242477417\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.06583609580993652  to: 0.06557579040527343\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.06557579040527343  to: 0.0654513657093048\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.0654513657093048  to: 0.06539487242698669\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.06539487242698669  to: 0.06538837552070617\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.06538837552070617  to: 0.06531869769096374\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.06531869769096374  to: 0.06509234309196472\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.06509234309196472  to: 0.06488217115402221\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.06488217115402221  to: 0.0648016631603241\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.0648016631603241  to: 0.06472728848457336\n",
      "Training iteration: 815\n",
      "Validation loss (no improvement): 0.06473764181137084\n",
      "Training iteration: 816\n",
      "Validation loss (no improvement): 0.06482988595962524\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.06472728848457336  to: 0.06451238393783569\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.06451238393783569  to: 0.06441580057144165\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.06441580057144165  to: 0.06431352496147155\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.06431352496147155  to: 0.0641874611377716\n",
      "Training iteration: 821\n",
      "Validation loss (no improvement): 0.06437589526176453\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.0641874611377716  to: 0.06418312788009643\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.06418312788009643  to: 0.06407245397567748\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.06407245397567748  to: 0.06399459838867187\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.06399459838867187  to: 0.06396986842155457\n",
      "Training iteration: 826\n",
      "Validation loss (no improvement): 0.06399060487747192\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.06396986842155457  to: 0.06383596658706665\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.06383596658706665  to: 0.06375835537910461\n",
      "Training iteration: 829\n",
      "Validation loss (no improvement): 0.06377146244049073\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.06375835537910461  to: 0.06357828974723816\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.06357828974723816  to: 0.06331698894500733\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.06331698894500733  to: 0.06318024396896363\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.06318024396896363  to: 0.0630976140499115\n",
      "Training iteration: 834\n",
      "Validation loss (no improvement): 0.06316394805908203\n",
      "Training iteration: 835\n",
      "Validation loss (no improvement): 0.0633007526397705\n",
      "Training iteration: 836\n",
      "Validation loss (no improvement): 0.06349166035652161\n",
      "Training iteration: 837\n",
      "Validation loss (no improvement): 0.06336798667907714\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.0630976140499115  to: 0.06296943426132202\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.06296943426132202  to: 0.06275626420974731\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.06275626420974731  to: 0.06272746920585633\n",
      "Training iteration: 841\n",
      "Validation loss (no improvement): 0.062836754322052\n",
      "Training iteration: 842\n",
      "Validation loss (no improvement): 0.0630987524986267\n",
      "Training iteration: 843\n",
      "Validation loss (no improvement): 0.06301159262657166\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.06272746920585633  to: 0.06252931356430054\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.06252931356430054  to: 0.0623357892036438\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.0623357892036438  to: 0.06231331825256348\n",
      "Training iteration: 847\n",
      "Validation loss (no improvement): 0.06232177019119263\n",
      "Training iteration: 848\n",
      "Validation loss (no improvement): 0.06257274746894836\n",
      "Training iteration: 849\n",
      "Validation loss (no improvement): 0.06259359121322632\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.06231331825256348  to: 0.06222299337387085\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.06222299337387085  to: 0.06206502914428711\n",
      "Training iteration: 852\n",
      "Validation loss (no improvement): 0.062069594860076904\n",
      "Training iteration: 853\n",
      "Validation loss (no improvement): 0.06220401525497436\n",
      "Training iteration: 854\n",
      "Validation loss (no improvement): 0.062162363529205324\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.06206502914428711  to: 0.06184130311012268\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.06184130311012268  to: 0.061653077602386475\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.061653077602386475  to: 0.06158233880996704\n",
      "Training iteration: 858\n",
      "Validation loss (no improvement): 0.061663579940795896\n",
      "Training iteration: 859\n",
      "Validation loss (no improvement): 0.0618932843208313\n",
      "Training iteration: 860\n",
      "Validation loss (no improvement): 0.061731159687042236\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.06158233880996704  to: 0.06129766702651977\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.06129766702651977  to: 0.06112290620803833\n",
      "Training iteration: 863\n",
      "Validation loss (no improvement): 0.061125552654266356\n",
      "Training iteration: 864\n",
      "Validation loss (no improvement): 0.061326068639755246\n",
      "Training iteration: 865\n",
      "Validation loss (no improvement): 0.06178314089775085\n",
      "Training iteration: 866\n",
      "Validation loss (no improvement): 0.06169431209564209\n",
      "Training iteration: 867\n",
      "Validation loss (no improvement): 0.061166012287139894\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.06112290620803833  to: 0.060947513580322264\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.060947513580322264  to: 0.0609189510345459\n",
      "Training iteration: 870\n",
      "Validation loss (no improvement): 0.06105290055274963\n",
      "Training iteration: 871\n",
      "Validation loss (no improvement): 0.06096607446670532\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.0609189510345459  to: 0.06076976656913757\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.06076976656913757  to: 0.06068929433822632\n",
      "Training iteration: 874\n",
      "Validation loss (no improvement): 0.06077980995178223\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.06068929433822632  to: 0.06064070463180542\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.06064070463180542  to: 0.06019738912582397\n",
      "Training iteration: 877\n",
      "Validation loss (no improvement): 0.060235786437988284\n",
      "Training iteration: 878\n",
      "Validation loss (no improvement): 0.0602490246295929\n",
      "Training iteration: 879\n",
      "Validation loss (no improvement): 0.06040982604026794\n",
      "Training iteration: 880\n",
      "Validation loss (no improvement): 0.06058416366577148\n",
      "Training iteration: 881\n",
      "Validation loss (no improvement): 0.060365551710128786\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.06019738912582397  to: 0.06001518964767456\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.06001518964767456  to: 0.059688138961791995\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.059688138961791995  to: 0.059565508365631105\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.059565508365631105  to: 0.05953758955001831\n",
      "Training iteration: 886\n",
      "Validation loss (no improvement): 0.05956683754920959\n",
      "Training iteration: 887\n",
      "Validation loss (no improvement): 0.059783023595809934\n",
      "Training iteration: 888\n",
      "Validation loss (no improvement): 0.05954834222793579\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.05953758955001831  to: 0.05945645570755005\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.05945645570755005  to: 0.05935814380645752\n",
      "Training iteration: 891\n",
      "Validation loss (no improvement): 0.05941485166549683\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.05935814380645752  to: 0.05911508798599243\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.05911508798599243  to: 0.0590442955493927\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.0590442955493927  to: 0.05888115763664246\n",
      "Training iteration: 895\n",
      "Validation loss (no improvement): 0.05892753601074219\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.05888115763664246  to: 0.05857831835746765\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.05857831835746765  to: 0.05841713547706604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 898\n",
      "Improved validation loss from: 0.05841713547706604  to: 0.05828891396522522\n",
      "Training iteration: 899\n",
      "Validation loss (no improvement): 0.058314311504364016\n",
      "Training iteration: 900\n",
      "Validation loss (no improvement): 0.05855036973953247\n",
      "Training iteration: 901\n",
      "Validation loss (no improvement): 0.05896695852279663\n",
      "Training iteration: 902\n",
      "Validation loss (no improvement): 0.05837361216545105\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.05828891396522522  to: 0.05803578495979309\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.05803578495979309  to: 0.05797202587127685\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.05797202587127685  to: 0.057696598768234256\n",
      "Training iteration: 906\n",
      "Validation loss (no improvement): 0.057897156476974486\n",
      "Training iteration: 907\n",
      "Validation loss (no improvement): 0.057967013120651244\n",
      "Training iteration: 908\n",
      "Validation loss (no improvement): 0.05782950520515442\n",
      "Training iteration: 909\n",
      "Validation loss (no improvement): 0.05776907205581665\n",
      "Training iteration: 910\n",
      "Validation loss (no improvement): 0.057717311382293704\n",
      "Training iteration: 911\n",
      "Validation loss (no improvement): 0.0580485463142395\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.057696598768234256  to: 0.05757163763046265\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.05757163763046265  to: 0.05748637318611145\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.05748637318611145  to: 0.05738000869750977\n",
      "Training iteration: 915\n",
      "Validation loss (no improvement): 0.05763112306594849\n",
      "Training iteration: 916\n",
      "Validation loss (no improvement): 0.05738893747329712\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.05738000869750977  to: 0.05704580545425415\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.05704580545425415  to: 0.056814569234848025\n",
      "Training iteration: 919\n",
      "Validation loss (no improvement): 0.05682385563850403\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.056814569234848025  to: 0.056394022703170774\n",
      "Training iteration: 921\n",
      "Validation loss (no improvement): 0.056462496519088745\n",
      "Training iteration: 922\n",
      "Validation loss (no improvement): 0.05648810863494873\n",
      "Training iteration: 923\n",
      "Validation loss (no improvement): 0.0569034218788147\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.056394022703170774  to: 0.05639000535011292\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.05639000535011292  to: 0.05613591074943543\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.05613591074943543  to: 0.055972117185592654\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.055972117185592654  to: 0.055559730529785155\n",
      "Training iteration: 928\n",
      "Validation loss (no improvement): 0.05582370162010193\n",
      "Training iteration: 929\n",
      "Validation loss (no improvement): 0.05622777938842773\n",
      "Training iteration: 930\n",
      "Validation loss (no improvement): 0.05617488622665405\n",
      "Training iteration: 931\n",
      "Validation loss (no improvement): 0.056372082233428954\n",
      "Training iteration: 932\n",
      "Validation loss (no improvement): 0.05601128339767456\n",
      "Training iteration: 933\n",
      "Validation loss (no improvement): 0.05571390390396118\n",
      "Training iteration: 934\n",
      "Validation loss (no improvement): 0.05563476085662842\n",
      "Training iteration: 935\n",
      "Validation loss (no improvement): 0.0558681845664978\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.055559730529785155  to: 0.05555474162101746\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.05555474162101746  to: 0.0552176296710968\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.0552176296710968  to: 0.054990172386169434\n",
      "Training iteration: 939\n",
      "Validation loss (no improvement): 0.05501409769058228\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.054990172386169434  to: 0.05495070219039917\n",
      "Training iteration: 941\n",
      "Validation loss (no improvement): 0.05501742362976074\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.05495070219039917  to: 0.05483948588371277\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.05483948588371277  to: 0.05428087115287781\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.05428087115287781  to: 0.054104411602020265\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.054104411602020265  to: 0.054084211587905884\n",
      "Training iteration: 946\n",
      "Validation loss (no improvement): 0.05436718463897705\n",
      "Training iteration: 947\n",
      "Validation loss (no improvement): 0.05448344349861145\n",
      "Training iteration: 948\n",
      "Validation loss (no improvement): 0.054207843542098996\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.054084211587905884  to: 0.05381612777709961\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.05381612777709961  to: 0.053778046369552614\n",
      "Training iteration: 951\n",
      "Validation loss (no improvement): 0.05393164157867432\n",
      "Training iteration: 952\n",
      "Validation loss (no improvement): 0.05435907244682312\n",
      "Training iteration: 953\n",
      "Validation loss (no improvement): 0.0538462221622467\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.053778046369552614  to: 0.05377209782600403\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.05377209782600403  to: 0.05371701717376709\n",
      "Training iteration: 956\n",
      "Validation loss (no improvement): 0.05394253134727478\n",
      "Training iteration: 957\n",
      "Validation loss (no improvement): 0.053832453489303586\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.05371701717376709  to: 0.05355356335639953\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.05355356335639953  to: 0.053516602516174315\n",
      "Training iteration: 960\n",
      "Validation loss (no improvement): 0.053640061616897584\n",
      "Training iteration: 961\n",
      "Validation loss (no improvement): 0.05352263450622559\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.053516602516174315  to: 0.05316680669784546\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.05316680669784546  to: 0.053106272220611574\n",
      "Training iteration: 964\n",
      "Validation loss (no improvement): 0.053287947177886964\n",
      "Training iteration: 965\n",
      "Validation loss (no improvement): 0.05353020429611206\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.053106272220611574  to: 0.052982234954833986\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.052982234954833986  to: 0.05263377428054809\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.05263377428054809  to: 0.05257113575935364\n",
      "Training iteration: 969\n",
      "Validation loss (no improvement): 0.05287731885910034\n",
      "Training iteration: 970\n",
      "Validation loss (no improvement): 0.05294756293296814\n",
      "Training iteration: 971\n",
      "Validation loss (no improvement): 0.05283651351928711\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.05257113575935364  to: 0.05224103331565857\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.05224103331565857  to: 0.05191659927368164\n",
      "Training iteration: 974\n",
      "Validation loss (no improvement): 0.051940429210662845\n",
      "Training iteration: 975\n",
      "Validation loss (no improvement): 0.05205467939376831\n",
      "Training iteration: 976\n",
      "Validation loss (no improvement): 0.05231481194496155\n",
      "Training iteration: 977\n",
      "Validation loss (no improvement): 0.05224367380142212\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.05191659927368164  to: 0.05172114372253418\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.05172114372253418  to: 0.05146018862724304\n",
      "Training iteration: 980\n",
      "Validation loss (no improvement): 0.05147930979728699\n",
      "Training iteration: 981\n",
      "Validation loss (no improvement): 0.05186324715614319\n",
      "Training iteration: 982\n",
      "Validation loss (no improvement): 0.051716917753219606\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.05146018862724304  to: 0.05127449631690979\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.05127449631690979  to: 0.05100260972976685\n",
      "Training iteration: 985\n",
      "Validation loss (no improvement): 0.05120978355407715\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.05100260972976685  to: 0.05098047852516174\n",
      "Training iteration: 987\n",
      "Validation loss (no improvement): 0.05107272863388061\n",
      "Training iteration: 988\n",
      "Validation loss (no improvement): 0.05099492073059082\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.05098047852516174  to: 0.05060848593711853\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.05060848593711853  to: 0.050556504726409913\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.050556504726409913  to: 0.05054616332054138\n",
      "Training iteration: 992\n",
      "Validation loss (no improvement): 0.050822716951370236\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.05054616332054138  to: 0.0501320481300354\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.0501320481300354  to: 0.050110137462615965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 995\n",
      "Improved validation loss from: 0.050110137462615965  to: 0.04979587495326996\n",
      "Training iteration: 996\n",
      "Validation loss (no improvement): 0.0503747820854187\n",
      "Training iteration: 997\n",
      "Validation loss (no improvement): 0.05032497048377991\n",
      "Training iteration: 998\n",
      "Validation loss (no improvement): 0.04997012615203857\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.04979587495326996  to: 0.0497852087020874\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.0497852087020874  to: 0.049546271562576294\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.049546271562576294  to: 0.04944028854370117\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.04944028854370117  to: 0.04940126836299896\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.04940126836299896  to: 0.04902001321315765\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.04902001321315765  to: 0.048701000213623044\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.048701000213623044  to: 0.04850308299064636\n",
      "Training iteration: 1006\n",
      "Validation loss (no improvement): 0.04881469309329987\n",
      "Training iteration: 1007\n",
      "Validation loss (no improvement): 0.04998255670070648\n",
      "Training iteration: 1008\n",
      "Validation loss (no improvement): 0.048748522996902466\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.04850308299064636  to: 0.048477596044540404\n",
      "Training iteration: 1010\n",
      "Validation loss (no improvement): 0.04853478968143463\n",
      "Training iteration: 1011\n",
      "Validation loss (no improvement): 0.04877802431583404\n",
      "Training iteration: 1012\n",
      "Validation loss (no improvement): 0.04894766807556152\n",
      "Training iteration: 1013\n",
      "Validation loss (no improvement): 0.0486576646566391\n",
      "Training iteration: 1014\n",
      "Validation loss (no improvement): 0.04873359203338623\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.048477596044540404  to: 0.047676253318786624\n",
      "Training iteration: 1016\n",
      "Validation loss (no improvement): 0.047919446229934694\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.047676253318786624  to: 0.04740062355995178\n",
      "Training iteration: 1018\n",
      "Validation loss (no improvement): 0.0479253351688385\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.04740062355995178  to: 0.047174233198165896\n",
      "Training iteration: 1020\n",
      "Validation loss (no improvement): 0.04722619950771332\n",
      "Training iteration: 1021\n",
      "Validation loss (no improvement): 0.04738658964633942\n",
      "Training iteration: 1022\n",
      "Validation loss (no improvement): 0.048022976517677306\n",
      "Training iteration: 1023\n",
      "Validation loss (no improvement): 0.04749590754508972\n",
      "Training iteration: 1024\n",
      "Validation loss (no improvement): 0.04718775749206543\n",
      "Training iteration: 1025\n",
      "Validation loss (no improvement): 0.0474953830242157\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.047174233198165896  to: 0.047097569704055785\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.047097569704055785  to: 0.0468752384185791\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.0468752384185791  to: 0.046807599067687986\n",
      "Training iteration: 1029\n",
      "Validation loss (no improvement): 0.04761412739753723\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.046807599067687986  to: 0.04621255397796631\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.04621255397796631  to: 0.04618050158023834\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.04618050158023834  to: 0.04611846506595611\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.04611846506595611  to: 0.04604884088039398\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.04604884088039398  to: 0.04531626701354981\n",
      "Training iteration: 1035\n",
      "Validation loss (no improvement): 0.04546758532524109\n",
      "Training iteration: 1036\n",
      "Validation loss (no improvement): 0.045411992073059085\n",
      "Training iteration: 1037\n",
      "Validation loss (no improvement): 0.047183218598365786\n",
      "Training iteration: 1038\n",
      "Validation loss (no improvement): 0.04654764533042908\n",
      "Training iteration: 1039\n",
      "Validation loss (no improvement): 0.04622830748558045\n",
      "Training iteration: 1040\n",
      "Validation loss (no improvement): 0.04602423310279846\n",
      "Training iteration: 1041\n",
      "Validation loss (no improvement): 0.04599024653434754\n",
      "Training iteration: 1042\n",
      "Validation loss (no improvement): 0.04544840753078461\n",
      "Training iteration: 1043\n",
      "Validation loss (no improvement): 0.04554819166660309\n",
      "Training iteration: 1044\n",
      "Validation loss (no improvement): 0.04566592276096344\n",
      "Training iteration: 1045\n",
      "Validation loss (no improvement): 0.0460894763469696\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.04531626701354981  to: 0.04485047459602356\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.04485047459602356  to: 0.04461353421211243\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.04461353421211243  to: 0.04428030550479889\n",
      "Training iteration: 1049\n",
      "Validation loss (no improvement): 0.044939985871315\n",
      "Training iteration: 1050\n",
      "Validation loss (no improvement): 0.044885057210922244\n",
      "Training iteration: 1051\n",
      "Validation loss (no improvement): 0.04514245092868805\n",
      "Training iteration: 1052\n",
      "Validation loss (no improvement): 0.044463396072387695\n",
      "Training iteration: 1053\n",
      "Validation loss (no improvement): 0.044295471906661985\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.04428030550479889  to: 0.04403166770935059\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.04403166770935059  to: 0.043955755233764646\n",
      "Training iteration: 1056\n",
      "Validation loss (no improvement): 0.044431114196777345\n",
      "Training iteration: 1057\n",
      "Validation loss (no improvement): 0.0451407253742218\n",
      "Training iteration: 1058\n",
      "Validation loss (no improvement): 0.04411117434501648\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.043955755233764646  to: 0.04348679482936859\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.04348679482936859  to: 0.04304286539554596\n",
      "Training iteration: 1061\n",
      "Validation loss (no improvement): 0.04358658790588379\n",
      "Training iteration: 1062\n",
      "Validation loss (no improvement): 0.043558305501937865\n",
      "Training iteration: 1063\n",
      "Validation loss (no improvement): 0.0445628821849823\n",
      "Training iteration: 1064\n",
      "Validation loss (no improvement): 0.04337228834629059\n",
      "Training iteration: 1065\n",
      "Validation loss (no improvement): 0.04410257339477539\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.04304286539554596  to: 0.04284703731536865\n",
      "Training iteration: 1067\n",
      "Validation loss (no improvement): 0.04449722170829773\n",
      "Training iteration: 1068\n",
      "Validation loss (no improvement): 0.046748533844947815\n",
      "Training iteration: 1069\n",
      "Validation loss (no improvement): 0.044303736090660094\n",
      "Training iteration: 1070\n",
      "Validation loss (no improvement): 0.04317689836025238\n",
      "Training iteration: 1071\n",
      "Validation loss (no improvement): 0.0450552225112915\n",
      "Training iteration: 1072\n",
      "Validation loss (no improvement): 0.04433131217956543\n",
      "Training iteration: 1073\n",
      "Validation loss (no improvement): 0.04418651461601257\n",
      "Training iteration: 1074\n",
      "Validation loss (no improvement): 0.04491452276706696\n",
      "Training iteration: 1075\n",
      "Validation loss (no improvement): 0.043529456853866576\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.04284703731536865  to: 0.04275773167610168\n",
      "Training iteration: 1077\n",
      "Validation loss (no improvement): 0.04359458386898041\n",
      "Training iteration: 1078\n",
      "Validation loss (no improvement): 0.04341679513454437\n",
      "Training iteration: 1079\n",
      "Validation loss (no improvement): 0.04476296305656433\n",
      "Training iteration: 1080\n",
      "Validation loss (no improvement): 0.04374770224094391\n",
      "Training iteration: 1081\n",
      "Validation loss (no improvement): 0.043487101793289185\n",
      "Training iteration: 1082\n",
      "Validation loss (no improvement): 0.043232622742652896\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.04275773167610168  to: 0.04196464419364929\n",
      "Training iteration: 1084\n",
      "Validation loss (no improvement): 0.04302499890327453\n",
      "Training iteration: 1085\n",
      "Validation loss (no improvement): 0.042383259534835814\n",
      "Training iteration: 1086\n",
      "Validation loss (no improvement): 0.04226425290107727\n",
      "Training iteration: 1087\n",
      "Validation loss (no improvement): 0.04239339232444763\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.04196464419364929  to: 0.04135984778404236\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.04135984778404236  to: 0.04128490388393402\n",
      "Training iteration: 1090\n",
      "Validation loss (no improvement): 0.04128961563110352\n",
      "Training iteration: 1091\n",
      "Validation loss (no improvement): 0.04164358973503113\n",
      "Training iteration: 1092\n",
      "Validation loss (no improvement): 0.04160444140434265\n",
      "Training iteration: 1093\n",
      "Validation loss (no improvement): 0.04156169891357422\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.04128490388393402  to: 0.041018503904342654\n",
      "Training iteration: 1095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.041018503904342654  to: 0.04079789221286774\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.04079789221286774  to: 0.04038743376731872\n",
      "Training iteration: 1097\n",
      "Validation loss (no improvement): 0.04061885476112366\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.04038743376731872  to: 0.04034198820590973\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.04034198820590973  to: 0.039994412660598756\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.039994412660598756  to: 0.03971090316772461\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.03971090316772461  to: 0.039483243227005006\n",
      "Training iteration: 1102\n",
      "Validation loss (no improvement): 0.03950253129005432\n",
      "Training iteration: 1103\n",
      "Validation loss (no improvement): 0.0398043155670166\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.039483243227005006  to: 0.03947476744651794\n",
      "Training iteration: 1105\n",
      "Validation loss (no improvement): 0.03975293040275574\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.03947476744651794  to: 0.03940611481666565\n",
      "Training iteration: 1107\n",
      "Validation loss (no improvement): 0.039878517389297485\n",
      "Training iteration: 1108\n",
      "Validation loss (no improvement): 0.039710605144500734\n",
      "Training iteration: 1109\n",
      "Validation loss (no improvement): 0.03985758125782013\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.03940611481666565  to: 0.03926556706428528\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.03926556706428528  to: 0.038918066024780276\n",
      "Training iteration: 1112\n",
      "Validation loss (no improvement): 0.03950834572315216\n",
      "Training iteration: 1113\n",
      "Validation loss (no improvement): 0.039191237092018126\n",
      "Training iteration: 1114\n",
      "Validation loss (no improvement): 0.03933600783348083\n",
      "Training iteration: 1115\n",
      "Validation loss (no improvement): 0.039746707677841185\n",
      "Training iteration: 1116\n",
      "Validation loss (no improvement): 0.03902998268604278\n",
      "Training iteration: 1117\n",
      "Validation loss (no improvement): 0.03894355297088623\n",
      "Training iteration: 1118\n",
      "Validation loss (no improvement): 0.03939867615699768\n",
      "Training iteration: 1119\n",
      "Validation loss (no improvement): 0.03923462927341461\n",
      "Training iteration: 1120\n",
      "Validation loss (no improvement): 0.03963078856468201\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.038918066024780276  to: 0.038207250833511355\n",
      "Training iteration: 1122\n",
      "Validation loss (no improvement): 0.03848375976085663\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.038207250833511355  to: 0.03804024755954742\n",
      "Training iteration: 1124\n",
      "Validation loss (no improvement): 0.03873948156833649\n",
      "Training iteration: 1125\n",
      "Validation loss (no improvement): 0.03921673595905304\n",
      "Training iteration: 1126\n",
      "Validation loss (no improvement): 0.039370307326316835\n",
      "Training iteration: 1127\n",
      "Validation loss (no improvement): 0.03883370459079742\n",
      "Training iteration: 1128\n",
      "Validation loss (no improvement): 0.039009422063827515\n",
      "Training iteration: 1129\n",
      "Validation loss (no improvement): 0.038540542125701904\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.03804024755954742  to: 0.03791974484920502\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.03791974484920502  to: 0.037569642066955566\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.037569642066955566  to: 0.037229222059249875\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.037229222059249875  to: 0.03706558346748352\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.03706558346748352  to: 0.036989182233810425\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.036989182233810425  to: 0.03687101304531097\n",
      "Training iteration: 1136\n",
      "Validation loss (no improvement): 0.03704922199249268\n",
      "Training iteration: 1137\n",
      "Validation loss (no improvement): 0.037424865365028384\n",
      "Training iteration: 1138\n",
      "Validation loss (no improvement): 0.03742868006229401\n",
      "Training iteration: 1139\n",
      "Validation loss (no improvement): 0.03733325302600861\n",
      "Training iteration: 1140\n",
      "Validation loss (no improvement): 0.03701877892017365\n",
      "Training iteration: 1141\n",
      "Validation loss (no improvement): 0.03714507520198822\n",
      "Training iteration: 1142\n",
      "Validation loss (no improvement): 0.03716385960578918\n",
      "Training iteration: 1143\n",
      "Validation loss (no improvement): 0.0369077205657959\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.03687101304531097  to: 0.03641800880432129\n",
      "Training iteration: 1145\n",
      "Validation loss (no improvement): 0.036432045698165896\n",
      "Training iteration: 1146\n",
      "Validation loss (no improvement): 0.03658529222011566\n",
      "Training iteration: 1147\n",
      "Validation loss (no improvement): 0.03684261739253998\n",
      "Training iteration: 1148\n",
      "Validation loss (no improvement): 0.037264344096183774\n",
      "Training iteration: 1149\n",
      "Validation loss (no improvement): 0.037342414259910583\n",
      "Training iteration: 1150\n",
      "Validation loss (no improvement): 0.037290376424789426\n",
      "Training iteration: 1151\n",
      "Validation loss (no improvement): 0.03704238533973694\n",
      "Training iteration: 1152\n",
      "Validation loss (no improvement): 0.0368845522403717\n",
      "Training iteration: 1153\n",
      "Validation loss (no improvement): 0.03670052886009216\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.03641800880432129  to: 0.03605720400810242\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.03605720400810242  to: 0.035426738858222964\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.035426738858222964  to: 0.0352262943983078\n",
      "Training iteration: 1157\n",
      "Validation loss (no improvement): 0.03536860048770905\n",
      "Training iteration: 1158\n",
      "Validation loss (no improvement): 0.03565952181816101\n",
      "Training iteration: 1159\n",
      "Validation loss (no improvement): 0.03557761907577515\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.0352262943983078  to: 0.03510749936103821\n",
      "Training iteration: 1161\n",
      "Validation loss (no improvement): 0.0353503942489624\n",
      "Training iteration: 1162\n",
      "Validation loss (no improvement): 0.03525932729244232\n",
      "Training iteration: 1163\n",
      "Validation loss (no improvement): 0.03581782877445221\n",
      "Training iteration: 1164\n",
      "Validation loss (no improvement): 0.03607625365257263\n",
      "Training iteration: 1165\n",
      "Validation loss (no improvement): 0.03536130785942078\n",
      "Training iteration: 1166\n",
      "Validation loss (no improvement): 0.036735862493515015\n",
      "Training iteration: 1167\n",
      "Validation loss (no improvement): 0.036280426383018496\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.03510749936103821  to: 0.034644901752471924\n",
      "Training iteration: 1169\n",
      "Validation loss (no improvement): 0.03496496379375458\n",
      "Training iteration: 1170\n",
      "Validation loss (no improvement): 0.03558997511863708\n",
      "Training iteration: 1171\n",
      "Validation loss (no improvement): 0.03595724999904633\n",
      "Training iteration: 1172\n",
      "Validation loss (no improvement): 0.03482239842414856\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.034644901752471924  to: 0.034373301267623904\n",
      "Training iteration: 1174\n",
      "Validation loss (no improvement): 0.03537812530994415\n",
      "Training iteration: 1175\n",
      "Validation loss (no improvement): 0.035713082551956175\n",
      "Training iteration: 1176\n",
      "Validation loss (no improvement): 0.034430623054504395\n",
      "Training iteration: 1177\n",
      "Validation loss (no improvement): 0.03510159850120544\n",
      "Training iteration: 1178\n",
      "Validation loss (no improvement): 0.03460049033164978\n",
      "Training iteration: 1179\n",
      "Validation loss (no improvement): 0.03526762425899506\n",
      "Training iteration: 1180\n",
      "Validation loss (no improvement): 0.03459857404232025\n",
      "Training iteration: 1181\n",
      "Validation loss (no improvement): 0.034674397110939024\n",
      "Training iteration: 1182\n",
      "Validation loss (no improvement): 0.03452444970607758\n",
      "Training iteration: 1183\n",
      "Validation loss (no improvement): 0.03516542017459869\n",
      "Training iteration: 1184\n",
      "Validation loss (no improvement): 0.034599018096923825\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.034373301267623904  to: 0.033676549792289734\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.033676549792289734  to: 0.03358938097953797\n",
      "Training iteration: 1187\n",
      "Validation loss (no improvement): 0.034762173891067505\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.03358938097953797  to: 0.03357412219047547\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.03357412219047547  to: 0.032902377843856814\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.032902377843856814  to: 0.032778510451316835\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.035051628947257996\n",
      "Training iteration: 1192\n",
      "Validation loss (no improvement): 0.03394744396209717\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.032778510451316835  to: 0.03252766728401184\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.03252766728401184  to: 0.03236401677131653\n",
      "Training iteration: 1195\n",
      "Validation loss (no improvement): 0.03270571827888489\n",
      "Training iteration: 1196\n",
      "Validation loss (no improvement): 0.03353843092918396\n",
      "Training iteration: 1197\n",
      "Validation loss (no improvement): 0.03316208124160767\n",
      "Training iteration: 1198\n",
      "Validation loss (no improvement): 0.03318851590156555\n",
      "Training iteration: 1199\n",
      "Validation loss (no improvement): 0.03332563042640686\n",
      "Training iteration: 1200\n",
      "Validation loss (no improvement): 0.03305419683456421\n",
      "Training iteration: 1201\n",
      "Validation loss (no improvement): 0.03259666860103607\n",
      "Training iteration: 1202\n",
      "Validation loss (no improvement): 0.032638758420944214\n",
      "Training iteration: 1203\n",
      "Validation loss (no improvement): 0.03299036622047424\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.03236401677131653  to: 0.03188858926296234\n",
      "Training iteration: 1205\n",
      "Validation loss (no improvement): 0.03206654787063599\n",
      "Training iteration: 1206\n",
      "Validation loss (no improvement): 0.03229089081287384\n",
      "Training iteration: 1207\n",
      "Validation loss (no improvement): 0.03507792949676514\n",
      "Training iteration: 1208\n",
      "Validation loss (no improvement): 0.03305214941501618\n",
      "Training iteration: 1209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.03188858926296234  to: 0.031771042943000795\n",
      "Training iteration: 1210\n",
      "Validation loss (no improvement): 0.031895557045936586\n",
      "Training iteration: 1211\n",
      "Validation loss (no improvement): 0.03429328799247742\n",
      "Training iteration: 1212\n",
      "Validation loss (no improvement): 0.03591578900814056\n",
      "Training iteration: 1213\n",
      "Validation loss (no improvement): 0.03242310881614685\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.031771042943000795  to: 0.03164714872837067\n",
      "Training iteration: 1215\n",
      "Validation loss (no improvement): 0.0316556304693222\n",
      "Training iteration: 1216\n",
      "Validation loss (no improvement): 0.03397011756896973\n",
      "Training iteration: 1217\n",
      "Validation loss (no improvement): 0.033649563789367676\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.03164714872837067  to: 0.03121526837348938\n",
      "Training iteration: 1219\n",
      "Validation loss (no improvement): 0.031231310963630677\n",
      "Training iteration: 1220\n",
      "Validation loss (no improvement): 0.031711050868034364\n",
      "Training iteration: 1221\n",
      "Validation loss (no improvement): 0.035068005323410034\n",
      "Training iteration: 1222\n",
      "Validation loss (no improvement): 0.032689014077186586\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.03121526837348938  to: 0.03070736527442932\n",
      "Training iteration: 1224\n",
      "Validation loss (no improvement): 0.030786892771720885\n",
      "Training iteration: 1225\n",
      "Validation loss (no improvement): 0.031019046902656555\n",
      "Training iteration: 1226\n",
      "Validation loss (no improvement): 0.03131973147392273\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.03070736527442932  to: 0.030394333600997924\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.030394333600997924  to: 0.030083176493644715\n",
      "Training iteration: 1229\n",
      "Validation loss (no improvement): 0.03043356239795685\n",
      "Training iteration: 1230\n",
      "Validation loss (no improvement): 0.030639171600341797\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.030083176493644715  to: 0.029968446493148802\n",
      "Training iteration: 1232\n",
      "Validation loss (no improvement): 0.030257153511047363\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.029968446493148802  to: 0.02980429232120514\n",
      "Training iteration: 1234\n",
      "Validation loss (no improvement): 0.029901105165481567\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.02980429232120514  to: 0.029659253358840943\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.029659253358840943  to: 0.029658132791519166\n",
      "Training iteration: 1237\n",
      "Validation loss (no improvement): 0.029691094160079957\n",
      "Training iteration: 1238\n",
      "Validation loss (no improvement): 0.02971896529197693\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.029658132791519166  to: 0.028777319192886352\n",
      "Training iteration: 1240\n",
      "Validation loss (no improvement): 0.02880939245223999\n",
      "Training iteration: 1241\n",
      "Validation loss (no improvement): 0.029763221740722656\n",
      "Training iteration: 1242\n",
      "Validation loss (no improvement): 0.02956826388835907\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.028777319192886352  to: 0.028521963953971864\n",
      "Training iteration: 1244\n",
      "Validation loss (no improvement): 0.028639012575149538\n",
      "Training iteration: 1245\n",
      "Validation loss (no improvement): 0.029716607928276063\n",
      "Training iteration: 1246\n",
      "Validation loss (no improvement): 0.029845672845840453\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.028521963953971864  to: 0.0283796489238739\n",
      "Training iteration: 1248\n",
      "Validation loss (no improvement): 0.028585445880889893\n",
      "Training iteration: 1249\n",
      "Validation loss (no improvement): 0.028742319345474242\n",
      "Training iteration: 1250\n",
      "Validation loss (no improvement): 0.029238519072532655\n",
      "Training iteration: 1251\n",
      "Validation loss (no improvement): 0.02957472801208496\n",
      "Training iteration: 1252\n",
      "Validation loss (no improvement): 0.028651472926139832\n",
      "Training iteration: 1253\n",
      "Validation loss (no improvement): 0.028387016057968138\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.0283796489238739  to: 0.02814021110534668\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.028568333387374877\n",
      "Training iteration: 1256\n",
      "Validation loss (no improvement): 0.029907912015914917\n",
      "Training iteration: 1257\n",
      "Validation loss (no improvement): 0.028400605916976927\n",
      "Training iteration: 1258\n",
      "Validation loss (no improvement): 0.02824116051197052\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.02814021110534668  to: 0.028075742721557616\n",
      "Training iteration: 1260\n",
      "Validation loss (no improvement): 0.028321194648742675\n",
      "Training iteration: 1261\n",
      "Validation loss (no improvement): 0.02831380069255829\n",
      "Training iteration: 1262\n",
      "Validation loss (no improvement): 0.028652995824813843\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.028075742721557616  to: 0.02771422266960144\n",
      "Training iteration: 1264\n",
      "Validation loss (no improvement): 0.028093156218528748\n",
      "Training iteration: 1265\n",
      "Validation loss (no improvement): 0.02799135148525238\n",
      "Training iteration: 1266\n",
      "Validation loss (no improvement): 0.028638392686843872\n",
      "Training iteration: 1267\n",
      "Validation loss (no improvement): 0.02882830798625946\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.02771422266960144  to: 0.02753616273403168\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.02753616273403168  to: 0.02746351957321167\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.02746351957321167  to: 0.027149945497512817\n",
      "Training iteration: 1271\n",
      "Validation loss (no improvement): 0.027445322275161742\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.027947229146957398\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.027149945497512817  to: 0.026895299553871155\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.026895299553871155  to: 0.02659517526626587\n",
      "Training iteration: 1275\n",
      "Validation loss (no improvement): 0.028904145956039427\n",
      "Training iteration: 1276\n",
      "Validation loss (no improvement): 0.03038177490234375\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.027910524606704713\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.027539953589439392\n",
      "Training iteration: 1279\n",
      "Validation loss (no improvement): 0.027417007088661193\n",
      "Training iteration: 1280\n",
      "Validation loss (no improvement): 0.029298555850982667\n",
      "Training iteration: 1281\n",
      "Validation loss (no improvement): 0.03051139712333679\n",
      "Training iteration: 1282\n",
      "Validation loss (no improvement): 0.028881126642227174\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.027923765778541564\n",
      "Training iteration: 1284\n",
      "Validation loss (no improvement): 0.02815670967102051\n",
      "Training iteration: 1285\n",
      "Validation loss (no improvement): 0.02860133647918701\n",
      "Training iteration: 1286\n",
      "Validation loss (no improvement): 0.028994110226631165\n",
      "Training iteration: 1287\n",
      "Validation loss (no improvement): 0.028763633966445924\n",
      "Training iteration: 1288\n",
      "Validation loss (no improvement): 0.027368766069412232\n",
      "Training iteration: 1289\n",
      "Validation loss (no improvement): 0.027494782209396364\n",
      "Training iteration: 1290\n",
      "Validation loss (no improvement): 0.027813655138015748\n",
      "Training iteration: 1291\n",
      "Validation loss (no improvement): 0.027069586515426635\n",
      "Training iteration: 1292\n",
      "Validation loss (no improvement): 0.026617321372032165\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.02659517526626587  to: 0.025665247440338136\n",
      "Training iteration: 1294\n",
      "Validation loss (no improvement): 0.026088348031044005\n",
      "Training iteration: 1295\n",
      "Validation loss (no improvement): 0.025848957896232604\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.025665247440338136  to: 0.02526065707206726\n",
      "Training iteration: 1297\n",
      "Validation loss (no improvement): 0.026025718450546263\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.025576686859130858\n",
      "Training iteration: 1299\n",
      "Validation loss (no improvement): 0.025508376955986022\n",
      "Training iteration: 1300\n",
      "Validation loss (no improvement): 0.025309643149375914\n",
      "Training iteration: 1301\n",
      "Validation loss (no improvement): 0.025971156358718873\n",
      "Training iteration: 1302\n",
      "Validation loss (no improvement): 0.025879797339439393\n",
      "Training iteration: 1303\n",
      "Validation loss (no improvement): 0.025756222009658814\n",
      "Training iteration: 1304\n",
      "Validation loss (no improvement): 0.025804463028907775\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.026883167028427125\n",
      "Training iteration: 1306\n",
      "Validation loss (no improvement): 0.02651592195034027\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.026047736406326294\n",
      "Training iteration: 1308\n",
      "Validation loss (no improvement): 0.025881442427635192\n",
      "Training iteration: 1309\n",
      "Validation loss (no improvement): 0.02635725438594818\n",
      "Training iteration: 1310\n",
      "Validation loss (no improvement): 0.025703424215316774\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.02526065707206726  to: 0.02516125738620758\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.02516125738620758  to: 0.02503177523612976\n",
      "Training iteration: 1313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.027125126123428343\n",
      "Training iteration: 1314\n",
      "Validation loss (no improvement): 0.026446792483329772\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.02503177523612976  to: 0.02428457736968994\n",
      "Training iteration: 1316\n",
      "Validation loss (no improvement): 0.0243289977312088\n",
      "Training iteration: 1317\n",
      "Validation loss (no improvement): 0.02543008029460907\n",
      "Training iteration: 1318\n",
      "Validation loss (no improvement): 0.025719699263572694\n",
      "Training iteration: 1319\n",
      "Validation loss (no improvement): 0.024883100390434267\n",
      "Training iteration: 1320\n",
      "Validation loss (no improvement): 0.025100502371788024\n",
      "Training iteration: 1321\n",
      "Validation loss (no improvement): 0.025587695837020873\n",
      "Training iteration: 1322\n",
      "Validation loss (no improvement): 0.026397356390953065\n",
      "Training iteration: 1323\n",
      "Validation loss (no improvement): 0.025464874505996705\n",
      "Training iteration: 1324\n",
      "Validation loss (no improvement): 0.0249116450548172\n",
      "Training iteration: 1325\n",
      "Validation loss (no improvement): 0.024704578518867492\n",
      "Training iteration: 1326\n",
      "Validation loss (no improvement): 0.025609695911407472\n",
      "Training iteration: 1327\n",
      "Validation loss (no improvement): 0.025667572021484376\n",
      "Training iteration: 1328\n",
      "Validation loss (no improvement): 0.024359790980815886\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.02428457736968994  to: 0.024105139076709747\n",
      "Training iteration: 1330\n",
      "Validation loss (no improvement): 0.024315926432609557\n",
      "Training iteration: 1331\n",
      "Validation loss (no improvement): 0.02483862340450287\n",
      "Training iteration: 1332\n",
      "Validation loss (no improvement): 0.025479817390441896\n",
      "Training iteration: 1333\n",
      "Validation loss (no improvement): 0.024299032986164093\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.024105139076709747  to: 0.023944911360740662\n",
      "Training iteration: 1335\n",
      "Validation loss (no improvement): 0.024356675148010255\n",
      "Training iteration: 1336\n",
      "Validation loss (no improvement): 0.025034198164939882\n",
      "Training iteration: 1337\n",
      "Validation loss (no improvement): 0.024179621040821074\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.023944911360740662  to: 0.02374732792377472\n",
      "Training iteration: 1339\n",
      "Validation loss (no improvement): 0.023793914914131166\n",
      "Training iteration: 1340\n",
      "Validation loss (no improvement): 0.024836353957653046\n",
      "Training iteration: 1341\n",
      "Validation loss (no improvement): 0.023947057127952576\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.02374732792377472  to: 0.023503987491130827\n",
      "Training iteration: 1343\n",
      "Validation loss (no improvement): 0.02377763092517853\n",
      "Training iteration: 1344\n",
      "Validation loss (no improvement): 0.025974366068840026\n",
      "Training iteration: 1345\n",
      "Validation loss (no improvement): 0.02575627863407135\n",
      "Training iteration: 1346\n",
      "Validation loss (no improvement): 0.024072396755218505\n",
      "Training iteration: 1347\n",
      "Validation loss (no improvement): 0.023771873116493224\n",
      "Training iteration: 1348\n",
      "Validation loss (no improvement): 0.024139551818370818\n",
      "Training iteration: 1349\n",
      "Validation loss (no improvement): 0.02479225695133209\n",
      "Training iteration: 1350\n",
      "Validation loss (no improvement): 0.024456290900707243\n",
      "Training iteration: 1351\n",
      "Validation loss (no improvement): 0.023818632960319518\n",
      "Training iteration: 1352\n",
      "Validation loss (no improvement): 0.023558168113231658\n",
      "Training iteration: 1353\n",
      "Validation loss (no improvement): 0.023578181862831116\n",
      "Training iteration: 1354\n",
      "Validation loss (no improvement): 0.02404216229915619\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.024357494711875916\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.023503987491130827  to: 0.02327902764081955\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.02327902764081955  to: 0.02300136536359787\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.024493391811847686\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.024725374579429627\n",
      "Training iteration: 1360\n",
      "Validation loss (no improvement): 0.023355360329151153\n",
      "Training iteration: 1361\n",
      "Validation loss (no improvement): 0.023360271751880646\n",
      "Training iteration: 1362\n",
      "Validation loss (no improvement): 0.023613591492176057\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.02484739273786545\n",
      "Training iteration: 1364\n",
      "Validation loss (no improvement): 0.02445054054260254\n",
      "Training iteration: 1365\n",
      "Validation loss (no improvement): 0.023450767993927\n",
      "Training iteration: 1366\n",
      "Validation loss (no improvement): 0.02398025095462799\n",
      "Training iteration: 1367\n",
      "Validation loss (no improvement): 0.024688005447387695\n",
      "Training iteration: 1368\n",
      "Validation loss (no improvement): 0.02386244535446167\n",
      "Training iteration: 1369\n",
      "Validation loss (no improvement): 0.02389783561229706\n",
      "Training iteration: 1370\n",
      "Validation loss (no improvement): 0.02331560105085373\n",
      "Training iteration: 1371\n",
      "Validation loss (no improvement): 0.023094408214092255\n",
      "Training iteration: 1372\n",
      "Validation loss (no improvement): 0.02392520159482956\n",
      "Training iteration: 1373\n",
      "Validation loss (no improvement): 0.02340191900730133\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.02300136536359787  to: 0.02273901253938675\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.02273901253938675  to: 0.022453658282756805\n",
      "Training iteration: 1376\n",
      "Validation loss (no improvement): 0.023448681831359862\n",
      "Training iteration: 1377\n",
      "Validation loss (no improvement): 0.023125648498535156\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.022515754401683807\n",
      "Training iteration: 1379\n",
      "Validation loss (no improvement): 0.022713521122932435\n",
      "Training iteration: 1380\n",
      "Validation loss (no improvement): 0.024394822120666505\n",
      "Training iteration: 1381\n",
      "Validation loss (no improvement): 0.02438032329082489\n",
      "Training iteration: 1382\n",
      "Validation loss (no improvement): 0.023101408779621125\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.023276782035827635\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.024358496069908142\n",
      "Training iteration: 1385\n",
      "Validation loss (no improvement): 0.025027868151664735\n",
      "Training iteration: 1386\n",
      "Validation loss (no improvement): 0.024025845527648925\n",
      "Training iteration: 1387\n",
      "Validation loss (no improvement): 0.023564982414245605\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.02360229045152664\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.024189552664756774\n",
      "Training iteration: 1390\n",
      "Validation loss (no improvement): 0.023246388137340545\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.022453658282756805  to: 0.022394509613513948\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.022394509613513948  to: 0.02235250025987625\n",
      "Training iteration: 1393\n",
      "Validation loss (no improvement): 0.02476329505443573\n",
      "Training iteration: 1394\n",
      "Validation loss (no improvement): 0.023387689888477326\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.02235250025987625  to: 0.021735814213752747\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.022069530189037324\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.023612967133522032\n",
      "Training iteration: 1398\n",
      "Validation loss (no improvement): 0.023136088252067567\n",
      "Training iteration: 1399\n",
      "Validation loss (no improvement): 0.02212907373905182\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.022645828127861024\n",
      "Training iteration: 1401\n",
      "Validation loss (no improvement): 0.022434695065021514\n",
      "Training iteration: 1402\n",
      "Validation loss (no improvement): 0.02191578894853592\n",
      "Training iteration: 1403\n",
      "Validation loss (no improvement): 0.02270786315202713\n",
      "Training iteration: 1404\n",
      "Validation loss (no improvement): 0.022875142097473145\n",
      "Training iteration: 1405\n",
      "Validation loss (no improvement): 0.02178429365158081\n",
      "Training iteration: 1406\n",
      "Validation loss (no improvement): 0.022107033431529997\n",
      "Training iteration: 1407\n",
      "Validation loss (no improvement): 0.0233767032623291\n",
      "Training iteration: 1408\n",
      "Validation loss (no improvement): 0.022912082076072694\n",
      "Training iteration: 1409\n",
      "Validation loss (no improvement): 0.02203315496444702\n",
      "Training iteration: 1410\n",
      "Validation loss (no improvement): 0.021810713410377502\n",
      "Training iteration: 1411\n",
      "Validation loss (no improvement): 0.02293315380811691\n",
      "Training iteration: 1412\n",
      "Validation loss (no improvement): 0.0225395604968071\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.021735814213752747  to: 0.021361668407917023\n",
      "Training iteration: 1414\n",
      "Validation loss (no improvement): 0.021701081097126006\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.023129816353321075\n",
      "Training iteration: 1416\n",
      "Validation loss (no improvement): 0.022933900356292725\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.021629853546619414\n",
      "Training iteration: 1418\n",
      "Validation loss (no improvement): 0.021569864451885225\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.022243992984294893\n",
      "Training iteration: 1420\n",
      "Validation loss (no improvement): 0.023113076388835908\n",
      "Training iteration: 1421\n",
      "Validation loss (no improvement): 0.02229725867509842\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.021361668407917023  to: 0.021072497963905333\n",
      "Training iteration: 1423\n",
      "Validation loss (no improvement): 0.021072883903980256\n",
      "Training iteration: 1424\n",
      "Validation loss (no improvement): 0.02262784242630005\n",
      "Training iteration: 1425\n",
      "Validation loss (no improvement): 0.02329910695552826\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.02216305285692215\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.021668581664562224\n",
      "Training iteration: 1428\n",
      "Validation loss (no improvement): 0.022127875685691835\n",
      "Training iteration: 1429\n",
      "Validation loss (no improvement): 0.023148408532142638\n",
      "Training iteration: 1430\n",
      "Validation loss (no improvement): 0.022373771667480467\n",
      "Training iteration: 1431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.021120145916938782\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.021254618465900422\n",
      "Training iteration: 1433\n",
      "Validation loss (no improvement): 0.02211231291294098\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.02288152724504471\n",
      "Training iteration: 1435\n",
      "Validation loss (no improvement): 0.022605612874031067\n",
      "Training iteration: 1436\n",
      "Validation loss (no improvement): 0.021582134068012238\n",
      "Training iteration: 1437\n",
      "Validation loss (no improvement): 0.02121644914150238\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.022000248730182647\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.0214246466755867\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.021072497963905333  to: 0.020766225457191468\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.020766225457191468  to: 0.020689532160758972\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.020826777815818785\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.020689532160758972  to: 0.020139093697071075\n",
      "Training iteration: 1444\n",
      "Validation loss (no improvement): 0.02020442485809326\n",
      "Training iteration: 1445\n",
      "Validation loss (no improvement): 0.02207990437746048\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.02181846648454666\n",
      "Training iteration: 1447\n",
      "Validation loss (no improvement): 0.02043180763721466\n",
      "Training iteration: 1448\n",
      "Validation loss (no improvement): 0.020343573391437532\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.0213205486536026\n",
      "Training iteration: 1450\n",
      "Validation loss (no improvement): 0.02136630713939667\n",
      "Training iteration: 1451\n",
      "Validation loss (no improvement): 0.020919492840766905\n",
      "Training iteration: 1452\n",
      "Validation loss (no improvement): 0.02118450850248337\n",
      "Training iteration: 1453\n",
      "Validation loss (no improvement): 0.02238907814025879\n",
      "Training iteration: 1454\n",
      "Validation loss (no improvement): 0.02145478278398514\n",
      "Training iteration: 1455\n",
      "Validation loss (no improvement): 0.020632147789001465\n",
      "Training iteration: 1456\n",
      "Validation loss (no improvement): 0.021201011538505555\n",
      "Training iteration: 1457\n",
      "Validation loss (no improvement): 0.023573832213878633\n",
      "Training iteration: 1458\n",
      "Validation loss (no improvement): 0.021797235310077667\n",
      "Training iteration: 1459\n",
      "Validation loss (no improvement): 0.02064882069826126\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.020508432388305665\n",
      "Training iteration: 1461\n",
      "Validation loss (no improvement): 0.020818448066711424\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.021105635166168212\n",
      "Training iteration: 1463\n",
      "Validation loss (no improvement): 0.02023290693759918\n",
      "Training iteration: 1464\n",
      "Validation loss (no improvement): 0.020690405368804933\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.020139093697071075  to: 0.01985038220882416\n",
      "Training iteration: 1466\n",
      "Validation loss (no improvement): 0.02013847082853317\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.020840673148632048\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.02036055028438568\n",
      "Training iteration: 1469\n",
      "Validation loss (no improvement): 0.021204634010791777\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.020842108130455016\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.01985038220882416  to: 0.019777067005634308\n",
      "Training iteration: 1472\n",
      "Validation loss (no improvement): 0.020339587330818178\n",
      "Training iteration: 1473\n",
      "Validation loss (no improvement): 0.020740675926208495\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.019777067005634308  to: 0.019353291392326354\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.020081710815429688\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.020070430636405946\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.01968808174133301\n",
      "Training iteration: 1478\n",
      "Validation loss (no improvement): 0.021612636744976044\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.020837089419364928\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.01952957808971405\n",
      "Training iteration: 1481\n",
      "Validation loss (no improvement): 0.020035536587238313\n",
      "Training iteration: 1482\n",
      "Validation loss (no improvement): 0.02226567566394806\n",
      "Training iteration: 1483\n",
      "Validation loss (no improvement): 0.022231678664684295\n",
      "Training iteration: 1484\n",
      "Validation loss (no improvement): 0.01998899281024933\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.01961267590522766\n",
      "Training iteration: 1486\n",
      "Validation loss (no improvement): 0.021232323348522188\n",
      "Training iteration: 1487\n",
      "Validation loss (no improvement): 0.021310415863990784\n",
      "Training iteration: 1488\n",
      "Validation loss (no improvement): 0.02013881653547287\n",
      "Training iteration: 1489\n",
      "Validation loss (no improvement): 0.01969885528087616\n",
      "Training iteration: 1490\n",
      "Validation loss (no improvement): 0.021362431347370148\n",
      "Training iteration: 1491\n",
      "Validation loss (no improvement): 0.02071867436170578\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.019353291392326354  to: 0.019168661534786226\n",
      "Training iteration: 1493\n",
      "Validation loss (no improvement): 0.019619444012641908\n",
      "Training iteration: 1494\n",
      "Validation loss (no improvement): 0.02095986157655716\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.019695205986499785\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.019168661534786226  to: 0.019069918990135194\n",
      "Training iteration: 1497\n",
      "Validation loss (no improvement): 0.02080193758010864\n",
      "Training iteration: 1498\n",
      "Validation loss (no improvement): 0.023680675029754638\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.02063029110431671\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.019080349802970888\n",
      "Training iteration: 1501\n",
      "Validation loss (no improvement): 0.019458411633968352\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.02136545181274414\n",
      "Training iteration: 1503\n",
      "Validation loss (no improvement): 0.020539899170398713\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.019069918990135194  to: 0.019034484028816225\n",
      "Training iteration: 1505\n",
      "Validation loss (no improvement): 0.019622579216957092\n",
      "Training iteration: 1506\n",
      "Validation loss (no improvement): 0.02204456031322479\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.022219237685203553\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.019745007157325745\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.019219931960105897\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.021871694922447206\n",
      "Training iteration: 1511\n",
      "Validation loss (no improvement): 0.022758486866950988\n",
      "Training iteration: 1512\n",
      "Validation loss (no improvement): 0.01947423964738846\n",
      "Training iteration: 1513\n",
      "Improved validation loss from: 0.019034484028816225  to: 0.018864354491233824\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.020148184895515443\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.020196227729320525\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.01961129605770111\n",
      "Training iteration: 1517\n",
      "Validation loss (no improvement): 0.01929320991039276\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.019670529663562773\n",
      "Training iteration: 1519\n",
      "Validation loss (no improvement): 0.019453611969947816\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.018864354491233824  to: 0.01876046359539032\n",
      "Training iteration: 1521\n",
      "Validation loss (no improvement): 0.019955474138259887\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.01876046359539032  to: 0.018661876022815705\n",
      "Training iteration: 1523\n",
      "Validation loss (no improvement): 0.01866348534822464\n",
      "Training iteration: 1524\n",
      "Validation loss (no improvement): 0.019829198718070984\n",
      "Training iteration: 1525\n",
      "Validation loss (no improvement): 0.021346405148506165\n",
      "Training iteration: 1526\n",
      "Validation loss (no improvement): 0.018740998208522798\n",
      "Training iteration: 1527\n",
      "Validation loss (no improvement): 0.01903603821992874\n",
      "Training iteration: 1528\n",
      "Validation loss (no improvement): 0.020232248306274413\n",
      "Training iteration: 1529\n",
      "Validation loss (no improvement): 0.020774832367897032\n",
      "Training iteration: 1530\n",
      "Validation loss (no improvement): 0.01950729191303253\n",
      "Training iteration: 1531\n",
      "Validation loss (no improvement): 0.018669962882995605\n",
      "Training iteration: 1532\n",
      "Validation loss (no improvement): 0.020238251984119417\n",
      "Training iteration: 1533\n",
      "Validation loss (no improvement): 0.02014053165912628\n",
      "Training iteration: 1534\n",
      "Validation loss (no improvement): 0.01919519454240799\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.01957166790962219\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.020604178309440613\n",
      "Training iteration: 1537\n",
      "Validation loss (no improvement): 0.019030340015888214\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.018699470162391662\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.021373708546161652\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.020504474639892578\n",
      "Training iteration: 1541\n",
      "Improved validation loss from: 0.018661876022815705  to: 0.018002772331237794\n",
      "Training iteration: 1542\n",
      "Validation loss (no improvement): 0.01895745247602463\n",
      "Training iteration: 1543\n",
      "Validation loss (no improvement): 0.023249395191669464\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.022248415648937224\n",
      "Training iteration: 1545\n",
      "Validation loss (no improvement): 0.019255273044109344\n",
      "Training iteration: 1546\n",
      "Validation loss (no improvement): 0.01961280554533005\n",
      "Training iteration: 1547\n",
      "Validation loss (no improvement): 0.022715234756469728\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.024320140480995178\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.021071024239063263\n",
      "Training iteration: 1550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.019456110894680023\n",
      "Training iteration: 1551\n",
      "Validation loss (no improvement): 0.018925093114376068\n",
      "Training iteration: 1552\n",
      "Validation loss (no improvement): 0.019739577174186708\n",
      "Training iteration: 1553\n",
      "Validation loss (no improvement): 0.01909184455871582\n",
      "Training iteration: 1554\n",
      "Validation loss (no improvement): 0.01849798709154129\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.020580117404460908\n",
      "Training iteration: 1556\n",
      "Validation loss (no improvement): 0.02317798137664795\n",
      "Training iteration: 1557\n",
      "Validation loss (no improvement): 0.020025794208049775\n",
      "Training iteration: 1558\n",
      "Validation loss (no improvement): 0.018115147948265076\n",
      "Training iteration: 1559\n",
      "Validation loss (no improvement): 0.02007368057966232\n",
      "Training iteration: 1560\n",
      "Validation loss (no improvement): 0.023940496146678925\n",
      "Training iteration: 1561\n",
      "Validation loss (no improvement): 0.02149609327316284\n",
      "Training iteration: 1562\n",
      "Validation loss (no improvement): 0.01846015453338623\n",
      "Training iteration: 1563\n",
      "Validation loss (no improvement): 0.019140413403511046\n",
      "Training iteration: 1564\n",
      "Validation loss (no improvement): 0.023705694079399108\n",
      "Training iteration: 1565\n",
      "Validation loss (no improvement): 0.02471731901168823\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.02105536013841629\n",
      "Training iteration: 1567\n",
      "Validation loss (no improvement): 0.020022611320018768\n",
      "Training iteration: 1568\n",
      "Validation loss (no improvement): 0.02179196774959564\n",
      "Training iteration: 1569\n",
      "Validation loss (no improvement): 0.02435167282819748\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.022406670451164245\n",
      "Training iteration: 1571\n",
      "Validation loss (no improvement): 0.019354549050331116\n",
      "Training iteration: 1572\n",
      "Validation loss (no improvement): 0.019267165660858156\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.02164054214954376\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.023194923996925354\n",
      "Training iteration: 1575\n",
      "Validation loss (no improvement): 0.019076988101005554\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.01864403933286667\n",
      "Training iteration: 1577\n",
      "Validation loss (no improvement): 0.021590550243854523\n",
      "Training iteration: 1578\n",
      "Validation loss (no improvement): 0.021664881706237794\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.01920301616191864\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.01868690997362137\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.020913271605968474\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.024014946818351746\n",
      "Training iteration: 1583\n",
      "Validation loss (no improvement): 0.02096737176179886\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.018560147285461424\n",
      "Training iteration: 1585\n",
      "Validation loss (no improvement): 0.019154831767082214\n",
      "Training iteration: 1586\n",
      "Validation loss (no improvement): 0.023241123557090758\n",
      "Training iteration: 1587\n",
      "Validation loss (no improvement): 0.023581700026988985\n",
      "Training iteration: 1588\n",
      "Validation loss (no improvement): 0.020379407703876494\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.01910940706729889\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.02038784921169281\n",
      "Training iteration: 1591\n",
      "Validation loss (no improvement): 0.02548515498638153\n",
      "Training iteration: 1592\n",
      "Validation loss (no improvement): 0.023472599685192108\n",
      "Training iteration: 1593\n",
      "Validation loss (no improvement): 0.019585271179676057\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.018734851479530336\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.02135479897260666\n",
      "Training iteration: 1596\n",
      "Validation loss (no improvement): 0.023706129193305968\n",
      "Training iteration: 1597\n",
      "Validation loss (no improvement): 0.02037622928619385\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.01829752027988434\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.019127006828784942\n",
      "Training iteration: 1600\n",
      "Validation loss (no improvement): 0.023404458165168764\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.023794908821582795\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.019372804462909697\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.018778136372566222\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.019774064421653748\n",
      "Training iteration: 1605\n",
      "Validation loss (no improvement): 0.022305211424827574\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.021855846047401428\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.019657923281192778\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.018574634194374086\n",
      "Training iteration: 1609\n",
      "Validation loss (no improvement): 0.019581757485866547\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.02288193702697754\n",
      "Training iteration: 1611\n",
      "Validation loss (no improvement): 0.02181064635515213\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.018443545699119566\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.01842478960752487\n",
      "Training iteration: 1614\n",
      "Validation loss (no improvement): 0.02086811512708664\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.020988965034484865\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.018099913001060487\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.018041971325874328\n",
      "Training iteration: 1618\n",
      "Validation loss (no improvement): 0.020009338855743408\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.020183292031288148\n",
      "Training iteration: 1620\n",
      "Improved validation loss from: 0.018002772331237794  to: 0.01795929968357086\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.019286362826824187\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.020162275433540343\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.019438441097736358\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.01812584698200226\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.018237921595573425\n",
      "Training iteration: 1626\n",
      "Validation loss (no improvement): 0.01816379129886627\n",
      "Training iteration: 1627\n",
      "Validation loss (no improvement): 0.018363063037395478\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.01795929968357086  to: 0.017812713980674744\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.01860790401697159\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.018583261966705324\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.017812713980674744  to: 0.01739061176776886\n",
      "Training iteration: 1632\n",
      "Validation loss (no improvement): 0.019125792384147643\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.02020295411348343\n",
      "Training iteration: 1634\n",
      "Validation loss (no improvement): 0.017909665405750275\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.01739061176776886  to: 0.01707743704319\n",
      "Training iteration: 1636\n",
      "Validation loss (no improvement): 0.018914684653282166\n",
      "Training iteration: 1637\n",
      "Validation loss (no improvement): 0.020101094245910646\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.01923827677965164\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.01783667504787445\n",
      "Training iteration: 1640\n",
      "Validation loss (no improvement): 0.018393418192863463\n",
      "Training iteration: 1641\n",
      "Validation loss (no improvement): 0.022061601281166077\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.0210371732711792\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.017639628052711485\n",
      "Training iteration: 1644\n",
      "Validation loss (no improvement): 0.01789851188659668\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.021712014079093934\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.021519997715950014\n",
      "Training iteration: 1647\n",
      "Validation loss (no improvement): 0.017755873501300812\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.018149015307426453\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.020176342129707335\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.02077019214630127\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.0181543231010437\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.01820127069950104\n",
      "Training iteration: 1653\n",
      "Validation loss (no improvement): 0.019609953463077544\n",
      "Training iteration: 1654\n",
      "Validation loss (no improvement): 0.018153096735477447\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.01707743704319  to: 0.016885337233543397\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.017962071299552917\n",
      "Training iteration: 1657\n",
      "Validation loss (no improvement): 0.022078807651996612\n",
      "Training iteration: 1658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02032259404659271\n",
      "Training iteration: 1659\n",
      "Improved validation loss from: 0.016885337233543397  to: 0.01672205775976181\n",
      "Training iteration: 1660\n",
      "Validation loss (no improvement): 0.017447593808174133\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.021707186102867128\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.02070510685443878\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.01786036789417267\n",
      "Training iteration: 1664\n",
      "Validation loss (no improvement): 0.018152080476284027\n",
      "Training iteration: 1665\n",
      "Validation loss (no improvement): 0.0185394823551178\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.01943293809890747\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.017515361309051514\n",
      "Training iteration: 1668\n",
      "Validation loss (no improvement): 0.017369794845581054\n",
      "Training iteration: 1669\n",
      "Validation loss (no improvement): 0.01891806870698929\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.01790401488542557\n",
      "Training iteration: 1671\n",
      "Validation loss (no improvement): 0.018104954063892363\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.017114627361297607\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.019024649262428285\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.017609366774559022\n",
      "Training iteration: 1675\n",
      "Validation loss (no improvement): 0.0173472136259079\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.019273433089256286\n",
      "Training iteration: 1677\n",
      "Validation loss (no improvement): 0.01816401332616806\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.017313703894615173\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.017367973923683167\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.018617552518844605\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.017498262226581573\n",
      "Training iteration: 1682\n",
      "Validation loss (no improvement): 0.018274366855621338\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.017822110652923585\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.01707700788974762\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.02023576498031616\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.019911691546440125\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.017023161053657532\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.017469321191310883\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.0197832390666008\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.018145112693309783\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.01672205775976181  to: 0.01647234559059143\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.01843235790729523\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.020234036445617675\n",
      "Training iteration: 1694\n",
      "Validation loss (no improvement): 0.01835872679948807\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.01711089313030243\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.018841278553009034\n",
      "Training iteration: 1697\n",
      "Validation loss (no improvement): 0.018879494071006774\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.01709817200899124\n",
      "Training iteration: 1699\n",
      "Validation loss (no improvement): 0.017703743278980257\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.019254927337169648\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.016891208291053773\n",
      "Training iteration: 1702\n",
      "Validation loss (no improvement): 0.01663234978914261\n",
      "Training iteration: 1703\n",
      "Validation loss (no improvement): 0.02007223665714264\n",
      "Training iteration: 1704\n",
      "Validation loss (no improvement): 0.018657983839511873\n",
      "Training iteration: 1705\n",
      "Improved validation loss from: 0.01647234559059143  to: 0.016057860851287842\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.01817227303981781\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.021172499656677245\n",
      "Training iteration: 1708\n",
      "Validation loss (no improvement): 0.019537810981273652\n",
      "Training iteration: 1709\n",
      "Validation loss (no improvement): 0.01627455949783325\n",
      "Training iteration: 1710\n",
      "Validation loss (no improvement): 0.01648756265640259\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.020560534298419954\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.019530606269836426\n",
      "Training iteration: 1713\n",
      "Improved validation loss from: 0.016057860851287842  to: 0.01604444831609726\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.01643477976322174\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.020920801162719726\n",
      "Training iteration: 1716\n",
      "Validation loss (no improvement): 0.02020160257816315\n",
      "Training iteration: 1717\n",
      "Validation loss (no improvement): 0.016903491318225862\n",
      "Training iteration: 1718\n",
      "Validation loss (no improvement): 0.017110733687877654\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.02108009308576584\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.020147076249122618\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.01719844341278076\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.016413304209709167\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.01960204839706421\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.02184317111968994\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.016637715697288512\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.01604444831609726  to: 0.015930576622486113\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.018574292957782745\n",
      "Training iteration: 1728\n",
      "Validation loss (no improvement): 0.01891651451587677\n",
      "Training iteration: 1729\n",
      "Validation loss (no improvement): 0.016197995841503145\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.01632477045059204\n",
      "Training iteration: 1731\n",
      "Validation loss (no improvement): 0.01907629519701004\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.018072737753391264\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.0164120152592659\n",
      "Training iteration: 1734\n",
      "Validation loss (no improvement): 0.017406542599201203\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.019115889072418214\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.017825034260749818\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.015930576622486113  to: 0.015864935517311097\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.01665227711200714\n",
      "Training iteration: 1739\n",
      "Validation loss (no improvement): 0.018421462178230284\n",
      "Training iteration: 1740\n",
      "Validation loss (no improvement): 0.017044684290885924\n",
      "Training iteration: 1741\n",
      "Improved validation loss from: 0.015864935517311097  to: 0.015513548254966735\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.01730736196041107\n",
      "Training iteration: 1743\n",
      "Validation loss (no improvement): 0.017754510045051575\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.015513548254966735  to: 0.015117496252059937\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.016027817130088808\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.01966181695461273\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.018272611498832702\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.015761175751686098\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.017089764773845672\n",
      "Training iteration: 1750\n",
      "Validation loss (no improvement): 0.018983371555805206\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.017858536541461946\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.016270697116851807\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.01763449013233185\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.016319918632507324\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.0159341499209404\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.017817310988903046\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.017989376187324525\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.0159586101770401\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.01730607748031616\n",
      "Training iteration: 1760\n",
      "Validation loss (no improvement): 0.018982672691345216\n",
      "Training iteration: 1761\n",
      "Validation loss (no improvement): 0.01603448987007141\n",
      "Training iteration: 1762\n",
      "Validation loss (no improvement): 0.016164276003837585\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.018871590495109558\n",
      "Training iteration: 1764\n",
      "Validation loss (no improvement): 0.016748508810997008\n",
      "Training iteration: 1765\n",
      "Improved validation loss from: 0.015117496252059937  to: 0.015056610107421875\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.016622075438499452\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.022214317321777345\n",
      "Training iteration: 1768\n",
      "Validation loss (no improvement): 0.018721847236156462\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.01549576222896576\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.01676264703273773\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.02347240746021271\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.022067780792713165\n",
      "Training iteration: 1773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.017048022150993346\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.017029590904712677\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.021593689918518066\n",
      "Training iteration: 1776\n",
      "Validation loss (no improvement): 0.023699605464935304\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.01850219815969467\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.016484294831752778\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.01668386310338974\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.021799711883068083\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.022070106863975526\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.017335382103919984\n",
      "Training iteration: 1783\n",
      "Validation loss (no improvement): 0.0164982408285141\n",
      "Training iteration: 1784\n",
      "Validation loss (no improvement): 0.017772799730300902\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.022222717106342316\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.01825951337814331\n",
      "Training iteration: 1787\n",
      "Validation loss (no improvement): 0.015436038374900818\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.01619006395339966\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.01959759443998337\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.01899344027042389\n",
      "Training iteration: 1791\n",
      "Validation loss (no improvement): 0.01550753116607666\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.015150254964828492\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.01713111698627472\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.017930714786052702\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.015587934851646423\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.01641698181629181\n",
      "Training iteration: 1797\n",
      "Validation loss (no improvement): 0.01658073365688324\n",
      "Training iteration: 1798\n",
      "Validation loss (no improvement): 0.01610371619462967\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.01797284334897995\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.016557897627353668\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.015056610107421875  to: 0.01467001885175705\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.01555359661579132\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.018131570518016817\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.016554613411426545\n",
      "Training iteration: 1805\n",
      "Validation loss (no improvement): 0.015414150059223175\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.016964706778526305\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.01746547967195511\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.01627960056066513\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.01691964566707611\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.016022711992263794\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.016887888312339783\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.016208747029304506\n",
      "Training iteration: 1813\n",
      "Validation loss (no improvement): 0.01496237814426422\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.016997244954109193\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.017001710832118988\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.015491156280040741\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.017198717594146727\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.01688447445631027\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.01649041622877121\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.014926391839981078\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.015621067583560943\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.016855759918689726\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.015210379660129548\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.01637164205312729\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.016409090161323546\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.015149620175361634\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.017781832814216615\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.018443360924720764\n",
      "Training iteration: 1829\n",
      "Validation loss (no improvement): 0.016507580876350403\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.016673895716667175\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.01861061304807663\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.017063461244106293\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.014975804090499877\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.017238797247409822\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.016901817917823792\n",
      "Training iteration: 1836\n",
      "Validation loss (no improvement): 0.015409746766090393\n",
      "Training iteration: 1837\n",
      "Validation loss (no improvement): 0.016780969500541688\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.019765503704547882\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.017499676346778868\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.015382559597492218\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.016648316383361818\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.021051077544689177\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.01839814633131027\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.015746702253818513\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.016748666763305664\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.020958249270915986\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.01881084740161896\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.015946409106254576\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.016682389378547668\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.017686958611011504\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.016086454689502715\n",
      "Training iteration: 1852\n",
      "Validation loss (no improvement): 0.017069348692893983\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.01627962291240692\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.017925797402858733\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.016396769881248476\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.015093953907489776\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.016505494713783264\n",
      "Training iteration: 1858\n",
      "Validation loss (no improvement): 0.020279355347156525\n",
      "Training iteration: 1859\n",
      "Validation loss (no improvement): 0.017070920765399934\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.014758901298046112\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.016222253441810608\n",
      "Training iteration: 1862\n",
      "Validation loss (no improvement): 0.01914600729942322\n",
      "Training iteration: 1863\n",
      "Validation loss (no improvement): 0.017817234992980956\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.015506784617900848\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.0166316494345665\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.017727681994438173\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.01669252961874008\n",
      "Training iteration: 1868\n",
      "Validation loss (no improvement): 0.01706383377313614\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.017659540474414825\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.017345401644706725\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.01631210744380951\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.017955465614795683\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.01789584904909134\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.01696930229663849\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.017968383431434632\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.015183468163013459\n",
      "Training iteration: 1877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01527649313211441\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.019378316402435303\n",
      "Training iteration: 1879\n",
      "Validation loss (no improvement): 0.018060779571533202\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.015155459940433502\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.015974435210227966\n",
      "Training iteration: 1882\n",
      "Validation loss (no improvement): 0.02003968209028244\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.02015872448682785\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.016574761271476744\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.015171831846237183\n",
      "Training iteration: 1886\n",
      "Validation loss (no improvement): 0.017110911011695863\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.017798896133899688\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.015390685200691223\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.01605815440416336\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.018265002965927125\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.01664995402097702\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.015550044178962708\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.017068402469158174\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.019099912047386168\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.01797715723514557\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.015516182780265808\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.016118961572647094\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.01842679679393768\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.016387742757797242\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.01482076346874237\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.017135396599769592\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_3 = krishnan_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_3.train_model()\n",
    "ensemble_model_3.model_inference()\n",
    "\n",
    "ensemble_mean_3 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_3 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_3 = np.sqrt(np.exp(ensemble_logvar_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 10.700105285644531\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 10.700105285644531  to: 8.401470947265626\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 8.401470947265626  to: 6.67865219116211\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 6.67865219116211  to: 5.3964073181152346\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 5.3964073181152346  to: 4.4194793701171875\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 4.4194793701171875  to: 3.651894378662109\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 3.651894378662109  to: 3.0353073120117187\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 3.0353073120117187  to: 2.534515953063965\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 2.534515953063965  to: 2.1257171630859375\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 2.1257171630859375  to: 1.7916669845581055\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.7916669845581055  to: 1.518486785888672\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 1.518486785888672  to: 1.294543170928955\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 1.294543170928955  to: 1.1105009078979493\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 1.1105009078979493  to: 0.9591208457946777\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.9591208457946777  to: 0.834705924987793\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.834705924987793  to: 0.7319752693176269\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.7319752693176269  to: 0.6467568874359131\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.6467568874359131  to: 0.5758967399597168\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.5758967399597168  to: 0.5168525218963623\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.5168525218963623  to: 0.46747851371765137\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.46747851371765137  to: 0.42609243392944335\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.42609243392944335  to: 0.39123382568359377\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.39123382568359377  to: 0.3617858409881592\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.3617858409881592  to: 0.3368134260177612\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.3368134260177612  to: 0.3155501842498779\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.3155501842498779  to: 0.2973815441131592\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.2973815441131592  to: 0.28179914951324464\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.28179914951324464  to: 0.2683849811553955\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.2683849811553955  to: 0.2567921161651611\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.2567921161651611  to: 0.24673538208007811\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.24673538208007811  to: 0.23798034191131592\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.23798034191131592  to: 0.230332612991333\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.230332612991333  to: 0.2236255168914795\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.2236255168914795  to: 0.2177225112915039\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.2177225112915039  to: 0.21250581741333008\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.21250581741333008  to: 0.20787663459777833\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.20787663459777833  to: 0.20376088619232177\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.20376088619232177  to: 0.20009033679962157\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.20009033679962157  to: 0.19680531024932862\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.19680531024932862  to: 0.19385620355606079\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.19385620355606079  to: 0.19120057821273803\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.19120057821273803  to: 0.1888022780418396\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.1888022780418396  to: 0.18662916421890258\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.18662916421890258  to: 0.18465449810028076\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.18465449810028076  to: 0.1828548789024353\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.1828548789024353  to: 0.18121011257171632\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.18121011257171632  to: 0.17970260381698608\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.17970260381698608  to: 0.17831699848175048\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.17831699848175048  to: 0.17703994512557983\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.17703994512557983  to: 0.17585976123809816\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.17585976123809816  to: 0.17476621866226197\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.17476621866226197  to: 0.17375013828277588\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.17375013828277588  to: 0.17280237674713134\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.17280237674713134  to: 0.17191486358642577\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.17191486358642577  to: 0.1710839867591858\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.1710839867591858  to: 0.17030433416366578\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.17030433416366578  to: 0.1695711135864258\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1695711135864258  to: 0.16888010501861572\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.16888010501861572  to: 0.1682271718978882\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.1682271718978882  to: 0.16760896444320678\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.16760896444320678  to: 0.16702260971069335\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.16702260971069335  to: 0.16646530628204345\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.16646530628204345  to: 0.1659347176551819\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.1659347176551819  to: 0.16542863845825195\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.16542863845825195  to: 0.16494516134262086\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.16494516134262086  to: 0.16448253393173218\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.16448253393173218  to: 0.16403878927230836\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.16403878927230836  to: 0.16361280679702758\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.16361280679702758  to: 0.16320329904556274\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.16320329904556274  to: 0.16280908584594728\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.16280908584594728  to: 0.16242802143096924\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.16242802143096924  to: 0.1620579957962036\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.1620579957962036  to: 0.1617004990577698\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.1617004990577698  to: 0.1613547086715698\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.1613547086715698  to: 0.16101932525634766\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.16101932525634766  to: 0.16069400310516357\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.16069400310516357  to: 0.16037862300872802\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.16037862300872802  to: 0.16007255315780639\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.16007255315780639  to: 0.15977542400360106\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.15977542400360106  to: 0.15948675870895385\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.15948675870895385  to: 0.1592060685157776\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.1592060685157776  to: 0.158932888507843\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.158932888507843  to: 0.15866698026657106\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.15866698026657106  to: 0.1584080934524536\n",
      "Training iteration: 84\n",
      "Improved validation loss from: 0.1584080934524536  to: 0.15815602540969848\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.15815602540969848  to: 0.15791037082672119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 86\n",
      "Improved validation loss from: 0.15791037082672119  to: 0.1576708436012268\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.1576708436012268  to: 0.15743699073791503\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.15743699073791503  to: 0.15720808506011963\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.15720808506011963  to: 0.15698453187942504\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.15698453187942504  to: 0.15676605701446533\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.15676605701446533  to: 0.15655221939086914\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.15655221939086914  to: 0.15633983612060548\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.15633983612060548  to: 0.15613162517547607\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.15613162517547607  to: 0.15592782497406005\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.15592782497406005  to: 0.15572837591171265\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.15572837591171265  to: 0.1555330276489258\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.1555330276489258  to: 0.15534161329269408\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.15534161329269408  to: 0.15515401363372802\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.15515401363372802  to: 0.15497015714645385\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.15497015714645385  to: 0.15478999614715577\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.15478999614715577  to: 0.1546133875846863\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.1546133875846863  to: 0.15444021224975585\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.15444021224975585  to: 0.1542702794075012\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.1542702794075012  to: 0.1541023850440979\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.1541023850440979  to: 0.15393632650375366\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.15393632650375366  to: 0.15377330780029297\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.15377330780029297  to: 0.15361301898956298\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.15361301898956298  to: 0.15345532894134523\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.15345532894134523  to: 0.15330007076263427\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.15330007076263427  to: 0.1531471848487854\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.1531471848487854  to: 0.15299640893936156\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.15299640893936156  to: 0.1528446912765503\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.1528446912765503  to: 0.1526951789855957\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.1526951789855957  to: 0.15254777669906616\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.15254777669906616  to: 0.1524024248123169\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.1524024248123169  to: 0.15225908756256104\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.15225908756256104  to: 0.15211770534515381\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.15211770534515381  to: 0.15197834968566895\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.15197834968566895  to: 0.15184098482131958\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.15184098482131958  to: 0.15170552730560302\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.15170552730560302  to: 0.1515719175338745\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.1515719175338745  to: 0.15144016742706298\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.15144016742706298  to: 0.15131018161773682\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.15131018161773682  to: 0.15118193626403809\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.15118193626403809  to: 0.1510530710220337\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.1510530710220337  to: 0.15092478990554808\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.15092478990554808  to: 0.1507979154586792\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.1507979154586792  to: 0.15067249536514282\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.15067249536514282  to: 0.15054845809936523\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.15054845809936523  to: 0.15042585134506226\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.15042585134506226  to: 0.15030462741851808\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.15030462741851808  to: 0.15018476247787477\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.15018476247787477  to: 0.15006625652313232\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.15006625652313232  to: 0.14994914531707765\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.14994914531707765  to: 0.14983342885971068\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.14983342885971068  to: 0.14971903562545777\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.14971903562545777  to: 0.14960600137710572\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14960600137710572  to: 0.1494942307472229\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.1494942307472229  to: 0.14938371181488036\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14938371181488036  to: 0.14927446842193604\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14927446842193604  to: 0.14916641712188722\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.14916641712188722  to: 0.14905956983566285\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14905956983566285  to: 0.14895389080047608\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14895389080047608  to: 0.14884935617446898\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14884935617446898  to: 0.14874598979949952\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14874598979949952  to: 0.1486423134803772\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.1486423134803772  to: 0.14853832721710206\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14853832721710206  to: 0.14843542575836183\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.14843542575836183  to: 0.14833362102508546\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.14833362102508546  to: 0.14823285341262818\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.14823285341262818  to: 0.14813311100006105\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.14813311100006105  to: 0.14803435802459716\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.14803435802459716  to: 0.14793660640716552\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.14793660640716552  to: 0.14783982038497925\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.14783982038497925  to: 0.14774396419525146\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.14774396419525146  to: 0.14764901399612426\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.14764901399612426  to: 0.1475549340248108\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.1475549340248108  to: 0.14746168851852418\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.14746168851852418  to: 0.14736930131912232\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.14736930131912232  to: 0.14727772474288942\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.14727772474288942  to: 0.1471869707107544\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.1471869707107544  to: 0.14709699153900146\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.14709699153900146  to: 0.14700781106948851\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.14700781106948851  to: 0.1469193696975708\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.1469193696975708  to: 0.14683167934417723\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.14683167934417723  to: 0.1467447280883789\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.1467447280883789  to: 0.1466585159301758\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.1466585159301758  to: 0.14657299518585204\n",
      "Training iteration: 169\n",
      "Improved validation loss from: 0.14657299518585204  to: 0.14648818969726562\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.14648818969726562  to: 0.14640409946441652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 171\n",
      "Improved validation loss from: 0.14640409946441652  to: 0.14632067680358887\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.14632067680358887  to: 0.14623792171478273\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.14623792171478273  to: 0.14615501165390016\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.14615501165390016  to: 0.14607206583023072\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.14607206583023072  to: 0.1459897756576538\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.1459897756576538  to: 0.14590780735015868\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.14590780735015868  to: 0.1458261489868164\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.1458261489868164  to: 0.14574512243270873\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.14574512243270873  to: 0.14566471576690673\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.14566471576690673  to: 0.14558489322662355\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.14558489322662355  to: 0.14550567865371705\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.14550567865371705  to: 0.14542703628540038\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.14542703628540038  to: 0.1453489899635315\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.1453489899635315  to: 0.14527149200439454\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.14527149200439454  to: 0.14519456624984742\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.14519456624984742  to: 0.14511818885803224\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.14511818885803224  to: 0.14504234790802\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.14504234790802  to: 0.14496704339981079\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.14496704339981079  to: 0.14489226341247557\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.14489226341247557  to: 0.14481807947158815\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.14481807947158815  to: 0.14474443197250367\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.14474443197250367  to: 0.14467135667800904\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.14467135667800904  to: 0.14459885358810426\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.14459885358810426  to: 0.1445268511772156\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.1445268511772156  to: 0.14445533752441406\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.14445533752441406  to: 0.14438433647155763\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.14438433647155763  to: 0.14431378841400147\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.14431378841400147  to: 0.14424372911453248\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.14424372911453248  to: 0.14417411088943483\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.14417411088943483  to: 0.1441049814224243\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.1441049814224243  to: 0.14403625726699829\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.14403625726699829  to: 0.14396798610687256\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.14396798610687256  to: 0.14390016794204713\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.14390016794204713  to: 0.14383275508880616\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.14383275508880616  to: 0.14376575946807862\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.14376575946807862  to: 0.1436991810798645\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.1436991810798645  to: 0.14363300800323486\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.14363300800323486  to: 0.1435672402381897\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.1435672402381897  to: 0.1435018539428711\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.1435018539428711  to: 0.14343687295913696\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.14343687295913696  to: 0.1433722496032715\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.1433722496032715  to: 0.14330799579620362\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.14330799579620362  to: 0.14324411153793334\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.14324411153793334  to: 0.14318004846572877\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.14318004846572877  to: 0.14311622381210326\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.14311622381210326  to: 0.1430527687072754\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.1430527687072754  to: 0.1429896354675293\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.1429896354675293  to: 0.14292685985565184\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.14292685985565184  to: 0.14286439418792723\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.14286439418792723  to: 0.14280227422714234\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.14280227422714234  to: 0.1427404761314392\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.1427404761314392  to: 0.14267866611480712\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.14267866611480712  to: 0.14261716604232788\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.14261716604232788  to: 0.14255599975585936\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.14255599975585936  to: 0.14249513149261475\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.14249513149261475  to: 0.14243459701538086\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.14243459701538086  to: 0.1423743486404419\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.1423743486404419  to: 0.14231441020965577\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.14231441020965577  to: 0.14225475788116454\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.14225475788116454  to: 0.14219539165496825\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.14219539165496825  to: 0.1421363115310669\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.1421363115310669  to: 0.14207751750946046\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.14207751750946046  to: 0.14201898574829103\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.14201898574829103  to: 0.1419607162475586\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1419607162475586  to: 0.14190273284912108\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.14190273284912108  to: 0.14184499979019166\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.14184499979019166  to: 0.14178750514984131\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.14178750514984131  to: 0.14173028469085694\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.14173028469085694  to: 0.1416733145713806\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.1416733145713806  to: 0.14161657094955443\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.14161657094955443  to: 0.14156004190444946\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.14156004190444946  to: 0.1415037155151367\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.1415037155151367  to: 0.1414475679397583\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.1414475679397583  to: 0.14139163494110107\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.14139163494110107  to: 0.141335928440094\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.141335928440094  to: 0.1412803888320923\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.1412803888320923  to: 0.14122499227523805\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.14122499227523805  to: 0.14116979837417604\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.14116979837417604  to: 0.14111478328704835\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.14111478328704835  to: 0.14105998277664183\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.14105998277664183  to: 0.14100550413131713\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.14100550413131713  to: 0.14095162153244017\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.14095162153244017  to: 0.14089791774749755\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.14089791774749755  to: 0.14084441661834718\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.14084441661834718  to: 0.14079108238220214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 256\n",
      "Improved validation loss from: 0.14079108238220214  to: 0.1407378911972046\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1407378911972046  to: 0.1406848907470703\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.1406848907470703  to: 0.14063202142715453\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.14063202142715453  to: 0.14057928323745728\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.14057928323745728  to: 0.14052671194076538\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.14052671194076538  to: 0.1404742956161499\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1404742956161499  to: 0.14042201042175292\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.14042201042175292  to: 0.14036986827850342\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.14036986827850342  to: 0.14031778573989867\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.14031778573989867  to: 0.1402658462524414\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.1402658462524414  to: 0.14021406173706055\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.14021406173706055  to: 0.1401624321937561\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.1401624321937561  to: 0.14011093378067016\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.14011093378067016  to: 0.14005956649780274\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.14005956649780274  to: 0.14000836610794068\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.14000836610794068  to: 0.13995726108551027\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.13995726108551027  to: 0.13990631103515624\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.13990631103515624  to: 0.13985546827316284\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13985546827316284  to: 0.13980476856231688\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.13980476856231688  to: 0.13975417613983154\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13975417613983154  to: 0.13970370292663575\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13970370292663575  to: 0.13965336084365845\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13965336084365845  to: 0.13960312604904174\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.13960312604904174  to: 0.13955302238464357\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13955302238464357  to: 0.13950302600860595\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13950302600860595  to: 0.13945314884185792\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13945314884185792  to: 0.1394033670425415\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.1394033670425415  to: 0.13935370445251466\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.13935370445251466  to: 0.13930414915084838\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13930414915084838  to: 0.13925471305847167\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13925471305847167  to: 0.13920542001724243\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.13920542001724243  to: 0.13915627002716063\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13915627002716063  to: 0.13910725116729736\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13910725116729736  to: 0.13905835151672363\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13905835151672363  to: 0.1390095829963684\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.1390095829963684  to: 0.13896095752716064\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.13896095752716064  to: 0.1389124274253845\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.1389124274253845  to: 0.1388640284538269\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.1388640284538269  to: 0.13881574869155883\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.13881574869155883  to: 0.13876755237579347\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.13876755237579347  to: 0.13871948719024657\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13871948719024657  to: 0.13867151737213135\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.13867151737213135  to: 0.13862364292144774\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13862364292144774  to: 0.13857587575912475\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13857587575912475  to: 0.13852821588516234\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13852821588516234  to: 0.13848063945770264\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13848063945770264  to: 0.13843321800231934\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.13843321800231934  to: 0.13838596343994142\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13838596343994142  to: 0.1383386492729187\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.1383386492729187  to: 0.13829128742218016\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.13829128742218016  to: 0.13824387788772582\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13824387788772582  to: 0.1381964325904846\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.1381964325904846  to: 0.1381489634513855\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.1381489634513855  to: 0.1381014585494995\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.1381014585494995  to: 0.13805391788482665\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.13805391788482665  to: 0.1380063772201538\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.1380063772201538  to: 0.13795883655548097\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.13795883655548097  to: 0.13791131973266602\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13791131973266602  to: 0.137863826751709\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.137863826751709  to: 0.13781635761260985\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13781635761260985  to: 0.13776893615722657\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.13776893615722657  to: 0.13772159814834595\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13772159814834595  to: 0.1376742959022522\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.1376742959022522  to: 0.13762706518173218\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.13762706518173218  to: 0.13757989406585694\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13757989406585694  to: 0.1375327706336975\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.1375327706336975  to: 0.13748571872711182\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13748571872711182  to: 0.13743875026702881\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13743875026702881  to: 0.13739181756973268\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13739181756973268  to: 0.13734496831893922\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13734496831893922  to: 0.13729817867279054\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13729817867279054  to: 0.13725146055221557\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13725146055221557  to: 0.13720481395721434\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.13720481395721434  to: 0.13715823888778686\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13715823888778686  to: 0.13711173534393312\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.13711173534393312  to: 0.1370653033256531\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.1370653033256531  to: 0.13701894283294677\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.13701894283294677  to: 0.1369726538658142\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.1369726538658142  to: 0.13692646026611327\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.13692646026611327  to: 0.13688032627105712\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13688032627105712  to: 0.1368342638015747\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.1368342638015747  to: 0.13678828477859498\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.13678828477859498  to: 0.13674238920211793\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.13674238920211793  to: 0.1366965651512146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 340\n",
      "Improved validation loss from: 0.1366965651512146  to: 0.13665082454681396\n",
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13665082454681396  to: 0.136605167388916\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.136605167388916  to: 0.1365595817565918\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.1365595817565918  to: 0.1365140914916992\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.1365140914916992  to: 0.13646867275238037\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.13646867275238037  to: 0.13642334938049316\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.13642334938049316  to: 0.13637809753417968\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.13637809753417968  to: 0.1363331198692322\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.1363331198692322  to: 0.13628822565078735\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13628822565078735  to: 0.13624345064163207\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13624345064163207  to: 0.13619877099990846\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13619877099990846  to: 0.13615416288375853\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.13615416288375853  to: 0.13610965013504028\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13610965013504028  to: 0.1360652208328247\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.1360652208328247  to: 0.13602088689804076\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.13602088689804076  to: 0.1359768271446228\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1359768271446228  to: 0.13593288660049438\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13593288660049438  to: 0.13588902950286866\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13588902950286866  to: 0.13584525585174562\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13584525585174562  to: 0.1358015775680542\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.1358015775680542  to: 0.13575799465179444\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13575799465179444  to: 0.1357144832611084\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.1357144832611084  to: 0.13567109107971193\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13567109107971193  to: 0.13562777042388915\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13562777042388915  to: 0.135584557056427\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.135584557056427  to: 0.13554141521453858\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.13554141521453858  to: 0.13549840450286865\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13549840450286865  to: 0.1354554533958435\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.1354554533958435  to: 0.1354126214981079\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.1354126214981079  to: 0.1353698968887329\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.1353698968887329  to: 0.1353272318840027\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1353272318840027  to: 0.135284686088562\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.135284686088562  to: 0.13524224758148193\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.13524224758148193  to: 0.13519989252090453\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.13519989252090453  to: 0.1351576566696167\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.1351576566696167  to: 0.13511550426483154\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.13511550426483154  to: 0.13507349491119386\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.13507349491119386  to: 0.1350315570831299\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.1350315570831299  to: 0.1349897265434265\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.1349897265434265  to: 0.13494802713394166\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.13494802713394166  to: 0.13490642309188844\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.13490642309188844  to: 0.1348649263381958\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.1348649263381958  to: 0.1348235487937927\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.1348235487937927  to: 0.13478227853775024\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.13478227853775024  to: 0.13474115133285522\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.13474115133285522  to: 0.13470010757446288\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.13470010757446288  to: 0.13465919494628906\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.13465919494628906  to: 0.13461841344833375\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.13461841344833375  to: 0.13457772731781006\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.13457772731781006  to: 0.1345371961593628\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.1345371961593628  to: 0.134496808052063\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.134496808052063  to: 0.13445650339126586\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.13445650339126586  to: 0.13441635370254518\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.13441635370254518  to: 0.13437633514404296\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.13437633514404296  to: 0.13433644771575928\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.13433644771575928  to: 0.13429667949676513\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.13429667949676513  to: 0.1342570662498474\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.1342570662498474  to: 0.1342176079750061\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.1342176079750061  to: 0.1341782569885254\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.1341782569885254  to: 0.134139084815979\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.134139084815979  to: 0.13410009145736695\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.13410009145736695  to: 0.13406126499176024\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.13406126499176024  to: 0.13402256965637208\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.13402256965637208  to: 0.13398401737213134\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.13398401737213134  to: 0.13394562005996705\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.13394562005996705  to: 0.13390735387802125\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.13390735387802125  to: 0.13386924266815187\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.13386924266815187  to: 0.13383127450942994\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.13383127450942994  to: 0.13379347324371338\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.13379347324371338  to: 0.13375580310821533\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.13375580310821533  to: 0.13371829986572265\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.13371829986572265  to: 0.1336809515953064\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1336809515953064  to: 0.13364375829696656\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.13364375829696656  to: 0.13360674381256105\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.13360674381256105  to: 0.13356988430023192\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.13356988430023192  to: 0.13353320360183715\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.13353320360183715  to: 0.13349676132202148\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.13349676132202148  to: 0.13346047401428224\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.13346047401428224  to: 0.13342435359954835\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.13342435359954835  to: 0.13338840007781982\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.13338840007781982  to: 0.1333526372909546\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.1333526372909546  to: 0.13331704139709472\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.13331704139709472  to: 0.1332816004753113\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.1332816004753113  to: 0.13324636220932007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 424\n",
      "Improved validation loss from: 0.13324636220932007  to: 0.13321168422698976\n",
      "Training iteration: 425\n",
      "Improved validation loss from: 0.13321168422698976  to: 0.13317759037017823\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.13317759037017823  to: 0.1331439733505249\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.1331439733505249  to: 0.1331108570098877\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.1331108570098877  to: 0.1330782175064087\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1330782175064087  to: 0.13304601907730101\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.13304601907730101  to: 0.13301420211791992\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.13301420211791992  to: 0.1329827666282654\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.1329827666282654  to: 0.1329517126083374\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.1329517126083374  to: 0.13292100429534912\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.13292100429534912  to: 0.13289061784744263\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.13289061784744263  to: 0.13286055326461793\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.13286055326461793  to: 0.13283077478408814\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.13283077478408814  to: 0.13280128240585326\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.13280128240585326  to: 0.13277208805084229\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.13277208805084229  to: 0.1327431321144104\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.1327431321144104  to: 0.13271442651748658\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.13271442651748658  to: 0.1326859712600708\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.1326859712600708  to: 0.13265773057937622\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.13265773057937622  to: 0.13262972831726075\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.13262972831726075  to: 0.1326019287109375\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.1326019287109375  to: 0.13257434368133544\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.13257434368133544  to: 0.13254696130752563\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.13254696130752563  to: 0.1325197696685791\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1325197696685791  to: 0.13249279260635377\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.13249279260635377  to: 0.13246599435806275\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.13246599435806275  to: 0.13243937492370605\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.13243937492370605  to: 0.13241294622421265\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.13241294622421265  to: 0.13238670825958251\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.13238670825958251  to: 0.13236064910888673\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.13236064910888673  to: 0.13233476877212524\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.13233476877212524  to: 0.13230905532836915\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.13230905532836915  to: 0.13228352069854737\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.13228352069854737  to: 0.132258141040802\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.132258141040802  to: 0.132232928276062\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.132232928276062  to: 0.13220787048339844\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.13220787048339844  to: 0.1321829915046692\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.1321829915046692  to: 0.1321582555770874\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.1321582555770874  to: 0.13213367462158204\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.13213367462158204  to: 0.13210924863815307\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.13210924863815307  to: 0.13208497762680055\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.13208497762680055  to: 0.13206087350845336\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.13206087350845336  to: 0.13203691244125365\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.13203691244125365  to: 0.13201310634613037\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.13201310634613037  to: 0.1319894552230835\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.1319894552230835  to: 0.1319659471511841\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.1319659471511841  to: 0.13194259405136108\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.13194259405136108  to: 0.13191937208175658\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.13191937208175658  to: 0.13189632892608644\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.13189632892608644  to: 0.13187341690063475\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.13187341690063475  to: 0.13185064792633056\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.13185064792633056  to: 0.13182804584503174\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.13182804584503174  to: 0.13180558681488036\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.13180558681488036  to: 0.13178327083587646\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.13178327083587646  to: 0.13176109790802001\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.13176109790802001  to: 0.13173909187316896\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.13173909187316896  to: 0.13171720504760742\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.13171720504760742  to: 0.13169548511505128\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.13169548511505128  to: 0.13167390823364258\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.13167390823364258  to: 0.13165246248245238\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.13165246248245238  to: 0.13163115978240966\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.13163115978240966  to: 0.13161001205444336\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.13161001205444336  to: 0.13158900737762452\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.13158900737762452  to: 0.13156816959381104\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.13156816959381104  to: 0.1315474510192871\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.1315474510192871  to: 0.1315268874168396\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.1315268874168396  to: 0.1315064549446106\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.1315064549446106  to: 0.1314861536026001\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.1314861536026001  to: 0.13146597146987915\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.13146597146987915  to: 0.13144593238830565\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.13144593238830565  to: 0.13142600059509277\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.13142600059509277  to: 0.13140623569488524\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.13140623569488524  to: 0.13138657808303833\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.13138657808303833  to: 0.1313670754432678\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.1313670754432678  to: 0.1313476800918579\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.1313476800918579  to: 0.13132845163345336\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.13132845163345336  to: 0.13130933046340942\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.13130933046340942  to: 0.13129035234451295\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.13129035234451295  to: 0.13127150535583496\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.13127150535583496  to: 0.13125278949737548\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.13125278949737548  to: 0.13123421669006347\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.13123421669006347  to: 0.13121578693389893\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.13121578693389893  to: 0.13119747638702392\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.13119747638702392  to: 0.13117929697036743\n",
      "Training iteration: 508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13117929697036743  to: 0.13116123676300048\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.13116123676300048  to: 0.131143319606781\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.131143319606781  to: 0.131125545501709\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.131125545501709  to: 0.13110787868499757\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.13110787868499757  to: 0.1310903549194336\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.1310903549194336  to: 0.13107293844223022\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.13107293844223022  to: 0.13105566501617433\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.13105566501617433  to: 0.13103849887847902\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.13103849887847902  to: 0.13102145195007325\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.13102145195007325  to: 0.13100454807281495\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.13100454807281495  to: 0.13098775148391723\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.13098775148391723  to: 0.13097106218338012\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.13097106218338012  to: 0.1309545159339905\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.1309545159339905  to: 0.13093807697296142\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.13093807697296142  to: 0.13092173337936402\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.13092173337936402  to: 0.13090550899505615\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.13090550899505615  to: 0.1308894157409668\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.1308894157409668  to: 0.1308734178543091\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.1308734178543091  to: 0.130857515335083\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.130857515335083  to: 0.13084170818328858\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.13084170818328858  to: 0.13082598447799682\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.13082598447799682  to: 0.13081035614013672\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.13081035614013672  to: 0.13079483509063722\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.13079483509063722  to: 0.13077940940856933\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.13077940940856933  to: 0.13076407909393312\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.13076407909393312  to: 0.13074884414672852\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.13074884414672852  to: 0.1307344079017639\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.1307344079017639  to: 0.13072071075439454\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.13072071075439454  to: 0.1307076930999756\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.1307076930999756  to: 0.13069528341293335\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.13069528341293335  to: 0.130683434009552\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.130683434009552  to: 0.13067207336425782\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.13067207336425782  to: 0.13066114187240602\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.13066114187240602  to: 0.13065062761306762\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.13065062761306762  to: 0.13064045906066896\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.13064045906066896  to: 0.13063061237335205\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.13063061237335205  to: 0.13062102794647218\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.13062102794647218  to: 0.13061170578002929\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.13061170578002929  to: 0.13060259819030762\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.13060259819030762  to: 0.13059364557266234\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.13059364557266234  to: 0.13058485984802246\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.13058485984802246  to: 0.13057619333267212\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.13057619333267212  to: 0.13056762218475343\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.13056762218475343  to: 0.1305591344833374\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.1305591344833374  to: 0.13055071830749512\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.13055071830749512  to: 0.13054234981536866\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.13054234981536866  to: 0.13053399324417114\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.13053399324417114  to: 0.1305256485939026\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.1305256485939026  to: 0.130517315864563\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.130517315864563  to: 0.1305089473724365\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.1305089473724365  to: 0.13050055503845215\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.13050055503845215  to: 0.13049215078353882\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.13049215078353882  to: 0.13048371076583862\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.13048371076583862  to: 0.13047525882720948\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.13047525882720948  to: 0.13046672344207763\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.13046672344207763  to: 0.13045815229415894\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.13045815229415894  to: 0.1304495096206665\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.1304495096206665  to: 0.1304407835006714\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.1304407835006714  to: 0.13043202161788942\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.13043202161788942  to: 0.13042315244674682\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.13042315244674682  to: 0.13041422367095948\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.13041422367095948  to: 0.1304051995277405\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.1304051995277405  to: 0.13039608001708985\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.13039608001708985  to: 0.13038688898086548\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.13038688898086548  to: 0.13037760257720948\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.13037760257720948  to: 0.13036824464797975\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.13036824464797975  to: 0.1303587794303894\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.1303587794303894  to: 0.1303492307662964\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.1303492307662964  to: 0.13033891916275026\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.13033891916275026  to: 0.13032792806625365\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.13032792806625365  to: 0.13031631708145142\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.13031631708145142  to: 0.13030414581298827\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.13030414581298827  to: 0.13029147386550904\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.13029147386550904  to: 0.13027833700180053\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.13027833700180053  to: 0.1302647829055786\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.1302647829055786  to: 0.13025085926055907\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.13025085926055907  to: 0.1302366614341736\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.1302366614341736  to: 0.13022215366363527\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.13022215366363527  to: 0.13020739555358887\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.13020739555358887  to: 0.1301924467086792\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.1301924467086792  to: 0.13017730712890624\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.13017730712890624  to: 0.13016202449798583\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.13016202449798583  to: 0.13014662265777588\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.13014662265777588  to: 0.13013114929199218\n",
      "Training iteration: 592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.13013114929199218  to: 0.13011562824249268\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.13011562824249268  to: 0.1301000714302063\n",
      "Training iteration: 594\n",
      "Improved validation loss from: 0.1301000714302063  to: 0.13008445501327515\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.13008445501327515  to: 0.130068838596344\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.130068838596344  to: 0.13005319833755494\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.13005319833755494  to: 0.1300376057624817\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.1300376057624817  to: 0.1300220012664795\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.1300220012664795  to: 0.13000645637512206\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.13000645637512206  to: 0.12999094724655152\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.12999094724655152  to: 0.1299755096435547\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.1299755096435547  to: 0.12996011972427368\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12996011972427368  to: 0.1299448013305664\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.1299448013305664  to: 0.1299295425415039\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.1299295425415039  to: 0.12991440296173096\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12991440296173096  to: 0.12989933490753175\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.12989933490753175  to: 0.12988436222076416\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12988436222076416  to: 0.12986948490142822\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12986948490142822  to: 0.1298547148704529\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.1298547148704529  to: 0.12984004020690917\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12984004020690917  to: 0.1298254609107971\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.1298254609107971  to: 0.12981101274490356\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12981101274490356  to: 0.12979663610458375\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.12979663610458375  to: 0.1297823667526245\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.1297823667526245  to: 0.12976821660995483\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.12976821660995483  to: 0.12975417375564574\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12975417375564574  to: 0.12974023818969727\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12974023818969727  to: 0.12972643375396728\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.12972643375396728  to: 0.12971271276474\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12971271276474  to: 0.12969911098480225\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12969911098480225  to: 0.12968558073043823\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.12968558073043823  to: 0.12967216968536377\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.12967216968536377  to: 0.12965881824493408\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12965881824493408  to: 0.12964558601379395\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.12964558601379395  to: 0.12963249683380126\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.12963249683380126  to: 0.12961947917938232\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12961947917938232  to: 0.1296065330505371\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1296065330505371  to: 0.12959367036819458\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.12959367036819458  to: 0.12958085536956787\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.12958085536956787  to: 0.12956809997558594\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12956809997558594  to: 0.12955540418624878\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.12955540418624878  to: 0.12954280376434327\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12954280376434327  to: 0.12953025102615356\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12953025102615356  to: 0.12951772212982177\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12951772212982177  to: 0.12950525283813477\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.12950525283813477  to: 0.12949281930923462\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12949281930923462  to: 0.12948040962219237\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12948040962219237  to: 0.12946803569793702\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.12946803569793702  to: 0.12945567369461058\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.12945567369461058  to: 0.12944334745407104\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12944334745407104  to: 0.12943103313446044\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12943103313446044  to: 0.1294187307357788\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.1294187307357788  to: 0.12940645217895508\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12940645217895508  to: 0.1293941855430603\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.1293941855430603  to: 0.12938191890716552\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12938191890716552  to: 0.12936965227127076\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12936965227127076  to: 0.12935739755630493\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12935739755630493  to: 0.12934513092041017\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12934513092041017  to: 0.12933288812637328\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12933288812637328  to: 0.12932062149047852\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.12932062149047852  to: 0.12930835485458375\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12930835485458375  to: 0.12929607629776002\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12929607629776002  to: 0.1292837977409363\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.1292837977409363  to: 0.1292715072631836\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.1292715072631836  to: 0.12925920486450196\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12925920486450196  to: 0.12924690246582032\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12924690246582032  to: 0.12923457622528076\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12923457622528076  to: 0.12922226190567015\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12922226190567015  to: 0.12920989990234374\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12920989990234374  to: 0.12919752597808837\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12919752597808837  to: 0.12918516397476196\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12918516397476196  to: 0.12917276620864868\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12917276620864868  to: 0.1291603684425354\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.1291603684425354  to: 0.1291479229927063\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.1291479229927063  to: 0.1291354775428772\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.1291354775428772  to: 0.12912299633026122\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12912299633026122  to: 0.1291105031967163\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.1291105031967163  to: 0.1290980100631714\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.1290980100631714  to: 0.12908549308776857\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.12908549308776857  to: 0.12907296419143677\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12907296419143677  to: 0.12906038761138916\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12906038761138916  to: 0.12904782295227052\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.12904782295227052  to: 0.1290352463722229\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.1290352463722229  to: 0.12902263402938843\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.12902263402938843  to: 0.12901003360748292\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12901003360748292  to: 0.12899739742279054\n",
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12899739742279054  to: 0.12898476123809816\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12898476123809816  to: 0.1289721131324768\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.1289721131324768  to: 0.12895944118499755\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.12895944118499755  to: 0.1289467692375183\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.1289467692375183  to: 0.12893407344818114\n",
      "Training iteration: 682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12893407344818114  to: 0.12892136573791504\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.12892136573791504  to: 0.12890864610671998\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12890864610671998  to: 0.12889589071273805\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12889589071273805  to: 0.12888313531875611\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12888313531875611  to: 0.12887035608291625\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12887035608291625  to: 0.1288575530052185\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.1288575530052185  to: 0.1288447141647339\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.1288447141647339  to: 0.12883187532424928\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12883187532424928  to: 0.12881901264190673\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.12881901264190673  to: 0.12880613803863525\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12880613803863525  to: 0.12879326343536376\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12879326343536376  to: 0.1287803530693054\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.1287803530693054  to: 0.12876744270324708\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12876744270324708  to: 0.1287544846534729\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1287544846534729  to: 0.12874152660369872\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12874152660369872  to: 0.1287285327911377\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.1287285327911377  to: 0.12871551513671875\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12871551513671875  to: 0.1287024736404419\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.1287024736404419  to: 0.1286894202232361\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.1286894202232361  to: 0.12867635488510132\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.12867635488510132  to: 0.12866324186325073\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12866324186325073  to: 0.12865011692047118\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.12865011692047118  to: 0.12863695621490479\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12863695621490479  to: 0.12862375974655152\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12862375974655152  to: 0.12861053943634032\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12861053943634032  to: 0.12859729528427125\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12859729528427125  to: 0.12858402729034424\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12858402729034424  to: 0.12857067584991455\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.12857067584991455  to: 0.1285569667816162\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.1285569667816162  to: 0.1285432457923889\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.1285432457923889  to: 0.12852948904037476\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12852948904037476  to: 0.12851576805114745\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12851576805114745  to: 0.12850202322006227\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12850202322006227  to: 0.12848825454711915\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12848825454711915  to: 0.12847448587417604\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12847448587417604  to: 0.12846071720123292\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.12846071720123292  to: 0.12844688892364503\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.12844688892364503  to: 0.12843307256698608\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12843307256698608  to: 0.1284192442893982\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.1284192442893982  to: 0.12840540409088136\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12840540409088136  to: 0.12839152812957763\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12839152812957763  to: 0.12837762832641603\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12837762832641603  to: 0.1283637285232544\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.1283637285232544  to: 0.12834978103637695\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12834978103637695  to: 0.12833582162857055\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12833582162857055  to: 0.1283218264579773\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.1283218264579773  to: 0.12830781936645508\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12830781936645508  to: 0.128293776512146\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.128293776512146  to: 0.128279709815979\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.128279709815979  to: 0.1282655954360962\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.1282655954360962  to: 0.12825146913528443\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.12825146913528443  to: 0.12823729515075682\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12823729515075682  to: 0.12822310924530028\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.12822310924530028  to: 0.12820886373519896\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12820886373519896  to: 0.12819461822509765\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12819461822509765  to: 0.12818033695220948\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12818033695220948  to: 0.1281660318374634\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.1281660318374634  to: 0.12815167903900146\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12815167903900146  to: 0.1281373143196106\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.1281373143196106  to: 0.12812292575836182\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12812292575836182  to: 0.12810852527618408\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12810852527618408  to: 0.1280941128730774\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.1280941128730774  to: 0.1280796766281128\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.1280796766281128  to: 0.12806538343429566\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12806538343429566  to: 0.12805120944976806\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12805120944976806  to: 0.12803714275360106\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12803714275360106  to: 0.12802318334579468\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12802318334579468  to: 0.12800915241241456\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12800915241241456  to: 0.1279947876930237\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.1279947876930237  to: 0.12798057794570922\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12798057794570922  to: 0.1279665231704712\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.1279665231704712  to: 0.12795250415802\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12795250415802  to: 0.12793859243392944\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12793859243392944  to: 0.12792476415634155\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12792476415634155  to: 0.12791101932525634\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12791101932525634  to: 0.12789732217788696\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.12789732217788696  to: 0.1278836965560913\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.1278836965560913  to: 0.12787010669708251\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12787010669708251  to: 0.12785656452178956\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12785656452178956  to: 0.12784302234649658\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12784302234649658  to: 0.12782952785491944\n",
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12782952785491944  to: 0.12781604528427123\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12781604528427123  to: 0.12780253887176513\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12780253887176513  to: 0.12778903245925904\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12778903245925904  to: 0.12777551412582397\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12777551412582397  to: 0.12776197195053102\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12776197195053102  to: 0.12774816751480103\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12774816751480103  to: 0.12773425579071046\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12773425579071046  to: 0.1277202844619751\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.1277202844619751  to: 0.12770626544952393\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12770626544952393  to: 0.12769218683242797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12769218683242797  to: 0.1276780366897583\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.1276780366897583  to: 0.12766382694244385\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12766382694244385  to: 0.12764955759048463\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.12764955759048463  to: 0.12763521671295167\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12763521671295167  to: 0.12762076854705812\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12762076854705812  to: 0.12760629653930664\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12760629653930664  to: 0.12759171724319457\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.12759171724319457  to: 0.1275770664215088\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.1275770664215088  to: 0.1275623321533203\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.1275623321533203  to: 0.1275475263595581\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.1275475263595581  to: 0.12753263711929322\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12753263711929322  to: 0.1275176763534546\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.1275176763534546  to: 0.12750260829925536\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12750260829925536  to: 0.12748749256134034\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12748749256134034  to: 0.12747228145599365\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.12747228145599365  to: 0.12745697498321534\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12745697498321534  to: 0.1274416208267212\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.1274416208267212  to: 0.1274261713027954\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.1274261713027954  to: 0.12741063833236693\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12741063833236693  to: 0.12739503383636475\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12739503383636475  to: 0.12737936973571778\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12737936973571778  to: 0.12736358642578124\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12736358642578124  to: 0.12734781503677367\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12734781503677367  to: 0.12733198404312135\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12733198404312135  to: 0.12731609344482422\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12731609344482422  to: 0.12730010747909545\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12730010747909545  to: 0.12728404998779297\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12728404998779297  to: 0.12726792097091674\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.12726792097091674  to: 0.1272517442703247\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.1272517442703247  to: 0.12723548412323\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.12723548412323  to: 0.12721914052963257\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.12721914052963257  to: 0.12720273733139037\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.12720273733139037  to: 0.12718625068664552\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.12718625068664552  to: 0.12716972827911377\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.12716972827911377  to: 0.12715318202972412\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.12715318202972412  to: 0.12713661193847656\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.12713661193847656  to: 0.1271200180053711\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.1271200180053711  to: 0.1271033763885498\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.1271033763885498  to: 0.12708671092987062\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.12708671092987062  to: 0.12707009315490722\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.12707009315490722  to: 0.12705355882644653\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.12705355882644653  to: 0.12703707218170165\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.12703707218170165  to: 0.12702064514160155\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.12702064514160155  to: 0.12700424194335938\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.12700424194335938  to: 0.12698788642883302\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.12698788642883302  to: 0.12697153091430663\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.12697153091430663  to: 0.12695519924163817\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.12695519924163817  to: 0.12693886756896972\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.12693886756896972  to: 0.12692253589630126\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.12692253589630126  to: 0.1269061803817749\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.1269061803817749  to: 0.1268898367881775\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.1268898367881775  to: 0.12687346935272217\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.12687346935272217  to: 0.12685706615447997\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.12685706615447997  to: 0.12684062719345093\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.12684062719345093  to: 0.12682416439056396\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.12682416439056396  to: 0.12680765390396118\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.12680765390396118  to: 0.1267911195755005\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.1267911195755005  to: 0.12677457332611083\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.12677457332611083  to: 0.1267579674720764\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.1267579674720764  to: 0.12674134969711304\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.12674134969711304  to: 0.1267246961593628\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.1267246961593628  to: 0.1267080068588257\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.1267080068588257  to: 0.1266912579536438\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.1266912579536438  to: 0.12667446136474608\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.12667446136474608  to: 0.12665761709213258\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.12665761709213258  to: 0.1266407012939453\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.1266407012939453  to: 0.1266237497329712\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.1266237497329712  to: 0.12660672664642333\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.12660672664642333  to: 0.12658965587615967\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.12658965587615967  to: 0.12657251358032226\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.12657251358032226  to: 0.12655532360076904\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.12655532360076904  to: 0.12653805017471315\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.12653805017471315  to: 0.1265207290649414\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.1265207290649414  to: 0.1265033721923828\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.1265033721923828  to: 0.12648595571517945\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.12648595571517945  to: 0.12646808624267578\n",
      "Training iteration: 849\n",
      "Improved validation loss from: 0.12646808624267578  to: 0.1264500856399536\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.1264500856399536  to: 0.12643203735351563\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.12643203735351563  to: 0.126413893699646\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.126413893699646  to: 0.12639569044113158\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.12639569044113158  to: 0.1263774275779724\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.1263774275779724  to: 0.12635910511016846\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.12635910511016846  to: 0.12634069919586183\n",
      "Training iteration: 856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12634069919586183  to: 0.12632224559783936\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.12632224559783936  to: 0.12630373239517212\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.12630373239517212  to: 0.1262851595878601\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.1262851595878601  to: 0.12626651525497437\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.12626651525497437  to: 0.12624781131744384\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.12624781131744384  to: 0.12622905969619752\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.12622905969619752  to: 0.1262102246284485\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.1262102246284485  to: 0.12619131803512573\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.12619131803512573  to: 0.12617237567901612\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.12617237567901612  to: 0.12615336179733277\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.12615336179733277  to: 0.1261343002319336\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.1261343002319336  to: 0.1261151671409607\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.1261151671409607  to: 0.12609598636627198\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.12609598636627198  to: 0.1260767698287964\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.1260767698287964  to: 0.1260574698448181\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.1260574698448181  to: 0.12603814601898194\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.12603814601898194  to: 0.12601873874664307\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.12601873874664307  to: 0.12599929571151733\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.12599929571151733  to: 0.12597980499267578\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.12597980499267578  to: 0.12596025466918945\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.12596025466918945  to: 0.12594066858291625\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.12594066858291625  to: 0.12592103481292724\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.12592103481292724  to: 0.12590136528015136\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.12590136528015136  to: 0.1258816361427307\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.1258816361427307  to: 0.12586187124252318\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.12586187124252318  to: 0.12584205865859985\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.12584205865859985  to: 0.12582221031188964\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.12582221031188964  to: 0.12580232620239257\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.12580232620239257  to: 0.12578239440917968\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.12578239440917968  to: 0.12576242685317993\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.12576242685317993  to: 0.12574241161346436\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.12574241161346436  to: 0.12572237253189086\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.12572237253189086  to: 0.12570230960845946\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.12570230960845946  to: 0.12568211555480957\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.12568211555480957  to: 0.12566152811050416\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.12566152811050416  to: 0.12564094066619874\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.12564094066619874  to: 0.1256203532218933\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.1256203532218933  to: 0.12559993267059327\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.12559993267059327  to: 0.12557952404022216\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.12557952404022216  to: 0.1255590796470642\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.1255590796470642  to: 0.12553865909576417\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.12553865909576417  to: 0.1255182147026062\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.1255182147026062  to: 0.1254977822303772\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.1254977822303772  to: 0.12547733783721923\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.12547733783721923  to: 0.12545688152313234\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.12545688152313234  to: 0.1254364252090454\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.1254364252090454  to: 0.12541595697402955\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.12541595697402955  to: 0.12539546489715575\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.12539546489715575  to: 0.12537496089935302\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.12537496089935302  to: 0.12535445690155028\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.12535445690155028  to: 0.12533392906188964\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.12533392906188964  to: 0.1253133773803711\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.1253133773803711  to: 0.12529280185699462\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.12529280185699462  to: 0.12527220249176024\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.12527220249176024  to: 0.12525155544281005\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.12525155544281005  to: 0.12523090839385986\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.12523090839385986  to: 0.1252102255821228\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.1252102255821228  to: 0.12518950700759887\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.12518950700759887  to: 0.12516876459121704\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.12516876459121704  to: 0.12514798641204833\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.12514798641204833  to: 0.12512717247009278\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.12512717247009278  to: 0.1251063346862793\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.1251063346862793  to: 0.12508546113967894\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.12508546113967894  to: 0.12506455183029175\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.12506455183029175  to: 0.12504360675811768\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.12504360675811768  to: 0.12502266168594361\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.12502266168594361  to: 0.12500168085098268\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.12500168085098268  to: 0.12498066425323487\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.12498066425323487  to: 0.12495958805084229\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.12495958805084229  to: 0.1249385118484497\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.1249385118484497  to: 0.12491739988327026\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.12491739988327026  to: 0.12489627599716187\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.12489627599716187  to: 0.12487517595291138\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.12487517595291138  to: 0.12485412359237671\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.12485412359237671  to: 0.124833083152771\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.124833083152771  to: 0.1248120903968811\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.1248120903968811  to: 0.12479110956192016\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.12479110956192016  to: 0.12477012872695922\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.12477012872695922  to: 0.12474915981292725\n",
      "Training iteration: 935\n",
      "Improved validation loss from: 0.12474915981292725  to: 0.12472817897796631\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.12472817897796631  to: 0.12470718622207641\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.12470718622207641  to: 0.12468616962432862\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.12468616962432862  to: 0.1246645450592041\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.1246645450592041  to: 0.12464257478713989\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.12464257478713989  to: 0.12462055683135986\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.12462055683135986  to: 0.12459847927093506\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.12459847927093506  to: 0.12457635402679443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 943\n",
      "Improved validation loss from: 0.12457635402679443  to: 0.12455416917800903\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.12455416917800903  to: 0.1245319128036499\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.1245319128036499  to: 0.12450957298278809\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.12450957298278809  to: 0.12448714971542359\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.12448714971542359  to: 0.12446466684341431\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.12446466684341431  to: 0.12444210052490234\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.12444210052490234  to: 0.12441942691802979\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.12441942691802979  to: 0.12439664602279663\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.12439664602279663  to: 0.12437379360198975\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.12437379360198975  to: 0.12435081005096435\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.12435081005096435  to: 0.12432775497436524\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.12432775497436524  to: 0.12430460453033447\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.12430460453033447  to: 0.12428138256072999\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.12428138256072999  to: 0.12425807714462281\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.12425807714462281  to: 0.12423468828201294\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.12423468828201294  to: 0.12421122789382935\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.12421122789382935  to: 0.12418768405914307\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.12418768405914307  to: 0.12416408061981202\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.12416408061981202  to: 0.12414038181304932\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.12414038181304932  to: 0.12411662340164184\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.12411662340164184  to: 0.1240928053855896\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.1240928053855896  to: 0.12406892776489258\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.12406892776489258  to: 0.12404496669769287\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.12404496669769287  to: 0.12402099370956421\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.12402099370956421  to: 0.12399699687957763\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.12399699687957763  to: 0.12397301197052002\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.12397301197052002  to: 0.12394900321960449\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.12394900321960449  to: 0.12392500638961793\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.12392500638961793  to: 0.12390105724334717\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.12390105724334717  to: 0.12387712001800537\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.12387712001800537  to: 0.12385320663452148\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.12385320663452148  to: 0.12382930517196655\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.12382930517196655  to: 0.12380540370941162\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.12380540370941162  to: 0.12378151416778564\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.12378151416778564  to: 0.12375762462615966\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.12375762462615966  to: 0.1237337827682495\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.1237337827682495  to: 0.12370990514755249\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.12370990514755249  to: 0.12368605136871338\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.12368605136871338  to: 0.12366218566894531\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.12366218566894531  to: 0.12363831996917725\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.12363831996917725  to: 0.12361443042755127\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.12361443042755127  to: 0.12359052896499634\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.12359052896499634  to: 0.1235666036605835\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.1235666036605835  to: 0.1235426425933838\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.1235426425933838  to: 0.12351868152618409\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.12351868152618409  to: 0.12349447011947631\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.12349447011947631  to: 0.12346971035003662\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.12346971035003662  to: 0.12344496250152588\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.12344496250152588  to: 0.12342020273208618\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.12342020273208618  to: 0.12339543104171753\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.12339543104171753  to: 0.12337062358856202\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.12337062358856202  to: 0.12334578037261963\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.12334578037261963  to: 0.12332093715667725\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.12332093715667725  to: 0.12329608201980591\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.12329608201980591  to: 0.12327121496200562\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.12327121496200562  to: 0.12324634790420533\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.12324634790420533  to: 0.12322146892547607\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.12322146892547607  to: 0.12319657802581788\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.12319657802581788  to: 0.12317168712615967\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.12317168712615967  to: 0.12314674854278565\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.12314674854278565  to: 0.12312178611755371\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.12312178611755371  to: 0.12309681177139283\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.12309681177139283  to: 0.12307183742523194\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.12307183742523194  to: 0.12304686307907105\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.12304686307907105  to: 0.12302190065383911\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.12302190065383911  to: 0.12299695014953613\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.12299695014953613  to: 0.12297201156616211\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.12297201156616211  to: 0.12294707298278809\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.12294707298278809  to: 0.12292211055755616\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.12292211055755616  to: 0.12289717197418212\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.12289717197418212  to: 0.1228722333908081\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.1228722333908081  to: 0.12284733057022094\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.12284733057022094  to: 0.12282241582870483\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.12282241582870483  to: 0.12279753684997559\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.12279753684997559  to: 0.12277264595031738\n",
      "Training iteration: 1018\n",
      "Improved validation loss from: 0.12277264595031738  to: 0.12274774312973022\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.12274774312973022  to: 0.12272284030914307\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.12272284030914307  to: 0.12269791364669799\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.12269791364669799  to: 0.12267299890518188\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.12267299890518188  to: 0.12264807224273681\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.12264807224273681  to: 0.1226231575012207\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.1226231575012207  to: 0.12259821891784668\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.12259821891784668  to: 0.12257330417633057\n",
      "Training iteration: 1026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12257330417633057  to: 0.12254831790924073\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.12254831790924073  to: 0.12252330780029297\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.12252330780029297  to: 0.1224982738494873\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.1224982738494873  to: 0.12247321605682374\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.12247321605682374  to: 0.1224481463432312\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.1224481463432312  to: 0.12242305278778076\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.12242305278778076  to: 0.12239792346954345\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.12239792346954345  to: 0.12237275838851928\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.12237275838851928  to: 0.12234756946563721\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.12234756946563721  to: 0.1223223328590393\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.1223223328590393  to: 0.12229706048965454\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.12229706048965454  to: 0.1222721815109253\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.1222721815109253  to: 0.12224767208099366\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.12224767208099366  to: 0.12222347259521485\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.12222347259521485  to: 0.12219951152801514\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.12219951152801514  to: 0.12217578887939454\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.12217578887939454  to: 0.12215219736099243\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.12215219736099243  to: 0.12212874889373779\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.12212874889373779  to: 0.1221053957939148\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.1221053957939148  to: 0.12208211421966553\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.12208211421966553  to: 0.12205889225006103\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.12205889225006103  to: 0.1220356822013855\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.1220356822013855  to: 0.12201249599456787\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.12201249599456787  to: 0.12198930978775024\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.12198930978775024  to: 0.12196612358093262\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.12196612358093262  to: 0.12194290161132812\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.12194290161132812  to: 0.12191964387893676\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.12191964387893676  to: 0.12189629077911376\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.12189629077911376  to: 0.12187283039093018\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.12187283039093018  to: 0.12184926271438598\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.12184926271438598  to: 0.12182559967041015\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.12182559967041015  to: 0.12180181741714477\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.12180181741714477  to: 0.1217779278755188\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.1217779278755188  to: 0.12175390720367432\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.12175390720367432  to: 0.12172976732254029\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.12172976732254029  to: 0.12170546054840088\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.12170546054840088  to: 0.12168099880218505\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.12168099880218505  to: 0.12165639400482178\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.12165639400482178  to: 0.12163164615631103\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.12163164615631103  to: 0.12160676717758179\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.12160676717758179  to: 0.1215817928314209\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.1215817928314209  to: 0.12155666351318359\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.12155666351318359  to: 0.12153141498565674\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.12153141498565674  to: 0.1215059995651245\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.1215059995651245  to: 0.1214805006980896\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.1214805006980896  to: 0.12145487070083619\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.12145487070083619  to: 0.12142913341522217\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.12142913341522217  to: 0.12140331268310547\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.12140331268310547  to: 0.12137734889984131\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.12137734889984131  to: 0.12135131359100342\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.12135131359100342  to: 0.12132517099380494\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.12132517099380494  to: 0.12129894495010377\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.12129894495010377  to: 0.12127265930175782\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.12127265930175782  to: 0.12124626636505127\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.12124626636505127  to: 0.12121977806091308\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.12121977806091308  to: 0.12119321823120117\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.12119321823120117  to: 0.12116668224334717\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.12116668224334717  to: 0.12114012241363525\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.12114012241363525  to: 0.12111355066299438\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.12111355066299438  to: 0.12108691930770873\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.12108691930770873  to: 0.121060311794281\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.121060311794281  to: 0.1210336685180664\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.1210336685180664  to: 0.12100697755813598\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.12100697755813598  to: 0.12098026275634766\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.12098026275634766  to: 0.12095351219177246\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.12095351219177246  to: 0.12092670202255248\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.12092670202255248  to: 0.12090002298355103\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.12090002298355103  to: 0.12087335586547851\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.12087335586547851  to: 0.12084667682647705\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.12084667682647705  to: 0.12082005739212036\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.12082005739212036  to: 0.12079341411590576\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.12079341411590576  to: 0.12076675891876221\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.12076675891876221  to: 0.12073988914489746\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.12073988914489746  to: 0.12071278095245361\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.12071278095245361  to: 0.12068545818328857\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.12068545818328857  to: 0.12065794467926025\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.12065794467926025  to: 0.12063024044036866\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.12063024044036866  to: 0.12060235738754273\n",
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.12060235738754273  to: 0.12057430744171142\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.12057430744171142  to: 0.12054610252380371\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.12054610252380371  to: 0.12051775455474853\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.12051775455474853  to: 0.12048934698104859\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.12048934698104859  to: 0.12046095132827758\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.12046095132827758  to: 0.12043251991271972\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.12043251991271972  to: 0.12040396928787231\n",
      "Training iteration: 1111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12040396928787231  to: 0.12037527561187744\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.12037527561187744  to: 0.12034646272659302\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.12034646272659302  to: 0.12031753063201904\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.12031753063201904  to: 0.1202884316444397\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.1202884316444397  to: 0.12025927305221558\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.12025927305221558  to: 0.12023007869720459\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.12023007869720459  to: 0.12020068168640137\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.12020068168640137  to: 0.12017112970352173\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.12017112970352173  to: 0.12014144659042358\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.12014144659042358  to: 0.12011184692382812\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.12011184692382812  to: 0.12008230686187744\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.12008230686187744  to: 0.12005269527435303\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.12005269527435303  to: 0.12002302408218384\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.12002302408218384  to: 0.11999324560165406\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.11999324560165406  to: 0.11996339559555054\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.11996339559555054  to: 0.11993350982666015\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.11993350982666015  to: 0.11990373134613037\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.11990373134613037  to: 0.11987407207489013\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.11987407207489013  to: 0.11984450817108154\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.11984450817108154  to: 0.11981484889984131\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.11981484889984131  to: 0.11978505849838257\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.11978505849838257  to: 0.119755220413208\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.119755220413208  to: 0.11972529888153076\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.11972529888153076  to: 0.11969527006149291\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.11969527006149291  to: 0.11966519355773926\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.11966519355773926  to: 0.11963521242141724\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.11963521242141724  to: 0.11960535049438477\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.11960535049438477  to: 0.11957554817199707\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.11957554817199707  to: 0.119545578956604\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.119545578956604  to: 0.11951541900634766\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.11951541900634766  to: 0.11948509216308593\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.11948509216308593  to: 0.11945457458496093\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.11945457458496093  to: 0.11942393779754638\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.11942393779754638  to: 0.11939332485198975\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.11939332485198975  to: 0.11936277151107788\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.11936277151107788  to: 0.11933218240737915\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.11933218240737915  to: 0.1193014144897461\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.1193014144897461  to: 0.11927047967910767\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.11927047967910767  to: 0.11923938989639282\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.11923938989639282  to: 0.11920821666717529\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.11920821666717529  to: 0.11917697191238404\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.11917697191238404  to: 0.1191457986831665\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.1191457986831665  to: 0.11911468505859375\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.11911468505859375  to: 0.11908342838287353\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.11908342838287353  to: 0.11905205249786377\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.11905205249786377  to: 0.11902073621749878\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.11902073621749878  to: 0.1189893364906311\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.1189893364906311  to: 0.11895784139633178\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.11895784139633178  to: 0.11892637014389038\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.11892637014389038  to: 0.11889492273330689\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.11889492273330689  to: 0.11886335611343384\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.11886335611343384  to: 0.11883164644241333\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.11883164644241333  to: 0.11879980564117432\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.11879980564117432  to: 0.1187678337097168\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.1187678337097168  to: 0.11873588562011719\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.11873588562011719  to: 0.11870396137237549\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.11870396137237549  to: 0.11867210865020753\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.11867210865020753  to: 0.11864011287689209\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.11864011287689209  to: 0.1186079740524292\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.1186079740524292  to: 0.11857564449310302\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.11857564449310302  to: 0.11854317188262939\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.11854317188262939  to: 0.11851060390472412\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.11851060390472412  to: 0.1184781551361084\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.1184781551361084  to: 0.11844576597213745\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.11844576597213745  to: 0.11841342449188233\n",
      "Training iteration: 1176\n",
      "Improved validation loss from: 0.11841342449188233  to: 0.1183809518814087\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.1183809518814087  to: 0.11834838390350341\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.11834838390350341  to: 0.11831568479537964\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.11831568479537964  to: 0.11828283071517945\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.11828283071517945  to: 0.11825002431869507\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.11825002431869507  to: 0.1182172417640686\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.1182172417640686  to: 0.11818450689315796\n",
      "Training iteration: 1183\n",
      "Improved validation loss from: 0.11818450689315796  to: 0.11815159320831299\n",
      "Training iteration: 1184\n",
      "Improved validation loss from: 0.11815159320831299  to: 0.11811847686767578\n",
      "Training iteration: 1185\n",
      "Improved validation loss from: 0.11811847686767578  to: 0.1180851936340332\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.1180851936340332  to: 0.11805167198181152\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.11805167198181152  to: 0.11801798343658447\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.11801798343658447  to: 0.11798427104949952\n",
      "Training iteration: 1189\n",
      "Improved validation loss from: 0.11798427104949952  to: 0.1179506540298462\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.1179506540298462  to: 0.11791712045669556\n",
      "Training iteration: 1191\n",
      "Improved validation loss from: 0.11791712045669556  to: 0.11788156032562255\n",
      "Training iteration: 1192\n",
      "Improved validation loss from: 0.11788156032562255  to: 0.11784396171569825\n",
      "Training iteration: 1193\n",
      "Improved validation loss from: 0.11784396171569825  to: 0.11780459880828857\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.11780459880828857  to: 0.11776374578475952\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.11776374578475952  to: 0.11772167682647705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.11772167682647705  to: 0.11767871379852295\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.11767871379852295  to: 0.11763503551483154\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.11763503551483154  to: 0.1175911545753479\n",
      "Training iteration: 1199\n",
      "Improved validation loss from: 0.1175911545753479  to: 0.11754728555679321\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.11754728555679321  to: 0.1175035834312439\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.1175035834312439  to: 0.11746017932891846\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.11746017932891846  to: 0.11741691827774048\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.11741691827774048  to: 0.11737390756607055\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.11737390756607055  to: 0.11733126640319824\n",
      "Training iteration: 1205\n",
      "Improved validation loss from: 0.11733126640319824  to: 0.11728988885879517\n",
      "Training iteration: 1206\n",
      "Improved validation loss from: 0.11728988885879517  to: 0.11724973917007446\n",
      "Training iteration: 1207\n",
      "Improved validation loss from: 0.11724973917007446  to: 0.1172107219696045\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.1172107219696045  to: 0.11717278957366943\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.11717278957366943  to: 0.11713585853576661\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.11713585853576661  to: 0.11709998846054077\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.11709998846054077  to: 0.11706504821777344\n",
      "Training iteration: 1212\n",
      "Improved validation loss from: 0.11706504821777344  to: 0.11703087091445923\n",
      "Training iteration: 1213\n",
      "Improved validation loss from: 0.11703087091445923  to: 0.11699707508087158\n",
      "Training iteration: 1214\n",
      "Improved validation loss from: 0.11699707508087158  to: 0.11696352958679199\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.11696352958679199  to: 0.11693007946014404\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.11693007946014404  to: 0.11689679622650147\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.11689679622650147  to: 0.11686408519744873\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.11686408519744873  to: 0.11683142185211182\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.11683142185211182  to: 0.11679861545562745\n",
      "Training iteration: 1220\n",
      "Improved validation loss from: 0.11679861545562745  to: 0.11676560640335083\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.11676560640335083  to: 0.11673202514648437\n",
      "Training iteration: 1222\n",
      "Improved validation loss from: 0.11673202514648437  to: 0.11669788360595704\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.11669788360595704  to: 0.11666316986083984\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.11666316986083984  to: 0.11662788391113281\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.11662788391113281  to: 0.1165920376777649\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.1165920376777649  to: 0.11655590534210206\n",
      "Training iteration: 1227\n",
      "Improved validation loss from: 0.11655590534210206  to: 0.11651952266693115\n",
      "Training iteration: 1228\n",
      "Improved validation loss from: 0.11651952266693115  to: 0.11648292541503906\n",
      "Training iteration: 1229\n",
      "Improved validation loss from: 0.11648292541503906  to: 0.11644587516784669\n",
      "Training iteration: 1230\n",
      "Improved validation loss from: 0.11644587516784669  to: 0.11640843152999877\n",
      "Training iteration: 1231\n",
      "Improved validation loss from: 0.11640843152999877  to: 0.11637084484100342\n",
      "Training iteration: 1232\n",
      "Improved validation loss from: 0.11637084484100342  to: 0.11633380651473998\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.11633380651473998  to: 0.11629674434661866\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.11629674434661866  to: 0.116259765625\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.116259765625  to: 0.1162225842475891\n",
      "Training iteration: 1236\n",
      "Improved validation loss from: 0.1162225842475891  to: 0.1161852478981018\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.1161852478981018  to: 0.11614798307418824\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.11614798307418824  to: 0.11611047983169556\n",
      "Training iteration: 1239\n",
      "Improved validation loss from: 0.11611047983169556  to: 0.1160728931427002\n",
      "Training iteration: 1240\n",
      "Improved validation loss from: 0.1160728931427002  to: 0.11603553295135498\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.11603553295135498  to: 0.11599833965301513\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.11599833965301513  to: 0.11596094369888306\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.11596094369888306  to: 0.11592327356338501\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.11592327356338501  to: 0.11588542461395264\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.11588542461395264  to: 0.11584718227386474\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.11584718227386474  to: 0.1158089280128479\n",
      "Training iteration: 1247\n",
      "Improved validation loss from: 0.1158089280128479  to: 0.11577063798904419\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.11577063798904419  to: 0.11573215723037719\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.11573215723037719  to: 0.11569318771362305\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.11569318771362305  to: 0.115653657913208\n",
      "Training iteration: 1251\n",
      "Improved validation loss from: 0.115653657913208  to: 0.11561353206634521\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.11561353206634521  to: 0.11557282209396362\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.11557282209396362  to: 0.1155318021774292\n",
      "Training iteration: 1254\n",
      "Improved validation loss from: 0.1155318021774292  to: 0.11549087762832641\n",
      "Training iteration: 1255\n",
      "Improved validation loss from: 0.11549087762832641  to: 0.11545000076293946\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.11545000076293946  to: 0.11540911197662354\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.11540911197662354  to: 0.11536790132522583\n",
      "Training iteration: 1258\n",
      "Improved validation loss from: 0.11536790132522583  to: 0.1153259515762329\n",
      "Training iteration: 1259\n",
      "Improved validation loss from: 0.1153259515762329  to: 0.11528337001800537\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.11528337001800537  to: 0.11523239612579346\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.11523239612579346  to: 0.11518218517303466\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.11518218517303466  to: 0.11513335704803467\n",
      "Training iteration: 1263\n",
      "Improved validation loss from: 0.11513335704803467  to: 0.11508327722549438\n",
      "Training iteration: 1264\n",
      "Improved validation loss from: 0.11508327722549438  to: 0.11503236293792725\n",
      "Training iteration: 1265\n",
      "Improved validation loss from: 0.11503236293792725  to: 0.11498057842254639\n",
      "Training iteration: 1266\n",
      "Improved validation loss from: 0.11498057842254639  to: 0.11492053270339966\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.11492053270339966  to: 0.11486167907714843\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.11486167907714843  to: 0.1148044228553772\n",
      "Training iteration: 1269\n",
      "Improved validation loss from: 0.1148044228553772  to: 0.11474862098693847\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.11474862098693847  to: 0.11469440460205078\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.11469440460205078  to: 0.11463420391082764\n",
      "Training iteration: 1272\n",
      "Improved validation loss from: 0.11463420391082764  to: 0.11457685232162476\n",
      "Training iteration: 1273\n",
      "Improved validation loss from: 0.11457685232162476  to: 0.11452277898788452\n",
      "Training iteration: 1274\n",
      "Improved validation loss from: 0.11452277898788452  to: 0.11446430683135986\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.11446430683135986  to: 0.11440223455429077\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.11440223455429077  to: 0.11434497833251953\n",
      "Training iteration: 1277\n",
      "Improved validation loss from: 0.11434497833251953  to: 0.11428468227386475\n",
      "Training iteration: 1278\n",
      "Improved validation loss from: 0.11428468227386475  to: 0.1142221212387085\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.1142221212387085  to: 0.11416573524475097\n",
      "Training iteration: 1280\n",
      "Improved validation loss from: 0.11416573524475097  to: 0.11411552429199219\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.11411552429199219  to: 0.11406347751617432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.11406347751617432  to: 0.11400949954986572\n",
      "Training iteration: 1283\n",
      "Improved validation loss from: 0.11400949954986572  to: 0.11395399570465088\n",
      "Training iteration: 1284\n",
      "Improved validation loss from: 0.11395399570465088  to: 0.11389815807342529\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.11389815807342529  to: 0.11384246349334717\n",
      "Training iteration: 1286\n",
      "Improved validation loss from: 0.11384246349334717  to: 0.11378719806671142\n",
      "Training iteration: 1287\n",
      "Improved validation loss from: 0.11378719806671142  to: 0.11373275518417358\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.11373275518417358  to: 0.11367920637130738\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.11367920637130738  to: 0.11362680196762084\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.11362680196762084  to: 0.11357614994049073\n",
      "Training iteration: 1291\n",
      "Improved validation loss from: 0.11357614994049073  to: 0.11352699995040894\n",
      "Training iteration: 1292\n",
      "Improved validation loss from: 0.11352699995040894  to: 0.11347968578338623\n",
      "Training iteration: 1293\n",
      "Improved validation loss from: 0.11347968578338623  to: 0.11343421936035156\n",
      "Training iteration: 1294\n",
      "Improved validation loss from: 0.11343421936035156  to: 0.11339011192321777\n",
      "Training iteration: 1295\n",
      "Improved validation loss from: 0.11339011192321777  to: 0.11334718465805053\n",
      "Training iteration: 1296\n",
      "Improved validation loss from: 0.11334718465805053  to: 0.11330547332763671\n",
      "Training iteration: 1297\n",
      "Improved validation loss from: 0.11330547332763671  to: 0.1132646918296814\n",
      "Training iteration: 1298\n",
      "Improved validation loss from: 0.1132646918296814  to: 0.1132242202758789\n",
      "Training iteration: 1299\n",
      "Improved validation loss from: 0.1132242202758789  to: 0.11318384408950806\n",
      "Training iteration: 1300\n",
      "Improved validation loss from: 0.11318384408950806  to: 0.11314359903335572\n",
      "Training iteration: 1301\n",
      "Improved validation loss from: 0.11314359903335572  to: 0.11310336589813233\n",
      "Training iteration: 1302\n",
      "Improved validation loss from: 0.11310336589813233  to: 0.11306257247924804\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.11306257247924804  to: 0.1130213975906372\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.1130213975906372  to: 0.11297967433929443\n",
      "Training iteration: 1305\n",
      "Improved validation loss from: 0.11297967433929443  to: 0.11293697357177734\n",
      "Training iteration: 1306\n",
      "Improved validation loss from: 0.11293697357177734  to: 0.11289317607879638\n",
      "Training iteration: 1307\n",
      "Improved validation loss from: 0.11289317607879638  to: 0.11284871101379394\n",
      "Training iteration: 1308\n",
      "Improved validation loss from: 0.11284871101379394  to: 0.11280368566513062\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.11280368566513062  to: 0.11275814771652222\n",
      "Training iteration: 1310\n",
      "Improved validation loss from: 0.11275814771652222  to: 0.11271178722381592\n",
      "Training iteration: 1311\n",
      "Improved validation loss from: 0.11271178722381592  to: 0.11266491413116456\n",
      "Training iteration: 1312\n",
      "Improved validation loss from: 0.11266491413116456  to: 0.11261801719665528\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.11261801719665528  to: 0.11257247924804688\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.11257247924804688  to: 0.11252810955047607\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.11252810955047607  to: 0.1124842882156372\n",
      "Training iteration: 1316\n",
      "Improved validation loss from: 0.1124842882156372  to: 0.11244074106216431\n",
      "Training iteration: 1317\n",
      "Improved validation loss from: 0.11244074106216431  to: 0.11239731311798096\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.11239731311798096  to: 0.11235432624816895\n",
      "Training iteration: 1319\n",
      "Improved validation loss from: 0.11235432624816895  to: 0.11231158971786499\n",
      "Training iteration: 1320\n",
      "Improved validation loss from: 0.11231158971786499  to: 0.11226880550384521\n",
      "Training iteration: 1321\n",
      "Improved validation loss from: 0.11226880550384521  to: 0.11222579479217529\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.11222579479217529  to: 0.11218218803405762\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.11218218803405762  to: 0.11213716268539428\n",
      "Training iteration: 1324\n",
      "Improved validation loss from: 0.11213716268539428  to: 0.11209065914154052\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.11209065914154052  to: 0.11204273700714111\n",
      "Training iteration: 1326\n",
      "Improved validation loss from: 0.11204273700714111  to: 0.11199409961700439\n",
      "Training iteration: 1327\n",
      "Improved validation loss from: 0.11199409961700439  to: 0.11194479465484619\n",
      "Training iteration: 1328\n",
      "Improved validation loss from: 0.11194479465484619  to: 0.1118949294090271\n",
      "Training iteration: 1329\n",
      "Improved validation loss from: 0.1118949294090271  to: 0.11184457540512086\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.11184457540512086  to: 0.1117932677268982\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.1117932677268982  to: 0.11174123287200928\n",
      "Training iteration: 1332\n",
      "Improved validation loss from: 0.11174123287200928  to: 0.11168935298919677\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.11168935298919677  to: 0.11163374185562133\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.11163374185562133  to: 0.11157490015029907\n",
      "Training iteration: 1335\n",
      "Improved validation loss from: 0.11157490015029907  to: 0.11151335239410401\n",
      "Training iteration: 1336\n",
      "Improved validation loss from: 0.11151335239410401  to: 0.11144914627075195\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.11144914627075195  to: 0.11138306856155396\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.11138306856155396  to: 0.11131589412689209\n",
      "Training iteration: 1339\n",
      "Improved validation loss from: 0.11131589412689209  to: 0.11124899387359619\n",
      "Training iteration: 1340\n",
      "Improved validation loss from: 0.11124899387359619  to: 0.1111828327178955\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.1111828327178955  to: 0.11111758947372437\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.11111758947372437  to: 0.11105273962020874\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.11105273962020874  to: 0.11098836660385132\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.11098836660385132  to: 0.11092466115951538\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.11092466115951538  to: 0.11086227893829345\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.11086227893829345  to: 0.11080105304718017\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.11080105304718017  to: 0.11074062585830688\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.11074062585830688  to: 0.1106798768043518\n",
      "Training iteration: 1349\n",
      "Improved validation loss from: 0.1106798768043518  to: 0.11061723232269287\n",
      "Training iteration: 1350\n",
      "Improved validation loss from: 0.11061723232269287  to: 0.1105528473854065\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.1105528473854065  to: 0.11048774719238282\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.11048774719238282  to: 0.11042231321334839\n",
      "Training iteration: 1353\n",
      "Improved validation loss from: 0.11042231321334839  to: 0.11035664081573486\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.11035664081573486  to: 0.11029075384140015\n",
      "Training iteration: 1355\n",
      "Improved validation loss from: 0.11029075384140015  to: 0.1102246642112732\n",
      "Training iteration: 1356\n",
      "Improved validation loss from: 0.1102246642112732  to: 0.11015737056732178\n",
      "Training iteration: 1357\n",
      "Improved validation loss from: 0.11015737056732178  to: 0.11008918285369873\n",
      "Training iteration: 1358\n",
      "Improved validation loss from: 0.11008918285369873  to: 0.11002147197723389\n",
      "Training iteration: 1359\n",
      "Improved validation loss from: 0.11002147197723389  to: 0.10995458364486695\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.10995458364486695  to: 0.10988881587982177\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.10988881587982177  to: 0.10982339382171631\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.10982339382171631  to: 0.10975860357284546\n",
      "Training iteration: 1363\n",
      "Improved validation loss from: 0.10975860357284546  to: 0.10969570875167847\n",
      "Training iteration: 1364\n",
      "Improved validation loss from: 0.10969570875167847  to: 0.10963479280471802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1365\n",
      "Improved validation loss from: 0.10963479280471802  to: 0.10957545042037964\n",
      "Training iteration: 1366\n",
      "Improved validation loss from: 0.10957545042037964  to: 0.10951728820800781\n",
      "Training iteration: 1367\n",
      "Improved validation loss from: 0.10951728820800781  to: 0.10945875644683838\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.10945875644683838  to: 0.10939962863922119\n",
      "Training iteration: 1369\n",
      "Improved validation loss from: 0.10939962863922119  to: 0.10934098958969116\n",
      "Training iteration: 1370\n",
      "Improved validation loss from: 0.10934098958969116  to: 0.1092827558517456\n",
      "Training iteration: 1371\n",
      "Improved validation loss from: 0.1092827558517456  to: 0.10922485589981079\n",
      "Training iteration: 1372\n",
      "Improved validation loss from: 0.10922485589981079  to: 0.10916588306427003\n",
      "Training iteration: 1373\n",
      "Improved validation loss from: 0.10916588306427003  to: 0.10910708904266357\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.10910708904266357  to: 0.10904842615127563\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.10904842615127563  to: 0.10898971557617188\n",
      "Training iteration: 1376\n",
      "Improved validation loss from: 0.10898971557617188  to: 0.10892961025238038\n",
      "Training iteration: 1377\n",
      "Improved validation loss from: 0.10892961025238038  to: 0.10886945724487304\n",
      "Training iteration: 1378\n",
      "Improved validation loss from: 0.10886945724487304  to: 0.1088093638420105\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.1088093638420105  to: 0.10874931812286377\n",
      "Training iteration: 1380\n",
      "Improved validation loss from: 0.10874931812286377  to: 0.1086878776550293\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.1086878776550293  to: 0.10862669944763184\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.10862669944763184  to: 0.10856584310531617\n",
      "Training iteration: 1383\n",
      "Improved validation loss from: 0.10856584310531617  to: 0.10850402116775512\n",
      "Training iteration: 1384\n",
      "Improved validation loss from: 0.10850402116775512  to: 0.10844296216964722\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.10844296216964722  to: 0.10838264226913452\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.10838264226913452  to: 0.10832282304763793\n",
      "Training iteration: 1387\n",
      "Improved validation loss from: 0.10832282304763793  to: 0.10826307535171509\n",
      "Training iteration: 1388\n",
      "Improved validation loss from: 0.10826307535171509  to: 0.10820105075836181\n",
      "Training iteration: 1389\n",
      "Improved validation loss from: 0.10820105075836181  to: 0.10813857316970825\n",
      "Training iteration: 1390\n",
      "Improved validation loss from: 0.10813857316970825  to: 0.10807558298110961\n",
      "Training iteration: 1391\n",
      "Improved validation loss from: 0.10807558298110961  to: 0.10801204442977905\n",
      "Training iteration: 1392\n",
      "Improved validation loss from: 0.10801204442977905  to: 0.10794609785079956\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.10794609785079956  to: 0.10788031816482543\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.10788031816482543  to: 0.1078149914741516\n",
      "Training iteration: 1395\n",
      "Improved validation loss from: 0.1078149914741516  to: 0.10775020122528076\n",
      "Training iteration: 1396\n",
      "Improved validation loss from: 0.10775020122528076  to: 0.10768578052520753\n",
      "Training iteration: 1397\n",
      "Improved validation loss from: 0.10768578052520753  to: 0.10762126445770263\n",
      "Training iteration: 1398\n",
      "Improved validation loss from: 0.10762126445770263  to: 0.10755430459976197\n",
      "Training iteration: 1399\n",
      "Improved validation loss from: 0.10755430459976197  to: 0.1074873685836792\n",
      "Training iteration: 1400\n",
      "Improved validation loss from: 0.1074873685836792  to: 0.10742075443267822\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.10742075443267822  to: 0.10735447406768799\n",
      "Training iteration: 1402\n",
      "Improved validation loss from: 0.10735447406768799  to: 0.10728851556777955\n",
      "Training iteration: 1403\n",
      "Improved validation loss from: 0.10728851556777955  to: 0.10722031593322753\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.10722031593322753  to: 0.10715265274047851\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.10715265274047851  to: 0.10708553791046142\n",
      "Training iteration: 1406\n",
      "Improved validation loss from: 0.10708553791046142  to: 0.10701888799667358\n",
      "Training iteration: 1407\n",
      "Improved validation loss from: 0.10701888799667358  to: 0.10695241689682007\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.10695241689682007  to: 0.10688569545745849\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.10688569545745849  to: 0.10681715011596679\n",
      "Training iteration: 1410\n",
      "Improved validation loss from: 0.10681715011596679  to: 0.106746506690979\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.106746506690979  to: 0.10667375326156617\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.10667375326156617  to: 0.10659617185592651\n",
      "Training iteration: 1413\n",
      "Improved validation loss from: 0.10659617185592651  to: 0.1065181851387024\n",
      "Training iteration: 1414\n",
      "Improved validation loss from: 0.1065181851387024  to: 0.10644102096557617\n",
      "Training iteration: 1415\n",
      "Improved validation loss from: 0.10644102096557617  to: 0.10636546611785888\n",
      "Training iteration: 1416\n",
      "Improved validation loss from: 0.10636546611785888  to: 0.10629169940948487\n",
      "Training iteration: 1417\n",
      "Improved validation loss from: 0.10629169940948487  to: 0.10621942281723022\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.10621942281723022  to: 0.10614807605743408\n",
      "Training iteration: 1419\n",
      "Improved validation loss from: 0.10614807605743408  to: 0.10607664585113526\n",
      "Training iteration: 1420\n",
      "Improved validation loss from: 0.10607664585113526  to: 0.1060039758682251\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.1060039758682251  to: 0.10592939853668212\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.10592939853668212  to: 0.10582971572875977\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.10582971572875977  to: 0.10573688745498658\n",
      "Training iteration: 1424\n",
      "Improved validation loss from: 0.10573688745498658  to: 0.10565435886383057\n",
      "Training iteration: 1425\n",
      "Improved validation loss from: 0.10565435886383057  to: 0.10558322668075562\n",
      "Training iteration: 1426\n",
      "Improved validation loss from: 0.10558322668075562  to: 0.1054983377456665\n",
      "Training iteration: 1427\n",
      "Improved validation loss from: 0.1054983377456665  to: 0.10540355443954467\n",
      "Training iteration: 1428\n",
      "Improved validation loss from: 0.10540355443954467  to: 0.1053153395652771\n",
      "Training iteration: 1429\n",
      "Improved validation loss from: 0.1053153395652771  to: 0.10523639917373658\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.10523639917373658  to: 0.1051670789718628\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.1051670789718628  to: 0.10508143901824951\n",
      "Training iteration: 1432\n",
      "Improved validation loss from: 0.10508143901824951  to: 0.10498453378677368\n",
      "Training iteration: 1433\n",
      "Improved validation loss from: 0.10498453378677368  to: 0.10490732192993164\n",
      "Training iteration: 1434\n",
      "Improved validation loss from: 0.10490732192993164  to: 0.10484911203384399\n",
      "Training iteration: 1435\n",
      "Improved validation loss from: 0.10484911203384399  to: 0.10478004217147827\n",
      "Training iteration: 1436\n",
      "Improved validation loss from: 0.10478004217147827  to: 0.10469971895217896\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.10469971895217896  to: 0.10461112260818481\n",
      "Training iteration: 1438\n",
      "Improved validation loss from: 0.10461112260818481  to: 0.10451985597610473\n",
      "Training iteration: 1439\n",
      "Improved validation loss from: 0.10451985597610473  to: 0.10443177223205566\n",
      "Training iteration: 1440\n",
      "Improved validation loss from: 0.10443177223205566  to: 0.1043748140335083\n",
      "Training iteration: 1441\n",
      "Improved validation loss from: 0.1043748140335083  to: 0.10431725978851318\n",
      "Training iteration: 1442\n",
      "Improved validation loss from: 0.10431725978851318  to: 0.10425357818603516\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.10425357818603516  to: 0.10418031215667725\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.10418031215667725  to: 0.10409737825393676\n",
      "Training iteration: 1445\n",
      "Improved validation loss from: 0.10409737825393676  to: 0.10400794744491577\n",
      "Training iteration: 1446\n",
      "Improved validation loss from: 0.10400794744491577  to: 0.10391693115234375\n",
      "Training iteration: 1447\n",
      "Improved validation loss from: 0.10391693115234375  to: 0.10383312702178955\n",
      "Training iteration: 1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10383312702178955  to: 0.10375713109970093\n",
      "Training iteration: 1449\n",
      "Improved validation loss from: 0.10375713109970093  to: 0.10368583202362061\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.10368583202362061  to: 0.10361388921737671\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.10361388921737671  to: 0.10353469848632812\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.10353469848632812  to: 0.10343931913375855\n",
      "Training iteration: 1453\n",
      "Improved validation loss from: 0.10343931913375855  to: 0.10333106517791749\n",
      "Training iteration: 1454\n",
      "Improved validation loss from: 0.10333106517791749  to: 0.10321859121322632\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.10321859121322632  to: 0.10311180353164673\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.10311180353164673  to: 0.10302263498306274\n",
      "Training iteration: 1457\n",
      "Improved validation loss from: 0.10302263498306274  to: 0.10295045375823975\n",
      "Training iteration: 1458\n",
      "Improved validation loss from: 0.10295045375823975  to: 0.10288294553756713\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.10288294553756713  to: 0.10281198024749756\n",
      "Training iteration: 1460\n",
      "Improved validation loss from: 0.10281198024749756  to: 0.10273175239562989\n",
      "Training iteration: 1461\n",
      "Improved validation loss from: 0.10273175239562989  to: 0.10264142751693725\n",
      "Training iteration: 1462\n",
      "Improved validation loss from: 0.10264142751693725  to: 0.10255059003829955\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.10255059003829955  to: 0.10246219635009765\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.10246219635009765  to: 0.10237718820571899\n",
      "Training iteration: 1465\n",
      "Improved validation loss from: 0.10237718820571899  to: 0.10228856801986694\n",
      "Training iteration: 1466\n",
      "Improved validation loss from: 0.10228856801986694  to: 0.10219781398773194\n",
      "Training iteration: 1467\n",
      "Improved validation loss from: 0.10219781398773194  to: 0.10211369991302491\n",
      "Training iteration: 1468\n",
      "Improved validation loss from: 0.10211369991302491  to: 0.10203481912612915\n",
      "Training iteration: 1469\n",
      "Improved validation loss from: 0.10203481912612915  to: 0.10195049047470092\n",
      "Training iteration: 1470\n",
      "Improved validation loss from: 0.10195049047470092  to: 0.1018599271774292\n",
      "Training iteration: 1471\n",
      "Improved validation loss from: 0.1018599271774292  to: 0.10177218914031982\n",
      "Training iteration: 1472\n",
      "Improved validation loss from: 0.10177218914031982  to: 0.10168707370758057\n",
      "Training iteration: 1473\n",
      "Improved validation loss from: 0.10168707370758057  to: 0.10160204172134399\n",
      "Training iteration: 1474\n",
      "Improved validation loss from: 0.10160204172134399  to: 0.10151398181915283\n",
      "Training iteration: 1475\n",
      "Improved validation loss from: 0.10151398181915283  to: 0.1014140009880066\n",
      "Training iteration: 1476\n",
      "Improved validation loss from: 0.1014140009880066  to: 0.1013073205947876\n",
      "Training iteration: 1477\n",
      "Improved validation loss from: 0.1013073205947876  to: 0.10120289325714112\n",
      "Training iteration: 1478\n",
      "Improved validation loss from: 0.10120289325714112  to: 0.1011155366897583\n",
      "Training iteration: 1479\n",
      "Improved validation loss from: 0.1011155366897583  to: 0.10104219913482666\n",
      "Training iteration: 1480\n",
      "Improved validation loss from: 0.10104219913482666  to: 0.10097206830978393\n",
      "Training iteration: 1481\n",
      "Improved validation loss from: 0.10097206830978393  to: 0.10089184045791626\n",
      "Training iteration: 1482\n",
      "Improved validation loss from: 0.10089184045791626  to: 0.10079348087310791\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.10079348087310791  to: 0.10067931413650513\n",
      "Training iteration: 1484\n",
      "Improved validation loss from: 0.10067931413650513  to: 0.10056006908416748\n",
      "Training iteration: 1485\n",
      "Improved validation loss from: 0.10056006908416748  to: 0.10044794082641602\n",
      "Training iteration: 1486\n",
      "Improved validation loss from: 0.10044794082641602  to: 0.10034244060516358\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.10034244060516358  to: 0.10024813413619996\n",
      "Training iteration: 1488\n",
      "Improved validation loss from: 0.10024813413619996  to: 0.10017169713973999\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.10017169713973999  to: 0.10010082721710205\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.10010082721710205  to: 0.10001921653747559\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.10001921653747559  to: 0.09991723895072938\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.09991723895072938  to: 0.09962434768676758\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.09962434768676758  to: 0.0994531273841858\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.0994531273841858  to: 0.09941335916519164\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.09946800470352173\n",
      "Training iteration: 1496\n",
      "Improved validation loss from: 0.09941335916519164  to: 0.09930217862129212\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.09930217862129212  to: 0.09897524118423462\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.09897524118423462  to: 0.0988115906715393\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.09889072179794312\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.09881436228752136\n",
      "Training iteration: 1501\n",
      "Improved validation loss from: 0.0988115906715393  to: 0.09854162335395814\n",
      "Training iteration: 1502\n",
      "Improved validation loss from: 0.09854162335395814  to: 0.09821904301643372\n",
      "Training iteration: 1503\n",
      "Improved validation loss from: 0.09821904301643372  to: 0.09819567799568177\n",
      "Training iteration: 1504\n",
      "Improved validation loss from: 0.09819567799568177  to: 0.09815295338630677\n",
      "Training iteration: 1505\n",
      "Improved validation loss from: 0.09815295338630677  to: 0.0980272889137268\n",
      "Training iteration: 1506\n",
      "Improved validation loss from: 0.0980272889137268  to: 0.09782883524894714\n",
      "Training iteration: 1507\n",
      "Improved validation loss from: 0.09782883524894714  to: 0.09762763977050781\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.0977288842201233\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.0977302372455597\n",
      "Training iteration: 1510\n",
      "Improved validation loss from: 0.09762763977050781  to: 0.09754073023796081\n",
      "Training iteration: 1511\n",
      "Improved validation loss from: 0.09754073023796081  to: 0.0972650706768036\n",
      "Training iteration: 1512\n",
      "Improved validation loss from: 0.0972650706768036  to: 0.09712883234024047\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.09713377952575683\n",
      "Training iteration: 1514\n",
      "Improved validation loss from: 0.09712883234024047  to: 0.09712028503417969\n",
      "Training iteration: 1515\n",
      "Improved validation loss from: 0.09712028503417969  to: 0.09700862765312195\n",
      "Training iteration: 1516\n",
      "Improved validation loss from: 0.09700862765312195  to: 0.09681015014648438\n",
      "Training iteration: 1517\n",
      "Improved validation loss from: 0.09681015014648438  to: 0.09660965204238892\n",
      "Training iteration: 1518\n",
      "Improved validation loss from: 0.09660965204238892  to: 0.09655458331108094\n",
      "Training iteration: 1519\n",
      "Improved validation loss from: 0.09655458331108094  to: 0.09650785326957703\n",
      "Training iteration: 1520\n",
      "Improved validation loss from: 0.09650785326957703  to: 0.09641531109809875\n",
      "Training iteration: 1521\n",
      "Improved validation loss from: 0.09641531109809875  to: 0.09626530408859253\n",
      "Training iteration: 1522\n",
      "Improved validation loss from: 0.09626530408859253  to: 0.09610010981559754\n",
      "Training iteration: 1523\n",
      "Improved validation loss from: 0.09610010981559754  to: 0.09596842527389526\n",
      "Training iteration: 1524\n",
      "Validation loss (no improvement): 0.09597976803779602\n",
      "Training iteration: 1525\n",
      "Improved validation loss from: 0.09596842527389526  to: 0.09592355489730835\n",
      "Training iteration: 1526\n",
      "Improved validation loss from: 0.09592355489730835  to: 0.09575923085212708\n",
      "Training iteration: 1527\n",
      "Improved validation loss from: 0.09575923085212708  to: 0.09556625485420227\n",
      "Training iteration: 1528\n",
      "Improved validation loss from: 0.09556625485420227  to: 0.09552119374275207\n",
      "Training iteration: 1529\n",
      "Improved validation loss from: 0.09552119374275207  to: 0.09547868967056275\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.09547868967056275  to: 0.09519670605659485\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.09519670605659485  to: 0.09514358639717102\n",
      "Training iteration: 1532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.09528142213821411\n",
      "Training iteration: 1533\n",
      "Improved validation loss from: 0.09514358639717102  to: 0.09501211047172546\n",
      "Training iteration: 1534\n",
      "Improved validation loss from: 0.09501211047172546  to: 0.09458252787590027\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.09465622901916504\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.09461331367492676\n",
      "Training iteration: 1537\n",
      "Improved validation loss from: 0.09458252787590027  to: 0.09434282183647155\n",
      "Training iteration: 1538\n",
      "Improved validation loss from: 0.09434282183647155  to: 0.09427093267440796\n",
      "Training iteration: 1539\n",
      "Improved validation loss from: 0.09427093267440796  to: 0.09406154751777648\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.0942274272441864\n",
      "Training iteration: 1541\n",
      "Validation loss (no improvement): 0.09407659769058227\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.09406154751777648  to: 0.09371835589408875\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.09371835589408875  to: 0.09360777139663697\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.09371644258499146\n",
      "Training iteration: 1545\n",
      "Validation loss (no improvement): 0.09383692741394042\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.09360777139663697  to: 0.09339630007743835\n",
      "Training iteration: 1547\n",
      "Improved validation loss from: 0.09339630007743835  to: 0.09322229623794556\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.09352997541427613\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.09334813952445983\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.09322229623794556  to: 0.09294691085815429\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.09294691085815429  to: 0.0929442048072815\n",
      "Training iteration: 1552\n",
      "Improved validation loss from: 0.0929442048072815  to: 0.09292556643486023\n",
      "Training iteration: 1553\n",
      "Improved validation loss from: 0.09292556643486023  to: 0.0927500605583191\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.0927500605583191  to: 0.09249590039253235\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.0926349937915802\n",
      "Training iteration: 1556\n",
      "Validation loss (no improvement): 0.09256150126457215\n",
      "Training iteration: 1557\n",
      "Improved validation loss from: 0.09249590039253235  to: 0.09221258163452148\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.09221258163452148  to: 0.09200164079666137\n",
      "Training iteration: 1559\n",
      "Improved validation loss from: 0.09200164079666137  to: 0.09200116395950317\n",
      "Training iteration: 1560\n",
      "Improved validation loss from: 0.09200116395950317  to: 0.09190675616264343\n",
      "Training iteration: 1561\n",
      "Improved validation loss from: 0.09190675616264343  to: 0.0916971206665039\n",
      "Training iteration: 1562\n",
      "Improved validation loss from: 0.0916971206665039  to: 0.09159711599349976\n",
      "Training iteration: 1563\n",
      "Improved validation loss from: 0.09159711599349976  to: 0.0915902316570282\n",
      "Training iteration: 1564\n",
      "Improved validation loss from: 0.0915902316570282  to: 0.091291743516922\n",
      "Training iteration: 1565\n",
      "Improved validation loss from: 0.091291743516922  to: 0.0912261188030243\n",
      "Training iteration: 1566\n",
      "Validation loss (no improvement): 0.09138699769973754\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.0912261188030243  to: 0.09108516573905945\n",
      "Training iteration: 1568\n",
      "Improved validation loss from: 0.09108516573905945  to: 0.09074589610099792\n",
      "Training iteration: 1569\n",
      "Validation loss (no improvement): 0.09089158177375793\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.0909223735332489\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.09074589610099792  to: 0.09061381220817566\n",
      "Training iteration: 1572\n",
      "Improved validation loss from: 0.09061381220817566  to: 0.09026500582695007\n",
      "Training iteration: 1573\n",
      "Validation loss (no improvement): 0.09039776921272277\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.09042917490005493\n",
      "Training iteration: 1575\n",
      "Improved validation loss from: 0.09026500582695007  to: 0.09000240564346314\n",
      "Training iteration: 1576\n",
      "Improved validation loss from: 0.09000240564346314  to: 0.08980433344841003\n",
      "Training iteration: 1577\n",
      "Validation loss (no improvement): 0.08989554643630981\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.08980433344841003  to: 0.08973786234855652\n",
      "Training iteration: 1579\n",
      "Improved validation loss from: 0.08973786234855652  to: 0.08963069915771485\n",
      "Training iteration: 1580\n",
      "Improved validation loss from: 0.08963069915771485  to: 0.08938833475112914\n",
      "Training iteration: 1581\n",
      "Improved validation loss from: 0.08938833475112914  to: 0.08929289579391479\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.08932212591171265\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.08929289579391479  to: 0.08916820287704467\n",
      "Training iteration: 1584\n",
      "Improved validation loss from: 0.08916820287704467  to: 0.08891563415527344\n",
      "Training iteration: 1585\n",
      "Improved validation loss from: 0.08891563415527344  to: 0.08884438276290893\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.08884438276290893  to: 0.0887520968914032\n",
      "Training iteration: 1587\n",
      "Improved validation loss from: 0.0887520968914032  to: 0.08861805200576782\n",
      "Training iteration: 1588\n",
      "Improved validation loss from: 0.08861805200576782  to: 0.08845810890197754\n",
      "Training iteration: 1589\n",
      "Improved validation loss from: 0.08845810890197754  to: 0.08830752372741699\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.08837001919746398\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.08830752372741699  to: 0.08829089999198914\n",
      "Training iteration: 1592\n",
      "Improved validation loss from: 0.08829089999198914  to: 0.08804105520248413\n",
      "Training iteration: 1593\n",
      "Improved validation loss from: 0.08804105520248413  to: 0.08782771825790406\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.08807964324951172\n",
      "Training iteration: 1595\n",
      "Validation loss (no improvement): 0.08814467191696167\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.08782771825790406  to: 0.08769950866699219\n",
      "Training iteration: 1597\n",
      "Improved validation loss from: 0.08769950866699219  to: 0.08748208284378052\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.08772057294845581\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.08801347613334656\n",
      "Training iteration: 1600\n",
      "Improved validation loss from: 0.08748208284378052  to: 0.08738452196121216\n",
      "Training iteration: 1601\n",
      "Improved validation loss from: 0.08738452196121216  to: 0.08707255125045776\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.08715564608573914\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.08746887445449829\n",
      "Training iteration: 1604\n",
      "Improved validation loss from: 0.08707255125045776  to: 0.08701165914535522\n",
      "Training iteration: 1605\n",
      "Improved validation loss from: 0.08701165914535522  to: 0.08662747144699097\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.08677255511283874\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.0869369387626648\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.08675948381423951\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.08662747144699097  to: 0.08640982508659363\n",
      "Training iteration: 1610\n",
      "Improved validation loss from: 0.08640982508659363  to: 0.086207115650177\n",
      "Training iteration: 1611\n",
      "Validation loss (no improvement): 0.08652119636535645\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.08658123016357422\n",
      "Training iteration: 1613\n",
      "Improved validation loss from: 0.086207115650177  to: 0.08603754043579101\n",
      "Training iteration: 1614\n",
      "Improved validation loss from: 0.08603754043579101  to: 0.08581245541572571\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.08605104684829712\n",
      "Training iteration: 1616\n",
      "Validation loss (no improvement): 0.086489999294281\n",
      "Training iteration: 1617\n",
      "Improved validation loss from: 0.08581245541572571  to: 0.08576976656913757\n",
      "Training iteration: 1618\n",
      "Improved validation loss from: 0.08576976656913757  to: 0.08538572192192077\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.08550346493721009\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.08608617782592773\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.08561853170394898\n",
      "Training iteration: 1622\n",
      "Improved validation loss from: 0.08538572192192077  to: 0.08518133163452149\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.08529573678970337\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.08543195724487304\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.0852150559425354\n",
      "Training iteration: 1626\n",
      "Improved validation loss from: 0.08518133163452149  to: 0.08498202562332154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1627\n",
      "Improved validation loss from: 0.08498202562332154  to: 0.08485390543937683\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.08485390543937683  to: 0.08483344912528992\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.084903085231781\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.08490936160087585\n",
      "Training iteration: 1631\n",
      "Improved validation loss from: 0.08483344912528992  to: 0.08474611043930054\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.08474611043930054  to: 0.08452528119087219\n",
      "Training iteration: 1633\n",
      "Validation loss (no improvement): 0.08453450202941895\n",
      "Training iteration: 1634\n",
      "Improved validation loss from: 0.08452528119087219  to: 0.08448840975761414\n",
      "Training iteration: 1635\n",
      "Improved validation loss from: 0.08448840975761414  to: 0.08432829976081849\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.08432829976081849  to: 0.08411986231803895\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.08411986231803895  to: 0.08394947052001953\n",
      "Training iteration: 1638\n",
      "Improved validation loss from: 0.08394947052001953  to: 0.08391863703727723\n",
      "Training iteration: 1639\n",
      "Improved validation loss from: 0.08391863703727723  to: 0.08386706113815308\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.08386706113815308  to: 0.08357146978378296\n",
      "Training iteration: 1641\n",
      "Improved validation loss from: 0.08357146978378296  to: 0.0834882378578186\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.08351373672485352\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.08352845907211304\n",
      "Training iteration: 1644\n",
      "Improved validation loss from: 0.0834882378578186  to: 0.08339096307754516\n",
      "Training iteration: 1645\n",
      "Improved validation loss from: 0.08339096307754516  to: 0.08315149545669556\n",
      "Training iteration: 1646\n",
      "Improved validation loss from: 0.08315149545669556  to: 0.08295797109603882\n",
      "Training iteration: 1647\n",
      "Improved validation loss from: 0.08295797109603882  to: 0.0829469084739685\n",
      "Training iteration: 1648\n",
      "Improved validation loss from: 0.0829469084739685  to: 0.08293949961662292\n",
      "Training iteration: 1649\n",
      "Improved validation loss from: 0.08293949961662292  to: 0.08284796476364135\n",
      "Training iteration: 1650\n",
      "Improved validation loss from: 0.08284796476364135  to: 0.08258320093154907\n",
      "Training iteration: 1651\n",
      "Improved validation loss from: 0.08258320093154907  to: 0.08255712389945984\n",
      "Training iteration: 1652\n",
      "Validation loss (no improvement): 0.0827465832233429\n",
      "Training iteration: 1653\n",
      "Validation loss (no improvement): 0.08274784088134765\n",
      "Training iteration: 1654\n",
      "Improved validation loss from: 0.08255712389945984  to: 0.08245534896850586\n",
      "Training iteration: 1655\n",
      "Improved validation loss from: 0.08245534896850586  to: 0.08219135403633118\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.08219649195671082\n",
      "Training iteration: 1657\n",
      "Validation loss (no improvement): 0.08238588571548462\n",
      "Training iteration: 1658\n",
      "Improved validation loss from: 0.08219135403633118  to: 0.08187679052352906\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.08243934512138366\n",
      "Training iteration: 1660\n",
      "Validation loss (no improvement): 0.08200294375419617\n",
      "Training iteration: 1661\n",
      "Improved validation loss from: 0.08187679052352906  to: 0.08154659271240235\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.08214908838272095\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.0821011245250702\n",
      "Training iteration: 1664\n",
      "Improved validation loss from: 0.08154659271240235  to: 0.08139424324035645\n",
      "Training iteration: 1665\n",
      "Validation loss (no improvement): 0.08166583776473998\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.08184015154838561\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.08151294589042664\n",
      "Training iteration: 1668\n",
      "Improved validation loss from: 0.08139424324035645  to: 0.08111574053764344\n",
      "Training iteration: 1669\n",
      "Validation loss (no improvement): 0.08175035715103149\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.08153907656669616\n",
      "Training iteration: 1671\n",
      "Improved validation loss from: 0.08111574053764344  to: 0.08082831501960755\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.08120995759963989\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.08147274255752564\n",
      "Training iteration: 1674\n",
      "Validation loss (no improvement): 0.08089076876640319\n",
      "Training iteration: 1675\n",
      "Improved validation loss from: 0.08082831501960755  to: 0.08080973625183105\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.08116849660873413\n",
      "Training iteration: 1677\n",
      "Improved validation loss from: 0.08080973625183105  to: 0.08077301979064941\n",
      "Training iteration: 1678\n",
      "Improved validation loss from: 0.08077301979064941  to: 0.08036445379257202\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.08077793121337891\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.08099408149719238\n",
      "Training iteration: 1681\n",
      "Improved validation loss from: 0.08036445379257202  to: 0.08035310506820678\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.08035310506820678  to: 0.0802739143371582\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.08081192970275879\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.08039689064025879\n",
      "Training iteration: 1685\n",
      "Improved validation loss from: 0.0802739143371582  to: 0.07985526323318481\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.08012953996658326\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.08041241765022278\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.07989017367362976\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.08001777529716492\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.08056950569152832\n",
      "Training iteration: 1691\n",
      "Improved validation loss from: 0.07985526323318481  to: 0.07983017563819886\n",
      "Training iteration: 1692\n",
      "Improved validation loss from: 0.07983017563819886  to: 0.07933554649353028\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.07967244982719421\n",
      "Training iteration: 1694\n",
      "Validation loss (no improvement): 0.08012809753417968\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.07962359189987182\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.07958171963691711\n",
      "Training iteration: 1697\n",
      "Validation loss (no improvement): 0.07962431907653808\n",
      "Training iteration: 1698\n",
      "Improved validation loss from: 0.07933554649353028  to: 0.07932353019714355\n",
      "Training iteration: 1699\n",
      "Improved validation loss from: 0.07932353019714355  to: 0.07918163537979125\n",
      "Training iteration: 1700\n",
      "Improved validation loss from: 0.07918163537979125  to: 0.07894620895385743\n",
      "Training iteration: 1701\n",
      "Validation loss (no improvement): 0.07909153699874878\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.07894620895385743  to: 0.07878930568695068\n",
      "Training iteration: 1703\n",
      "Validation loss (no improvement): 0.07907407879829406\n",
      "Training iteration: 1704\n",
      "Improved validation loss from: 0.07878930568695068  to: 0.07873286008834839\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.07892593145370483\n",
      "Training iteration: 1706\n",
      "Improved validation loss from: 0.07873286008834839  to: 0.07844997048377991\n",
      "Training iteration: 1707\n",
      "Validation loss (no improvement): 0.07858470678329468\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.07844997048377991  to: 0.07843815684318542\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.07843815684318542  to: 0.07790873050689698\n",
      "Training iteration: 1710\n",
      "Validation loss (no improvement): 0.07833786010742187\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.07857317924499511\n",
      "Training iteration: 1712\n",
      "Improved validation loss from: 0.07790873050689698  to: 0.07770089507102966\n",
      "Training iteration: 1713\n",
      "Validation loss (no improvement): 0.07813732624053955\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.07872712016105651\n",
      "Training iteration: 1715\n",
      "Improved validation loss from: 0.07770089507102966  to: 0.07735304832458496\n",
      "Training iteration: 1716\n",
      "Validation loss (no improvement): 0.0774163544178009\n",
      "Training iteration: 1717\n",
      "Validation loss (no improvement): 0.07832218408584594\n",
      "Training iteration: 1718\n",
      "Improved validation loss from: 0.07735304832458496  to: 0.07728497385978698\n",
      "Training iteration: 1719\n",
      "Improved validation loss from: 0.07728497385978698  to: 0.07676210403442382\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.07725175619125366\n",
      "Training iteration: 1721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.07788522839546204\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.07686878442764282\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.07689740061759949\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.07748202085494996\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.07683351635932922\n",
      "Training iteration: 1726\n",
      "Improved validation loss from: 0.07676210403442382  to: 0.07642111778259278\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.07663626670837402\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.07642111778259278  to: 0.07618838548660278\n",
      "Training iteration: 1729\n",
      "Improved validation loss from: 0.07618838548660278  to: 0.07605057954788208\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.07620344161987305\n",
      "Training iteration: 1731\n",
      "Improved validation loss from: 0.07605057954788208  to: 0.07590075135231018\n",
      "Training iteration: 1732\n",
      "Improved validation loss from: 0.07590075135231018  to: 0.075853031873703\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.0760206401348114\n",
      "Training iteration: 1734\n",
      "Improved validation loss from: 0.075853031873703  to: 0.07539917826652527\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.07587019801139831\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.07541137337684631\n",
      "Training iteration: 1737\n",
      "Improved validation loss from: 0.07539917826652527  to: 0.07518573999404907\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.07549402117729187\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.07518573999404907  to: 0.07481639981269836\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.07481639981269836  to: 0.07459443807601929\n",
      "Training iteration: 1741\n",
      "Validation loss (no improvement): 0.07557295560836792\n",
      "Training iteration: 1742\n",
      "Improved validation loss from: 0.07459443807601929  to: 0.07448753118515014\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.07448753118515014  to: 0.07416059970855712\n",
      "Training iteration: 1744\n",
      "Validation loss (no improvement): 0.07487285733222962\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.07457624673843384\n",
      "Training iteration: 1746\n",
      "Improved validation loss from: 0.07416059970855712  to: 0.0740327000617981\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.07453073263168335\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.07544395327568054\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.07430315017700195\n",
      "Training iteration: 1750\n",
      "Improved validation loss from: 0.0740327000617981  to: 0.0739515781402588\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.07448493242263794\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.07420567274093628\n",
      "Training iteration: 1753\n",
      "Improved validation loss from: 0.0739515781402588  to: 0.073602294921875\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.07377744913101196\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.07443934679031372\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.07376526594161988\n",
      "Training iteration: 1757\n",
      "Improved validation loss from: 0.073602294921875  to: 0.07340157628059388\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.07373222112655639\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.07358822822570801\n",
      "Training iteration: 1760\n",
      "Improved validation loss from: 0.07340157628059388  to: 0.07307693958282471\n",
      "Training iteration: 1761\n",
      "Validation loss (no improvement): 0.07337232828140258\n",
      "Training iteration: 1762\n",
      "Validation loss (no improvement): 0.07309526801109315\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.07318503856658935\n",
      "Training iteration: 1764\n",
      "Improved validation loss from: 0.07307693958282471  to: 0.07263621091842651\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.07284178733825683\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.07272396683692932\n",
      "Training iteration: 1767\n",
      "Improved validation loss from: 0.07263621091842651  to: 0.07208161950111389\n",
      "Training iteration: 1768\n",
      "Validation loss (no improvement): 0.07235467433929443\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.07293367385864258\n",
      "Training iteration: 1770\n",
      "Improved validation loss from: 0.07208161950111389  to: 0.07178426384925843\n",
      "Training iteration: 1771\n",
      "Improved validation loss from: 0.07178426384925843  to: 0.07167391777038574\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.07207702398300171\n",
      "Training iteration: 1773\n",
      "Improved validation loss from: 0.07167391777038574  to: 0.07140787243843079\n",
      "Training iteration: 1774\n",
      "Improved validation loss from: 0.07140787243843079  to: 0.07132338285446167\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.07175667285919189\n",
      "Training iteration: 1776\n",
      "Improved validation loss from: 0.07132338285446167  to: 0.07114019393920898\n",
      "Training iteration: 1777\n",
      "Improved validation loss from: 0.07114019393920898  to: 0.07106887698173522\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.07152752876281739\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.07113434076309204\n",
      "Training iteration: 1780\n",
      "Improved validation loss from: 0.07106887698173522  to: 0.07063862085342407\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.07091194987297059\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.07125827670097351\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.07063862085342407  to: 0.07059630751609802\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.07059630751609802  to: 0.07046054005622863\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.07072794437408447\n",
      "Training iteration: 1786\n",
      "Improved validation loss from: 0.07046054005622863  to: 0.07028585076332092\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.07028585076332092  to: 0.07000314593315124\n",
      "Training iteration: 1788\n",
      "Validation loss (no improvement): 0.07021207809448242\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.07010671496391296\n",
      "Training iteration: 1790\n",
      "Improved validation loss from: 0.07000314593315124  to: 0.06982764601707458\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.06982764601707458  to: 0.0695417046546936\n",
      "Training iteration: 1792\n",
      "Validation loss (no improvement): 0.06968251466751099\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.06970645189285278\n",
      "Training iteration: 1794\n",
      "Improved validation loss from: 0.0695417046546936  to: 0.0692175269126892\n",
      "Training iteration: 1795\n",
      "Improved validation loss from: 0.0692175269126892  to: 0.06906842589378356\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.06940587759017944\n",
      "Training iteration: 1797\n",
      "Improved validation loss from: 0.06906842589378356  to: 0.06905620694160461\n",
      "Training iteration: 1798\n",
      "Improved validation loss from: 0.06905620694160461  to: 0.06867513060569763\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.06877191066741943\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.06927964091300964\n",
      "Training iteration: 1801\n",
      "Improved validation loss from: 0.06867513060569763  to: 0.06862355470657348\n",
      "Training iteration: 1802\n",
      "Improved validation loss from: 0.06862355470657348  to: 0.06835153698921204\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.06851083636283875\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.06866881847381592\n",
      "Training iteration: 1805\n",
      "Improved validation loss from: 0.06835153698921204  to: 0.06827105283737182\n",
      "Training iteration: 1806\n",
      "Improved validation loss from: 0.06827105283737182  to: 0.06819319128990173\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.06843844652175904\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.06824101209640503\n",
      "Training iteration: 1809\n",
      "Improved validation loss from: 0.06819319128990173  to: 0.06796448230743408\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.0681158423423767\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.06813305020332336\n",
      "Training iteration: 1812\n",
      "Validation loss (no improvement): 0.06798679232597352\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.06796448230743408  to: 0.06788471937179566\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.06794143319129944\n",
      "Training iteration: 1815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.06788471937179566  to: 0.06776524782180786\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.06800657510757446\n",
      "Training iteration: 1817\n",
      "Improved validation loss from: 0.06776524782180786  to: 0.06765646934509277\n",
      "Training iteration: 1818\n",
      "Improved validation loss from: 0.06765646934509277  to: 0.06747792959213257\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.0677133321762085\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.06752210855484009\n",
      "Training iteration: 1821\n",
      "Improved validation loss from: 0.06747792959213257  to: 0.06730965375900269\n",
      "Training iteration: 1822\n",
      "Improved validation loss from: 0.06730965375900269  to: 0.06730936765670777\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.06744173169136047\n",
      "Training iteration: 1824\n",
      "Improved validation loss from: 0.06730936765670777  to: 0.06722358465194703\n",
      "Training iteration: 1825\n",
      "Improved validation loss from: 0.06722358465194703  to: 0.06698644161224365\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.06708280444145202\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.0673845112323761\n",
      "Training iteration: 1828\n",
      "Improved validation loss from: 0.06698644161224365  to: 0.06669318079948425\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.06669318079948425  to: 0.06660779118537903\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.066899573802948\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.06708279848098755\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.06670835614204407\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.06668203473091125\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.0667211651802063\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.06691678762435913\n",
      "Training iteration: 1836\n",
      "Improved validation loss from: 0.06660779118537903  to: 0.0663805365562439\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.0663805365562439  to: 0.06624053120613098\n",
      "Training iteration: 1838\n",
      "Validation loss (no improvement): 0.06664343476295471\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.06689667701721191\n",
      "Training iteration: 1840\n",
      "Improved validation loss from: 0.06624053120613098  to: 0.06620481610298157\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.06622080802917481\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.06679646372795105\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.06677660942077637\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.0662439227104187\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.06623018980026245\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.06678535342216492\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.0671503484249115\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.06641863584518433\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.06662116646766662\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.06646953225135803\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.06684465408325195\n",
      "Training iteration: 1852\n",
      "Validation loss (no improvement): 0.06653249263763428\n",
      "Training iteration: 1853\n",
      "Improved validation loss from: 0.06620481610298157  to: 0.06616045236587524\n",
      "Training iteration: 1854\n",
      "Improved validation loss from: 0.06616045236587524  to: 0.06605650186538696\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.06624253392219544\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.06674578189849853\n",
      "Training iteration: 1857\n",
      "Improved validation loss from: 0.06605650186538696  to: 0.06596554517745971\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.06596554517745971  to: 0.06586915254592896\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.06586915254592896  to: 0.06577601432800292\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.06599092483520508\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.06585828065872193\n",
      "Training iteration: 1862\n",
      "Improved validation loss from: 0.06577601432800292  to: 0.06546124219894409\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.06546124219894409  to: 0.06531164050102234\n",
      "Training iteration: 1864\n",
      "Validation loss (no improvement): 0.06541088819503785\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.06554136872291565\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.0654763400554657\n",
      "Training iteration: 1867\n",
      "Improved validation loss from: 0.06531164050102234  to: 0.0651917278766632\n",
      "Training iteration: 1868\n",
      "Improved validation loss from: 0.0651917278766632  to: 0.06515082120895385\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.06517878770828248\n",
      "Training iteration: 1870\n",
      "Improved validation loss from: 0.06515082120895385  to: 0.06510778665542602\n",
      "Training iteration: 1871\n",
      "Improved validation loss from: 0.06510778665542602  to: 0.06454184651374817\n",
      "Training iteration: 1872\n",
      "Improved validation loss from: 0.06454184651374817  to: 0.06436174511909484\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.06458508372306823\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.0645534873008728\n",
      "Training iteration: 1875\n",
      "Improved validation loss from: 0.06436174511909484  to: 0.064260596036911\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.0642721951007843\n",
      "Training iteration: 1877\n",
      "Validation loss (no improvement): 0.06433407068252564\n",
      "Training iteration: 1878\n",
      "Improved validation loss from: 0.064260596036911  to: 0.06426016092300416\n",
      "Training iteration: 1879\n",
      "Improved validation loss from: 0.06426016092300416  to: 0.06403586864471436\n",
      "Training iteration: 1880\n",
      "Improved validation loss from: 0.06403586864471436  to: 0.06392852663993835\n",
      "Training iteration: 1881\n",
      "Improved validation loss from: 0.06392852663993835  to: 0.06384917497634887\n",
      "Training iteration: 1882\n",
      "Improved validation loss from: 0.06384917497634887  to: 0.06372456550598145\n",
      "Training iteration: 1883\n",
      "Improved validation loss from: 0.06372456550598145  to: 0.06370584368705749\n",
      "Training iteration: 1884\n",
      "Improved validation loss from: 0.06370584368705749  to: 0.06361559629440308\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.06365717053413392\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.06361559629440308  to: 0.06360212564468384\n",
      "Training iteration: 1887\n",
      "Improved validation loss from: 0.06360212564468384  to: 0.06349629163742065\n",
      "Training iteration: 1888\n",
      "Improved validation loss from: 0.06349629163742065  to: 0.06329620480537415\n",
      "Training iteration: 1889\n",
      "Improved validation loss from: 0.06329620480537415  to: 0.0632332980632782\n",
      "Training iteration: 1890\n",
      "Improved validation loss from: 0.0632332980632782  to: 0.06301876306533813\n",
      "Training iteration: 1891\n",
      "Improved validation loss from: 0.06301876306533813  to: 0.06294851899147033\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.06300196647644044\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.06301474571228027\n",
      "Training iteration: 1894\n",
      "Validation loss (no improvement): 0.06297235488891602\n",
      "Training iteration: 1895\n",
      "Improved validation loss from: 0.06294851899147033  to: 0.06287330389022827\n",
      "Training iteration: 1896\n",
      "Improved validation loss from: 0.06287330389022827  to: 0.06268818974494934\n",
      "Training iteration: 1897\n",
      "Improved validation loss from: 0.06268818974494934  to: 0.06254266500473023\n",
      "Training iteration: 1898\n",
      "Improved validation loss from: 0.06254266500473023  to: 0.06252192854881286\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.06259217858314514\n",
      "Training iteration: 1900\n",
      "Improved validation loss from: 0.06252192854881286  to: 0.06247326731681824\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.06256242990493774\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.06247326731681824  to: 0.062440347671508786\n",
      "Training iteration: 1903\n",
      "Improved validation loss from: 0.062440347671508786  to: 0.06223899722099304\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.06227809190750122\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.06233433485031128\n",
      "Training iteration: 1906\n",
      "Improved validation loss from: 0.06223899722099304  to: 0.062125658988952635\n",
      "Training iteration: 1907\n",
      "Improved validation loss from: 0.062125658988952635  to: 0.06211473345756531\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.06228116154670715\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.062150228023529056\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.06215960383415222\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.06215850710868835\n",
      "Training iteration: 1912\n",
      "Improved validation loss from: 0.06211473345756531  to: 0.06208721399307251\n",
      "Training iteration: 1913\n",
      "Improved validation loss from: 0.06208721399307251  to: 0.06194469332695007\n",
      "Training iteration: 1914\n",
      "Improved validation loss from: 0.06194469332695007  to: 0.061879271268844606\n",
      "Training iteration: 1915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.061879271268844606  to: 0.06186684370040894\n",
      "Training iteration: 1916\n",
      "Validation loss (no improvement): 0.06198354959487915\n",
      "Training iteration: 1917\n",
      "Improved validation loss from: 0.06186684370040894  to: 0.061794668436050415\n",
      "Training iteration: 1918\n",
      "Improved validation loss from: 0.061794668436050415  to: 0.06170973777770996\n",
      "Training iteration: 1919\n",
      "Improved validation loss from: 0.06170973777770996  to: 0.06165324449539185\n",
      "Training iteration: 1920\n",
      "Improved validation loss from: 0.06165324449539185  to: 0.06142401695251465\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.06142401695251465  to: 0.06126782894134521\n",
      "Training iteration: 1922\n",
      "Improved validation loss from: 0.06126782894134521  to: 0.061181682348251346\n",
      "Training iteration: 1923\n",
      "Validation loss (no improvement): 0.06126194000244141\n",
      "Training iteration: 1924\n",
      "Improved validation loss from: 0.061181682348251346  to: 0.060991179943084714\n",
      "Training iteration: 1925\n",
      "Improved validation loss from: 0.060991179943084714  to: 0.060961687564849855\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.06102361083030701\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.06099320650100708\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.06098621487617493\n",
      "Training iteration: 1929\n",
      "Improved validation loss from: 0.060961687564849855  to: 0.06072872877120972\n",
      "Training iteration: 1930\n",
      "Improved validation loss from: 0.06072872877120972  to: 0.06072292923927307\n",
      "Training iteration: 1931\n",
      "Improved validation loss from: 0.06072292923927307  to: 0.06053619384765625\n",
      "Training iteration: 1932\n",
      "Improved validation loss from: 0.06053619384765625  to: 0.06041455864906311\n",
      "Training iteration: 1933\n",
      "Improved validation loss from: 0.06041455864906311  to: 0.06038156747817993\n",
      "Training iteration: 1934\n",
      "Improved validation loss from: 0.06038156747817993  to: 0.06037710309028625\n",
      "Training iteration: 1935\n",
      "Improved validation loss from: 0.06037710309028625  to: 0.060311079025268555\n",
      "Training iteration: 1936\n",
      "Improved validation loss from: 0.060311079025268555  to: 0.06024168729782105\n",
      "Training iteration: 1937\n",
      "Improved validation loss from: 0.06024168729782105  to: 0.060137951374053956\n",
      "Training iteration: 1938\n",
      "Improved validation loss from: 0.060137951374053956  to: 0.059983307123184205\n",
      "Training iteration: 1939\n",
      "Improved validation loss from: 0.059983307123184205  to: 0.05989610552787781\n",
      "Training iteration: 1940\n",
      "Improved validation loss from: 0.05989610552787781  to: 0.05974893569946289\n",
      "Training iteration: 1941\n",
      "Improved validation loss from: 0.05974893569946289  to: 0.05960887670516968\n",
      "Training iteration: 1942\n",
      "Improved validation loss from: 0.05960887670516968  to: 0.05951738953590393\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.05951738953590393  to: 0.05948750376701355\n",
      "Training iteration: 1944\n",
      "Improved validation loss from: 0.05948750376701355  to: 0.05940122604370117\n",
      "Training iteration: 1945\n",
      "Improved validation loss from: 0.05940122604370117  to: 0.059363645315170285\n",
      "Training iteration: 1946\n",
      "Improved validation loss from: 0.059363645315170285  to: 0.05924756526947021\n",
      "Training iteration: 1947\n",
      "Improved validation loss from: 0.05924756526947021  to: 0.0590857982635498\n",
      "Training iteration: 1948\n",
      "Improved validation loss from: 0.0590857982635498  to: 0.05899645090103149\n",
      "Training iteration: 1949\n",
      "Improved validation loss from: 0.05899645090103149  to: 0.05887317657470703\n",
      "Training iteration: 1950\n",
      "Improved validation loss from: 0.05887317657470703  to: 0.0588115930557251\n",
      "Training iteration: 1951\n",
      "Validation loss (no improvement): 0.05883924961090088\n",
      "Training iteration: 1952\n",
      "Validation loss (no improvement): 0.05882306098937988\n",
      "Training iteration: 1953\n",
      "Improved validation loss from: 0.0588115930557251  to: 0.05877926349639893\n",
      "Training iteration: 1954\n",
      "Validation loss (no improvement): 0.05881391763687134\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.05884760022163391\n",
      "Training iteration: 1956\n",
      "Validation loss (no improvement): 0.058821046352386476\n",
      "Training iteration: 1957\n",
      "Validation loss (no improvement): 0.058948826789855954\n",
      "Training iteration: 1958\n",
      "Improved validation loss from: 0.05877926349639893  to: 0.05867992639541626\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.05873567461967468\n",
      "Training iteration: 1960\n",
      "Improved validation loss from: 0.05867992639541626  to: 0.058637064695358274\n",
      "Training iteration: 1961\n",
      "Improved validation loss from: 0.058637064695358274  to: 0.058636474609375\n",
      "Training iteration: 1962\n",
      "Validation loss (no improvement): 0.05866385698318481\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.058648151159286496\n",
      "Training iteration: 1964\n",
      "Improved validation loss from: 0.058636474609375  to: 0.058630913496017456\n",
      "Training iteration: 1965\n",
      "Improved validation loss from: 0.058630913496017456  to: 0.05853198766708374\n",
      "Training iteration: 1966\n",
      "Improved validation loss from: 0.05853198766708374  to: 0.058406639099121097\n",
      "Training iteration: 1967\n",
      "Improved validation loss from: 0.058406639099121097  to: 0.05820770263671875\n",
      "Training iteration: 1968\n",
      "Improved validation loss from: 0.05820770263671875  to: 0.05819618105888367\n",
      "Training iteration: 1969\n",
      "Improved validation loss from: 0.05819618105888367  to: 0.05813812613487244\n",
      "Training iteration: 1970\n",
      "Improved validation loss from: 0.05813812613487244  to: 0.05806809663772583\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.05815840363502502\n",
      "Training iteration: 1972\n",
      "Validation loss (no improvement): 0.05809852480888367\n",
      "Training iteration: 1973\n",
      "Improved validation loss from: 0.05806809663772583  to: 0.05795363187789917\n",
      "Training iteration: 1974\n",
      "Improved validation loss from: 0.05795363187789917  to: 0.057814037799835204\n",
      "Training iteration: 1975\n",
      "Improved validation loss from: 0.057814037799835204  to: 0.05771490931510925\n",
      "Training iteration: 1976\n",
      "Improved validation loss from: 0.05771490931510925  to: 0.05765389204025269\n",
      "Training iteration: 1977\n",
      "Improved validation loss from: 0.05765389204025269  to: 0.05762578248977661\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.057628792524337766\n",
      "Training iteration: 1979\n",
      "Improved validation loss from: 0.05762578248977661  to: 0.057609778642654416\n",
      "Training iteration: 1980\n",
      "Improved validation loss from: 0.057609778642654416  to: 0.05747911334037781\n",
      "Training iteration: 1981\n",
      "Improved validation loss from: 0.05747911334037781  to: 0.057314622402191165\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.057367068529129026\n",
      "Training iteration: 1983\n",
      "Improved validation loss from: 0.057314622402191165  to: 0.05720570683479309\n",
      "Training iteration: 1984\n",
      "Improved validation loss from: 0.05720570683479309  to: 0.05713135004043579\n",
      "Training iteration: 1985\n",
      "Validation loss (no improvement): 0.05719382762908935\n",
      "Training iteration: 1986\n",
      "Validation loss (no improvement): 0.05720294713973999\n",
      "Training iteration: 1987\n",
      "Validation loss (no improvement): 0.05730077624320984\n",
      "Training iteration: 1988\n",
      "Validation loss (no improvement): 0.057185697555541995\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.057201707363128663\n",
      "Training iteration: 1990\n",
      "Improved validation loss from: 0.05713135004043579  to: 0.05705116987228394\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.05706127882003784\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.057197201251983645\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.05730618834495545\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.057149994373321536\n",
      "Training iteration: 1995\n",
      "Validation loss (no improvement): 0.057196348905563354\n",
      "Training iteration: 1996\n",
      "Validation loss (no improvement): 0.057108283042907715\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.057154643535614016\n",
      "Training iteration: 1998\n",
      "Improved validation loss from: 0.05705116987228394  to: 0.05700668692588806\n",
      "Training iteration: 1999\n",
      "Improved validation loss from: 0.05700668692588806  to: 0.05691278576850891\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.05691278576850891  to: 0.056856846809387206\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.056856846809387206  to: 0.05684465765953064\n",
      "Training iteration: 2002\n",
      "Improved validation loss from: 0.05684465765953064  to: 0.05679086446762085\n",
      "Training iteration: 2003\n",
      "Improved validation loss from: 0.05679086446762085  to: 0.056684684753417966\n",
      "Training iteration: 2004\n",
      "Improved validation loss from: 0.056684684753417966  to: 0.05653141736984253\n",
      "Training iteration: 2005\n",
      "Improved validation loss from: 0.05653141736984253  to: 0.05642516016960144\n",
      "Training iteration: 2006\n",
      "Improved validation loss from: 0.05642516016960144  to: 0.05632935762405396\n",
      "Training iteration: 2007\n",
      "Improved validation loss from: 0.05632935762405396  to: 0.0562135100364685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.0562135100364685  to: 0.05614006519317627\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.05614006519317627  to: 0.05600619316101074\n",
      "Training iteration: 2010\n",
      "Improved validation loss from: 0.05600619316101074  to: 0.05599268674850464\n",
      "Training iteration: 2011\n",
      "Improved validation loss from: 0.05599268674850464  to: 0.0558521568775177\n",
      "Training iteration: 2012\n",
      "Improved validation loss from: 0.0558521568775177  to: 0.05575501322746277\n",
      "Training iteration: 2013\n",
      "Improved validation loss from: 0.05575501322746277  to: 0.055628979206085206\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.055643343925476076\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.055644255876541135\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.055628979206085206  to: 0.055580157041549685\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.05560445785522461\n",
      "Training iteration: 2018\n",
      "Improved validation loss from: 0.055580157041549685  to: 0.05555888414382935\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.05555888414382935  to: 0.05553125143051148\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.05553125143051148  to: 0.05541146993637085\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.055412352085113525\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.055567729473114016\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.055805152654647826\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.055618506669998166\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.055909478664398195\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.05585792064666748\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.05614467263221741\n",
      "Training iteration: 2028\n",
      "Validation loss (no improvement): 0.05592038631439209\n",
      "Training iteration: 2029\n",
      "Validation loss (no improvement): 0.056046700477600096\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.05612894892692566\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.056611049175262454\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.05613422989845276\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.056385147571563723\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.05620626211166382\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.05654100775718689\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.05649309754371643\n",
      "Training iteration: 2037\n",
      "Validation loss (no improvement): 0.056233829259872435\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.05624946355819702\n",
      "Training iteration: 2039\n",
      "Validation loss (no improvement): 0.05607098340988159\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.05624918937683106\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.05576396584510803\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.05562097430229187\n",
      "Training iteration: 2043\n",
      "Improved validation loss from: 0.05541146993637085  to: 0.0552902340888977\n",
      "Training iteration: 2044\n",
      "Improved validation loss from: 0.0552902340888977  to: 0.055257880687713624\n",
      "Training iteration: 2045\n",
      "Improved validation loss from: 0.055257880687713624  to: 0.05524817705154419\n",
      "Training iteration: 2046\n",
      "Improved validation loss from: 0.05524817705154419  to: 0.05469626784324646\n",
      "Training iteration: 2047\n",
      "Improved validation loss from: 0.05469626784324646  to: 0.0545538604259491\n",
      "Training iteration: 2048\n",
      "Improved validation loss from: 0.0545538604259491  to: 0.054316604137420656\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.054365575313568115\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.054316604137420656  to: 0.05426846742630005\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.05426846742630005  to: 0.053910404443740845\n",
      "Training iteration: 2052\n",
      "Improved validation loss from: 0.053910404443740845  to: 0.05376287698745728\n",
      "Training iteration: 2053\n",
      "Improved validation loss from: 0.05376287698745728  to: 0.05357214212417603\n",
      "Training iteration: 2054\n",
      "Validation loss (no improvement): 0.053692400455474854\n",
      "Training iteration: 2055\n",
      "Validation loss (no improvement): 0.053593885898590085\n",
      "Training iteration: 2056\n",
      "Improved validation loss from: 0.05357214212417603  to: 0.05333266258239746\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.053347593545913695\n",
      "Training iteration: 2058\n",
      "Improved validation loss from: 0.05333266258239746  to: 0.05331385135650635\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.0533355176448822\n",
      "Training iteration: 2060\n",
      "Improved validation loss from: 0.05331385135650635  to: 0.0531688392162323\n",
      "Training iteration: 2061\n",
      "Improved validation loss from: 0.0531688392162323  to: 0.05300580263137818\n",
      "Training iteration: 2062\n",
      "Improved validation loss from: 0.05300580263137818  to: 0.05295902490615845\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): 0.05300934314727783\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.05316994786262512\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.05317710638046265\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.05315852165222168\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.05315162539482117\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.05309678316116333\n",
      "Training iteration: 2069\n",
      "Validation loss (no improvement): 0.053109943866729736\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.05295902490615845  to: 0.05286120176315308\n",
      "Training iteration: 2071\n",
      "Improved validation loss from: 0.05286120176315308  to: 0.05279747247695923\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.0528164803981781\n",
      "Training iteration: 2073\n",
      "Validation loss (no improvement): 0.05295180082321167\n",
      "Training iteration: 2074\n",
      "Validation loss (no improvement): 0.05292690396308899\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.052814590930938723\n",
      "Training iteration: 2076\n",
      "Improved validation loss from: 0.05279747247695923  to: 0.05268889665603638\n",
      "Training iteration: 2077\n",
      "Improved validation loss from: 0.05268889665603638  to: 0.05258058309555054\n",
      "Training iteration: 2078\n",
      "Improved validation loss from: 0.05258058309555054  to: 0.05257173776626587\n",
      "Training iteration: 2079\n",
      "Improved validation loss from: 0.05257173776626587  to: 0.05244941711425781\n",
      "Training iteration: 2080\n",
      "Improved validation loss from: 0.05244941711425781  to: 0.0524228036403656\n",
      "Training iteration: 2081\n",
      "Improved validation loss from: 0.0524228036403656  to: 0.05237218737602234\n",
      "Training iteration: 2082\n",
      "Improved validation loss from: 0.05237218737602234  to: 0.052155065536499026\n",
      "Training iteration: 2083\n",
      "Improved validation loss from: 0.052155065536499026  to: 0.052005499601364136\n",
      "Training iteration: 2084\n",
      "Improved validation loss from: 0.052005499601364136  to: 0.05186251401901245\n",
      "Training iteration: 2085\n",
      "Improved validation loss from: 0.05186251401901245  to: 0.0517942488193512\n",
      "Training iteration: 2086\n",
      "Improved validation loss from: 0.0517942488193512  to: 0.051790541410446166\n",
      "Training iteration: 2087\n",
      "Improved validation loss from: 0.051790541410446166  to: 0.05176531672477722\n",
      "Training iteration: 2088\n",
      "Improved validation loss from: 0.05176531672477722  to: 0.051664048433303834\n",
      "Training iteration: 2089\n",
      "Improved validation loss from: 0.051664048433303834  to: 0.05151013135910034\n",
      "Training iteration: 2090\n",
      "Improved validation loss from: 0.05151013135910034  to: 0.05147184133529663\n",
      "Training iteration: 2091\n",
      "Improved validation loss from: 0.05147184133529663  to: 0.05136253833770752\n",
      "Training iteration: 2092\n",
      "Improved validation loss from: 0.05136253833770752  to: 0.05135585069656372\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.051426041126251223\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.05144102573394775\n",
      "Training iteration: 2095\n",
      "Improved validation loss from: 0.05135585069656372  to: 0.05125693082809448\n",
      "Training iteration: 2096\n",
      "Improved validation loss from: 0.05125693082809448  to: 0.05105951428413391\n",
      "Training iteration: 2097\n",
      "Improved validation loss from: 0.05105951428413391  to: 0.05097675323486328\n",
      "Training iteration: 2098\n",
      "Validation loss (no improvement): 0.05102130174636841\n",
      "Training iteration: 2099\n",
      "Improved validation loss from: 0.05097675323486328  to: 0.050975239276885985\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.051044195890426636\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.051115894317626955\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.05117564201354981\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.05121324062347412\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.05101373791694641\n",
      "Training iteration: 2105\n",
      "Improved validation loss from: 0.050975239276885985  to: 0.05095340013504028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.05105687379837036\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.05095340013504028  to: 0.050810837745666505\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.05100669264793396\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.05097507238388062\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.05146435499191284\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.05153132677078247\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.05186727046966553\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.0512178361415863\n",
      "Training iteration: 2114\n",
      "Validation loss (no improvement): 0.05209502577781677\n",
      "Training iteration: 2115\n",
      "Validation loss (no improvement): 0.051393556594848636\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.0518150269985199\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.05184035897254944\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): 0.052181017398834226\n",
      "Training iteration: 2119\n",
      "Validation loss (no improvement): 0.05232040882110596\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.05248621702194214\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.05253360867500305\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.05213671326637268\n",
      "Training iteration: 2123\n",
      "Validation loss (no improvement): 0.05225631594657898\n",
      "Training iteration: 2124\n",
      "Validation loss (no improvement): 0.0515651524066925\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.05152859091758728\n",
      "Training iteration: 2126\n",
      "Validation loss (no improvement): 0.05114501118659973\n",
      "Training iteration: 2127\n",
      "Validation loss (no improvement): 0.051379501819610596\n",
      "Training iteration: 2128\n",
      "Validation loss (no improvement): 0.05099588632583618\n",
      "Training iteration: 2129\n",
      "Improved validation loss from: 0.050810837745666505  to: 0.05079841613769531\n",
      "Training iteration: 2130\n",
      "Improved validation loss from: 0.05079841613769531  to: 0.050515872240066526\n",
      "Training iteration: 2131\n",
      "Improved validation loss from: 0.050515872240066526  to: 0.05038598775863647\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.050867170095443726\n",
      "Training iteration: 2133\n",
      "Improved validation loss from: 0.05038598775863647  to: 0.04991739690303802\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.050326138734817505\n",
      "Training iteration: 2135\n",
      "Improved validation loss from: 0.04991739690303802  to: 0.049876832962036134\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.05072786808013916\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.050273454189300536\n",
      "Training iteration: 2138\n",
      "Validation loss (no improvement): 0.050139826536178586\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.05034741163253784\n",
      "Training iteration: 2140\n",
      "Validation loss (no improvement): 0.050531595945358276\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.05097439885139465\n",
      "Training iteration: 2142\n",
      "Validation loss (no improvement): 0.05065850019454956\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.050712805986404416\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.05019648671150208\n",
      "Training iteration: 2145\n",
      "Validation loss (no improvement): 0.050515884160995485\n",
      "Training iteration: 2146\n",
      "Validation loss (no improvement): 0.05035364031791687\n",
      "Training iteration: 2147\n",
      "Improved validation loss from: 0.049876832962036134  to: 0.049594339728355405\n",
      "Training iteration: 2148\n",
      "Improved validation loss from: 0.049594339728355405  to: 0.04948083758354187\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.04960994124412536\n",
      "Training iteration: 2150\n",
      "Validation loss (no improvement): 0.05021381378173828\n",
      "Training iteration: 2151\n",
      "Validation loss (no improvement): 0.050222575664520264\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.05014734268188477\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.04991067051887512\n",
      "Training iteration: 2154\n",
      "Validation loss (no improvement): 0.04965627193450928\n",
      "Training iteration: 2155\n",
      "Validation loss (no improvement): 0.050175780057907106\n",
      "Training iteration: 2156\n",
      "Improved validation loss from: 0.04948083758354187  to: 0.04918941557407379\n",
      "Training iteration: 2157\n",
      "Improved validation loss from: 0.04918941557407379  to: 0.049167332053184507\n",
      "Training iteration: 2158\n",
      "Improved validation loss from: 0.049167332053184507  to: 0.048974594473838805\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): 0.04945977330207825\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.049564528465271\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.0489943265914917\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.049103865027427675\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.04905482232570648\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.04970771372318268\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): 0.050213086605072024\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): 0.04960196614265442\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.05001614689826965\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.04927166998386383\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.049388614296913144\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.049908247590065\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.04903578758239746\n",
      "Training iteration: 2172\n",
      "Improved validation loss from: 0.048974594473838805  to: 0.048543152213096616\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): 0.048559895157814024\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): 0.048826032876968385\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.049404627084732054\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.04918212890625\n",
      "Training iteration: 2177\n",
      "Validation loss (no improvement): 0.049012431502342226\n",
      "Training iteration: 2178\n",
      "Validation loss (no improvement): 0.04854649007320404\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.048543152213096616  to: 0.04849471151828766\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): 0.04917743802070618\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.04849471151828766  to: 0.0480677992105484\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.0480677992105484  to: 0.04777429699897766\n",
      "Training iteration: 2183\n",
      "Validation loss (no improvement): 0.047921037673950194\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.04845985770225525\n",
      "Training iteration: 2185\n",
      "Validation loss (no improvement): 0.049058908224105836\n",
      "Training iteration: 2186\n",
      "Validation loss (no improvement): 0.048476728796958926\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): 0.047981709241867065\n",
      "Training iteration: 2188\n",
      "Improved validation loss from: 0.04777429699897766  to: 0.047570142149925235\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.04789316654205322\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): 0.04836124777793884\n",
      "Training iteration: 2191\n",
      "Improved validation loss from: 0.047570142149925235  to: 0.04744315147399902\n",
      "Training iteration: 2192\n",
      "Improved validation loss from: 0.04744315147399902  to: 0.04739975333213806\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.04758884906768799\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.04772879481315613\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.04791812896728516\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.0476240873336792\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.04743470549583435\n",
      "Training iteration: 2198\n",
      "Improved validation loss from: 0.04739975333213806  to: 0.04718455374240875\n",
      "Training iteration: 2199\n",
      "Improved validation loss from: 0.04718455374240875  to: 0.047124871611595155\n",
      "Training iteration: 2200\n",
      "Validation loss (no improvement): 0.04729354381561279\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): 0.047526150941848755\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): 0.04756918847560883\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.04738698601722717\n",
      "Training iteration: 2204\n",
      "Improved validation loss from: 0.047124871611595155  to: 0.04683946073055267\n",
      "Training iteration: 2205\n",
      "Improved validation loss from: 0.04683946073055267  to: 0.046221619844436644\n",
      "Training iteration: 2206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.046221619844436644  to: 0.04603308737277985\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.046054881811141965\n",
      "Training iteration: 2208\n",
      "Improved validation loss from: 0.04603308737277985  to: 0.04602251946926117\n",
      "Training iteration: 2209\n",
      "Improved validation loss from: 0.04602251946926117  to: 0.045770388841629026\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.04580960869789123\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.04609277844429016\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.0463052898645401\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.04621595442295075\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.04600335657596588\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.0459782212972641\n",
      "Training iteration: 2216\n",
      "Validation loss (no improvement): 0.046032142639160153\n",
      "Training iteration: 2217\n",
      "Validation loss (no improvement): 0.04620523452758789\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.04616694450378418\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.04585159420967102\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.045770388841629026  to: 0.045664891600608826\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.045664891600608826  to: 0.04561578333377838\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.04571237564086914\n",
      "Training iteration: 2223\n",
      "Improved validation loss from: 0.04561578333377838  to: 0.04551618099212647\n",
      "Training iteration: 2224\n",
      "Improved validation loss from: 0.04551618099212647  to: 0.0454751193523407\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): 0.04555743634700775\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.045601433515548705\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.045610538125038146\n",
      "Training iteration: 2228\n",
      "Improved validation loss from: 0.0454751193523407  to: 0.045229563117027284\n",
      "Training iteration: 2229\n",
      "Improved validation loss from: 0.045229563117027284  to: 0.045145589113235476\n",
      "Training iteration: 2230\n",
      "Improved validation loss from: 0.045145589113235476  to: 0.044914379715919495\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.04493280947208404\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.04502732753753662\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.04533699154853821\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.04515230655670166\n",
      "Training iteration: 2235\n",
      "Improved validation loss from: 0.044914379715919495  to: 0.04476292729377747\n",
      "Training iteration: 2236\n",
      "Improved validation loss from: 0.04476292729377747  to: 0.04460558295249939\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.04463491439819336\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.044845014810562134\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.044989728927612306\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.04477914273738861\n",
      "Training iteration: 2241\n",
      "Improved validation loss from: 0.04460558295249939  to: 0.04443641602993011\n",
      "Training iteration: 2242\n",
      "Improved validation loss from: 0.04443641602993011  to: 0.04428761601448059\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.04430325925350189\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): 0.04446442723274231\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.0445577323436737\n",
      "Training iteration: 2246\n",
      "Improved validation loss from: 0.04428761601448059  to: 0.04417999386787415\n",
      "Training iteration: 2247\n",
      "Improved validation loss from: 0.04417999386787415  to: 0.0439753919839859\n",
      "Training iteration: 2248\n",
      "Improved validation loss from: 0.0439753919839859  to: 0.0438822329044342\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.04395200610160828\n",
      "Training iteration: 2250\n",
      "Validation loss (no improvement): 0.04419674873352051\n",
      "Training iteration: 2251\n",
      "Validation loss (no improvement): 0.044317889213562014\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.04401314854621887\n",
      "Training iteration: 2253\n",
      "Improved validation loss from: 0.0438822329044342  to: 0.04383176863193512\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.04393123686313629\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.044197410345077515\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.04413260519504547\n",
      "Training iteration: 2257\n",
      "Improved validation loss from: 0.04383176863193512  to: 0.043796390295028687\n",
      "Training iteration: 2258\n",
      "Improved validation loss from: 0.043796390295028687  to: 0.043437260389328006\n",
      "Training iteration: 2259\n",
      "Improved validation loss from: 0.043437260389328006  to: 0.043403157591819765\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.043403783440589906\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.04370521605014801\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.044147104024887085\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.043995732069015504\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.04343446791172027\n",
      "Training iteration: 2265\n",
      "Improved validation loss from: 0.043403157591819765  to: 0.043217006325721743\n",
      "Training iteration: 2266\n",
      "Improved validation loss from: 0.043217006325721743  to: 0.04320222437381745\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): 0.04338076114654541\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.04382551312446594\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): 0.043901705741882326\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.043539699912071225\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.04324992299079895\n",
      "Training iteration: 2272\n",
      "Improved validation loss from: 0.04320222437381745  to: 0.04299271702766418\n",
      "Training iteration: 2273\n",
      "Improved validation loss from: 0.04299271702766418  to: 0.042900609970092776\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): 0.04295768737792969\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): 0.04302676320075989\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): 0.04325281083583832\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): 0.04346771240234375\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.04353482723236084\n",
      "Training iteration: 2279\n",
      "Validation loss (no improvement): 0.043303641676902774\n",
      "Training iteration: 2280\n",
      "Validation loss (no improvement): 0.043078461289405824\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.0429330438375473\n",
      "Training iteration: 2282\n",
      "Improved validation loss from: 0.042900609970092776  to: 0.0428358256816864\n",
      "Training iteration: 2283\n",
      "Improved validation loss from: 0.0428358256816864  to: 0.04282738566398621\n",
      "Training iteration: 2284\n",
      "Improved validation loss from: 0.04282738566398621  to: 0.04272947907447815\n",
      "Training iteration: 2285\n",
      "Improved validation loss from: 0.04272947907447815  to: 0.04248291552066803\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): 0.04248950481414795\n",
      "Training iteration: 2287\n",
      "Improved validation loss from: 0.04248291552066803  to: 0.04237306714057922\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.042406687140464784\n",
      "Training iteration: 2289\n",
      "Validation loss (no improvement): 0.04242564141750336\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): 0.04242731034755707\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.042521697282791135\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.04267679750919342\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): 0.04267326295375824\n",
      "Training iteration: 2294\n",
      "Improved validation loss from: 0.04237306714057922  to: 0.042362481355667114\n",
      "Training iteration: 2295\n",
      "Improved validation loss from: 0.042362481355667114  to: 0.04221890568733215\n",
      "Training iteration: 2296\n",
      "Improved validation loss from: 0.04221890568733215  to: 0.042062965035438535\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.04210960268974304\n",
      "Training iteration: 2298\n",
      "Improved validation loss from: 0.042062965035438535  to: 0.04190444052219391\n",
      "Training iteration: 2299\n",
      "Improved validation loss from: 0.04190444052219391  to: 0.04184629023075104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.04199712872505188\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): 0.04230012893676758\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.042053517699241635\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.04184629023075104  to: 0.04159129559993744\n",
      "Training iteration: 2304\n",
      "Validation loss (no improvement): 0.04173178672790527\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.041644778847694394\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): 0.042045006155967714\n",
      "Training iteration: 2307\n",
      "Validation loss (no improvement): 0.04246145188808441\n",
      "Training iteration: 2308\n",
      "Validation loss (no improvement): 0.04222412109375\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): 0.04190759658813477\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): 0.04185901284217834\n",
      "Training iteration: 2311\n",
      "Validation loss (no improvement): 0.04182496666908264\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.04183065891265869\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.04206236004829407\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.04219558835029602\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.04242310523986816\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.04210616946220398\n",
      "Training iteration: 2317\n",
      "Improved validation loss from: 0.04159129559993744  to: 0.04155181050300598\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.041680487990379336\n",
      "Training iteration: 2319\n",
      "Improved validation loss from: 0.04155181050300598  to: 0.041418331861495974\n",
      "Training iteration: 2320\n",
      "Improved validation loss from: 0.041418331861495974  to: 0.04139087200164795\n",
      "Training iteration: 2321\n",
      "Validation loss (no improvement): 0.04166224002838135\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): 0.041927894949913024\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.041983109712600705\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.041974622011184695\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): 0.041848549246788026\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): 0.041456174850463864\n",
      "Training iteration: 2327\n",
      "Improved validation loss from: 0.04139087200164795  to: 0.04129640460014343\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.041332978010177615\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.04160236716270447\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): 0.041596990823745725\n",
      "Training iteration: 2331\n",
      "Improved validation loss from: 0.04129640460014343  to: 0.04107026159763336\n",
      "Training iteration: 2332\n",
      "Improved validation loss from: 0.04107026159763336  to: 0.040516901016235354\n",
      "Training iteration: 2333\n",
      "Improved validation loss from: 0.040516901016235354  to: 0.040396252274513246\n",
      "Training iteration: 2334\n",
      "Improved validation loss from: 0.040396252274513246  to: 0.040158557891845706\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): 0.040392670035362246\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): 0.040890449285507204\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): 0.0406076192855835\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.04026973247528076\n",
      "Training iteration: 2339\n",
      "Improved validation loss from: 0.040158557891845706  to: 0.04008297324180603\n",
      "Training iteration: 2340\n",
      "Validation loss (no improvement): 0.04034141004085541\n",
      "Training iteration: 2341\n",
      "Validation loss (no improvement): 0.040117159485816956\n",
      "Training iteration: 2342\n",
      "Validation loss (no improvement): 0.040403860807418826\n",
      "Training iteration: 2343\n",
      "Validation loss (no improvement): 0.040689677000045776\n",
      "Training iteration: 2344\n",
      "Validation loss (no improvement): 0.04034244120121002\n",
      "Training iteration: 2345\n",
      "Improved validation loss from: 0.04008297324180603  to: 0.03954527974128723\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.03960922956466675\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.03955356180667877\n",
      "Training iteration: 2348\n",
      "Improved validation loss from: 0.03954527974128723  to: 0.03944575786590576\n",
      "Training iteration: 2349\n",
      "Validation loss (no improvement): 0.039893752336502074\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.04058504104614258\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.0400467574596405\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.039491182565689086\n",
      "Training iteration: 2353\n",
      "Improved validation loss from: 0.03944575786590576  to: 0.03931296765804291\n",
      "Training iteration: 2354\n",
      "Validation loss (no improvement): 0.039337795972824094\n",
      "Training iteration: 2355\n",
      "Validation loss (no improvement): 0.039606648683547976\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.04029388427734375\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.04030846655368805\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.039560440182685855\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.03931296765804291  to: 0.0391127347946167\n",
      "Training iteration: 2360\n",
      "Improved validation loss from: 0.0391127347946167  to: 0.03899028599262237\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.03906129002571106\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.0392777144908905\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.0400655210018158\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.03985952734947205\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.03929587006568909\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.03919557929039001\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.03921591341495514\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.03910887837409973\n",
      "Training iteration: 2369\n",
      "Improved validation loss from: 0.03899028599262237  to: 0.038562139868736266\n",
      "Training iteration: 2370\n",
      "Improved validation loss from: 0.038562139868736266  to: 0.03850846886634827\n",
      "Training iteration: 2371\n",
      "Improved validation loss from: 0.03850846886634827  to: 0.038459965586662294\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.03891043066978454\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.03875095248222351\n",
      "Training iteration: 2374\n",
      "Improved validation loss from: 0.038459965586662294  to: 0.03816015124320984\n",
      "Training iteration: 2375\n",
      "Improved validation loss from: 0.03816015124320984  to: 0.03796989321708679\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.03799907863140106\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.03832903504371643\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.03875269591808319\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.03854871392250061\n",
      "Training iteration: 2380\n",
      "Improved validation loss from: 0.03796989321708679  to: 0.03793272376060486\n",
      "Training iteration: 2381\n",
      "Improved validation loss from: 0.03793272376060486  to: 0.03765243887901306\n",
      "Training iteration: 2382\n",
      "Improved validation loss from: 0.03765243887901306  to: 0.03761505782604217\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.03781742751598358\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.0385135680437088\n",
      "Training iteration: 2385\n",
      "Validation loss (no improvement): 0.0388016551733017\n",
      "Training iteration: 2386\n",
      "Validation loss (no improvement): 0.03845253884792328\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.03768184781074524\n",
      "Training iteration: 2388\n",
      "Validation loss (no improvement): 0.037619581818580626\n",
      "Training iteration: 2389\n",
      "Validation loss (no improvement): 0.03772996068000793\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.038088181614875795\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): 0.03903678059577942\n",
      "Training iteration: 2392\n",
      "Validation loss (no improvement): 0.03926343619823456\n",
      "Training iteration: 2393\n",
      "Validation loss (no improvement): 0.038384878635406496\n",
      "Training iteration: 2394\n",
      "Validation loss (no improvement): 0.03792943060398102\n",
      "Training iteration: 2395\n",
      "Validation loss (no improvement): 0.03768884539604187\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): 0.03762873113155365\n",
      "Training iteration: 2397\n",
      "Improved validation loss from: 0.03761505782604217  to: 0.03747423887252808\n",
      "Training iteration: 2398\n",
      "Improved validation loss from: 0.03747423887252808  to: 0.03742985725402832\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.037531095743179324\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.037779226899147034\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.037467175722122194\n",
      "Training iteration: 2402\n",
      "Improved validation loss from: 0.03742985725402832  to: 0.03685650825500488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2403\n",
      "Improved validation loss from: 0.03685650825500488  to: 0.036687126755714415\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.03687901496887207\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.03727715909481048\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.03689212203025818\n",
      "Training iteration: 2407\n",
      "Improved validation loss from: 0.036687126755714415  to: 0.03641687631607056\n",
      "Training iteration: 2408\n",
      "Improved validation loss from: 0.03641687631607056  to: 0.036231580376625064\n",
      "Training iteration: 2409\n",
      "Validation loss (no improvement): 0.03630343973636627\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): 0.036451488733291626\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.036843737959861754\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.036735153198242186\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.03628031313419342\n",
      "Training iteration: 2414\n",
      "Improved validation loss from: 0.036231580376625064  to: 0.036153048276901245\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.03616251349449158\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.036371073126792906\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.03684278130531311\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.036906766891479495\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.03642335534095764\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.036287516355514526\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.03638014197349548\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.03641161322593689\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.03670230507850647\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.03742832243442536\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.0371088981628418\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.03658226132392883\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.036402386426925656\n",
      "Training iteration: 2428\n",
      "Validation loss (no improvement): 0.036502796411514285\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.036714759469032285\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.03660186529159546\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.036719787120819095\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.03666575253009796\n",
      "Training iteration: 2433\n",
      "Improved validation loss from: 0.036153048276901245  to: 0.03606249094009399\n",
      "Training iteration: 2434\n",
      "Improved validation loss from: 0.03606249094009399  to: 0.035962560772895814\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.03600030541419983\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.03658049404621124\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.03653375506401062\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.03605847954750061\n",
      "Training iteration: 2439\n",
      "Improved validation loss from: 0.035962560772895814  to: 0.035793477296829225\n",
      "Training iteration: 2440\n",
      "Improved validation loss from: 0.035793477296829225  to: 0.03574894070625305\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.035932451486587524\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.03605048060417175\n",
      "Training iteration: 2443\n",
      "Improved validation loss from: 0.03574894070625305  to: 0.03539714217185974\n",
      "Training iteration: 2444\n",
      "Improved validation loss from: 0.03539714217185974  to: 0.035240578651428225\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.03536320924758911\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): 0.03592741191387176\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.035835000872612\n",
      "Training iteration: 2448\n",
      "Validation loss (no improvement): 0.035339176654815674\n",
      "Training iteration: 2449\n",
      "Improved validation loss from: 0.035240578651428225  to: 0.035230106115341185\n",
      "Training iteration: 2450\n",
      "Validation loss (no improvement): 0.035777950286865236\n",
      "Training iteration: 2451\n",
      "Validation loss (no improvement): 0.035425609350204466\n",
      "Training iteration: 2452\n",
      "Validation loss (no improvement): 0.03575663864612579\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.036203807592391966\n",
      "Training iteration: 2454\n",
      "Validation loss (no improvement): 0.036021488904953006\n",
      "Training iteration: 2455\n",
      "Validation loss (no improvement): 0.0353453129529953\n",
      "Training iteration: 2456\n",
      "Validation loss (no improvement): 0.03530004322528839\n",
      "Training iteration: 2457\n",
      "Validation loss (no improvement): 0.035374611616134644\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.03581838607788086\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.03622565865516662\n",
      "Training iteration: 2460\n",
      "Validation loss (no improvement): 0.03599408268928528\n",
      "Training iteration: 2461\n",
      "Validation loss (no improvement): 0.035400408506393435\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.03531856536865234\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.035360592603683474\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.035867565870285036\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.03652814030647278\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.03639545738697052\n",
      "Training iteration: 2467\n",
      "Improved validation loss from: 0.035230106115341185  to: 0.03522252142429352\n",
      "Training iteration: 2468\n",
      "Improved validation loss from: 0.03522252142429352  to: 0.03497282266616821\n",
      "Training iteration: 2469\n",
      "Improved validation loss from: 0.03497282266616821  to: 0.03496925830841065\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.03525996208190918\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.03589513897895813\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.03563417792320252\n",
      "Training iteration: 2473\n",
      "Improved validation loss from: 0.03496925830841065  to: 0.03496679365634918\n",
      "Training iteration: 2474\n",
      "Improved validation loss from: 0.03496679365634918  to: 0.03481684625148773\n",
      "Training iteration: 2475\n",
      "Improved validation loss from: 0.03481684625148773  to: 0.03474118709564209\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.035124319791793826\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.03475998044013977\n",
      "Training iteration: 2478\n",
      "Improved validation loss from: 0.03474118709564209  to: 0.03458249270915985\n",
      "Training iteration: 2479\n",
      "Improved validation loss from: 0.03458249270915985  to: 0.034558406472206114\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.03474584221839905\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.03468053936958313\n",
      "Training iteration: 2482\n",
      "Validation loss (no improvement): 0.034605053067207334\n",
      "Training iteration: 2483\n",
      "Improved validation loss from: 0.034558406472206114  to: 0.03448073267936706\n",
      "Training iteration: 2484\n",
      "Improved validation loss from: 0.03448073267936706  to: 0.03409855961799622\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.03417454659938812\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.03448951840400696\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.03439663350582123\n",
      "Training iteration: 2488\n",
      "Improved validation loss from: 0.03409855961799622  to: 0.03381278812885284\n",
      "Training iteration: 2489\n",
      "Improved validation loss from: 0.03381278812885284  to: 0.03365090191364288\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): 0.03374217748641968\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.0340302437543869\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.033711329102516174\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.03371978402137756\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.033995595574378965\n",
      "Training iteration: 2495\n",
      "Improved validation loss from: 0.03365090191364288  to: 0.03359626233577728\n",
      "Training iteration: 2496\n",
      "Validation loss (no improvement): 0.03361736834049225\n",
      "Training iteration: 2497\n",
      "Validation loss (no improvement): 0.03383230566978455\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.03359626233577728  to: 0.033528342843055725\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.033528342843055725  to: 0.033500072360038755\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.03363687992095947\n",
      "Training iteration: 2501\n",
      "Validation loss (no improvement): 0.03412580788135529\n",
      "Training iteration: 2502\n",
      "Validation loss (no improvement): 0.03372277319431305\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.03364036977291107\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.03361586630344391\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.03351881206035614\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.03359585404396057\n",
      "Training iteration: 2507\n",
      "Improved validation loss from: 0.033500072360038755  to: 0.03311830163002014\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.03346118927001953\n",
      "Training iteration: 2509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03333564102649689\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.03327704071998596\n",
      "Training iteration: 2511\n",
      "Improved validation loss from: 0.03311830163002014  to: 0.0326773464679718\n",
      "Training iteration: 2512\n",
      "Improved validation loss from: 0.0326773464679718  to: 0.03255592882633209\n",
      "Training iteration: 2513\n",
      "Improved validation loss from: 0.03255592882633209  to: 0.032467854022979734\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.032722371816635135\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.033443665504455565\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): 0.03309341967105865\n",
      "Training iteration: 2517\n",
      "Improved validation loss from: 0.032467854022979734  to: 0.03219628930091858\n",
      "Training iteration: 2518\n",
      "Improved validation loss from: 0.03219628930091858  to: 0.031997451186180116\n",
      "Training iteration: 2519\n",
      "Improved validation loss from: 0.031997451186180116  to: 0.03198372721672058\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.03222623765468598\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): 0.033125418424606326\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.033150529861450194\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.032121962308883666\n",
      "Training iteration: 2524\n",
      "Improved validation loss from: 0.03198372721672058  to: 0.03198102414608002\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.03202702403068543\n",
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.03222252726554871\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): 0.033259472250938414\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.03305376172065735\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): 0.03210384547710419\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.03198102414608002  to: 0.03194742202758789\n",
      "Training iteration: 2531\n",
      "Improved validation loss from: 0.03194742202758789  to: 0.031930667161941526\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.03234966993331909\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.03251011669635773\n",
      "Training iteration: 2534\n",
      "Validation loss (no improvement): 0.032024848461151126\n",
      "Training iteration: 2535\n",
      "Improved validation loss from: 0.031930667161941526  to: 0.031371647119522096\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.031371647119522096  to: 0.03136193454265594\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.031412062048912046\n",
      "Training iteration: 2538\n",
      "Validation loss (no improvement): 0.031512030959129335\n",
      "Training iteration: 2539\n",
      "Validation loss (no improvement): 0.03263744115829468\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.032343429327011106\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.031685161590576175\n",
      "Training iteration: 2542\n",
      "Improved validation loss from: 0.03136193454265594  to: 0.031305789947509766\n",
      "Training iteration: 2543\n",
      "Validation loss (no improvement): 0.031311804056167604\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.03170515894889832\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.03197098672389984\n",
      "Training iteration: 2546\n",
      "Validation loss (no improvement): 0.03225886821746826\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.03194311261177063\n",
      "Training iteration: 2548\n",
      "Improved validation loss from: 0.031305789947509766  to: 0.031101936101913454\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.031393274664878845\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.031680384278297426\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.0312866747379303\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.032573023438453676\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.032183772325515746\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.03152011632919312\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.03121071457862854\n",
      "Training iteration: 2556\n",
      "Improved validation loss from: 0.031101936101913454  to: 0.031079524755477907\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.03132416009902954\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.03207683861255646\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.03235405385494232\n",
      "Training iteration: 2560\n",
      "Validation loss (no improvement): 0.031579285860061646\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.031387796998023985\n",
      "Training iteration: 2562\n",
      "Validation loss (no improvement): 0.031287765502929686\n",
      "Training iteration: 2563\n",
      "Validation loss (no improvement): 0.031567221879959105\n",
      "Training iteration: 2564\n",
      "Validation loss (no improvement): 0.031998687982559205\n",
      "Training iteration: 2565\n",
      "Validation loss (no improvement): 0.03175866007804871\n",
      "Training iteration: 2566\n",
      "Validation loss (no improvement): 0.03108542561531067\n",
      "Training iteration: 2567\n",
      "Improved validation loss from: 0.031079524755477907  to: 0.030919867753982543\n",
      "Training iteration: 2568\n",
      "Validation loss (no improvement): 0.03100912868976593\n",
      "Training iteration: 2569\n",
      "Validation loss (no improvement): 0.03141855597496033\n",
      "Training iteration: 2570\n",
      "Validation loss (no improvement): 0.03101029396057129\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.030919867753982543  to: 0.0307614266872406\n",
      "Training iteration: 2572\n",
      "Improved validation loss from: 0.0307614266872406  to: 0.030516228079795836\n",
      "Training iteration: 2573\n",
      "Improved validation loss from: 0.030516228079795836  to: 0.030401095747947693\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.031002214550971983\n",
      "Training iteration: 2575\n",
      "Validation loss (no improvement): 0.03106137812137604\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.030557286739349366\n",
      "Training iteration: 2577\n",
      "Improved validation loss from: 0.030401095747947693  to: 0.03033199906349182\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.030427923798561095\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.03086014688014984\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.03155807256698608\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.03145796358585358\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.031245338916778564\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.030487757921218873\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.031256431341171266\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): 0.031053319573402405\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.03077099323272705\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.03198959529399872\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.031710857152938844\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.03137548565864563\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.03091413378715515\n",
      "Training iteration: 2591\n",
      "Validation loss (no improvement): 0.030691391229629515\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.0310148686170578\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.030714312195777894\n",
      "Training iteration: 2594\n",
      "Validation loss (no improvement): 0.030930423736572267\n",
      "Training iteration: 2595\n",
      "Validation loss (no improvement): 0.030863997340202332\n",
      "Training iteration: 2596\n",
      "Improved validation loss from: 0.03033199906349182  to: 0.03025246262550354\n",
      "Training iteration: 2597\n",
      "Improved validation loss from: 0.03025246262550354  to: 0.029983490705490112\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.03006404936313629\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.030495578050613405\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.030316263437271118\n",
      "Training iteration: 2601\n",
      "Improved validation loss from: 0.029983490705490112  to: 0.0296965628862381\n",
      "Training iteration: 2602\n",
      "Improved validation loss from: 0.0296965628862381  to: 0.02955714464187622\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.029706859588623048\n",
      "Training iteration: 2604\n",
      "Improved validation loss from: 0.02955714464187622  to: 0.029436558485031128\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.029496103525161743\n",
      "Training iteration: 2606\n",
      "Improved validation loss from: 0.029436558485031128  to: 0.029432764649391173\n",
      "Training iteration: 2607\n",
      "Improved validation loss from: 0.029432764649391173  to: 0.029366892576217652\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.029612815380096434\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.029366892576217652  to: 0.029311850666999817\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.029311850666999817  to: 0.02920544445514679\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.02920544445514679  to: 0.028903520107269286\n",
      "Training iteration: 2612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.028903520107269286  to: 0.02879185676574707\n",
      "Training iteration: 2613\n",
      "Improved validation loss from: 0.02879185676574707  to: 0.028742703795433044\n",
      "Training iteration: 2614\n",
      "Validation loss (no improvement): 0.029327839612960815\n",
      "Training iteration: 2615\n",
      "Validation loss (no improvement): 0.029272276163101196\n",
      "Training iteration: 2616\n",
      "Validation loss (no improvement): 0.02885148227214813\n",
      "Training iteration: 2617\n",
      "Improved validation loss from: 0.028742703795433044  to: 0.028398799896240234\n",
      "Training iteration: 2618\n",
      "Validation loss (no improvement): 0.028515970706939696\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.029008135199546814\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.030007213354110718\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.029768741130828856\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.029121357202529907\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.029081887006759642\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.029401811957359313\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.029969340562820433\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.03079049289226532\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.03076061010360718\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.029774945974349976\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.02983780801296234\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.02993592619895935\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.03002110719680786\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.029912909865379332\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.029412466287612914\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.029062259197235107\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.028985577821731567\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.029350370168685913\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.028992468118667604\n",
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.028450605273246766\n",
      "Training iteration: 2639\n",
      "Improved validation loss from: 0.028398799896240234  to: 0.028178441524505615\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.028181546926498414\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.028872844576835633\n",
      "Training iteration: 2642\n",
      "Validation loss (no improvement): 0.028637009859085082\n",
      "Training iteration: 2643\n",
      "Improved validation loss from: 0.028178441524505615  to: 0.027796411514282228\n",
      "Training iteration: 2644\n",
      "Improved validation loss from: 0.027796411514282228  to: 0.027576765418052672\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.027648034691810607\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.028131604194641113\n",
      "Training iteration: 2647\n",
      "Validation loss (no improvement): 0.028841397166252135\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.028525862097740173\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.02818153500556946\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.028216972947120667\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.02798633575439453\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.028487905859947205\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.028436893224716188\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.02815936505794525\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.027851849794387817\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.027806320786476137\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.028091821074485778\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.028938549757003783\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.02895284295082092\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.028257939219474792\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.02793973982334137\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.02794874906539917\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.028295791149139403\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.028162309527397157\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.02797985076904297\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.02761703133583069\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.027724486589431763\n",
      "Training iteration: 2668\n",
      "Improved validation loss from: 0.027576765418052672  to: 0.027541381120681763\n",
      "Training iteration: 2669\n",
      "Validation loss (no improvement): 0.027947434782981874\n",
      "Training iteration: 2670\n",
      "Validation loss (no improvement): 0.027926793694496153\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.027561575174331665\n",
      "Training iteration: 2672\n",
      "Improved validation loss from: 0.027541381120681763  to: 0.02727142870426178\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.02727142870426178  to: 0.02721472382545471\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.027675387263298035\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.027825167775154112\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.027662426233291626\n",
      "Training iteration: 2677\n",
      "Improved validation loss from: 0.02721472382545471  to: 0.027172204852104188\n",
      "Training iteration: 2678\n",
      "Improved validation loss from: 0.027172204852104188  to: 0.027153599262237548\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.027504163980484008\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.02808147668838501\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): 0.028116828203201293\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.027623242139816283\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.027276402711868285\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): 0.027498805522918703\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.02742883563041687\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.02746317386627197\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.02757469117641449\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.027308639883995057\n",
      "Training iteration: 2689\n",
      "Improved validation loss from: 0.027153599262237548  to: 0.026989632844924928\n",
      "Training iteration: 2690\n",
      "Improved validation loss from: 0.026989632844924928  to: 0.026922780275344848\n",
      "Training iteration: 2691\n",
      "Validation loss (no improvement): 0.027157536149024962\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.027207058668136597\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.026922780275344848  to: 0.026809191703796385\n",
      "Training iteration: 2694\n",
      "Improved validation loss from: 0.026809191703796385  to: 0.02672080397605896\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.02678973078727722\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.02680099904537201\n",
      "Training iteration: 2697\n",
      "Improved validation loss from: 0.02672080397605896  to: 0.026567012071609497\n",
      "Training iteration: 2698\n",
      "Validation loss (no improvement): 0.026620516180992128\n",
      "Training iteration: 2699\n",
      "Validation loss (no improvement): 0.026738211512565613\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.027360981702804564\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.027459284663200377\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.026826423406600953\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.026780179142951964\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.02669960558414459\n",
      "Training iteration: 2705\n",
      "Improved validation loss from: 0.026567012071609497  to: 0.026444032788276672\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.0269317626953125\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.02673110067844391\n",
      "Training iteration: 2708\n",
      "Improved validation loss from: 0.026444032788276672  to: 0.026335617899894713\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.026472610235214234\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.026485925912857054\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.02697669863700867\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.026916655898094177\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.026341482996940613\n",
      "Training iteration: 2714\n",
      "Improved validation loss from: 0.026335617899894713  to: 0.026148256659507752\n",
      "Training iteration: 2715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.026148256659507752  to: 0.026032689213752746\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.026278936862945558\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.026188534498214722\n",
      "Training iteration: 2718\n",
      "Improved validation loss from: 0.026032689213752746  to: 0.02596903145313263\n",
      "Training iteration: 2719\n",
      "Improved validation loss from: 0.02596903145313263  to: 0.025915616750717164\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.025997447967529296\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.026320251822471618\n",
      "Training iteration: 2722\n",
      "Validation loss (no improvement): 0.02596319317817688\n",
      "Training iteration: 2723\n",
      "Validation loss (no improvement): 0.02592431604862213\n",
      "Training iteration: 2724\n",
      "Improved validation loss from: 0.025915616750717164  to: 0.025814780592918397\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.025955256819725037\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.02618442475795746\n",
      "Training iteration: 2727\n",
      "Validation loss (no improvement): 0.025969752669334413\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.025814780592918397  to: 0.025789701938629152\n",
      "Training iteration: 2729\n",
      "Improved validation loss from: 0.025789701938629152  to: 0.025507181882858276\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.025578641891479494\n",
      "Training iteration: 2731\n",
      "Validation loss (no improvement): 0.02560895085334778\n",
      "Training iteration: 2732\n",
      "Improved validation loss from: 0.025507181882858276  to: 0.02542150616645813\n",
      "Training iteration: 2733\n",
      "Improved validation loss from: 0.02542150616645813  to: 0.02511977553367615\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.02529643177986145\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): 0.025173649191856384\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.025183245539665222\n",
      "Training iteration: 2737\n",
      "Improved validation loss from: 0.02511977553367615  to: 0.025039759278297425\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.025277748703956604\n",
      "Training iteration: 2739\n",
      "Validation loss (no improvement): 0.025210869312286378\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.02528354525566101\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.025318440794944764\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.025091633200645447\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.02523576617240906\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.02561606764793396\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.025830620527267457\n",
      "Training iteration: 2746\n",
      "Validation loss (no improvement): 0.02574462294578552\n",
      "Training iteration: 2747\n",
      "Validation loss (no improvement): 0.025339752435684204\n",
      "Training iteration: 2748\n",
      "Validation loss (no improvement): 0.025321105122566225\n",
      "Training iteration: 2749\n",
      "Validation loss (no improvement): 0.02522163689136505\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.025503939390182494\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.02543303370475769\n",
      "Training iteration: 2752\n",
      "Validation loss (no improvement): 0.025159081816673277\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.025039759278297425  to: 0.024890899658203125\n",
      "Training iteration: 2754\n",
      "Improved validation loss from: 0.024890899658203125  to: 0.024807536602020265\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.025281077623367308\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.02542884349822998\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.02483755797147751\n",
      "Training iteration: 2758\n",
      "Improved validation loss from: 0.024807536602020265  to: 0.024417352676391602\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.024428386986255646\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.024865946173667906\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.025686177611351012\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.025519046187400817\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.024761518836021422\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.024616572260856628\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.02490086555480957\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.02524365484714508\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.025698989629745483\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.025735962390899658\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.025107839703559877\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): 0.02534398138523102\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.02515951991081238\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.02537058889865875\n",
      "Training iteration: 2773\n",
      "Validation loss (no improvement): 0.02523910105228424\n",
      "Training iteration: 2774\n",
      "Validation loss (no improvement): 0.025146442651748657\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.024654746055603027\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.024520036578178406\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.024902839958667756\n",
      "Training iteration: 2778\n",
      "Validation loss (no improvement): 0.024764123558998107\n",
      "Training iteration: 2779\n",
      "Improved validation loss from: 0.024417352676391602  to: 0.024344901740550994\n",
      "Training iteration: 2780\n",
      "Improved validation loss from: 0.024344901740550994  to: 0.023926372826099395\n",
      "Training iteration: 2781\n",
      "Improved validation loss from: 0.023926372826099395  to: 0.023853024840354918\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): 0.02436918318271637\n",
      "Training iteration: 2783\n",
      "Validation loss (no improvement): 0.024945826828479768\n",
      "Training iteration: 2784\n",
      "Validation loss (no improvement): 0.024567055702209472\n",
      "Training iteration: 2785\n",
      "Validation loss (no improvement): 0.024002154171466828\n",
      "Training iteration: 2786\n",
      "Validation loss (no improvement): 0.02390991002321243\n",
      "Training iteration: 2787\n",
      "Validation loss (no improvement): 0.02457270622253418\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.025064319372177124\n",
      "Training iteration: 2789\n",
      "Validation loss (no improvement): 0.02555193305015564\n",
      "Training iteration: 2790\n",
      "Validation loss (no improvement): 0.025671154260635376\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.024631372094154357\n",
      "Training iteration: 2792\n",
      "Validation loss (no improvement): 0.02519024908542633\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.02502661943435669\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.024185340106487273\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.02436559945344925\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.024409332871437074\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): 0.024279293417930604\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): 0.023965677618980406\n",
      "Training iteration: 2799\n",
      "Improved validation loss from: 0.023853024840354918  to: 0.02372225224971771\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.02390044629573822\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.024362704157829283\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.024949567019939424\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.024363823235034943\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.023807565867900848\n",
      "Training iteration: 2805\n",
      "Improved validation loss from: 0.02372225224971771  to: 0.023566766083240508\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.02360547035932541\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): 0.023839661478996278\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): 0.024220474064350128\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.023906807601451873\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.023655875027179717\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.023883560299873353\n",
      "Training iteration: 2812\n",
      "Improved validation loss from: 0.023566766083240508  to: 0.023418602347373963\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.023910704255104064\n",
      "Training iteration: 2814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.023770146071910858\n",
      "Training iteration: 2815\n",
      "Improved validation loss from: 0.023418602347373963  to: 0.023339371383190154\n",
      "Training iteration: 2816\n",
      "Improved validation loss from: 0.023339371383190154  to: 0.023324647545814516\n",
      "Training iteration: 2817\n",
      "Improved validation loss from: 0.023324647545814516  to: 0.02325786054134369\n",
      "Training iteration: 2818\n",
      "Validation loss (no improvement): 0.02348305881023407\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.02381129711866379\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.023614129424095152\n",
      "Training iteration: 2821\n",
      "Validation loss (no improvement): 0.023280110955238343\n",
      "Training iteration: 2822\n",
      "Improved validation loss from: 0.02325786054134369  to: 0.02287510335445404\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.022939424216747283\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.023171262443065645\n",
      "Training iteration: 2825\n",
      "Validation loss (no improvement): 0.023531076312065125\n",
      "Training iteration: 2826\n",
      "Validation loss (no improvement): 0.023257653415203094\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.022923359274864198\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.022979232668876647\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.02287510335445404  to: 0.02271726131439209\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.02285226881504059\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): 0.022794942557811736\n",
      "Training iteration: 2832\n",
      "Improved validation loss from: 0.02271726131439209  to: 0.022381286323070525\n",
      "Training iteration: 2833\n",
      "Improved validation loss from: 0.022381286323070525  to: 0.0221652626991272\n",
      "Training iteration: 2834\n",
      "Improved validation loss from: 0.0221652626991272  to: 0.022066406905651093\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.022380802035331725\n",
      "Training iteration: 2836\n",
      "Validation loss (no improvement): 0.02215237319469452\n",
      "Training iteration: 2837\n",
      "Improved validation loss from: 0.022066406905651093  to: 0.02194485366344452\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): 0.022047436237335204\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.022039178013801574\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.022303251922130583\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.022403693199157713\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.022387130558490752\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.022572919726371765\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.022464290261268616\n",
      "Training iteration: 2845\n",
      "Validation loss (no improvement): 0.022688913345336913\n",
      "Training iteration: 2846\n",
      "Validation loss (no improvement): 0.022738881409168243\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.02271346151828766\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.022610747814178468\n",
      "Training iteration: 2849\n",
      "Validation loss (no improvement): 0.02252175360918045\n",
      "Training iteration: 2850\n",
      "Validation loss (no improvement): 0.02252827137708664\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.02217903584241867\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.022538897395133973\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.022631868720054626\n",
      "Training iteration: 2854\n",
      "Validation loss (no improvement): 0.022648139297962187\n",
      "Training iteration: 2855\n",
      "Validation loss (no improvement): 0.022019645571708678\n",
      "Training iteration: 2856\n",
      "Improved validation loss from: 0.02194485366344452  to: 0.021938574314117432\n",
      "Training iteration: 2857\n",
      "Validation loss (no improvement): 0.022381071746349335\n",
      "Training iteration: 2858\n",
      "Validation loss (no improvement): 0.022690601646900177\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.02257550656795502\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.02196653187274933\n",
      "Training iteration: 2861\n",
      "Improved validation loss from: 0.021938574314117432  to: 0.02184312343597412\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): 0.02205653637647629\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.02252488136291504\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.022408132255077363\n",
      "Training iteration: 2865\n",
      "Validation loss (no improvement): 0.021894845366477966\n",
      "Training iteration: 2866\n",
      "Improved validation loss from: 0.02184312343597412  to: 0.02147466391324997\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.02158661335706711\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.021784333884716033\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.02198559492826462\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): 0.021819357573986054\n",
      "Training iteration: 2871\n",
      "Improved validation loss from: 0.02147466391324997  to: 0.02134992629289627\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.02137560397386551\n",
      "Training iteration: 2873\n",
      "Validation loss (no improvement): 0.02145562469959259\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.021923616528511047\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.021651923656463623\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.02134992629289627  to: 0.02125508487224579\n",
      "Training iteration: 2877\n",
      "Validation loss (no improvement): 0.021434763073921205\n",
      "Training iteration: 2878\n",
      "Improved validation loss from: 0.02125508487224579  to: 0.020990073680877686\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.02131562978029251\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.021579892933368684\n",
      "Training iteration: 2881\n",
      "Validation loss (no improvement): 0.021948647499084473\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.021751534938812257\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.02171700894832611\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.021991705894470213\n",
      "Training iteration: 2885\n",
      "Validation loss (no improvement): 0.022041475772857665\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.02186685800552368\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.02137486934661865\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): 0.021209874749183656\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.021307849884033205\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): 0.02165146768093109\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): 0.022164086997509002\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.02185358703136444\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.021558623015880584\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.021509747207164764\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.021502439677715302\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.021477146446704863\n",
      "Training iteration: 2897\n",
      "Improved validation loss from: 0.020990073680877686  to: 0.02098352015018463\n",
      "Training iteration: 2898\n",
      "Improved validation loss from: 0.02098352015018463  to: 0.02082771360874176\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.02089967727661133\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.02137196958065033\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.02113073766231537\n",
      "Training iteration: 2902\n",
      "Improved validation loss from: 0.02082771360874176  to: 0.020631805062294006\n",
      "Training iteration: 2903\n",
      "Improved validation loss from: 0.020631805062294006  to: 0.020612744987010954\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.02070980966091156\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.020999917387962343\n",
      "Training iteration: 2906\n",
      "Validation loss (no improvement): 0.020826101303100586\n",
      "Training iteration: 2907\n",
      "Improved validation loss from: 0.020612744987010954  to: 0.02059938609600067\n",
      "Training iteration: 2908\n",
      "Validation loss (no improvement): 0.020901469886302947\n",
      "Training iteration: 2909\n",
      "Validation loss (no improvement): 0.0206025630235672\n",
      "Training iteration: 2910\n",
      "Validation loss (no improvement): 0.02095980942249298\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.021000111103057863\n",
      "Training iteration: 2912\n",
      "Improved validation loss from: 0.02059938609600067  to: 0.02055070102214813\n",
      "Training iteration: 2913\n",
      "Improved validation loss from: 0.02055070102214813  to: 0.020238494873046874\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.020238494873046874  to: 0.020207814872264862\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): 0.02047791928052902\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.020738391578197478\n",
      "Training iteration: 2917\n",
      "Validation loss (no improvement): 0.020416870713233948\n",
      "Training iteration: 2918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02058844119310379\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.020884685218334198\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): 0.021450504660606384\n",
      "Training iteration: 2921\n",
      "Validation loss (no improvement): 0.021285662055015565\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.020581862330436705\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.020569059252738952\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.020410323143005372\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.020747025310993195\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.020878250896930694\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.02050008773803711\n",
      "Training iteration: 2928\n",
      "Improved validation loss from: 0.020207814872264862  to: 0.02012694627046585\n",
      "Training iteration: 2929\n",
      "Validation loss (no improvement): 0.020308008790016173\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.02055881917476654\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.020838251709938048\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.020933684706687928\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.020678584277629853\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.02072187215089798\n",
      "Training iteration: 2935\n",
      "Validation loss (no improvement): 0.020385856926441192\n",
      "Training iteration: 2936\n",
      "Validation loss (no improvement): 0.020164699852466585\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.02050813436508179\n",
      "Training iteration: 2938\n",
      "Improved validation loss from: 0.02012694627046585  to: 0.020093312859535216\n",
      "Training iteration: 2939\n",
      "Improved validation loss from: 0.020093312859535216  to: 0.020002444088459016\n",
      "Training iteration: 2940\n",
      "Improved validation loss from: 0.020002444088459016  to: 0.019971585273742674\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.020392851531505586\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.02066277265548706\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.020406922698020934\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.020092813670635222\n",
      "Training iteration: 2945\n",
      "Validation loss (no improvement): 0.0202055424451828\n",
      "Training iteration: 2946\n",
      "Validation loss (no improvement): 0.020506437122821807\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.02013700008392334\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.019988706707954405\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.020208625495433806\n",
      "Training iteration: 2950\n",
      "Improved validation loss from: 0.019971585273742674  to: 0.019916160404682158\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.019922073185443877\n",
      "Training iteration: 2952\n",
      "Improved validation loss from: 0.019916160404682158  to: 0.01981676071882248\n",
      "Training iteration: 2953\n",
      "Improved validation loss from: 0.01981676071882248  to: 0.019753512740135194\n",
      "Training iteration: 2954\n",
      "Improved validation loss from: 0.019753512740135194  to: 0.01958683431148529\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.01959261894226074\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.020038357377052306\n",
      "Training iteration: 2957\n",
      "Validation loss (no improvement): 0.01962890625\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.01958683431148529  to: 0.019493748247623444\n",
      "Training iteration: 2959\n",
      "Improved validation loss from: 0.019493748247623444  to: 0.019090691208839418\n",
      "Training iteration: 2960\n",
      "Improved validation loss from: 0.019090691208839418  to: 0.018934468924999236\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.019689598679542543\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.019642610847949982\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.019219441711902617\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): 0.019007398188114165\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.019054217636585234\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.01961250603199005\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.020659080147743224\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.0202887624502182\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.01935742050409317\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.019336798787117006\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.019699636101722717\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.020043811202049254\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.020518389344215394\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): 0.020596972107887267\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.01980041265487671\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.01997431218624115\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.019327111542224884\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.01944625675678253\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.0193849042057991\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.0195114403963089\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.01985052675008774\n",
      "Training iteration: 2982\n",
      "Validation loss (no improvement): 0.01955396980047226\n",
      "Training iteration: 2983\n",
      "Validation loss (no improvement): 0.01983223408460617\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.01990809738636017\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.01917712688446045\n",
      "Training iteration: 2986\n",
      "Validation loss (no improvement): 0.02031009495258331\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.020256829261779786\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.019542621076107027\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.019052164256572725\n",
      "Training iteration: 2990\n",
      "Improved validation loss from: 0.018934468924999236  to: 0.018919813632965087\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.019287779927253723\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.019721785187721254\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.01956835240125656\n",
      "Training iteration: 2994\n",
      "Validation loss (no improvement): 0.019085115194320677\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.01929696500301361\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.019271326065063477\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.019824311137199402\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.019863662123680115\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.019535866379737855\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.018982824683189393\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.019023768603801727\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.019167080521583557\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.019225648045539855\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.019291457533836365\n",
      "Training iteration: 3005\n",
      "Improved validation loss from: 0.018919813632965087  to: 0.018826352059841157\n",
      "Training iteration: 3006\n",
      "Improved validation loss from: 0.018826352059841157  to: 0.018733954429626463\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.018852095305919647\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.018835583329200746\n",
      "Training iteration: 3009\n",
      "Validation loss (no improvement): 0.018876507878303528\n",
      "Training iteration: 3010\n",
      "Improved validation loss from: 0.018733954429626463  to: 0.018385681509971618\n",
      "Training iteration: 3011\n",
      "Improved validation loss from: 0.018385681509971618  to: 0.01821538507938385\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): 0.018243809044361115\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.018708889186382294\n",
      "Training iteration: 3014\n",
      "Improved validation loss from: 0.01821538507938385  to: 0.018119163811206818\n",
      "Training iteration: 3015\n",
      "Improved validation loss from: 0.018119163811206818  to: 0.017912878096103667\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.018166592717170714\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.018315924704074858\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.018496327102184296\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.018198911845684052\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.017928412556648253\n",
      "Training iteration: 3021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.018007662892341614\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): 0.0180481880903244\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.01874229609966278\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.01859157085418701\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.01843809485435486\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.01821001321077347\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.018292291462421416\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.018953259289264678\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.02011679708957672\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.019692319631576537\n",
      "Training iteration: 3031\n",
      "Validation loss (no improvement): 0.018734145164489745\n",
      "Training iteration: 3032\n",
      "Validation loss (no improvement): 0.01870143413543701\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.018776743113994597\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.018968626856803894\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.019857880473136903\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.02020076513290405\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.018936863541603087\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.01915963590145111\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.019898031651973725\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.018814441561698914\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.018601235747337342\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.019772203266620637\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.019123396277427672\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.018482470512390138\n",
      "Training iteration: 3045\n",
      "Validation loss (no improvement): 0.018414273858070374\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.018026064336299896\n",
      "Training iteration: 3047\n",
      "Validation loss (no improvement): 0.018113619089126586\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.018362373113632202\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.017918701469898223\n",
      "Training iteration: 3050\n",
      "Improved validation loss from: 0.017912878096103667  to: 0.01771111935377121\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.017763832211494447\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.018181833624839782\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.018176518380641937\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.018277034163475037\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.01849902719259262\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.017952603101730347\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.017747311294078826\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.01789412945508957\n",
      "Training iteration: 3059\n",
      "Improved validation loss from: 0.01771111935377121  to: 0.0173922598361969\n",
      "Training iteration: 3060\n",
      "Improved validation loss from: 0.0173922598361969  to: 0.01736599951982498\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.01746370792388916\n",
      "Training iteration: 3062\n",
      "Improved validation loss from: 0.01736599951982498  to: 0.017271585762500763\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.017389777302742004\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.017365744709968566\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.017707261443138122\n",
      "Training iteration: 3066\n",
      "Improved validation loss from: 0.017271585762500763  to: 0.01716320067644119\n",
      "Training iteration: 3067\n",
      "Improved validation loss from: 0.01716320067644119  to: 0.017010092735290527\n",
      "Training iteration: 3068\n",
      "Improved validation loss from: 0.017010092735290527  to: 0.016963425278663635\n",
      "Training iteration: 3069\n",
      "Improved validation loss from: 0.016963425278663635  to: 0.01660904586315155\n",
      "Training iteration: 3070\n",
      "Improved validation loss from: 0.01660904586315155  to: 0.01660788953304291\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.016813030838966368\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.017525355517864227\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.017180998623371125\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.01683487743139267\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.016784341633319856\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.017113029956817627\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.01810363233089447\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.017683547735214234\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.017204394936561583\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.017267921566963197\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.01732321232557297\n",
      "Training iteration: 3082\n",
      "Validation loss (no improvement): 0.018149837851524353\n",
      "Training iteration: 3083\n",
      "Validation loss (no improvement): 0.018006762862205504\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.017514991760253906\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.017532114684581757\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.017630612850189208\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.018261012434959412\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.018145182728767396\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.01760377436876297\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.017197948694229127\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.017007195949554445\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.016843244433403015\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.0169965997338295\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.017471417784690857\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.01733623445034027\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.01707145869731903\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.017328785359859468\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.017254051566123963\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.01745840013027191\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.016711780428886415\n",
      "Training iteration: 3101\n",
      "Improved validation loss from: 0.01660788953304291  to: 0.016394977271556855\n",
      "Training iteration: 3102\n",
      "Improved validation loss from: 0.016394977271556855  to: 0.016111202538013458\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.016312476992607117\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.01726875305175781\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.017589184641838073\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.01725047081708908\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.017428374290466307\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.017949938774108887\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.018156783282756807\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.017944575846195222\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.01732637584209442\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.017074911296367644\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.01682668477296829\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.0173710897564888\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.017322875559329987\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.017215004563331603\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.017190903425216675\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.017386212944984436\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.017681317031383516\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.017569224536418914\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.01705515831708908\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.017048631608486176\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.01659986823797226\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.017059922218322754\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.017366477847099306\n",
      "Training iteration: 3126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01681269407272339\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.017937278747558592\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.01759529560804367\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.017520451545715333\n",
      "Training iteration: 3130\n",
      "Validation loss (no improvement): 0.01722886264324188\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.017477069795131684\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.017431405186653138\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.017999084293842317\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.018305750191211702\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.018036098778247835\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.017171208560466767\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.01666916012763977\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.01659633219242096\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.0162384033203125\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.0169536754488945\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.016711661219596864\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.016809329390525818\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.01715901643037796\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.01711408793926239\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.017342934012413026\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.01710408180952072\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.016741950809955598\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.0161697655916214\n",
      "Training iteration: 3149\n",
      "Improved validation loss from: 0.016111202538013458  to: 0.016038110852241515\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.016112031042575838\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.0165393128991127\n",
      "Training iteration: 3152\n",
      "Validation loss (no improvement): 0.01677181273698807\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.017052941024303436\n",
      "Training iteration: 3154\n",
      "Validation loss (no improvement): 0.016835972666740417\n",
      "Training iteration: 3155\n",
      "Validation loss (no improvement): 0.016893287003040314\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.017107486724853516\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.016789042949676515\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.016607317328453063\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.016477809846401216\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.016432981193065643\n",
      "Training iteration: 3161\n",
      "Improved validation loss from: 0.016038110852241515  to: 0.01583179533481598\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.01583600342273712\n",
      "Training iteration: 3163\n",
      "Improved validation loss from: 0.01583179533481598  to: 0.015809260308742523\n",
      "Training iteration: 3164\n",
      "Improved validation loss from: 0.015809260308742523  to: 0.01559583842754364\n",
      "Training iteration: 3165\n",
      "Improved validation loss from: 0.01559583842754364  to: 0.01541305035352707\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.01555686891078949\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.015786440670490266\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.016219039261341096\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.016665247082710267\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.01627601534128189\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.016205424070358278\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.015906982123851776\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.016352060437202453\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.01615650802850723\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.016425110399723053\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.0160791277885437\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.016198202967643738\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.016035246849060058\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): 0.01639987379312515\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.016796204447746276\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.016425931453704835\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.016042371094226838\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.015942952036857604\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.016253206133842468\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.015791091322898864\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.015794675052165984\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.0155899778008461\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.015504606068134308\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.016460168361663818\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.01643097251653671\n",
      "Training iteration: 3191\n",
      "Validation loss (no improvement): 0.01614481210708618\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.01595374345779419\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.01573488563299179\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.01609756499528885\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.01615624576807022\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.015916995704174042\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.015859249234199523\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.015965433418750764\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.016483543813228606\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.017089858651161194\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.01699582636356354\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.016589590907096864\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.016075721383094786\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.01585870385169983\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.01610027253627777\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.01595701426267624\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.015674138069152833\n",
      "Training iteration: 3208\n",
      "Improved validation loss from: 0.01541305035352707  to: 0.015315599739551544\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.01547166407108307\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.01591423749923706\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.01609569489955902\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.015598781406879425\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.01541638970375061\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.015451937913894653\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.01544678509235382\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.01578199565410614\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.015432186424732208\n",
      "Training iteration: 3218\n",
      "Improved validation loss from: 0.015315599739551544  to: 0.015289001166820526\n",
      "Training iteration: 3219\n",
      "Improved validation loss from: 0.015289001166820526  to: 0.015033704042434693\n",
      "Training iteration: 3220\n",
      "Improved validation loss from: 0.015033704042434693  to: 0.014616414904594421\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.015234433114528656\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.015694114565849304\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.015913572907447816\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.01568872630596161\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.01599498689174652\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.0163672536611557\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.01681227684020996\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.01641480028629303\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): 0.015961992740631103\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.015740254521369935\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.016032485663890837\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.016188088059425353\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.01639557331800461\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.01676724702119827\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.016097383201122285\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.016146405041217803\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.01631835997104645\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.015640595555305482\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.015705385804176332\n",
      "Training iteration: 3240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.016306939721107482\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.016171565651893614\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.015638311207294465\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.015071186423301696\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.01502925604581833\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.015449777245521545\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.01589812934398651\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.015851590037345886\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.01576192080974579\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.016069865226745604\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.016744376718997957\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.01708327978849411\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.01728285402059555\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.016712971031665802\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.016448712348937987\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.0160151869058609\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.015454675257205962\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.015537720918655396\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.015704062581062318\n",
      "Training iteration: 3259\n",
      "Validation loss (no improvement): 0.015551938116550446\n",
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.015132133662700654\n",
      "Training iteration: 3261\n",
      "Validation loss (no improvement): 0.014945070445537566\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.014920842647552491\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.015272073447704315\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.015246883034706116\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.014893630146980285\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.01470952033996582\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.0148284912109375\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.014980430901050567\n",
      "Training iteration: 3269\n",
      "Improved validation loss from: 0.014616414904594421  to: 0.014564380049705505\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.014866060018539429\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.015040262043476105\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.01595630347728729\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.016354922950267792\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.016121530532836915\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.016209204494953156\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.015931281447410583\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.015714648365974426\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.01528674215078354\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.015132129192352295\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.015358957648277282\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.015699665248394012\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): 0.015742239356040955\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.015353919565677642\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.015034495294094086\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.014838588237762452\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.014858704805374146\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.014789864420890808\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.014913253486156464\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.0151670902967453\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.015150412917137146\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.014826227724552155\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.01462428867816925\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.015064874291419983\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.015246817469596862\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.015258902311325073\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.01506604701280594\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.01468873918056488\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.01525069922208786\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.01588187664747238\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.015378531813621522\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.01599026620388031\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.015832263231277465\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.01634579002857208\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.015843215584754943\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.015558841824531554\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.015820547938346863\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.01593150645494461\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.015872757136821746\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.015673565864562988\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.01555752158164978\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.015914687514305116\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.016180381178855896\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.016273769736289977\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.015808606147766115\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.015324410796165467\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.01533309817314148\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.01494167149066925\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.015160642564296722\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.015065772831439972\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.014733059704303742\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.014754018187522889\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.01465737521648407\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.014805397391319275\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.014879712462425232\n",
      "Training iteration: 3325\n",
      "Improved validation loss from: 0.014564380049705505  to: 0.014444597065448761\n",
      "Training iteration: 3326\n",
      "Improved validation loss from: 0.014444597065448761  to: 0.014194245636463165\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.014399127662181854\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.015012437105178833\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.015874476730823518\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.01571017801761627\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.015732108056545256\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.016008436679840088\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.01612100899219513\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.016467788815498353\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.016192933917045592\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.015617790818214416\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.015817877650260926\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.015848419070243834\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.015941108763217925\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.016099460422992706\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.01567147970199585\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.015414586663246155\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.015519927442073821\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.01559578776359558\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.015609191358089447\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.015021875500679016\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.014784875512123107\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.014765477180480957\n",
      "Training iteration: 3349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015278439223766326\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.015329553186893463\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.0152245432138443\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.015149649977684022\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.014815059304237366\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.014326424896717071\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.014267972111701966\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.014320804178714753\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.01425289809703827\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.014267756044864655\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.014521093666553497\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.014597615599632264\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.014468051493167877\n",
      "Training iteration: 3362\n",
      "Validation loss (no improvement): 0.014339759945869446\n",
      "Training iteration: 3363\n",
      "Validation loss (no improvement): 0.01475130021572113\n",
      "Training iteration: 3364\n",
      "Validation loss (no improvement): 0.015187931060791016\n",
      "Training iteration: 3365\n",
      "Validation loss (no improvement): 0.015055382251739502\n",
      "Training iteration: 3366\n",
      "Validation loss (no improvement): 0.014603491127490997\n",
      "Training iteration: 3367\n",
      "Validation loss (no improvement): 0.014414086937904358\n",
      "Training iteration: 3368\n",
      "Validation loss (no improvement): 0.014560016989707946\n",
      "Training iteration: 3369\n",
      "Validation loss (no improvement): 0.01428021490573883\n",
      "Training iteration: 3370\n",
      "Validation loss (no improvement): 0.014377167820930481\n",
      "Training iteration: 3371\n",
      "Validation loss (no improvement): 0.014928922057151794\n",
      "Training iteration: 3372\n",
      "Validation loss (no improvement): 0.015422901511192322\n",
      "Training iteration: 3373\n",
      "Validation loss (no improvement): 0.0156421422958374\n",
      "Training iteration: 3374\n",
      "Validation loss (no improvement): 0.015782639384269714\n",
      "Training iteration: 3375\n",
      "Validation loss (no improvement): 0.01565113365650177\n",
      "Training iteration: 3376\n",
      "Validation loss (no improvement): 0.015356038510799409\n",
      "Training iteration: 3377\n",
      "Validation loss (no improvement): 0.01484459787607193\n",
      "Training iteration: 3378\n",
      "Validation loss (no improvement): 0.014363229274749756\n",
      "Training iteration: 3379\n",
      "Validation loss (no improvement): 0.014311391115188598\n",
      "Training iteration: 3380\n",
      "Improved validation loss from: 0.014194245636463165  to: 0.013885696232318879\n",
      "Training iteration: 3381\n",
      "Validation loss (no improvement): 0.013940660655498505\n",
      "Training iteration: 3382\n",
      "Validation loss (no improvement): 0.014421288669109345\n",
      "Training iteration: 3383\n",
      "Validation loss (no improvement): 0.014958330988883972\n",
      "Training iteration: 3384\n",
      "Validation loss (no improvement): 0.015079070627689362\n",
      "Training iteration: 3385\n",
      "Validation loss (no improvement): 0.015335334837436676\n",
      "Training iteration: 3386\n",
      "Validation loss (no improvement): 0.01520906388759613\n",
      "Training iteration: 3387\n",
      "Validation loss (no improvement): 0.014896789193153381\n",
      "Training iteration: 3388\n",
      "Validation loss (no improvement): 0.014912359416484833\n",
      "Training iteration: 3389\n",
      "Validation loss (no improvement): 0.014623664319515228\n",
      "Training iteration: 3390\n",
      "Validation loss (no improvement): 0.014549638330936431\n",
      "Training iteration: 3391\n",
      "Validation loss (no improvement): 0.014456228911876678\n",
      "Training iteration: 3392\n",
      "Validation loss (no improvement): 0.014233018457889556\n",
      "Training iteration: 3393\n",
      "Validation loss (no improvement): 0.014526240527629852\n",
      "Training iteration: 3394\n",
      "Validation loss (no improvement): 0.014911694824695588\n",
      "Training iteration: 3395\n",
      "Validation loss (no improvement): 0.014622990787029267\n",
      "Training iteration: 3396\n",
      "Validation loss (no improvement): 0.014574854075908661\n",
      "Training iteration: 3397\n",
      "Validation loss (no improvement): 0.014736703038215638\n",
      "Training iteration: 3398\n",
      "Validation loss (no improvement): 0.015116277337074279\n",
      "Training iteration: 3399\n",
      "Validation loss (no improvement): 0.014958515763282776\n",
      "Training iteration: 3400\n",
      "Validation loss (no improvement): 0.014555878937244415\n",
      "Training iteration: 3401\n",
      "Validation loss (no improvement): 0.014555086195468903\n",
      "Training iteration: 3402\n",
      "Validation loss (no improvement): 0.015038922429084778\n",
      "Training iteration: 3403\n",
      "Validation loss (no improvement): 0.015594515204429626\n",
      "Training iteration: 3404\n",
      "Validation loss (no improvement): 0.014746741950511932\n",
      "Training iteration: 3405\n",
      "Validation loss (no improvement): 0.014840061962604522\n",
      "Training iteration: 3406\n",
      "Validation loss (no improvement): 0.015089902281761169\n",
      "Training iteration: 3407\n",
      "Validation loss (no improvement): 0.015839609503746032\n",
      "Training iteration: 3408\n",
      "Validation loss (no improvement): 0.015471598505973816\n",
      "Training iteration: 3409\n",
      "Validation loss (no improvement): 0.015420830249786377\n",
      "Training iteration: 3410\n",
      "Validation loss (no improvement): 0.015348246693611145\n",
      "Training iteration: 3411\n",
      "Validation loss (no improvement): 0.015141883492469787\n",
      "Training iteration: 3412\n",
      "Validation loss (no improvement): 0.014756710827350616\n",
      "Training iteration: 3413\n",
      "Validation loss (no improvement): 0.014651572704315186\n",
      "Training iteration: 3414\n",
      "Validation loss (no improvement): 0.01462724506855011\n",
      "Training iteration: 3415\n",
      "Validation loss (no improvement): 0.014604365825653077\n",
      "Training iteration: 3416\n",
      "Validation loss (no improvement): 0.014678926765918731\n",
      "Training iteration: 3417\n",
      "Validation loss (no improvement): 0.014856915175914764\n",
      "Training iteration: 3418\n",
      "Validation loss (no improvement): 0.014874550700187682\n",
      "Training iteration: 3419\n",
      "Validation loss (no improvement): 0.014300307631492615\n",
      "Training iteration: 3420\n",
      "Validation loss (no improvement): 0.014000201225280761\n",
      "Training iteration: 3421\n",
      "Improved validation loss from: 0.013885696232318879  to: 0.013852018117904662\n",
      "Training iteration: 3422\n",
      "Validation loss (no improvement): 0.014148680865764618\n",
      "Training iteration: 3423\n",
      "Validation loss (no improvement): 0.014235147833824157\n",
      "Training iteration: 3424\n",
      "Validation loss (no improvement): 0.014095991849899292\n",
      "Training iteration: 3425\n",
      "Validation loss (no improvement): 0.014479704201221466\n",
      "Training iteration: 3426\n",
      "Validation loss (no improvement): 0.015124544501304626\n",
      "Training iteration: 3427\n",
      "Validation loss (no improvement): 0.015637485682964324\n",
      "Training iteration: 3428\n",
      "Validation loss (no improvement): 0.014928534626960754\n",
      "Training iteration: 3429\n",
      "Validation loss (no improvement): 0.014770877361297608\n",
      "Training iteration: 3430\n",
      "Validation loss (no improvement): 0.014995820820331573\n",
      "Training iteration: 3431\n",
      "Validation loss (no improvement): 0.015366785228252411\n",
      "Training iteration: 3432\n",
      "Validation loss (no improvement): 0.015447105467319488\n",
      "Training iteration: 3433\n",
      "Validation loss (no improvement): 0.015396113693714141\n",
      "Training iteration: 3434\n",
      "Validation loss (no improvement): 0.015048940479755402\n",
      "Training iteration: 3435\n",
      "Validation loss (no improvement): 0.014421597123146057\n",
      "Training iteration: 3436\n",
      "Validation loss (no improvement): 0.014297696948051452\n",
      "Training iteration: 3437\n",
      "Validation loss (no improvement): 0.01434241384267807\n",
      "Training iteration: 3438\n",
      "Validation loss (no improvement): 0.013865357637405396\n",
      "Training iteration: 3439\n",
      "Improved validation loss from: 0.013852018117904662  to: 0.013840292394161225\n",
      "Training iteration: 3440\n",
      "Validation loss (no improvement): 0.014490586519241334\n",
      "Training iteration: 3441\n",
      "Validation loss (no improvement): 0.014929528534412383\n",
      "Training iteration: 3442\n",
      "Validation loss (no improvement): 0.014514896273612975\n",
      "Training iteration: 3443\n",
      "Validation loss (no improvement): 0.014099916815757752\n",
      "Training iteration: 3444\n",
      "Improved validation loss from: 0.013840292394161225  to: 0.01377633810043335\n",
      "Training iteration: 3445\n",
      "Validation loss (no improvement): 0.014105629920959473\n",
      "Training iteration: 3446\n",
      "Validation loss (no improvement): 0.014186188578605652\n",
      "Training iteration: 3447\n",
      "Validation loss (no improvement): 0.014544329047203064\n",
      "Training iteration: 3448\n",
      "Validation loss (no improvement): 0.014529073238372802\n",
      "Training iteration: 3449\n",
      "Validation loss (no improvement): 0.014854279160499573\n",
      "Training iteration: 3450\n",
      "Validation loss (no improvement): 0.015108473598957062\n",
      "Training iteration: 3451\n",
      "Validation loss (no improvement): 0.014222733676433563\n",
      "Training iteration: 3452\n",
      "Validation loss (no improvement): 0.014260636270046234\n",
      "Training iteration: 3453\n",
      "Validation loss (no improvement): 0.01427185833454132\n",
      "Training iteration: 3454\n",
      "Validation loss (no improvement): 0.014670193195343018\n",
      "Training iteration: 3455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.014395515620708465\n",
      "Training iteration: 3456\n",
      "Validation loss (no improvement): 0.014164997637271881\n",
      "Training iteration: 3457\n",
      "Validation loss (no improvement): 0.014290609955787658\n",
      "Training iteration: 3458\n",
      "Validation loss (no improvement): 0.014719365537166596\n",
      "Training iteration: 3459\n",
      "Validation loss (no improvement): 0.014594724774360657\n",
      "Training iteration: 3460\n",
      "Validation loss (no improvement): 0.01425657570362091\n",
      "Training iteration: 3461\n",
      "Validation loss (no improvement): 0.014159047603607177\n",
      "Training iteration: 3462\n",
      "Validation loss (no improvement): 0.014647540450096131\n",
      "Training iteration: 3463\n",
      "Validation loss (no improvement): 0.014873519539833069\n",
      "Training iteration: 3464\n",
      "Validation loss (no improvement): 0.014696106314659119\n",
      "Training iteration: 3465\n",
      "Validation loss (no improvement): 0.014313602447509765\n",
      "Training iteration: 3466\n",
      "Validation loss (no improvement): 0.014313709735870362\n",
      "Training iteration: 3467\n",
      "Validation loss (no improvement): 0.014190569519996643\n",
      "Training iteration: 3468\n",
      "Validation loss (no improvement): 0.013793951272964478\n",
      "Training iteration: 3469\n",
      "Validation loss (no improvement): 0.013971561193466186\n",
      "Training iteration: 3470\n",
      "Validation loss (no improvement): 0.014141221344470978\n",
      "Training iteration: 3471\n",
      "Validation loss (no improvement): 0.014778494834899902\n",
      "Training iteration: 3472\n",
      "Validation loss (no improvement): 0.014807641506195068\n",
      "Training iteration: 3473\n",
      "Validation loss (no improvement): 0.014386908710002899\n",
      "Training iteration: 3474\n",
      "Validation loss (no improvement): 0.014030149579048157\n",
      "Training iteration: 3475\n",
      "Validation loss (no improvement): 0.013886040449142456\n",
      "Training iteration: 3476\n",
      "Improved validation loss from: 0.01377633810043335  to: 0.013420470058918\n",
      "Training iteration: 3477\n",
      "Validation loss (no improvement): 0.014019207656383514\n",
      "Training iteration: 3478\n",
      "Validation loss (no improvement): 0.014458929002285004\n",
      "Training iteration: 3479\n",
      "Validation loss (no improvement): 0.014184725284576417\n",
      "Training iteration: 3480\n",
      "Validation loss (no improvement): 0.014350132644176483\n",
      "Training iteration: 3481\n",
      "Validation loss (no improvement): 0.014868417382240295\n",
      "Training iteration: 3482\n",
      "Validation loss (no improvement): 0.014638976752758026\n",
      "Training iteration: 3483\n",
      "Validation loss (no improvement): 0.014674931764602661\n",
      "Training iteration: 3484\n",
      "Validation loss (no improvement): 0.014600157737731934\n",
      "Training iteration: 3485\n",
      "Validation loss (no improvement): 0.014143411815166474\n",
      "Training iteration: 3486\n",
      "Validation loss (no improvement): 0.014004908502101898\n",
      "Training iteration: 3487\n",
      "Validation loss (no improvement): 0.013957707583904267\n",
      "Training iteration: 3488\n",
      "Validation loss (no improvement): 0.014115548133850098\n",
      "Training iteration: 3489\n",
      "Validation loss (no improvement): 0.013971489667892457\n",
      "Training iteration: 3490\n",
      "Validation loss (no improvement): 0.013773290812969208\n",
      "Training iteration: 3491\n",
      "Validation loss (no improvement): 0.0141029492020607\n",
      "Training iteration: 3492\n",
      "Validation loss (no improvement): 0.014585855603218078\n",
      "Training iteration: 3493\n",
      "Validation loss (no improvement): 0.014366206526756287\n",
      "Training iteration: 3494\n",
      "Validation loss (no improvement): 0.014149057865142822\n",
      "Training iteration: 3495\n",
      "Validation loss (no improvement): 0.013939881324768066\n",
      "Training iteration: 3496\n",
      "Validation loss (no improvement): 0.01390746533870697\n",
      "Training iteration: 3497\n",
      "Validation loss (no improvement): 0.014174720644950867\n",
      "Training iteration: 3498\n",
      "Validation loss (no improvement): 0.01419217735528946\n",
      "Training iteration: 3499\n",
      "Validation loss (no improvement): 0.014088229835033416\n",
      "Training iteration: 3500\n",
      "Validation loss (no improvement): 0.013673529028892517\n",
      "Training iteration: 3501\n",
      "Validation loss (no improvement): 0.013522335886955261\n",
      "Training iteration: 3502\n",
      "Improved validation loss from: 0.013420470058918  to: 0.013419525325298309\n",
      "Training iteration: 3503\n",
      "Validation loss (no improvement): 0.013533616065979004\n",
      "Training iteration: 3504\n",
      "Validation loss (no improvement): 0.01384231448173523\n",
      "Training iteration: 3505\n",
      "Validation loss (no improvement): 0.014133039116859435\n",
      "Training iteration: 3506\n",
      "Improved validation loss from: 0.013419525325298309  to: 0.013385453820228576\n",
      "Training iteration: 3507\n",
      "Improved validation loss from: 0.013385453820228576  to: 0.013173815608024598\n",
      "Training iteration: 3508\n",
      "Validation loss (no improvement): 0.013339325785636902\n",
      "Training iteration: 3509\n",
      "Validation loss (no improvement): 0.013905060291290284\n",
      "Training iteration: 3510\n",
      "Validation loss (no improvement): 0.013504722714424133\n",
      "Training iteration: 3511\n",
      "Validation loss (no improvement): 0.013798384368419648\n",
      "Training iteration: 3512\n",
      "Validation loss (no improvement): 0.014395536482334137\n",
      "Training iteration: 3513\n",
      "Validation loss (no improvement): 0.014464591443538666\n",
      "Training iteration: 3514\n",
      "Validation loss (no improvement): 0.014065256714820862\n",
      "Training iteration: 3515\n",
      "Validation loss (no improvement): 0.01395643651485443\n",
      "Training iteration: 3516\n",
      "Validation loss (no improvement): 0.01465710550546646\n",
      "Training iteration: 3517\n",
      "Validation loss (no improvement): 0.01503256857395172\n",
      "Training iteration: 3518\n",
      "Validation loss (no improvement): 0.01469503492116928\n",
      "Training iteration: 3519\n",
      "Validation loss (no improvement): 0.014839501678943634\n",
      "Training iteration: 3520\n",
      "Validation loss (no improvement): 0.014933371543884277\n",
      "Training iteration: 3521\n",
      "Validation loss (no improvement): 0.014725491404533386\n",
      "Training iteration: 3522\n",
      "Validation loss (no improvement): 0.013755646347999573\n",
      "Training iteration: 3523\n",
      "Improved validation loss from: 0.013173815608024598  to: 0.013058342039585114\n",
      "Training iteration: 3524\n",
      "Improved validation loss from: 0.013058342039585114  to: 0.012973460555076598\n",
      "Training iteration: 3525\n",
      "Validation loss (no improvement): 0.013114723563194274\n",
      "Training iteration: 3526\n",
      "Validation loss (no improvement): 0.013168089091777802\n",
      "Training iteration: 3527\n",
      "Validation loss (no improvement): 0.013410541415214538\n",
      "Training iteration: 3528\n",
      "Validation loss (no improvement): 0.013938434422016144\n",
      "Training iteration: 3529\n",
      "Validation loss (no improvement): 0.014021912217140197\n",
      "Training iteration: 3530\n",
      "Validation loss (no improvement): 0.013540232181549072\n",
      "Training iteration: 3531\n",
      "Validation loss (no improvement): 0.01322161704301834\n",
      "Training iteration: 3532\n",
      "Validation loss (no improvement): 0.01307649165391922\n",
      "Training iteration: 3533\n",
      "Validation loss (no improvement): 0.013371784985065461\n",
      "Training iteration: 3534\n",
      "Validation loss (no improvement): 0.013521352410316467\n",
      "Training iteration: 3535\n",
      "Validation loss (no improvement): 0.013831703364849091\n",
      "Training iteration: 3536\n",
      "Validation loss (no improvement): 0.013941031694412232\n",
      "Training iteration: 3537\n",
      "Validation loss (no improvement): 0.01400165855884552\n",
      "Training iteration: 3538\n",
      "Validation loss (no improvement): 0.014282731711864472\n",
      "Training iteration: 3539\n",
      "Validation loss (no improvement): 0.013953351974487304\n",
      "Training iteration: 3540\n",
      "Validation loss (no improvement): 0.013688768446445464\n",
      "Training iteration: 3541\n",
      "Validation loss (no improvement): 0.013814619183540345\n",
      "Training iteration: 3542\n",
      "Validation loss (no improvement): 0.01405019611120224\n",
      "Training iteration: 3543\n",
      "Validation loss (no improvement): 0.013350608944892883\n",
      "Training iteration: 3544\n",
      "Validation loss (no improvement): 0.013411752879619598\n",
      "Training iteration: 3545\n",
      "Validation loss (no improvement): 0.013617008924484253\n",
      "Training iteration: 3546\n",
      "Validation loss (no improvement): 0.013645873963832855\n",
      "Training iteration: 3547\n",
      "Validation loss (no improvement): 0.01376086324453354\n",
      "Training iteration: 3548\n",
      "Validation loss (no improvement): 0.013773930072784425\n",
      "Training iteration: 3549\n",
      "Validation loss (no improvement): 0.013857819139957428\n",
      "Training iteration: 3550\n",
      "Validation loss (no improvement): 0.014120247960090638\n",
      "Training iteration: 3551\n",
      "Validation loss (no improvement): 0.01401773989200592\n",
      "Training iteration: 3552\n",
      "Validation loss (no improvement): 0.01334056407213211\n",
      "Training iteration: 3553\n",
      "Validation loss (no improvement): 0.013064588606357574\n",
      "Training iteration: 3554\n",
      "Validation loss (no improvement): 0.013813064992427826\n",
      "Training iteration: 3555\n",
      "Validation loss (no improvement): 0.014133629202842713\n",
      "Training iteration: 3556\n",
      "Validation loss (no improvement): 0.013510823249816895\n",
      "Training iteration: 3557\n",
      "Validation loss (no improvement): 0.013894346356391907\n",
      "Training iteration: 3558\n",
      "Validation loss (no improvement): 0.014699529111385345\n",
      "Training iteration: 3559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015212750434875489\n",
      "Training iteration: 3560\n",
      "Validation loss (no improvement): 0.01423511803150177\n",
      "Training iteration: 3561\n",
      "Validation loss (no improvement): 0.013883380591869355\n",
      "Training iteration: 3562\n",
      "Validation loss (no improvement): 0.013805030286312104\n",
      "Training iteration: 3563\n",
      "Validation loss (no improvement): 0.013918422162532806\n",
      "Training iteration: 3564\n",
      "Validation loss (no improvement): 0.013370971381664275\n",
      "Training iteration: 3565\n",
      "Validation loss (no improvement): 0.013245947659015656\n",
      "Training iteration: 3566\n",
      "Validation loss (no improvement): 0.01333123743534088\n",
      "Training iteration: 3567\n",
      "Validation loss (no improvement): 0.013409599661827087\n",
      "Training iteration: 3568\n",
      "Validation loss (no improvement): 0.013187642395496368\n",
      "Training iteration: 3569\n",
      "Validation loss (no improvement): 0.01315082460641861\n",
      "Training iteration: 3570\n",
      "Improved validation loss from: 0.012973460555076598  to: 0.012920048832893372\n",
      "Training iteration: 3571\n",
      "Improved validation loss from: 0.012920048832893372  to: 0.012817630171775818\n",
      "Training iteration: 3572\n",
      "Validation loss (no improvement): 0.012898650765419007\n",
      "Training iteration: 3573\n",
      "Validation loss (no improvement): 0.012827534973621369\n",
      "Training iteration: 3574\n",
      "Improved validation loss from: 0.012817630171775818  to: 0.0127092644572258\n",
      "Training iteration: 3575\n",
      "Improved validation loss from: 0.0127092644572258  to: 0.0125364750623703\n",
      "Training iteration: 3576\n",
      "Validation loss (no improvement): 0.012930880486965179\n",
      "Training iteration: 3577\n",
      "Validation loss (no improvement): 0.0138465017080307\n",
      "Training iteration: 3578\n",
      "Validation loss (no improvement): 0.01369740515947342\n",
      "Training iteration: 3579\n",
      "Validation loss (no improvement): 0.013423772156238556\n",
      "Training iteration: 3580\n",
      "Validation loss (no improvement): 0.013691341876983643\n",
      "Training iteration: 3581\n",
      "Validation loss (no improvement): 0.014633375406265258\n",
      "Training iteration: 3582\n",
      "Validation loss (no improvement): 0.01446373164653778\n",
      "Training iteration: 3583\n",
      "Validation loss (no improvement): 0.013592496514320374\n",
      "Training iteration: 3584\n",
      "Validation loss (no improvement): 0.013258282840251923\n",
      "Training iteration: 3585\n",
      "Validation loss (no improvement): 0.013313280045986175\n",
      "Training iteration: 3586\n",
      "Validation loss (no improvement): 0.013351668417453767\n",
      "Training iteration: 3587\n",
      "Validation loss (no improvement): 0.013464504480361938\n",
      "Training iteration: 3588\n",
      "Validation loss (no improvement): 0.013380677998065948\n",
      "Training iteration: 3589\n",
      "Validation loss (no improvement): 0.013597311079502105\n",
      "Training iteration: 3590\n",
      "Validation loss (no improvement): 0.013540086150169373\n",
      "Training iteration: 3591\n",
      "Validation loss (no improvement): 0.013168801367282868\n",
      "Training iteration: 3592\n",
      "Validation loss (no improvement): 0.013207647204399108\n",
      "Training iteration: 3593\n",
      "Validation loss (no improvement): 0.013055753707885743\n",
      "Training iteration: 3594\n",
      "Validation loss (no improvement): 0.012844407558441162\n",
      "Training iteration: 3595\n",
      "Validation loss (no improvement): 0.012985479831695557\n",
      "Training iteration: 3596\n",
      "Validation loss (no improvement): 0.01312277615070343\n",
      "Training iteration: 3597\n",
      "Validation loss (no improvement): 0.012898588180541992\n",
      "Training iteration: 3598\n",
      "Validation loss (no improvement): 0.012651140987873077\n",
      "Training iteration: 3599\n",
      "Validation loss (no improvement): 0.01279076337814331\n",
      "Training iteration: 3600\n",
      "Validation loss (no improvement): 0.012895525991916656\n",
      "Training iteration: 3601\n",
      "Validation loss (no improvement): 0.012869200110435486\n",
      "Training iteration: 3602\n",
      "Validation loss (no improvement): 0.012705373764038085\n",
      "Training iteration: 3603\n",
      "Validation loss (no improvement): 0.012840978801250458\n",
      "Training iteration: 3604\n",
      "Validation loss (no improvement): 0.012747557461261749\n",
      "Training iteration: 3605\n",
      "Validation loss (no improvement): 0.01307709515094757\n",
      "Training iteration: 3606\n",
      "Validation loss (no improvement): 0.01346602439880371\n",
      "Training iteration: 3607\n",
      "Validation loss (no improvement): 0.013092024624347687\n",
      "Training iteration: 3608\n",
      "Validation loss (no improvement): 0.013116100430488586\n",
      "Training iteration: 3609\n",
      "Validation loss (no improvement): 0.012978523969650269\n",
      "Training iteration: 3610\n",
      "Validation loss (no improvement): 0.012752968072891235\n",
      "Training iteration: 3611\n",
      "Validation loss (no improvement): 0.01287938803434372\n",
      "Training iteration: 3612\n",
      "Validation loss (no improvement): 0.01284058839082718\n",
      "Training iteration: 3613\n",
      "Validation loss (no improvement): 0.012591217458248139\n",
      "Training iteration: 3614\n",
      "Validation loss (no improvement): 0.012778587639331818\n",
      "Training iteration: 3615\n",
      "Validation loss (no improvement): 0.01268637478351593\n",
      "Training iteration: 3616\n",
      "Validation loss (no improvement): 0.01264130175113678\n",
      "Training iteration: 3617\n",
      "Validation loss (no improvement): 0.012887080013751984\n",
      "Training iteration: 3618\n",
      "Validation loss (no improvement): 0.013697901368141174\n",
      "Training iteration: 3619\n",
      "Validation loss (no improvement): 0.013077548146247864\n",
      "Training iteration: 3620\n",
      "Validation loss (no improvement): 0.012837007641792297\n",
      "Training iteration: 3621\n",
      "Validation loss (no improvement): 0.013452364504337311\n",
      "Training iteration: 3622\n",
      "Validation loss (no improvement): 0.013661321997642518\n",
      "Training iteration: 3623\n",
      "Validation loss (no improvement): 0.013251872360706329\n",
      "Training iteration: 3624\n",
      "Validation loss (no improvement): 0.01345743089914322\n",
      "Training iteration: 3625\n",
      "Validation loss (no improvement): 0.014907160401344299\n",
      "Training iteration: 3626\n",
      "Validation loss (no improvement): 0.015488481521606446\n",
      "Training iteration: 3627\n",
      "Validation loss (no improvement): 0.01441701054573059\n",
      "Training iteration: 3628\n",
      "Validation loss (no improvement): 0.014377126097679138\n",
      "Training iteration: 3629\n",
      "Validation loss (no improvement): 0.014603784680366516\n",
      "Training iteration: 3630\n",
      "Validation loss (no improvement): 0.015170666575431823\n",
      "Training iteration: 3631\n",
      "Validation loss (no improvement): 0.014016394317150117\n",
      "Training iteration: 3632\n",
      "Validation loss (no improvement): 0.013646826148033142\n",
      "Training iteration: 3633\n",
      "Validation loss (no improvement): 0.013941110670566558\n",
      "Training iteration: 3634\n",
      "Validation loss (no improvement): 0.015001983940601349\n",
      "Training iteration: 3635\n",
      "Validation loss (no improvement): 0.013893935084342956\n",
      "Training iteration: 3636\n",
      "Validation loss (no improvement): 0.013777253031730653\n",
      "Training iteration: 3637\n",
      "Validation loss (no improvement): 0.013654556870460511\n",
      "Training iteration: 3638\n",
      "Validation loss (no improvement): 0.014475925266742707\n",
      "Training iteration: 3639\n",
      "Validation loss (no improvement): 0.014294314384460449\n",
      "Training iteration: 3640\n",
      "Validation loss (no improvement): 0.013631194829940796\n",
      "Training iteration: 3641\n",
      "Validation loss (no improvement): 0.013456404209136963\n",
      "Training iteration: 3642\n",
      "Validation loss (no improvement): 0.013514715433120727\n",
      "Training iteration: 3643\n",
      "Validation loss (no improvement): 0.013963177800178528\n",
      "Training iteration: 3644\n",
      "Validation loss (no improvement): 0.01382221132516861\n",
      "Training iteration: 3645\n",
      "Validation loss (no improvement): 0.013095740973949433\n",
      "Training iteration: 3646\n",
      "Improved validation loss from: 0.0125364750623703  to: 0.012468808889389038\n",
      "Training iteration: 3647\n",
      "Validation loss (no improvement): 0.012477093935012817\n",
      "Training iteration: 3648\n",
      "Validation loss (no improvement): 0.01266339123249054\n",
      "Training iteration: 3649\n",
      "Improved validation loss from: 0.012468808889389038  to: 0.011809203773736954\n",
      "Training iteration: 3650\n",
      "Validation loss (no improvement): 0.01200134977698326\n",
      "Training iteration: 3651\n",
      "Validation loss (no improvement): 0.013827915489673614\n",
      "Training iteration: 3652\n",
      "Validation loss (no improvement): 0.014336296916007995\n",
      "Training iteration: 3653\n",
      "Validation loss (no improvement): 0.013084308803081512\n",
      "Training iteration: 3654\n",
      "Validation loss (no improvement): 0.013302090764045715\n",
      "Training iteration: 3655\n",
      "Validation loss (no improvement): 0.014679992198944091\n",
      "Training iteration: 3656\n",
      "Validation loss (no improvement): 0.016019991040229796\n",
      "Training iteration: 3657\n",
      "Validation loss (no improvement): 0.014062558114528657\n",
      "Training iteration: 3658\n",
      "Validation loss (no improvement): 0.014260753989219666\n",
      "Training iteration: 3659\n",
      "Validation loss (no improvement): 0.01449439525604248\n",
      "Training iteration: 3660\n",
      "Validation loss (no improvement): 0.016456811130046843\n",
      "Training iteration: 3661\n",
      "Validation loss (no improvement): 0.016663873195648195\n",
      "Training iteration: 3662\n",
      "Validation loss (no improvement): 0.015129239857196808\n",
      "Training iteration: 3663\n",
      "Validation loss (no improvement): 0.014810022711753846\n",
      "Training iteration: 3664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.015000244975090027\n",
      "Training iteration: 3665\n",
      "Validation loss (no improvement): 0.01550535261631012\n",
      "Training iteration: 3666\n",
      "Validation loss (no improvement): 0.01402294635772705\n",
      "Training iteration: 3667\n",
      "Validation loss (no improvement): 0.01337045431137085\n",
      "Training iteration: 3668\n",
      "Validation loss (no improvement): 0.0133877232670784\n",
      "Training iteration: 3669\n",
      "Validation loss (no improvement): 0.013670089840888976\n",
      "Training iteration: 3670\n",
      "Validation loss (no improvement): 0.013827867805957794\n",
      "Training iteration: 3671\n",
      "Validation loss (no improvement): 0.01325020045042038\n",
      "Training iteration: 3672\n",
      "Validation loss (no improvement): 0.012962831556797028\n",
      "Training iteration: 3673\n",
      "Validation loss (no improvement): 0.013003480434417725\n",
      "Training iteration: 3674\n",
      "Validation loss (no improvement): 0.01222914233803749\n",
      "Training iteration: 3675\n",
      "Validation loss (no improvement): 0.012170346081256866\n",
      "Training iteration: 3676\n",
      "Validation loss (no improvement): 0.012264092266559602\n",
      "Training iteration: 3677\n",
      "Validation loss (no improvement): 0.012281937897205353\n",
      "Training iteration: 3678\n",
      "Validation loss (no improvement): 0.012689769268035889\n",
      "Training iteration: 3679\n",
      "Validation loss (no improvement): 0.012901365756988525\n",
      "Training iteration: 3680\n",
      "Validation loss (no improvement): 0.013043035566806794\n",
      "Training iteration: 3681\n",
      "Validation loss (no improvement): 0.012988993525505066\n",
      "Training iteration: 3682\n",
      "Validation loss (no improvement): 0.013068172335624694\n",
      "Training iteration: 3683\n",
      "Validation loss (no improvement): 0.013405779004096985\n",
      "Training iteration: 3684\n",
      "Validation loss (no improvement): 0.013723549246788026\n",
      "Training iteration: 3685\n",
      "Validation loss (no improvement): 0.013442020118236541\n",
      "Training iteration: 3686\n",
      "Validation loss (no improvement): 0.012980198860168457\n",
      "Training iteration: 3687\n",
      "Validation loss (no improvement): 0.012973752617836\n",
      "Training iteration: 3688\n",
      "Validation loss (no improvement): 0.013288339972496033\n",
      "Training iteration: 3689\n",
      "Validation loss (no improvement): 0.012631353735923768\n",
      "Training iteration: 3690\n",
      "Validation loss (no improvement): 0.012935280799865723\n",
      "Training iteration: 3691\n",
      "Validation loss (no improvement): 0.013735418021678925\n",
      "Training iteration: 3692\n",
      "Validation loss (no improvement): 0.014057716727256775\n",
      "Training iteration: 3693\n",
      "Validation loss (no improvement): 0.01361154466867447\n",
      "Training iteration: 3694\n",
      "Validation loss (no improvement): 0.013035836815834045\n",
      "Training iteration: 3695\n",
      "Validation loss (no improvement): 0.013741295039653777\n",
      "Training iteration: 3696\n",
      "Validation loss (no improvement): 0.01484958827495575\n",
      "Training iteration: 3697\n",
      "Validation loss (no improvement): 0.014606435596942902\n",
      "Training iteration: 3698\n",
      "Validation loss (no improvement): 0.013213865458965302\n",
      "Training iteration: 3699\n",
      "Validation loss (no improvement): 0.012562116980552674\n",
      "Training iteration: 3700\n",
      "Validation loss (no improvement): 0.012754206359386445\n",
      "Training iteration: 3701\n",
      "Validation loss (no improvement): 0.014648178219795227\n",
      "Training iteration: 3702\n",
      "Validation loss (no improvement): 0.013819119334220887\n",
      "Training iteration: 3703\n",
      "Validation loss (no improvement): 0.013058893382549286\n",
      "Training iteration: 3704\n",
      "Validation loss (no improvement): 0.01327742338180542\n",
      "Training iteration: 3705\n",
      "Validation loss (no improvement): 0.014203216135501861\n",
      "Training iteration: 3706\n",
      "Validation loss (no improvement): 0.014514780044555664\n",
      "Training iteration: 3707\n",
      "Validation loss (no improvement): 0.013590243458747864\n",
      "Training iteration: 3708\n",
      "Validation loss (no improvement): 0.013279005885124207\n",
      "Training iteration: 3709\n",
      "Validation loss (no improvement): 0.013349151611328125\n",
      "Training iteration: 3710\n",
      "Validation loss (no improvement): 0.014824011921882629\n",
      "Training iteration: 3711\n",
      "Validation loss (no improvement): 0.014755228161811828\n",
      "Training iteration: 3712\n",
      "Validation loss (no improvement): 0.013902485370635986\n",
      "Training iteration: 3713\n",
      "Validation loss (no improvement): 0.013760188221931457\n",
      "Training iteration: 3714\n",
      "Validation loss (no improvement): 0.014484818279743194\n",
      "Training iteration: 3715\n",
      "Validation loss (no improvement): 0.014985892176628112\n",
      "Training iteration: 3716\n",
      "Validation loss (no improvement): 0.014149627089500428\n",
      "Training iteration: 3717\n",
      "Validation loss (no improvement): 0.012729552388191224\n",
      "Training iteration: 3718\n",
      "Validation loss (no improvement): 0.012242436408996582\n",
      "Training iteration: 3719\n",
      "Validation loss (no improvement): 0.013025403022766113\n",
      "Training iteration: 3720\n",
      "Validation loss (no improvement): 0.01325872391462326\n",
      "Training iteration: 3721\n",
      "Validation loss (no improvement): 0.012271362543106078\n",
      "Training iteration: 3722\n",
      "Validation loss (no improvement): 0.012218163907527923\n",
      "Training iteration: 3723\n",
      "Validation loss (no improvement): 0.01327097713947296\n",
      "Training iteration: 3724\n",
      "Validation loss (no improvement): 0.0137888103723526\n",
      "Training iteration: 3725\n",
      "Validation loss (no improvement): 0.013068766891956329\n",
      "Training iteration: 3726\n",
      "Validation loss (no improvement): 0.012493660300970077\n",
      "Training iteration: 3727\n",
      "Validation loss (no improvement): 0.012599866092205047\n",
      "Training iteration: 3728\n",
      "Validation loss (no improvement): 0.013678306341171264\n",
      "Training iteration: 3729\n",
      "Validation loss (no improvement): 0.013572612404823303\n",
      "Training iteration: 3730\n",
      "Validation loss (no improvement): 0.012742188572883607\n",
      "Training iteration: 3731\n",
      "Validation loss (no improvement): 0.012509088218212127\n",
      "Training iteration: 3732\n",
      "Validation loss (no improvement): 0.013197250664234161\n",
      "Training iteration: 3733\n",
      "Validation loss (no improvement): 0.013300217688083649\n",
      "Training iteration: 3734\n",
      "Validation loss (no improvement): 0.012661199271678924\n",
      "Training iteration: 3735\n",
      "Validation loss (no improvement): 0.012244787067174911\n",
      "Training iteration: 3736\n",
      "Validation loss (no improvement): 0.012653525173664092\n",
      "Training iteration: 3737\n",
      "Validation loss (no improvement): 0.01272800862789154\n",
      "Training iteration: 3738\n",
      "Validation loss (no improvement): 0.0125690296292305\n",
      "Training iteration: 3739\n",
      "Validation loss (no improvement): 0.01244940385222435\n",
      "Training iteration: 3740\n",
      "Validation loss (no improvement): 0.01297009438276291\n",
      "Training iteration: 3741\n",
      "Validation loss (no improvement): 0.013053914904594422\n",
      "Training iteration: 3742\n",
      "Validation loss (no improvement): 0.012373322248458862\n",
      "Training iteration: 3743\n",
      "Validation loss (no improvement): 0.012221095710992813\n",
      "Training iteration: 3744\n",
      "Validation loss (no improvement): 0.012805676460266114\n",
      "Training iteration: 3745\n",
      "Validation loss (no improvement): 0.012702348828315734\n",
      "Training iteration: 3746\n",
      "Validation loss (no improvement): 0.01245512142777443\n",
      "Training iteration: 3747\n",
      "Validation loss (no improvement): 0.01221691519021988\n",
      "Training iteration: 3748\n",
      "Validation loss (no improvement): 0.012123020738363266\n",
      "Training iteration: 3749\n",
      "Validation loss (no improvement): 0.012092991918325424\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_4 = krishnan_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_4.train_model()\n",
    "ensemble_model_4.model_inference()\n",
    "\n",
    "ensemble_mean_4 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_4 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_4 = np.sqrt(np.exp(ensemble_logvar_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Improved validation loss from: inf  to: 10.688059997558593\n",
      "Training iteration: 1\n",
      "Improved validation loss from: 10.688059997558593  to: 7.9868614196777346\n",
      "Training iteration: 2\n",
      "Improved validation loss from: 7.9868614196777346  to: 5.989473724365235\n",
      "Training iteration: 3\n",
      "Improved validation loss from: 5.989473724365235  to: 4.5242431640625\n",
      "Training iteration: 4\n",
      "Improved validation loss from: 4.5242431640625  to: 3.445748138427734\n",
      "Training iteration: 5\n",
      "Improved validation loss from: 3.445748138427734  to: 2.6558094024658203\n",
      "Training iteration: 6\n",
      "Improved validation loss from: 2.6558094024658203  to: 2.0662612915039062\n",
      "Training iteration: 7\n",
      "Improved validation loss from: 2.0662612915039062  to: 1.623709487915039\n",
      "Training iteration: 8\n",
      "Improved validation loss from: 1.623709487915039  to: 1.2887039184570312\n",
      "Training iteration: 9\n",
      "Improved validation loss from: 1.2887039184570312  to: 1.0349239349365233\n",
      "Training iteration: 10\n",
      "Improved validation loss from: 1.0349239349365233  to: 0.8422145843505859\n",
      "Training iteration: 11\n",
      "Improved validation loss from: 0.8422145843505859  to: 0.6942413330078125\n",
      "Training iteration: 12\n",
      "Improved validation loss from: 0.6942413330078125  to: 0.5801796436309814\n",
      "Training iteration: 13\n",
      "Improved validation loss from: 0.5801796436309814  to: 0.49194979667663574\n",
      "Training iteration: 14\n",
      "Improved validation loss from: 0.49194979667663574  to: 0.4233090400695801\n",
      "Training iteration: 15\n",
      "Improved validation loss from: 0.4233090400695801  to: 0.36965279579162597\n",
      "Training iteration: 16\n",
      "Improved validation loss from: 0.36965279579162597  to: 0.32750251293182375\n",
      "Training iteration: 17\n",
      "Improved validation loss from: 0.32750251293182375  to: 0.29423043727874754\n",
      "Training iteration: 18\n",
      "Improved validation loss from: 0.29423043727874754  to: 0.2678029775619507\n",
      "Training iteration: 19\n",
      "Improved validation loss from: 0.2678029775619507  to: 0.24670166969299318\n",
      "Training iteration: 20\n",
      "Improved validation loss from: 0.24670166969299318  to: 0.229750394821167\n",
      "Training iteration: 21\n",
      "Improved validation loss from: 0.229750394821167  to: 0.21606833934783937\n",
      "Training iteration: 22\n",
      "Improved validation loss from: 0.21606833934783937  to: 0.20495498180389404\n",
      "Training iteration: 23\n",
      "Improved validation loss from: 0.20495498180389404  to: 0.19587881565093995\n",
      "Training iteration: 24\n",
      "Improved validation loss from: 0.19587881565093995  to: 0.1884239912033081\n",
      "Training iteration: 25\n",
      "Improved validation loss from: 0.1884239912033081  to: 0.18226922750473024\n",
      "Training iteration: 26\n",
      "Improved validation loss from: 0.18226922750473024  to: 0.17716919183731078\n",
      "Training iteration: 27\n",
      "Improved validation loss from: 0.17716919183731078  to: 0.1729193925857544\n",
      "Training iteration: 28\n",
      "Improved validation loss from: 0.1729193925857544  to: 0.1693647623062134\n",
      "Training iteration: 29\n",
      "Improved validation loss from: 0.1693647623062134  to: 0.16637375354766845\n",
      "Training iteration: 30\n",
      "Improved validation loss from: 0.16637375354766845  to: 0.16384719610214232\n",
      "Training iteration: 31\n",
      "Improved validation loss from: 0.16384719610214232  to: 0.16170421838760377\n",
      "Training iteration: 32\n",
      "Improved validation loss from: 0.16170421838760377  to: 0.1598793864250183\n",
      "Training iteration: 33\n",
      "Improved validation loss from: 0.1598793864250183  to: 0.15831903219223023\n",
      "Training iteration: 34\n",
      "Improved validation loss from: 0.15831903219223023  to: 0.15697896480560303\n",
      "Training iteration: 35\n",
      "Improved validation loss from: 0.15697896480560303  to: 0.15582540035247802\n",
      "Training iteration: 36\n",
      "Improved validation loss from: 0.15582540035247802  to: 0.1548243761062622\n",
      "Training iteration: 37\n",
      "Improved validation loss from: 0.1548243761062622  to: 0.1539537191390991\n",
      "Training iteration: 38\n",
      "Improved validation loss from: 0.1539537191390991  to: 0.1531937837600708\n",
      "Training iteration: 39\n",
      "Improved validation loss from: 0.1531937837600708  to: 0.15252785682678222\n",
      "Training iteration: 40\n",
      "Improved validation loss from: 0.15252785682678222  to: 0.15194199085235596\n",
      "Training iteration: 41\n",
      "Improved validation loss from: 0.15194199085235596  to: 0.15142419338226318\n",
      "Training iteration: 42\n",
      "Improved validation loss from: 0.15142419338226318  to: 0.15096460580825805\n",
      "Training iteration: 43\n",
      "Improved validation loss from: 0.15096460580825805  to: 0.15055496692657472\n",
      "Training iteration: 44\n",
      "Improved validation loss from: 0.15055496692657472  to: 0.15018811225891113\n",
      "Training iteration: 45\n",
      "Improved validation loss from: 0.15018811225891113  to: 0.14985806941986085\n",
      "Training iteration: 46\n",
      "Improved validation loss from: 0.14985806941986085  to: 0.1495596647262573\n",
      "Training iteration: 47\n",
      "Improved validation loss from: 0.1495596647262573  to: 0.14928661584854125\n",
      "Training iteration: 48\n",
      "Improved validation loss from: 0.14928661584854125  to: 0.14903653860092164\n",
      "Training iteration: 49\n",
      "Improved validation loss from: 0.14903653860092164  to: 0.14880592823028566\n",
      "Training iteration: 50\n",
      "Improved validation loss from: 0.14880592823028566  to: 0.14859310388565064\n",
      "Training iteration: 51\n",
      "Improved validation loss from: 0.14859310388565064  to: 0.14839513301849366\n",
      "Training iteration: 52\n",
      "Improved validation loss from: 0.14839513301849366  to: 0.14821064472198486\n",
      "Training iteration: 53\n",
      "Improved validation loss from: 0.14821064472198486  to: 0.14803788661956788\n",
      "Training iteration: 54\n",
      "Improved validation loss from: 0.14803788661956788  to: 0.14787544012069703\n",
      "Training iteration: 55\n",
      "Improved validation loss from: 0.14787544012069703  to: 0.14772207736968995\n",
      "Training iteration: 56\n",
      "Improved validation loss from: 0.14772207736968995  to: 0.1475766897201538\n",
      "Training iteration: 57\n",
      "Improved validation loss from: 0.1475766897201538  to: 0.14743831157684326\n",
      "Training iteration: 58\n",
      "Improved validation loss from: 0.14743831157684326  to: 0.14730620384216309\n",
      "Training iteration: 59\n",
      "Improved validation loss from: 0.14730620384216309  to: 0.14717957973480225\n",
      "Training iteration: 60\n",
      "Improved validation loss from: 0.14717957973480225  to: 0.1470576286315918\n",
      "Training iteration: 61\n",
      "Improved validation loss from: 0.1470576286315918  to: 0.14693849086761473\n",
      "Training iteration: 62\n",
      "Improved validation loss from: 0.14693849086761473  to: 0.14682323932647706\n",
      "Training iteration: 63\n",
      "Improved validation loss from: 0.14682323932647706  to: 0.14671145677566527\n",
      "Training iteration: 64\n",
      "Improved validation loss from: 0.14671145677566527  to: 0.14660267829895018\n",
      "Training iteration: 65\n",
      "Improved validation loss from: 0.14660267829895018  to: 0.14649665355682373\n",
      "Training iteration: 66\n",
      "Improved validation loss from: 0.14649665355682373  to: 0.14639307260513307\n",
      "Training iteration: 67\n",
      "Improved validation loss from: 0.14639307260513307  to: 0.14629170894622803\n",
      "Training iteration: 68\n",
      "Improved validation loss from: 0.14629170894622803  to: 0.14619234800338746\n",
      "Training iteration: 69\n",
      "Improved validation loss from: 0.14619234800338746  to: 0.14609477519989014\n",
      "Training iteration: 70\n",
      "Improved validation loss from: 0.14609477519989014  to: 0.14599883556365967\n",
      "Training iteration: 71\n",
      "Improved validation loss from: 0.14599883556365967  to: 0.14590440988540648\n",
      "Training iteration: 72\n",
      "Improved validation loss from: 0.14590440988540648  to: 0.14581133127212526\n",
      "Training iteration: 73\n",
      "Improved validation loss from: 0.14581133127212526  to: 0.14571950435638428\n",
      "Training iteration: 74\n",
      "Improved validation loss from: 0.14571950435638428  to: 0.14562883377075195\n",
      "Training iteration: 75\n",
      "Improved validation loss from: 0.14562883377075195  to: 0.14553921222686766\n",
      "Training iteration: 76\n",
      "Improved validation loss from: 0.14553921222686766  to: 0.1454503655433655\n",
      "Training iteration: 77\n",
      "Improved validation loss from: 0.1454503655433655  to: 0.14536243677139282\n",
      "Training iteration: 78\n",
      "Improved validation loss from: 0.14536243677139282  to: 0.14527537822723388\n",
      "Training iteration: 79\n",
      "Improved validation loss from: 0.14527537822723388  to: 0.14518911838531495\n",
      "Training iteration: 80\n",
      "Improved validation loss from: 0.14518911838531495  to: 0.14510360956192017\n",
      "Training iteration: 81\n",
      "Improved validation loss from: 0.14510360956192017  to: 0.1450188159942627\n",
      "Training iteration: 82\n",
      "Improved validation loss from: 0.1450188159942627  to: 0.14493401050567628\n",
      "Training iteration: 83\n",
      "Improved validation loss from: 0.14493401050567628  to: 0.1448500156402588\n",
      "Training iteration: 84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.1448500156402588  to: 0.14476678371429444\n",
      "Training iteration: 85\n",
      "Improved validation loss from: 0.14476678371429444  to: 0.14468424320220946\n",
      "Training iteration: 86\n",
      "Improved validation loss from: 0.14468424320220946  to: 0.14460233449935914\n",
      "Training iteration: 87\n",
      "Improved validation loss from: 0.14460233449935914  to: 0.14452104568481444\n",
      "Training iteration: 88\n",
      "Improved validation loss from: 0.14452104568481444  to: 0.14444030523300172\n",
      "Training iteration: 89\n",
      "Improved validation loss from: 0.14444030523300172  to: 0.14436013698577882\n",
      "Training iteration: 90\n",
      "Improved validation loss from: 0.14436013698577882  to: 0.14428048133850097\n",
      "Training iteration: 91\n",
      "Improved validation loss from: 0.14428048133850097  to: 0.1442013144493103\n",
      "Training iteration: 92\n",
      "Improved validation loss from: 0.1442013144493103  to: 0.14412262439727783\n",
      "Training iteration: 93\n",
      "Improved validation loss from: 0.14412262439727783  to: 0.1440443754196167\n",
      "Training iteration: 94\n",
      "Improved validation loss from: 0.1440443754196167  to: 0.14396657943725585\n",
      "Training iteration: 95\n",
      "Improved validation loss from: 0.14396657943725585  to: 0.14388911724090575\n",
      "Training iteration: 96\n",
      "Improved validation loss from: 0.14388911724090575  to: 0.1438119888305664\n",
      "Training iteration: 97\n",
      "Improved validation loss from: 0.1438119888305664  to: 0.1437351942062378\n",
      "Training iteration: 98\n",
      "Improved validation loss from: 0.1437351942062378  to: 0.14365873336791993\n",
      "Training iteration: 99\n",
      "Improved validation loss from: 0.14365873336791993  to: 0.14358259439468385\n",
      "Training iteration: 100\n",
      "Improved validation loss from: 0.14358259439468385  to: 0.14350675344467162\n",
      "Training iteration: 101\n",
      "Improved validation loss from: 0.14350675344467162  to: 0.14343119859695436\n",
      "Training iteration: 102\n",
      "Improved validation loss from: 0.14343119859695436  to: 0.14335590600967407\n",
      "Training iteration: 103\n",
      "Improved validation loss from: 0.14335590600967407  to: 0.14328091144561766\n",
      "Training iteration: 104\n",
      "Improved validation loss from: 0.14328091144561766  to: 0.14320616722106932\n",
      "Training iteration: 105\n",
      "Improved validation loss from: 0.14320616722106932  to: 0.14313169717788696\n",
      "Training iteration: 106\n",
      "Improved validation loss from: 0.14313169717788696  to: 0.1430575132369995\n",
      "Training iteration: 107\n",
      "Improved validation loss from: 0.1430575132369995  to: 0.1429835796356201\n",
      "Training iteration: 108\n",
      "Improved validation loss from: 0.1429835796356201  to: 0.14290990829467773\n",
      "Training iteration: 109\n",
      "Improved validation loss from: 0.14290990829467773  to: 0.14283649921417235\n",
      "Training iteration: 110\n",
      "Improved validation loss from: 0.14283649921417235  to: 0.14276336431503295\n",
      "Training iteration: 111\n",
      "Improved validation loss from: 0.14276336431503295  to: 0.1426904797554016\n",
      "Training iteration: 112\n",
      "Improved validation loss from: 0.1426904797554016  to: 0.14261784553527831\n",
      "Training iteration: 113\n",
      "Improved validation loss from: 0.14261784553527831  to: 0.14254546165466309\n",
      "Training iteration: 114\n",
      "Improved validation loss from: 0.14254546165466309  to: 0.1424729347229004\n",
      "Training iteration: 115\n",
      "Improved validation loss from: 0.1424729347229004  to: 0.14240022897720336\n",
      "Training iteration: 116\n",
      "Improved validation loss from: 0.14240022897720336  to: 0.14232776165008545\n",
      "Training iteration: 117\n",
      "Improved validation loss from: 0.14232776165008545  to: 0.1422551155090332\n",
      "Training iteration: 118\n",
      "Improved validation loss from: 0.1422551155090332  to: 0.1421826720237732\n",
      "Training iteration: 119\n",
      "Improved validation loss from: 0.1421826720237732  to: 0.14211046695709229\n",
      "Training iteration: 120\n",
      "Improved validation loss from: 0.14211046695709229  to: 0.14203850030899048\n",
      "Training iteration: 121\n",
      "Improved validation loss from: 0.14203850030899048  to: 0.1419666290283203\n",
      "Training iteration: 122\n",
      "Improved validation loss from: 0.1419666290283203  to: 0.14189498424530028\n",
      "Training iteration: 123\n",
      "Improved validation loss from: 0.14189498424530028  to: 0.14182355403900146\n",
      "Training iteration: 124\n",
      "Improved validation loss from: 0.14182355403900146  to: 0.14175236225128174\n",
      "Training iteration: 125\n",
      "Improved validation loss from: 0.14175236225128174  to: 0.14168140888214112\n",
      "Training iteration: 126\n",
      "Improved validation loss from: 0.14168140888214112  to: 0.14161067008972167\n",
      "Training iteration: 127\n",
      "Improved validation loss from: 0.14161067008972167  to: 0.14154014587402344\n",
      "Training iteration: 128\n",
      "Improved validation loss from: 0.14154014587402344  to: 0.1414698600769043\n",
      "Training iteration: 129\n",
      "Improved validation loss from: 0.1414698600769043  to: 0.14139978885650634\n",
      "Training iteration: 130\n",
      "Improved validation loss from: 0.14139978885650634  to: 0.14132993221282958\n",
      "Training iteration: 131\n",
      "Improved validation loss from: 0.14132993221282958  to: 0.14126030206680298\n",
      "Training iteration: 132\n",
      "Improved validation loss from: 0.14126030206680298  to: 0.14119088649749756\n",
      "Training iteration: 133\n",
      "Improved validation loss from: 0.14119088649749756  to: 0.1411216974258423\n",
      "Training iteration: 134\n",
      "Improved validation loss from: 0.1411216974258423  to: 0.1410526990890503\n",
      "Training iteration: 135\n",
      "Improved validation loss from: 0.1410526990890503  to: 0.1409839391708374\n",
      "Training iteration: 136\n",
      "Improved validation loss from: 0.1409839391708374  to: 0.1409153699874878\n",
      "Training iteration: 137\n",
      "Improved validation loss from: 0.1409153699874878  to: 0.14084700345993043\n",
      "Training iteration: 138\n",
      "Improved validation loss from: 0.14084700345993043  to: 0.14077883958816528\n",
      "Training iteration: 139\n",
      "Improved validation loss from: 0.14077883958816528  to: 0.14071089029312134\n",
      "Training iteration: 140\n",
      "Improved validation loss from: 0.14071089029312134  to: 0.14064311981201172\n",
      "Training iteration: 141\n",
      "Improved validation loss from: 0.14064311981201172  to: 0.1405755400657654\n",
      "Training iteration: 142\n",
      "Improved validation loss from: 0.1405755400657654  to: 0.14050817489624023\n",
      "Training iteration: 143\n",
      "Improved validation loss from: 0.14050817489624023  to: 0.14044090509414672\n",
      "Training iteration: 144\n",
      "Improved validation loss from: 0.14044090509414672  to: 0.14037374258041382\n",
      "Training iteration: 145\n",
      "Improved validation loss from: 0.14037374258041382  to: 0.14030671119689941\n",
      "Training iteration: 146\n",
      "Improved validation loss from: 0.14030671119689941  to: 0.14023978710174562\n",
      "Training iteration: 147\n",
      "Improved validation loss from: 0.14023978710174562  to: 0.14017298221588134\n",
      "Training iteration: 148\n",
      "Improved validation loss from: 0.14017298221588134  to: 0.1401062250137329\n",
      "Training iteration: 149\n",
      "Improved validation loss from: 0.1401062250137329  to: 0.1400395631790161\n",
      "Training iteration: 150\n",
      "Improved validation loss from: 0.1400395631790161  to: 0.13997294902801513\n",
      "Training iteration: 151\n",
      "Improved validation loss from: 0.13997294902801513  to: 0.1399064540863037\n",
      "Training iteration: 152\n",
      "Improved validation loss from: 0.1399064540863037  to: 0.13984001874923707\n",
      "Training iteration: 153\n",
      "Improved validation loss from: 0.13984001874923707  to: 0.139773690700531\n",
      "Training iteration: 154\n",
      "Improved validation loss from: 0.139773690700531  to: 0.13970743417739867\n",
      "Training iteration: 155\n",
      "Improved validation loss from: 0.13970743417739867  to: 0.13964112997055053\n",
      "Training iteration: 156\n",
      "Improved validation loss from: 0.13964112997055053  to: 0.1395747184753418\n",
      "Training iteration: 157\n",
      "Improved validation loss from: 0.1395747184753418  to: 0.1395084261894226\n",
      "Training iteration: 158\n",
      "Improved validation loss from: 0.1395084261894226  to: 0.13944224119186402\n",
      "Training iteration: 159\n",
      "Improved validation loss from: 0.13944224119186402  to: 0.1393761396408081\n",
      "Training iteration: 160\n",
      "Improved validation loss from: 0.1393761396408081  to: 0.13931018114089966\n",
      "Training iteration: 161\n",
      "Improved validation loss from: 0.13931018114089966  to: 0.13924459218978882\n",
      "Training iteration: 162\n",
      "Improved validation loss from: 0.13924459218978882  to: 0.13917936086654664\n",
      "Training iteration: 163\n",
      "Improved validation loss from: 0.13917936086654664  to: 0.13911424875259398\n",
      "Training iteration: 164\n",
      "Improved validation loss from: 0.13911424875259398  to: 0.139049232006073\n",
      "Training iteration: 165\n",
      "Improved validation loss from: 0.139049232006073  to: 0.1389843463897705\n",
      "Training iteration: 166\n",
      "Improved validation loss from: 0.1389843463897705  to: 0.13891955614089965\n",
      "Training iteration: 167\n",
      "Improved validation loss from: 0.13891955614089965  to: 0.13885490894317626\n",
      "Training iteration: 168\n",
      "Improved validation loss from: 0.13885490894317626  to: 0.1387903571128845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 169\n",
      "Improved validation loss from: 0.1387903571128845  to: 0.13872594833374025\n",
      "Training iteration: 170\n",
      "Improved validation loss from: 0.13872594833374025  to: 0.13866164684295654\n",
      "Training iteration: 171\n",
      "Improved validation loss from: 0.13866164684295654  to: 0.13859747648239135\n",
      "Training iteration: 172\n",
      "Improved validation loss from: 0.13859747648239135  to: 0.13853342533111573\n",
      "Training iteration: 173\n",
      "Improved validation loss from: 0.13853342533111573  to: 0.13846948146820068\n",
      "Training iteration: 174\n",
      "Improved validation loss from: 0.13846948146820068  to: 0.1384056806564331\n",
      "Training iteration: 175\n",
      "Improved validation loss from: 0.1384056806564331  to: 0.138342022895813\n",
      "Training iteration: 176\n",
      "Improved validation loss from: 0.138342022895813  to: 0.13827836513519287\n",
      "Training iteration: 177\n",
      "Improved validation loss from: 0.13827836513519287  to: 0.13821486234664918\n",
      "Training iteration: 178\n",
      "Improved validation loss from: 0.13821486234664918  to: 0.13815147876739503\n",
      "Training iteration: 179\n",
      "Improved validation loss from: 0.13815147876739503  to: 0.13808823823928834\n",
      "Training iteration: 180\n",
      "Improved validation loss from: 0.13808823823928834  to: 0.1380251169204712\n",
      "Training iteration: 181\n",
      "Improved validation loss from: 0.1380251169204712  to: 0.13796212673187255\n",
      "Training iteration: 182\n",
      "Improved validation loss from: 0.13796212673187255  to: 0.13789925575256348\n",
      "Training iteration: 183\n",
      "Improved validation loss from: 0.13789925575256348  to: 0.1378365159034729\n",
      "Training iteration: 184\n",
      "Improved validation loss from: 0.1378365159034729  to: 0.13777389526367187\n",
      "Training iteration: 185\n",
      "Improved validation loss from: 0.13777389526367187  to: 0.1377113938331604\n",
      "Training iteration: 186\n",
      "Improved validation loss from: 0.1377113938331604  to: 0.13764902353286743\n",
      "Training iteration: 187\n",
      "Improved validation loss from: 0.13764902353286743  to: 0.13758678436279298\n",
      "Training iteration: 188\n",
      "Improved validation loss from: 0.13758678436279298  to: 0.13752466440200806\n",
      "Training iteration: 189\n",
      "Improved validation loss from: 0.13752466440200806  to: 0.13746267557144165\n",
      "Training iteration: 190\n",
      "Improved validation loss from: 0.13746267557144165  to: 0.13740081787109376\n",
      "Training iteration: 191\n",
      "Improved validation loss from: 0.13740081787109376  to: 0.13733909130096436\n",
      "Training iteration: 192\n",
      "Improved validation loss from: 0.13733909130096436  to: 0.13727750778198242\n",
      "Training iteration: 193\n",
      "Improved validation loss from: 0.13727750778198242  to: 0.13721596002578734\n",
      "Training iteration: 194\n",
      "Improved validation loss from: 0.13721596002578734  to: 0.13715455532073975\n",
      "Training iteration: 195\n",
      "Improved validation loss from: 0.13715455532073975  to: 0.13709328174591065\n",
      "Training iteration: 196\n",
      "Improved validation loss from: 0.13709328174591065  to: 0.13703213930130004\n",
      "Training iteration: 197\n",
      "Improved validation loss from: 0.13703213930130004  to: 0.1369711399078369\n",
      "Training iteration: 198\n",
      "Improved validation loss from: 0.1369711399078369  to: 0.13691028356552123\n",
      "Training iteration: 199\n",
      "Improved validation loss from: 0.13691028356552123  to: 0.13684955835342408\n",
      "Training iteration: 200\n",
      "Improved validation loss from: 0.13684955835342408  to: 0.1367891550064087\n",
      "Training iteration: 201\n",
      "Improved validation loss from: 0.1367891550064087  to: 0.13672897815704346\n",
      "Training iteration: 202\n",
      "Improved validation loss from: 0.13672897815704346  to: 0.1366689085960388\n",
      "Training iteration: 203\n",
      "Improved validation loss from: 0.1366689085960388  to: 0.1366090416908264\n",
      "Training iteration: 204\n",
      "Improved validation loss from: 0.1366090416908264  to: 0.13654930591583253\n",
      "Training iteration: 205\n",
      "Improved validation loss from: 0.13654930591583253  to: 0.13648977279663085\n",
      "Training iteration: 206\n",
      "Improved validation loss from: 0.13648977279663085  to: 0.13643039464950563\n",
      "Training iteration: 207\n",
      "Improved validation loss from: 0.13643039464950563  to: 0.13637101650238037\n",
      "Training iteration: 208\n",
      "Improved validation loss from: 0.13637101650238037  to: 0.13631160259246827\n",
      "Training iteration: 209\n",
      "Improved validation loss from: 0.13631160259246827  to: 0.13625222444534302\n",
      "Training iteration: 210\n",
      "Improved validation loss from: 0.13625222444534302  to: 0.13619284629821776\n",
      "Training iteration: 211\n",
      "Improved validation loss from: 0.13619284629821776  to: 0.1361335039138794\n",
      "Training iteration: 212\n",
      "Improved validation loss from: 0.1361335039138794  to: 0.13607418537139893\n",
      "Training iteration: 213\n",
      "Improved validation loss from: 0.13607418537139893  to: 0.13601491451263428\n",
      "Training iteration: 214\n",
      "Improved validation loss from: 0.13601491451263428  to: 0.13595571517944335\n",
      "Training iteration: 215\n",
      "Improved validation loss from: 0.13595571517944335  to: 0.13589656352996826\n",
      "Training iteration: 216\n",
      "Improved validation loss from: 0.13589656352996826  to: 0.13583753108978272\n",
      "Training iteration: 217\n",
      "Improved validation loss from: 0.13583753108978272  to: 0.135778546333313\n",
      "Training iteration: 218\n",
      "Improved validation loss from: 0.135778546333313  to: 0.1357196569442749\n",
      "Training iteration: 219\n",
      "Improved validation loss from: 0.1357196569442749  to: 0.13566086292266846\n",
      "Training iteration: 220\n",
      "Improved validation loss from: 0.13566086292266846  to: 0.13560221195220948\n",
      "Training iteration: 221\n",
      "Improved validation loss from: 0.13560221195220948  to: 0.1355436325073242\n",
      "Training iteration: 222\n",
      "Improved validation loss from: 0.1355436325073242  to: 0.13548519611358642\n",
      "Training iteration: 223\n",
      "Improved validation loss from: 0.13548519611358642  to: 0.13542695045471193\n",
      "Training iteration: 224\n",
      "Improved validation loss from: 0.13542695045471193  to: 0.13536914587020873\n",
      "Training iteration: 225\n",
      "Improved validation loss from: 0.13536914587020873  to: 0.13531147241592406\n",
      "Training iteration: 226\n",
      "Improved validation loss from: 0.13531147241592406  to: 0.13525396585464478\n",
      "Training iteration: 227\n",
      "Improved validation loss from: 0.13525396585464478  to: 0.13519659042358398\n",
      "Training iteration: 228\n",
      "Improved validation loss from: 0.13519659042358398  to: 0.1351393938064575\n",
      "Training iteration: 229\n",
      "Improved validation loss from: 0.1351393938064575  to: 0.13508232831954955\n",
      "Training iteration: 230\n",
      "Improved validation loss from: 0.13508232831954955  to: 0.1350254535675049\n",
      "Training iteration: 231\n",
      "Improved validation loss from: 0.1350254535675049  to: 0.13496874570846557\n",
      "Training iteration: 232\n",
      "Improved validation loss from: 0.13496874570846557  to: 0.13491222858428956\n",
      "Training iteration: 233\n",
      "Improved validation loss from: 0.13491222858428956  to: 0.13485589027404785\n",
      "Training iteration: 234\n",
      "Improved validation loss from: 0.13485589027404785  to: 0.1347997307777405\n",
      "Training iteration: 235\n",
      "Improved validation loss from: 0.1347997307777405  to: 0.1347437858581543\n",
      "Training iteration: 236\n",
      "Improved validation loss from: 0.1347437858581543  to: 0.13468801975250244\n",
      "Training iteration: 237\n",
      "Improved validation loss from: 0.13468801975250244  to: 0.1346324563026428\n",
      "Training iteration: 238\n",
      "Improved validation loss from: 0.1346324563026428  to: 0.13457711935043334\n",
      "Training iteration: 239\n",
      "Improved validation loss from: 0.13457711935043334  to: 0.1345219612121582\n",
      "Training iteration: 240\n",
      "Improved validation loss from: 0.1345219612121582  to: 0.1344670295715332\n",
      "Training iteration: 241\n",
      "Improved validation loss from: 0.1344670295715332  to: 0.1344123125076294\n",
      "Training iteration: 242\n",
      "Improved validation loss from: 0.1344123125076294  to: 0.13435781002044678\n",
      "Training iteration: 243\n",
      "Improved validation loss from: 0.13435781002044678  to: 0.13430354595184327\n",
      "Training iteration: 244\n",
      "Improved validation loss from: 0.13430354595184327  to: 0.1342495083808899\n",
      "Training iteration: 245\n",
      "Improved validation loss from: 0.1342495083808899  to: 0.13419570922851562\n",
      "Training iteration: 246\n",
      "Improved validation loss from: 0.13419570922851562  to: 0.13414214849472045\n",
      "Training iteration: 247\n",
      "Improved validation loss from: 0.13414214849472045  to: 0.13408881425857544\n",
      "Training iteration: 248\n",
      "Improved validation loss from: 0.13408881425857544  to: 0.13403571844100953\n",
      "Training iteration: 249\n",
      "Improved validation loss from: 0.13403571844100953  to: 0.13398287296295167\n",
      "Training iteration: 250\n",
      "Improved validation loss from: 0.13398287296295167  to: 0.1339302659034729\n",
      "Training iteration: 251\n",
      "Improved validation loss from: 0.1339302659034729  to: 0.1338779091835022\n",
      "Training iteration: 252\n",
      "Improved validation loss from: 0.1338779091835022  to: 0.13382580280303955\n",
      "Training iteration: 253\n",
      "Improved validation loss from: 0.13382580280303955  to: 0.13377395868301392\n",
      "Training iteration: 254\n",
      "Improved validation loss from: 0.13377395868301392  to: 0.13372238874435424\n",
      "Training iteration: 255\n",
      "Improved validation loss from: 0.13372238874435424  to: 0.13367106914520263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 256\n",
      "Improved validation loss from: 0.13367106914520263  to: 0.1336200475692749\n",
      "Training iteration: 257\n",
      "Improved validation loss from: 0.1336200475692749  to: 0.13356927633285523\n",
      "Training iteration: 258\n",
      "Improved validation loss from: 0.13356927633285523  to: 0.1335188031196594\n",
      "Training iteration: 259\n",
      "Improved validation loss from: 0.1335188031196594  to: 0.13346855640411376\n",
      "Training iteration: 260\n",
      "Improved validation loss from: 0.13346855640411376  to: 0.13341858386993408\n",
      "Training iteration: 261\n",
      "Improved validation loss from: 0.13341858386993408  to: 0.1333688974380493\n",
      "Training iteration: 262\n",
      "Improved validation loss from: 0.1333688974380493  to: 0.13331950902938844\n",
      "Training iteration: 263\n",
      "Improved validation loss from: 0.13331950902938844  to: 0.13327043056488036\n",
      "Training iteration: 264\n",
      "Improved validation loss from: 0.13327043056488036  to: 0.13322165012359619\n",
      "Training iteration: 265\n",
      "Improved validation loss from: 0.13322165012359619  to: 0.13317317962646485\n",
      "Training iteration: 266\n",
      "Improved validation loss from: 0.13317317962646485  to: 0.13312503099441528\n",
      "Training iteration: 267\n",
      "Improved validation loss from: 0.13312503099441528  to: 0.1330772042274475\n",
      "Training iteration: 268\n",
      "Improved validation loss from: 0.1330772042274475  to: 0.13302968740463256\n",
      "Training iteration: 269\n",
      "Improved validation loss from: 0.13302968740463256  to: 0.13298249244689941\n",
      "Training iteration: 270\n",
      "Improved validation loss from: 0.13298249244689941  to: 0.1329360008239746\n",
      "Training iteration: 271\n",
      "Improved validation loss from: 0.1329360008239746  to: 0.1328902006149292\n",
      "Training iteration: 272\n",
      "Improved validation loss from: 0.1328902006149292  to: 0.1328450322151184\n",
      "Training iteration: 273\n",
      "Improved validation loss from: 0.1328450322151184  to: 0.13280047178268434\n",
      "Training iteration: 274\n",
      "Improved validation loss from: 0.13280047178268434  to: 0.132756507396698\n",
      "Training iteration: 275\n",
      "Improved validation loss from: 0.132756507396698  to: 0.13271310329437255\n",
      "Training iteration: 276\n",
      "Improved validation loss from: 0.13271310329437255  to: 0.13267024755477905\n",
      "Training iteration: 277\n",
      "Improved validation loss from: 0.13267024755477905  to: 0.13262791633605958\n",
      "Training iteration: 278\n",
      "Improved validation loss from: 0.13262791633605958  to: 0.1325860619544983\n",
      "Training iteration: 279\n",
      "Improved validation loss from: 0.1325860619544983  to: 0.13254470825195314\n",
      "Training iteration: 280\n",
      "Improved validation loss from: 0.13254470825195314  to: 0.13250380754470825\n",
      "Training iteration: 281\n",
      "Improved validation loss from: 0.13250380754470825  to: 0.13246334791183473\n",
      "Training iteration: 282\n",
      "Improved validation loss from: 0.13246334791183473  to: 0.13242334127426147\n",
      "Training iteration: 283\n",
      "Improved validation loss from: 0.13242334127426147  to: 0.1323833703994751\n",
      "Training iteration: 284\n",
      "Improved validation loss from: 0.1323833703994751  to: 0.13234344720840455\n",
      "Training iteration: 285\n",
      "Improved validation loss from: 0.13234344720840455  to: 0.13230358362197875\n",
      "Training iteration: 286\n",
      "Improved validation loss from: 0.13230358362197875  to: 0.1322637677192688\n",
      "Training iteration: 287\n",
      "Improved validation loss from: 0.1322637677192688  to: 0.13222405910491944\n",
      "Training iteration: 288\n",
      "Improved validation loss from: 0.13222405910491944  to: 0.13218445777893068\n",
      "Training iteration: 289\n",
      "Improved validation loss from: 0.13218445777893068  to: 0.13214499950408937\n",
      "Training iteration: 290\n",
      "Improved validation loss from: 0.13214499950408937  to: 0.13210569620132445\n",
      "Training iteration: 291\n",
      "Improved validation loss from: 0.13210569620132445  to: 0.1320665717124939\n",
      "Training iteration: 292\n",
      "Improved validation loss from: 0.1320665717124939  to: 0.1320276141166687\n",
      "Training iteration: 293\n",
      "Improved validation loss from: 0.1320276141166687  to: 0.13198884725570678\n",
      "Training iteration: 294\n",
      "Improved validation loss from: 0.13198884725570678  to: 0.1319502830505371\n",
      "Training iteration: 295\n",
      "Improved validation loss from: 0.1319502830505371  to: 0.1319119453430176\n",
      "Training iteration: 296\n",
      "Improved validation loss from: 0.1319119453430176  to: 0.13187382221221924\n",
      "Training iteration: 297\n",
      "Improved validation loss from: 0.13187382221221924  to: 0.1318359136581421\n",
      "Training iteration: 298\n",
      "Improved validation loss from: 0.1318359136581421  to: 0.13179824352264405\n",
      "Training iteration: 299\n",
      "Improved validation loss from: 0.13179824352264405  to: 0.13176084756851197\n",
      "Training iteration: 300\n",
      "Improved validation loss from: 0.13176084756851197  to: 0.13172367811203003\n",
      "Training iteration: 301\n",
      "Improved validation loss from: 0.13172367811203003  to: 0.13168678283691407\n",
      "Training iteration: 302\n",
      "Improved validation loss from: 0.13168678283691407  to: 0.1316501498222351\n",
      "Training iteration: 303\n",
      "Improved validation loss from: 0.1316501498222351  to: 0.13161379098892212\n",
      "Training iteration: 304\n",
      "Improved validation loss from: 0.13161379098892212  to: 0.13157771825790404\n",
      "Training iteration: 305\n",
      "Improved validation loss from: 0.13157771825790404  to: 0.131541907787323\n",
      "Training iteration: 306\n",
      "Improved validation loss from: 0.131541907787323  to: 0.13150641918182374\n",
      "Training iteration: 307\n",
      "Improved validation loss from: 0.13150641918182374  to: 0.13147125244140626\n",
      "Training iteration: 308\n",
      "Improved validation loss from: 0.13147125244140626  to: 0.13143651485443114\n",
      "Training iteration: 309\n",
      "Improved validation loss from: 0.13143651485443114  to: 0.1314021110534668\n",
      "Training iteration: 310\n",
      "Improved validation loss from: 0.1314021110534668  to: 0.1313680052757263\n",
      "Training iteration: 311\n",
      "Improved validation loss from: 0.1313680052757263  to: 0.13133423328399657\n",
      "Training iteration: 312\n",
      "Improved validation loss from: 0.13133423328399657  to: 0.1313007354736328\n",
      "Training iteration: 313\n",
      "Improved validation loss from: 0.1313007354736328  to: 0.13126757144927978\n",
      "Training iteration: 314\n",
      "Improved validation loss from: 0.13126757144927978  to: 0.1312347173690796\n",
      "Training iteration: 315\n",
      "Improved validation loss from: 0.1312347173690796  to: 0.13120216131210327\n",
      "Training iteration: 316\n",
      "Improved validation loss from: 0.13120216131210327  to: 0.1311699390411377\n",
      "Training iteration: 317\n",
      "Improved validation loss from: 0.1311699390411377  to: 0.13113802671432495\n",
      "Training iteration: 318\n",
      "Improved validation loss from: 0.13113802671432495  to: 0.13110635280609131\n",
      "Training iteration: 319\n",
      "Improved validation loss from: 0.13110635280609131  to: 0.1310749888420105\n",
      "Training iteration: 320\n",
      "Improved validation loss from: 0.1310749888420105  to: 0.13104394674301148\n",
      "Training iteration: 321\n",
      "Improved validation loss from: 0.13104394674301148  to: 0.13101320266723632\n",
      "Training iteration: 322\n",
      "Improved validation loss from: 0.13101320266723632  to: 0.13098278045654296\n",
      "Training iteration: 323\n",
      "Improved validation loss from: 0.13098278045654296  to: 0.13095266819000245\n",
      "Training iteration: 324\n",
      "Improved validation loss from: 0.13095266819000245  to: 0.13092284202575682\n",
      "Training iteration: 325\n",
      "Improved validation loss from: 0.13092284202575682  to: 0.13089332580566407\n",
      "Training iteration: 326\n",
      "Improved validation loss from: 0.13089332580566407  to: 0.13086411952972413\n",
      "Training iteration: 327\n",
      "Improved validation loss from: 0.13086411952972413  to: 0.13083518743515016\n",
      "Training iteration: 328\n",
      "Improved validation loss from: 0.13083518743515016  to: 0.1308065414428711\n",
      "Training iteration: 329\n",
      "Improved validation loss from: 0.1308065414428711  to: 0.13077819347381592\n",
      "Training iteration: 330\n",
      "Improved validation loss from: 0.13077819347381592  to: 0.1307501435279846\n",
      "Training iteration: 331\n",
      "Improved validation loss from: 0.1307501435279846  to: 0.13072235584259034\n",
      "Training iteration: 332\n",
      "Improved validation loss from: 0.13072235584259034  to: 0.1306948781013489\n",
      "Training iteration: 333\n",
      "Improved validation loss from: 0.1306948781013489  to: 0.13066765069961547\n",
      "Training iteration: 334\n",
      "Improved validation loss from: 0.13066765069961547  to: 0.1306407332420349\n",
      "Training iteration: 335\n",
      "Improved validation loss from: 0.1306407332420349  to: 0.13061411380767823\n",
      "Training iteration: 336\n",
      "Improved validation loss from: 0.13061411380767823  to: 0.1305877685546875\n",
      "Training iteration: 337\n",
      "Improved validation loss from: 0.1305877685546875  to: 0.1305617570877075\n",
      "Training iteration: 338\n",
      "Improved validation loss from: 0.1305617570877075  to: 0.1305360436439514\n",
      "Training iteration: 339\n",
      "Improved validation loss from: 0.1305360436439514  to: 0.13051059246063232\n",
      "Training iteration: 340\n",
      "Improved validation loss from: 0.13051059246063232  to: 0.13048545122146607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 341\n",
      "Improved validation loss from: 0.13048545122146607  to: 0.13046057224273683\n",
      "Training iteration: 342\n",
      "Improved validation loss from: 0.13046057224273683  to: 0.1304359555244446\n",
      "Training iteration: 343\n",
      "Improved validation loss from: 0.1304359555244446  to: 0.13041160106658936\n",
      "Training iteration: 344\n",
      "Improved validation loss from: 0.13041160106658936  to: 0.1303875207901001\n",
      "Training iteration: 345\n",
      "Improved validation loss from: 0.1303875207901001  to: 0.1303644061088562\n",
      "Training iteration: 346\n",
      "Improved validation loss from: 0.1303644061088562  to: 0.1303421974182129\n",
      "Training iteration: 347\n",
      "Improved validation loss from: 0.1303421974182129  to: 0.1303207755088806\n",
      "Training iteration: 348\n",
      "Improved validation loss from: 0.1303207755088806  to: 0.13030011653900148\n",
      "Training iteration: 349\n",
      "Improved validation loss from: 0.13030011653900148  to: 0.13028014898300172\n",
      "Training iteration: 350\n",
      "Improved validation loss from: 0.13028014898300172  to: 0.13026081323623656\n",
      "Training iteration: 351\n",
      "Improved validation loss from: 0.13026081323623656  to: 0.1302421808242798\n",
      "Training iteration: 352\n",
      "Improved validation loss from: 0.1302421808242798  to: 0.13022416830062866\n",
      "Training iteration: 353\n",
      "Improved validation loss from: 0.13022416830062866  to: 0.13020668029785157\n",
      "Training iteration: 354\n",
      "Improved validation loss from: 0.13020668029785157  to: 0.1301896333694458\n",
      "Training iteration: 355\n",
      "Improved validation loss from: 0.1301896333694458  to: 0.1301730155944824\n",
      "Training iteration: 356\n",
      "Improved validation loss from: 0.1301730155944824  to: 0.13015676736831666\n",
      "Training iteration: 357\n",
      "Improved validation loss from: 0.13015676736831666  to: 0.13014087677001954\n",
      "Training iteration: 358\n",
      "Improved validation loss from: 0.13014087677001954  to: 0.13012529611587526\n",
      "Training iteration: 359\n",
      "Improved validation loss from: 0.13012529611587526  to: 0.130109965801239\n",
      "Training iteration: 360\n",
      "Improved validation loss from: 0.130109965801239  to: 0.13009493350982665\n",
      "Training iteration: 361\n",
      "Improved validation loss from: 0.13009493350982665  to: 0.13008010387420654\n",
      "Training iteration: 362\n",
      "Improved validation loss from: 0.13008010387420654  to: 0.13006547689437867\n",
      "Training iteration: 363\n",
      "Improved validation loss from: 0.13006547689437867  to: 0.13005104064941406\n",
      "Training iteration: 364\n",
      "Improved validation loss from: 0.13005104064941406  to: 0.13003677129745483\n",
      "Training iteration: 365\n",
      "Improved validation loss from: 0.13003677129745483  to: 0.1300226330757141\n",
      "Training iteration: 366\n",
      "Improved validation loss from: 0.1300226330757141  to: 0.13000862598419188\n",
      "Training iteration: 367\n",
      "Improved validation loss from: 0.13000862598419188  to: 0.1299947142601013\n",
      "Training iteration: 368\n",
      "Improved validation loss from: 0.1299947142601013  to: 0.1299809217453003\n",
      "Training iteration: 369\n",
      "Improved validation loss from: 0.1299809217453003  to: 0.12996717691421508\n",
      "Training iteration: 370\n",
      "Improved validation loss from: 0.12996717691421508  to: 0.1299535036087036\n",
      "Training iteration: 371\n",
      "Improved validation loss from: 0.1299535036087036  to: 0.12993987798690795\n",
      "Training iteration: 372\n",
      "Improved validation loss from: 0.12993987798690795  to: 0.12992632389068604\n",
      "Training iteration: 373\n",
      "Improved validation loss from: 0.12992632389068604  to: 0.12991278171539306\n",
      "Training iteration: 374\n",
      "Improved validation loss from: 0.12991278171539306  to: 0.12989925146102904\n",
      "Training iteration: 375\n",
      "Improved validation loss from: 0.12989925146102904  to: 0.12988574504852296\n",
      "Training iteration: 376\n",
      "Improved validation loss from: 0.12988574504852296  to: 0.1298722505569458\n",
      "Training iteration: 377\n",
      "Improved validation loss from: 0.1298722505569458  to: 0.12985875606536865\n",
      "Training iteration: 378\n",
      "Improved validation loss from: 0.12985875606536865  to: 0.12984522581100463\n",
      "Training iteration: 379\n",
      "Improved validation loss from: 0.12984522581100463  to: 0.12983169555664062\n",
      "Training iteration: 380\n",
      "Improved validation loss from: 0.12983169555664062  to: 0.12981812953948973\n",
      "Training iteration: 381\n",
      "Improved validation loss from: 0.12981812953948973  to: 0.129804527759552\n",
      "Training iteration: 382\n",
      "Improved validation loss from: 0.129804527759552  to: 0.12979087829589844\n",
      "Training iteration: 383\n",
      "Improved validation loss from: 0.12979087829589844  to: 0.129777193069458\n",
      "Training iteration: 384\n",
      "Improved validation loss from: 0.129777193069458  to: 0.1297634482383728\n",
      "Training iteration: 385\n",
      "Improved validation loss from: 0.1297634482383728  to: 0.12974965572357178\n",
      "Training iteration: 386\n",
      "Improved validation loss from: 0.12974965572357178  to: 0.12973568439483643\n",
      "Training iteration: 387\n",
      "Improved validation loss from: 0.12973568439483643  to: 0.12972162961959838\n",
      "Training iteration: 388\n",
      "Improved validation loss from: 0.12972162961959838  to: 0.1297075033187866\n",
      "Training iteration: 389\n",
      "Improved validation loss from: 0.1297075033187866  to: 0.129693341255188\n",
      "Training iteration: 390\n",
      "Improved validation loss from: 0.129693341255188  to: 0.12967910766601562\n",
      "Training iteration: 391\n",
      "Improved validation loss from: 0.12967910766601562  to: 0.12966480255126953\n",
      "Training iteration: 392\n",
      "Improved validation loss from: 0.12966480255126953  to: 0.12965044975280762\n",
      "Training iteration: 393\n",
      "Improved validation loss from: 0.12965044975280762  to: 0.12963602542877198\n",
      "Training iteration: 394\n",
      "Improved validation loss from: 0.12963602542877198  to: 0.12962154150009156\n",
      "Training iteration: 395\n",
      "Improved validation loss from: 0.12962154150009156  to: 0.1296069860458374\n",
      "Training iteration: 396\n",
      "Improved validation loss from: 0.1296069860458374  to: 0.1295923948287964\n",
      "Training iteration: 397\n",
      "Improved validation loss from: 0.1295923948287964  to: 0.12957777976989746\n",
      "Training iteration: 398\n",
      "Improved validation loss from: 0.12957777976989746  to: 0.12956314086914061\n",
      "Training iteration: 399\n",
      "Improved validation loss from: 0.12956314086914061  to: 0.12954847812652587\n",
      "Training iteration: 400\n",
      "Improved validation loss from: 0.12954847812652587  to: 0.12953381538391112\n",
      "Training iteration: 401\n",
      "Improved validation loss from: 0.12953381538391112  to: 0.12951916456222534\n",
      "Training iteration: 402\n",
      "Improved validation loss from: 0.12951916456222534  to: 0.1295045256614685\n",
      "Training iteration: 403\n",
      "Improved validation loss from: 0.1295045256614685  to: 0.12948989868164062\n",
      "Training iteration: 404\n",
      "Improved validation loss from: 0.12948989868164062  to: 0.12947527170181275\n",
      "Training iteration: 405\n",
      "Improved validation loss from: 0.12947527170181275  to: 0.12946064472198487\n",
      "Training iteration: 406\n",
      "Improved validation loss from: 0.12946064472198487  to: 0.12944602966308594\n",
      "Training iteration: 407\n",
      "Improved validation loss from: 0.12944602966308594  to: 0.1294313907623291\n",
      "Training iteration: 408\n",
      "Improved validation loss from: 0.1294313907623291  to: 0.12941673994064332\n",
      "Training iteration: 409\n",
      "Improved validation loss from: 0.12941673994064332  to: 0.1294020414352417\n",
      "Training iteration: 410\n",
      "Improved validation loss from: 0.1294020414352417  to: 0.1293873071670532\n",
      "Training iteration: 411\n",
      "Improved validation loss from: 0.1293873071670532  to: 0.1293725848197937\n",
      "Training iteration: 412\n",
      "Improved validation loss from: 0.1293725848197937  to: 0.12935763597488403\n",
      "Training iteration: 413\n",
      "Improved validation loss from: 0.12935763597488403  to: 0.1293426513671875\n",
      "Training iteration: 414\n",
      "Improved validation loss from: 0.1293426513671875  to: 0.129327654838562\n",
      "Training iteration: 415\n",
      "Improved validation loss from: 0.129327654838562  to: 0.1293126344680786\n",
      "Training iteration: 416\n",
      "Improved validation loss from: 0.1293126344680786  to: 0.12929775714874267\n",
      "Training iteration: 417\n",
      "Improved validation loss from: 0.12929775714874267  to: 0.1292830228805542\n",
      "Training iteration: 418\n",
      "Improved validation loss from: 0.1292830228805542  to: 0.1292683959007263\n",
      "Training iteration: 419\n",
      "Improved validation loss from: 0.1292683959007263  to: 0.1292538523674011\n",
      "Training iteration: 420\n",
      "Improved validation loss from: 0.1292538523674011  to: 0.12923942804336547\n",
      "Training iteration: 421\n",
      "Improved validation loss from: 0.12923942804336547  to: 0.1292250394821167\n",
      "Training iteration: 422\n",
      "Improved validation loss from: 0.1292250394821167  to: 0.1292107582092285\n",
      "Training iteration: 423\n",
      "Improved validation loss from: 0.1292107582092285  to: 0.12919650077819825\n",
      "Training iteration: 424\n",
      "Improved validation loss from: 0.12919650077819825  to: 0.12918226718902587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 425\n",
      "Improved validation loss from: 0.12918226718902587  to: 0.1291680693626404\n",
      "Training iteration: 426\n",
      "Improved validation loss from: 0.1291680693626404  to: 0.12915388345718384\n",
      "Training iteration: 427\n",
      "Improved validation loss from: 0.12915388345718384  to: 0.12913968563079833\n",
      "Training iteration: 428\n",
      "Improved validation loss from: 0.12913968563079833  to: 0.1291255235671997\n",
      "Training iteration: 429\n",
      "Improved validation loss from: 0.1291255235671997  to: 0.12911132574081421\n",
      "Training iteration: 430\n",
      "Improved validation loss from: 0.12911132574081421  to: 0.1290971279144287\n",
      "Training iteration: 431\n",
      "Improved validation loss from: 0.1290971279144287  to: 0.12908289432525635\n",
      "Training iteration: 432\n",
      "Improved validation loss from: 0.12908289432525635  to: 0.12906862497329713\n",
      "Training iteration: 433\n",
      "Improved validation loss from: 0.12906862497329713  to: 0.12905433177947997\n",
      "Training iteration: 434\n",
      "Improved validation loss from: 0.12905433177947997  to: 0.12904005050659179\n",
      "Training iteration: 435\n",
      "Improved validation loss from: 0.12904005050659179  to: 0.1290257453918457\n",
      "Training iteration: 436\n",
      "Improved validation loss from: 0.1290257453918457  to: 0.12901140451431276\n",
      "Training iteration: 437\n",
      "Improved validation loss from: 0.12901140451431276  to: 0.12899700403213502\n",
      "Training iteration: 438\n",
      "Improved validation loss from: 0.12899700403213502  to: 0.1289825677871704\n",
      "Training iteration: 439\n",
      "Improved validation loss from: 0.1289825677871704  to: 0.12896808385848998\n",
      "Training iteration: 440\n",
      "Improved validation loss from: 0.12896808385848998  to: 0.1289535641670227\n",
      "Training iteration: 441\n",
      "Improved validation loss from: 0.1289535641670227  to: 0.12893898487091066\n",
      "Training iteration: 442\n",
      "Improved validation loss from: 0.12893898487091066  to: 0.12892434597015381\n",
      "Training iteration: 443\n",
      "Improved validation loss from: 0.12892434597015381  to: 0.12890965938568116\n",
      "Training iteration: 444\n",
      "Improved validation loss from: 0.12890965938568116  to: 0.12889492511749268\n",
      "Training iteration: 445\n",
      "Improved validation loss from: 0.12889492511749268  to: 0.12888011932373047\n",
      "Training iteration: 446\n",
      "Improved validation loss from: 0.12888011932373047  to: 0.12886526584625244\n",
      "Training iteration: 447\n",
      "Improved validation loss from: 0.12886526584625244  to: 0.1288503646850586\n",
      "Training iteration: 448\n",
      "Improved validation loss from: 0.1288503646850586  to: 0.12883541584014893\n",
      "Training iteration: 449\n",
      "Improved validation loss from: 0.12883541584014893  to: 0.12882041931152344\n",
      "Training iteration: 450\n",
      "Improved validation loss from: 0.12882041931152344  to: 0.12880538702011107\n",
      "Training iteration: 451\n",
      "Improved validation loss from: 0.12880538702011107  to: 0.12879031896591187\n",
      "Training iteration: 452\n",
      "Improved validation loss from: 0.12879031896591187  to: 0.12877522706985473\n",
      "Training iteration: 453\n",
      "Improved validation loss from: 0.12877522706985473  to: 0.12876008749008178\n",
      "Training iteration: 454\n",
      "Improved validation loss from: 0.12876008749008178  to: 0.12874491214752198\n",
      "Training iteration: 455\n",
      "Improved validation loss from: 0.12874491214752198  to: 0.1287297010421753\n",
      "Training iteration: 456\n",
      "Improved validation loss from: 0.1287297010421753  to: 0.1287144660949707\n",
      "Training iteration: 457\n",
      "Improved validation loss from: 0.1287144660949707  to: 0.1286991834640503\n",
      "Training iteration: 458\n",
      "Improved validation loss from: 0.1286991834640503  to: 0.12868409156799315\n",
      "Training iteration: 459\n",
      "Improved validation loss from: 0.12868409156799315  to: 0.1286689519882202\n",
      "Training iteration: 460\n",
      "Improved validation loss from: 0.1286689519882202  to: 0.12865378856658935\n",
      "Training iteration: 461\n",
      "Improved validation loss from: 0.12865378856658935  to: 0.1286385774612427\n",
      "Training iteration: 462\n",
      "Improved validation loss from: 0.1286385774612427  to: 0.12862330675125122\n",
      "Training iteration: 463\n",
      "Improved validation loss from: 0.12862330675125122  to: 0.1286080002784729\n",
      "Training iteration: 464\n",
      "Improved validation loss from: 0.1286080002784729  to: 0.12859264612197877\n",
      "Training iteration: 465\n",
      "Improved validation loss from: 0.12859264612197877  to: 0.12857723236083984\n",
      "Training iteration: 466\n",
      "Improved validation loss from: 0.12857723236083984  to: 0.12856178283691405\n",
      "Training iteration: 467\n",
      "Improved validation loss from: 0.12856178283691405  to: 0.12854629755020142\n",
      "Training iteration: 468\n",
      "Improved validation loss from: 0.12854629755020142  to: 0.12853076457977294\n",
      "Training iteration: 469\n",
      "Improved validation loss from: 0.12853076457977294  to: 0.12851519584655763\n",
      "Training iteration: 470\n",
      "Improved validation loss from: 0.12851519584655763  to: 0.12849959135055541\n",
      "Training iteration: 471\n",
      "Improved validation loss from: 0.12849959135055541  to: 0.12848390340805055\n",
      "Training iteration: 472\n",
      "Improved validation loss from: 0.12848390340805055  to: 0.1284682035446167\n",
      "Training iteration: 473\n",
      "Improved validation loss from: 0.1284682035446167  to: 0.12845242023468018\n",
      "Training iteration: 474\n",
      "Improved validation loss from: 0.12845242023468018  to: 0.12843658924102783\n",
      "Training iteration: 475\n",
      "Improved validation loss from: 0.12843658924102783  to: 0.1284206986427307\n",
      "Training iteration: 476\n",
      "Improved validation loss from: 0.1284206986427307  to: 0.12840477228164673\n",
      "Training iteration: 477\n",
      "Improved validation loss from: 0.12840477228164673  to: 0.12838877439498902\n",
      "Training iteration: 478\n",
      "Improved validation loss from: 0.12838877439498902  to: 0.12837274074554444\n",
      "Training iteration: 479\n",
      "Improved validation loss from: 0.12837274074554444  to: 0.1283566474914551\n",
      "Training iteration: 480\n",
      "Improved validation loss from: 0.1283566474914551  to: 0.1283405065536499\n",
      "Training iteration: 481\n",
      "Improved validation loss from: 0.1283405065536499  to: 0.12832434177398683\n",
      "Training iteration: 482\n",
      "Improved validation loss from: 0.12832434177398683  to: 0.12830812931060792\n",
      "Training iteration: 483\n",
      "Improved validation loss from: 0.12830812931060792  to: 0.12829186916351318\n",
      "Training iteration: 484\n",
      "Improved validation loss from: 0.12829186916351318  to: 0.12827550172805785\n",
      "Training iteration: 485\n",
      "Improved validation loss from: 0.12827550172805785  to: 0.12825911045074462\n",
      "Training iteration: 486\n",
      "Improved validation loss from: 0.12825911045074462  to: 0.12824270725250245\n",
      "Training iteration: 487\n",
      "Improved validation loss from: 0.12824270725250245  to: 0.12822628021240234\n",
      "Training iteration: 488\n",
      "Improved validation loss from: 0.12822628021240234  to: 0.1282098412513733\n",
      "Training iteration: 489\n",
      "Improved validation loss from: 0.1282098412513733  to: 0.12819335460662842\n",
      "Training iteration: 490\n",
      "Improved validation loss from: 0.12819335460662842  to: 0.12817682027816774\n",
      "Training iteration: 491\n",
      "Improved validation loss from: 0.12817682027816774  to: 0.12816027402877808\n",
      "Training iteration: 492\n",
      "Improved validation loss from: 0.12816027402877808  to: 0.12814368009567262\n",
      "Training iteration: 493\n",
      "Improved validation loss from: 0.12814368009567262  to: 0.12812702655792235\n",
      "Training iteration: 494\n",
      "Improved validation loss from: 0.12812702655792235  to: 0.1281103014945984\n",
      "Training iteration: 495\n",
      "Improved validation loss from: 0.1281103014945984  to: 0.12809355258941652\n",
      "Training iteration: 496\n",
      "Improved validation loss from: 0.12809355258941652  to: 0.12807666063308715\n",
      "Training iteration: 497\n",
      "Improved validation loss from: 0.12807666063308715  to: 0.12805970907211303\n",
      "Training iteration: 498\n",
      "Improved validation loss from: 0.12805970907211303  to: 0.12804268598556517\n",
      "Training iteration: 499\n",
      "Improved validation loss from: 0.12804268598556517  to: 0.1280255913734436\n",
      "Training iteration: 500\n",
      "Improved validation loss from: 0.1280255913734436  to: 0.1280085563659668\n",
      "Training iteration: 501\n",
      "Improved validation loss from: 0.1280085563659668  to: 0.1279915452003479\n",
      "Training iteration: 502\n",
      "Improved validation loss from: 0.1279915452003479  to: 0.12797445058822632\n",
      "Training iteration: 503\n",
      "Improved validation loss from: 0.12797445058822632  to: 0.127957284450531\n",
      "Training iteration: 504\n",
      "Improved validation loss from: 0.127957284450531  to: 0.12794005870819092\n",
      "Training iteration: 505\n",
      "Improved validation loss from: 0.12794005870819092  to: 0.12792272567749025\n",
      "Training iteration: 506\n",
      "Improved validation loss from: 0.12792272567749025  to: 0.12790533304214477\n",
      "Training iteration: 507\n",
      "Improved validation loss from: 0.12790533304214477  to: 0.12788746356964112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 508\n",
      "Improved validation loss from: 0.12788746356964112  to: 0.1278694748878479\n",
      "Training iteration: 509\n",
      "Improved validation loss from: 0.1278694748878479  to: 0.12785139083862304\n",
      "Training iteration: 510\n",
      "Improved validation loss from: 0.12785139083862304  to: 0.12783323526382445\n",
      "Training iteration: 511\n",
      "Improved validation loss from: 0.12783323526382445  to: 0.12781499624252318\n",
      "Training iteration: 512\n",
      "Improved validation loss from: 0.12781499624252318  to: 0.1277966856956482\n",
      "Training iteration: 513\n",
      "Improved validation loss from: 0.1277966856956482  to: 0.12777827978134154\n",
      "Training iteration: 514\n",
      "Improved validation loss from: 0.12777827978134154  to: 0.12775980234146117\n",
      "Training iteration: 515\n",
      "Improved validation loss from: 0.12775980234146117  to: 0.12774124145507812\n",
      "Training iteration: 516\n",
      "Improved validation loss from: 0.12774124145507812  to: 0.12772260904312133\n",
      "Training iteration: 517\n",
      "Improved validation loss from: 0.12772260904312133  to: 0.1277039170265198\n",
      "Training iteration: 518\n",
      "Improved validation loss from: 0.1277039170265198  to: 0.12768514156341554\n",
      "Training iteration: 519\n",
      "Improved validation loss from: 0.12768514156341554  to: 0.12766630649566652\n",
      "Training iteration: 520\n",
      "Improved validation loss from: 0.12766630649566652  to: 0.12764737606048585\n",
      "Training iteration: 521\n",
      "Improved validation loss from: 0.12764737606048585  to: 0.12762844562530518\n",
      "Training iteration: 522\n",
      "Improved validation loss from: 0.12762844562530518  to: 0.1276094675064087\n",
      "Training iteration: 523\n",
      "Improved validation loss from: 0.1276094675064087  to: 0.12759047746658325\n",
      "Training iteration: 524\n",
      "Improved validation loss from: 0.12759047746658325  to: 0.12757145166397094\n",
      "Training iteration: 525\n",
      "Improved validation loss from: 0.12757145166397094  to: 0.12755239009857178\n",
      "Training iteration: 526\n",
      "Improved validation loss from: 0.12755239009857178  to: 0.12753331661224365\n",
      "Training iteration: 527\n",
      "Improved validation loss from: 0.12753331661224365  to: 0.12751420736312866\n",
      "Training iteration: 528\n",
      "Improved validation loss from: 0.12751420736312866  to: 0.1274951219558716\n",
      "Training iteration: 529\n",
      "Improved validation loss from: 0.1274951219558716  to: 0.12747595310211182\n",
      "Training iteration: 530\n",
      "Improved validation loss from: 0.12747595310211182  to: 0.12745676040649415\n",
      "Training iteration: 531\n",
      "Improved validation loss from: 0.12745676040649415  to: 0.1274375319480896\n",
      "Training iteration: 532\n",
      "Improved validation loss from: 0.1274375319480896  to: 0.12741826772689818\n",
      "Training iteration: 533\n",
      "Improved validation loss from: 0.12741826772689818  to: 0.12739896774291992\n",
      "Training iteration: 534\n",
      "Improved validation loss from: 0.12739896774291992  to: 0.12737962007522582\n",
      "Training iteration: 535\n",
      "Improved validation loss from: 0.12737962007522582  to: 0.127360200881958\n",
      "Training iteration: 536\n",
      "Improved validation loss from: 0.127360200881958  to: 0.127340567111969\n",
      "Training iteration: 537\n",
      "Improved validation loss from: 0.127340567111969  to: 0.12732090950012206\n",
      "Training iteration: 538\n",
      "Improved validation loss from: 0.12732090950012206  to: 0.1273011565208435\n",
      "Training iteration: 539\n",
      "Improved validation loss from: 0.1273011565208435  to: 0.1272813558578491\n",
      "Training iteration: 540\n",
      "Improved validation loss from: 0.1272813558578491  to: 0.12726147174835206\n",
      "Training iteration: 541\n",
      "Improved validation loss from: 0.12726147174835206  to: 0.12724153995513915\n",
      "Training iteration: 542\n",
      "Improved validation loss from: 0.12724153995513915  to: 0.12722150087356568\n",
      "Training iteration: 543\n",
      "Improved validation loss from: 0.12722150087356568  to: 0.12720139026641847\n",
      "Training iteration: 544\n",
      "Improved validation loss from: 0.12720139026641847  to: 0.12718122005462645\n",
      "Training iteration: 545\n",
      "Improved validation loss from: 0.12718122005462645  to: 0.12716094255447388\n",
      "Training iteration: 546\n",
      "Improved validation loss from: 0.12716094255447388  to: 0.12714059352874757\n",
      "Training iteration: 547\n",
      "Improved validation loss from: 0.12714059352874757  to: 0.12712018489837645\n",
      "Training iteration: 548\n",
      "Improved validation loss from: 0.12712018489837645  to: 0.12709968090057372\n",
      "Training iteration: 549\n",
      "Improved validation loss from: 0.12709968090057372  to: 0.12707908153533937\n",
      "Training iteration: 550\n",
      "Improved validation loss from: 0.12707908153533937  to: 0.12705841064453124\n",
      "Training iteration: 551\n",
      "Improved validation loss from: 0.12705841064453124  to: 0.12703765630722047\n",
      "Training iteration: 552\n",
      "Improved validation loss from: 0.12703765630722047  to: 0.12701680660247802\n",
      "Training iteration: 553\n",
      "Improved validation loss from: 0.12701680660247802  to: 0.12699587345123292\n",
      "Training iteration: 554\n",
      "Improved validation loss from: 0.12699587345123292  to: 0.1269749641418457\n",
      "Training iteration: 555\n",
      "Improved validation loss from: 0.1269749641418457  to: 0.12695401906967163\n",
      "Training iteration: 556\n",
      "Improved validation loss from: 0.12695401906967163  to: 0.1269330859184265\n",
      "Training iteration: 557\n",
      "Improved validation loss from: 0.1269330859184265  to: 0.12691209316253663\n",
      "Training iteration: 558\n",
      "Improved validation loss from: 0.12691209316253663  to: 0.12689108848571778\n",
      "Training iteration: 559\n",
      "Improved validation loss from: 0.12689108848571778  to: 0.1268700361251831\n",
      "Training iteration: 560\n",
      "Improved validation loss from: 0.1268700361251831  to: 0.12684895992279052\n",
      "Training iteration: 561\n",
      "Improved validation loss from: 0.12684895992279052  to: 0.12682783603668213\n",
      "Training iteration: 562\n",
      "Improved validation loss from: 0.12682783603668213  to: 0.12680667638778687\n",
      "Training iteration: 563\n",
      "Improved validation loss from: 0.12680667638778687  to: 0.12678548097610473\n",
      "Training iteration: 564\n",
      "Improved validation loss from: 0.12678548097610473  to: 0.12676421403884888\n",
      "Training iteration: 565\n",
      "Improved validation loss from: 0.12676421403884888  to: 0.1267428994178772\n",
      "Training iteration: 566\n",
      "Improved validation loss from: 0.1267428994178772  to: 0.12672150135040283\n",
      "Training iteration: 567\n",
      "Improved validation loss from: 0.12672150135040283  to: 0.1267000675201416\n",
      "Training iteration: 568\n",
      "Improved validation loss from: 0.1267000675201416  to: 0.12667853832244874\n",
      "Training iteration: 569\n",
      "Improved validation loss from: 0.12667853832244874  to: 0.12665696144104005\n",
      "Training iteration: 570\n",
      "Improved validation loss from: 0.12665696144104005  to: 0.12663530111312865\n",
      "Training iteration: 571\n",
      "Improved validation loss from: 0.12663530111312865  to: 0.1266135573387146\n",
      "Training iteration: 572\n",
      "Improved validation loss from: 0.1266135573387146  to: 0.12659175395965577\n",
      "Training iteration: 573\n",
      "Improved validation loss from: 0.12659175395965577  to: 0.12656986713409424\n",
      "Training iteration: 574\n",
      "Improved validation loss from: 0.12656986713409424  to: 0.1265479326248169\n",
      "Training iteration: 575\n",
      "Improved validation loss from: 0.1265479326248169  to: 0.12652591466903687\n",
      "Training iteration: 576\n",
      "Improved validation loss from: 0.12652591466903687  to: 0.12650392055511475\n",
      "Training iteration: 577\n",
      "Improved validation loss from: 0.12650392055511475  to: 0.1264819860458374\n",
      "Training iteration: 578\n",
      "Improved validation loss from: 0.1264819860458374  to: 0.12645995616912842\n",
      "Training iteration: 579\n",
      "Improved validation loss from: 0.12645995616912842  to: 0.1264379858970642\n",
      "Training iteration: 580\n",
      "Improved validation loss from: 0.1264379858970642  to: 0.12641607522964476\n",
      "Training iteration: 581\n",
      "Improved validation loss from: 0.12641607522964476  to: 0.1263941764831543\n",
      "Training iteration: 582\n",
      "Improved validation loss from: 0.1263941764831543  to: 0.12637240886688234\n",
      "Training iteration: 583\n",
      "Improved validation loss from: 0.12637240886688234  to: 0.12635074853897094\n",
      "Training iteration: 584\n",
      "Improved validation loss from: 0.12635074853897094  to: 0.12632917165756224\n",
      "Training iteration: 585\n",
      "Improved validation loss from: 0.12632917165756224  to: 0.12630767822265626\n",
      "Training iteration: 586\n",
      "Improved validation loss from: 0.12630767822265626  to: 0.12628624439239503\n",
      "Training iteration: 587\n",
      "Improved validation loss from: 0.12628624439239503  to: 0.1262648582458496\n",
      "Training iteration: 588\n",
      "Improved validation loss from: 0.1262648582458496  to: 0.12624351978302\n",
      "Training iteration: 589\n",
      "Improved validation loss from: 0.12624351978302  to: 0.12622215747833251\n",
      "Training iteration: 590\n",
      "Improved validation loss from: 0.12622215747833251  to: 0.12620084285736083\n",
      "Training iteration: 591\n",
      "Improved validation loss from: 0.12620084285736083  to: 0.12617956399917601\n",
      "Training iteration: 592\n",
      "Improved validation loss from: 0.12617956399917601  to: 0.12615829706192017\n",
      "Training iteration: 593\n",
      "Improved validation loss from: 0.12615829706192017  to: 0.12613701820373535\n",
      "Training iteration: 594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.12613701820373535  to: 0.12611576318740844\n",
      "Training iteration: 595\n",
      "Improved validation loss from: 0.12611576318740844  to: 0.12609446048736572\n",
      "Training iteration: 596\n",
      "Improved validation loss from: 0.12609446048736572  to: 0.12607311010360717\n",
      "Training iteration: 597\n",
      "Improved validation loss from: 0.12607311010360717  to: 0.1260517120361328\n",
      "Training iteration: 598\n",
      "Improved validation loss from: 0.1260517120361328  to: 0.12603026628494263\n",
      "Training iteration: 599\n",
      "Improved validation loss from: 0.12603026628494263  to: 0.1260087251663208\n",
      "Training iteration: 600\n",
      "Improved validation loss from: 0.1260087251663208  to: 0.1259871244430542\n",
      "Training iteration: 601\n",
      "Improved validation loss from: 0.1259871244430542  to: 0.12596547603607178\n",
      "Training iteration: 602\n",
      "Improved validation loss from: 0.12596547603607178  to: 0.12594373226165773\n",
      "Training iteration: 603\n",
      "Improved validation loss from: 0.12594373226165773  to: 0.12592194080352784\n",
      "Training iteration: 604\n",
      "Improved validation loss from: 0.12592194080352784  to: 0.12589988708496094\n",
      "Training iteration: 605\n",
      "Improved validation loss from: 0.12589988708496094  to: 0.12587738037109375\n",
      "Training iteration: 606\n",
      "Improved validation loss from: 0.12587738037109375  to: 0.1258547782897949\n",
      "Training iteration: 607\n",
      "Improved validation loss from: 0.1258547782897949  to: 0.12583208084106445\n",
      "Training iteration: 608\n",
      "Improved validation loss from: 0.12583208084106445  to: 0.12580928802490235\n",
      "Training iteration: 609\n",
      "Improved validation loss from: 0.12580928802490235  to: 0.1257863759994507\n",
      "Training iteration: 610\n",
      "Improved validation loss from: 0.1257863759994507  to: 0.12576338052749633\n",
      "Training iteration: 611\n",
      "Improved validation loss from: 0.12576338052749633  to: 0.12574026584625245\n",
      "Training iteration: 612\n",
      "Improved validation loss from: 0.12574026584625245  to: 0.12571704387664795\n",
      "Training iteration: 613\n",
      "Improved validation loss from: 0.12571704387664795  to: 0.1256937265396118\n",
      "Training iteration: 614\n",
      "Improved validation loss from: 0.1256937265396118  to: 0.12567028999328614\n",
      "Training iteration: 615\n",
      "Improved validation loss from: 0.12567028999328614  to: 0.1256465196609497\n",
      "Training iteration: 616\n",
      "Improved validation loss from: 0.1256465196609497  to: 0.12562257051467896\n",
      "Training iteration: 617\n",
      "Improved validation loss from: 0.12562257051467896  to: 0.12559864521026612\n",
      "Training iteration: 618\n",
      "Improved validation loss from: 0.12559864521026612  to: 0.1255747675895691\n",
      "Training iteration: 619\n",
      "Improved validation loss from: 0.1255747675895691  to: 0.12555091381072997\n",
      "Training iteration: 620\n",
      "Improved validation loss from: 0.12555091381072997  to: 0.12552703619003297\n",
      "Training iteration: 621\n",
      "Improved validation loss from: 0.12552703619003297  to: 0.1255031943321228\n",
      "Training iteration: 622\n",
      "Improved validation loss from: 0.1255031943321228  to: 0.1254793405532837\n",
      "Training iteration: 623\n",
      "Improved validation loss from: 0.1254793405532837  to: 0.12545549869537354\n",
      "Training iteration: 624\n",
      "Improved validation loss from: 0.12545549869537354  to: 0.1254316449165344\n",
      "Training iteration: 625\n",
      "Improved validation loss from: 0.1254316449165344  to: 0.1254077672958374\n",
      "Training iteration: 626\n",
      "Improved validation loss from: 0.1254077672958374  to: 0.12538385391235352\n",
      "Training iteration: 627\n",
      "Improved validation loss from: 0.12538385391235352  to: 0.1253598928451538\n",
      "Training iteration: 628\n",
      "Improved validation loss from: 0.1253598928451538  to: 0.1253358840942383\n",
      "Training iteration: 629\n",
      "Improved validation loss from: 0.1253358840942383  to: 0.12531182765960694\n",
      "Training iteration: 630\n",
      "Improved validation loss from: 0.12531182765960694  to: 0.12528769969940184\n",
      "Training iteration: 631\n",
      "Improved validation loss from: 0.12528769969940184  to: 0.125263512134552\n",
      "Training iteration: 632\n",
      "Improved validation loss from: 0.125263512134552  to: 0.12523925304412842\n",
      "Training iteration: 633\n",
      "Improved validation loss from: 0.12523925304412842  to: 0.12521488666534425\n",
      "Training iteration: 634\n",
      "Improved validation loss from: 0.12521488666534425  to: 0.12519047260284424\n",
      "Training iteration: 635\n",
      "Improved validation loss from: 0.12519047260284424  to: 0.1251659631729126\n",
      "Training iteration: 636\n",
      "Improved validation loss from: 0.1251659631729126  to: 0.12514150142669678\n",
      "Training iteration: 637\n",
      "Improved validation loss from: 0.12514150142669678  to: 0.12511709928512574\n",
      "Training iteration: 638\n",
      "Improved validation loss from: 0.12511709928512574  to: 0.125092613697052\n",
      "Training iteration: 639\n",
      "Improved validation loss from: 0.125092613697052  to: 0.1250680685043335\n",
      "Training iteration: 640\n",
      "Improved validation loss from: 0.1250680685043335  to: 0.12504346370697023\n",
      "Training iteration: 641\n",
      "Improved validation loss from: 0.12504346370697023  to: 0.12501878738403321\n",
      "Training iteration: 642\n",
      "Improved validation loss from: 0.12501878738403321  to: 0.12499399185180664\n",
      "Training iteration: 643\n",
      "Improved validation loss from: 0.12499399185180664  to: 0.12496910095214844\n",
      "Training iteration: 644\n",
      "Improved validation loss from: 0.12496910095214844  to: 0.12494411468505859\n",
      "Training iteration: 645\n",
      "Improved validation loss from: 0.12494411468505859  to: 0.12491899728775024\n",
      "Training iteration: 646\n",
      "Improved validation loss from: 0.12491899728775024  to: 0.12489378452301025\n",
      "Training iteration: 647\n",
      "Improved validation loss from: 0.12489378452301025  to: 0.12486844062805176\n",
      "Training iteration: 648\n",
      "Improved validation loss from: 0.12486844062805176  to: 0.12484300136566162\n",
      "Training iteration: 649\n",
      "Improved validation loss from: 0.12484300136566162  to: 0.12481745481491088\n",
      "Training iteration: 650\n",
      "Improved validation loss from: 0.12481745481491088  to: 0.1247915506362915\n",
      "Training iteration: 651\n",
      "Improved validation loss from: 0.1247915506362915  to: 0.12476532459259033\n",
      "Training iteration: 652\n",
      "Improved validation loss from: 0.12476532459259033  to: 0.12473878860473633\n",
      "Training iteration: 653\n",
      "Improved validation loss from: 0.12473878860473633  to: 0.12471196651458741\n",
      "Training iteration: 654\n",
      "Improved validation loss from: 0.12471196651458741  to: 0.12468512058258056\n",
      "Training iteration: 655\n",
      "Improved validation loss from: 0.12468512058258056  to: 0.12465827465057373\n",
      "Training iteration: 656\n",
      "Improved validation loss from: 0.12465827465057373  to: 0.12463153600692749\n",
      "Training iteration: 657\n",
      "Improved validation loss from: 0.12463153600692749  to: 0.12460482120513916\n",
      "Training iteration: 658\n",
      "Improved validation loss from: 0.12460482120513916  to: 0.12457804679870606\n",
      "Training iteration: 659\n",
      "Improved validation loss from: 0.12457804679870606  to: 0.12455122470855713\n",
      "Training iteration: 660\n",
      "Improved validation loss from: 0.12455122470855713  to: 0.12452433109283448\n",
      "Training iteration: 661\n",
      "Improved validation loss from: 0.12452433109283448  to: 0.12449740171432495\n",
      "Training iteration: 662\n",
      "Improved validation loss from: 0.12449740171432495  to: 0.12447011470794678\n",
      "Training iteration: 663\n",
      "Improved validation loss from: 0.12447011470794678  to: 0.12444251775741577\n",
      "Training iteration: 664\n",
      "Improved validation loss from: 0.12444251775741577  to: 0.12441494464874267\n",
      "Training iteration: 665\n",
      "Improved validation loss from: 0.12441494464874267  to: 0.12438732385635376\n",
      "Training iteration: 666\n",
      "Improved validation loss from: 0.12438732385635376  to: 0.12435970306396485\n",
      "Training iteration: 667\n",
      "Improved validation loss from: 0.12435970306396485  to: 0.12433208227157592\n",
      "Training iteration: 668\n",
      "Improved validation loss from: 0.12433208227157592  to: 0.12430413961410522\n",
      "Training iteration: 669\n",
      "Improved validation loss from: 0.12430413961410522  to: 0.12427622079849243\n",
      "Training iteration: 670\n",
      "Improved validation loss from: 0.12427622079849243  to: 0.12424829006195068\n",
      "Training iteration: 671\n",
      "Improved validation loss from: 0.12424829006195068  to: 0.12422009706497192\n",
      "Training iteration: 672\n",
      "Improved validation loss from: 0.12422009706497192  to: 0.1241918921470642\n",
      "Training iteration: 673\n",
      "Improved validation loss from: 0.1241918921470642  to: 0.12416369915008545\n",
      "Training iteration: 674\n",
      "Improved validation loss from: 0.12416369915008545  to: 0.1241355299949646\n",
      "Training iteration: 675\n",
      "Improved validation loss from: 0.1241355299949646  to: 0.12410705089569092\n",
      "Training iteration: 676\n",
      "Improved validation loss from: 0.12410705089569092  to: 0.12407863140106201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 677\n",
      "Improved validation loss from: 0.12407863140106201  to: 0.12405025959014893\n",
      "Training iteration: 678\n",
      "Improved validation loss from: 0.12405025959014893  to: 0.12402156591415406\n",
      "Training iteration: 679\n",
      "Improved validation loss from: 0.12402156591415406  to: 0.1239929437637329\n",
      "Training iteration: 680\n",
      "Improved validation loss from: 0.1239929437637329  to: 0.12396509647369384\n",
      "Training iteration: 681\n",
      "Improved validation loss from: 0.12396509647369384  to: 0.12393729686737061\n",
      "Training iteration: 682\n",
      "Improved validation loss from: 0.12393729686737061  to: 0.12390918731689453\n",
      "Training iteration: 683\n",
      "Improved validation loss from: 0.12390918731689453  to: 0.12388079166412354\n",
      "Training iteration: 684\n",
      "Improved validation loss from: 0.12388079166412354  to: 0.12385250329971313\n",
      "Training iteration: 685\n",
      "Improved validation loss from: 0.12385250329971313  to: 0.12382432222366332\n",
      "Training iteration: 686\n",
      "Improved validation loss from: 0.12382432222366332  to: 0.12379622459411621\n",
      "Training iteration: 687\n",
      "Improved validation loss from: 0.12379622459411621  to: 0.12376819849014283\n",
      "Training iteration: 688\n",
      "Improved validation loss from: 0.12376819849014283  to: 0.12374022006988525\n",
      "Training iteration: 689\n",
      "Improved validation loss from: 0.12374022006988525  to: 0.12371227741241456\n",
      "Training iteration: 690\n",
      "Improved validation loss from: 0.12371227741241456  to: 0.1236840009689331\n",
      "Training iteration: 691\n",
      "Improved validation loss from: 0.1236840009689331  to: 0.12365541458129883\n",
      "Training iteration: 692\n",
      "Improved validation loss from: 0.12365541458129883  to: 0.12362653017044067\n",
      "Training iteration: 693\n",
      "Improved validation loss from: 0.12362653017044067  to: 0.12359771728515626\n",
      "Training iteration: 694\n",
      "Improved validation loss from: 0.12359771728515626  to: 0.12356898784637452\n",
      "Training iteration: 695\n",
      "Improved validation loss from: 0.12356898784637452  to: 0.1235403299331665\n",
      "Training iteration: 696\n",
      "Improved validation loss from: 0.1235403299331665  to: 0.12351175546646118\n",
      "Training iteration: 697\n",
      "Improved validation loss from: 0.12351175546646118  to: 0.12348315715789795\n",
      "Training iteration: 698\n",
      "Improved validation loss from: 0.12348315715789795  to: 0.12345463037490845\n",
      "Training iteration: 699\n",
      "Improved validation loss from: 0.12345463037490845  to: 0.12342548370361328\n",
      "Training iteration: 700\n",
      "Improved validation loss from: 0.12342548370361328  to: 0.12339594364166259\n",
      "Training iteration: 701\n",
      "Improved validation loss from: 0.12339594364166259  to: 0.12336609363555909\n",
      "Training iteration: 702\n",
      "Improved validation loss from: 0.12336609363555909  to: 0.12333632707595825\n",
      "Training iteration: 703\n",
      "Improved validation loss from: 0.12333632707595825  to: 0.1233066439628601\n",
      "Training iteration: 704\n",
      "Improved validation loss from: 0.1233066439628601  to: 0.12327700853347778\n",
      "Training iteration: 705\n",
      "Improved validation loss from: 0.12327700853347778  to: 0.12324740886688232\n",
      "Training iteration: 706\n",
      "Improved validation loss from: 0.12324740886688232  to: 0.12321792840957642\n",
      "Training iteration: 707\n",
      "Improved validation loss from: 0.12321792840957642  to: 0.12318801879882812\n",
      "Training iteration: 708\n",
      "Improved validation loss from: 0.12318801879882812  to: 0.12315772771835327\n",
      "Training iteration: 709\n",
      "Improved validation loss from: 0.12315772771835327  to: 0.1231275200843811\n",
      "Training iteration: 710\n",
      "Improved validation loss from: 0.1231275200843811  to: 0.12309737205505371\n",
      "Training iteration: 711\n",
      "Improved validation loss from: 0.12309737205505371  to: 0.12306729555130005\n",
      "Training iteration: 712\n",
      "Improved validation loss from: 0.12306729555130005  to: 0.12303680181503296\n",
      "Training iteration: 713\n",
      "Improved validation loss from: 0.12303680181503296  to: 0.12300593852996826\n",
      "Training iteration: 714\n",
      "Improved validation loss from: 0.12300593852996826  to: 0.12297519445419311\n",
      "Training iteration: 715\n",
      "Improved validation loss from: 0.12297519445419311  to: 0.12294453382492065\n",
      "Training iteration: 716\n",
      "Improved validation loss from: 0.12294453382492065  to: 0.12291395664215088\n",
      "Training iteration: 717\n",
      "Improved validation loss from: 0.12291395664215088  to: 0.1228830099105835\n",
      "Training iteration: 718\n",
      "Improved validation loss from: 0.1228830099105835  to: 0.1228521466255188\n",
      "Training iteration: 719\n",
      "Improved validation loss from: 0.1228521466255188  to: 0.12282087802886962\n",
      "Training iteration: 720\n",
      "Improved validation loss from: 0.12282087802886962  to: 0.12278975248336792\n",
      "Training iteration: 721\n",
      "Improved validation loss from: 0.12278975248336792  to: 0.12275871038436889\n",
      "Training iteration: 722\n",
      "Improved validation loss from: 0.12275871038436889  to: 0.12272727489471436\n",
      "Training iteration: 723\n",
      "Improved validation loss from: 0.12272727489471436  to: 0.12269595861434937\n",
      "Training iteration: 724\n",
      "Improved validation loss from: 0.12269595861434937  to: 0.12266426086425782\n",
      "Training iteration: 725\n",
      "Improved validation loss from: 0.12266426086425782  to: 0.12263267040252686\n",
      "Training iteration: 726\n",
      "Improved validation loss from: 0.12263267040252686  to: 0.12260124683380128\n",
      "Training iteration: 727\n",
      "Improved validation loss from: 0.12260124683380128  to: 0.12256940603256225\n",
      "Training iteration: 728\n",
      "Improved validation loss from: 0.12256940603256225  to: 0.12253773212432861\n",
      "Training iteration: 729\n",
      "Improved validation loss from: 0.12253773212432861  to: 0.12250566482543945\n",
      "Training iteration: 730\n",
      "Improved validation loss from: 0.12250566482543945  to: 0.12247377634048462\n",
      "Training iteration: 731\n",
      "Improved validation loss from: 0.12247377634048462  to: 0.12244203090667724\n",
      "Training iteration: 732\n",
      "Improved validation loss from: 0.12244203090667724  to: 0.1224098801612854\n",
      "Training iteration: 733\n",
      "Improved validation loss from: 0.1224098801612854  to: 0.12237786054611206\n",
      "Training iteration: 734\n",
      "Improved validation loss from: 0.12237786054611206  to: 0.1223454475402832\n",
      "Training iteration: 735\n",
      "Improved validation loss from: 0.1223454475402832  to: 0.12231320142745972\n",
      "Training iteration: 736\n",
      "Improved validation loss from: 0.12231320142745972  to: 0.12228109836578369\n",
      "Training iteration: 737\n",
      "Improved validation loss from: 0.12228109836578369  to: 0.12224855422973632\n",
      "Training iteration: 738\n",
      "Improved validation loss from: 0.12224855422973632  to: 0.12221561670303345\n",
      "Training iteration: 739\n",
      "Improved validation loss from: 0.12221561670303345  to: 0.12218286991119384\n",
      "Training iteration: 740\n",
      "Improved validation loss from: 0.12218286991119384  to: 0.12215030193328857\n",
      "Training iteration: 741\n",
      "Improved validation loss from: 0.12215030193328857  to: 0.12211790084838867\n",
      "Training iteration: 742\n",
      "Improved validation loss from: 0.12211790084838867  to: 0.12208505868911743\n",
      "Training iteration: 743\n",
      "Improved validation loss from: 0.12208505868911743  to: 0.12205184698104858\n",
      "Training iteration: 744\n",
      "Improved validation loss from: 0.12205184698104858  to: 0.12201887369155884\n",
      "Training iteration: 745\n",
      "Improved validation loss from: 0.12201887369155884  to: 0.12198610305786133\n",
      "Training iteration: 746\n",
      "Improved validation loss from: 0.12198610305786133  to: 0.12195290327072143\n",
      "Training iteration: 747\n",
      "Improved validation loss from: 0.12195290327072143  to: 0.12191991806030274\n",
      "Training iteration: 748\n",
      "Improved validation loss from: 0.12191991806030274  to: 0.12188650369644165\n",
      "Training iteration: 749\n",
      "Improved validation loss from: 0.12188650369644165  to: 0.12185328006744385\n",
      "Training iteration: 750\n",
      "Improved validation loss from: 0.12185328006744385  to: 0.12182019948959351\n",
      "Training iteration: 751\n",
      "Improved validation loss from: 0.12182019948959351  to: 0.12178665399551392\n",
      "Training iteration: 752\n",
      "Improved validation loss from: 0.12178665399551392  to: 0.12175261974334717\n",
      "Training iteration: 753\n",
      "Improved validation loss from: 0.12175261974334717  to: 0.12171881198883057\n",
      "Training iteration: 754\n",
      "Improved validation loss from: 0.12171881198883057  to: 0.12168515920639038\n",
      "Training iteration: 755\n",
      "Improved validation loss from: 0.12168515920639038  to: 0.12165100574493408\n",
      "Training iteration: 756\n",
      "Improved validation loss from: 0.12165100574493408  to: 0.12161638736724853\n",
      "Training iteration: 757\n",
      "Improved validation loss from: 0.12161638736724853  to: 0.1215820074081421\n",
      "Training iteration: 758\n",
      "Improved validation loss from: 0.1215820074081421  to: 0.12154786586761475\n",
      "Training iteration: 759\n",
      "Improved validation loss from: 0.12154786586761475  to: 0.12151448726654053\n",
      "Training iteration: 760\n",
      "Improved validation loss from: 0.12151448726654053  to: 0.12148110866546631\n",
      "Training iteration: 761\n",
      "Improved validation loss from: 0.12148110866546631  to: 0.12144771814346314\n",
      "Training iteration: 762\n",
      "Improved validation loss from: 0.12144771814346314  to: 0.12141427993774415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 763\n",
      "Improved validation loss from: 0.12141427993774415  to: 0.12138077020645141\n",
      "Training iteration: 764\n",
      "Improved validation loss from: 0.12138077020645141  to: 0.12134784460067749\n",
      "Training iteration: 765\n",
      "Improved validation loss from: 0.12134784460067749  to: 0.12131401300430297\n",
      "Training iteration: 766\n",
      "Improved validation loss from: 0.12131401300430297  to: 0.12127975225448609\n",
      "Training iteration: 767\n",
      "Improved validation loss from: 0.12127975225448609  to: 0.12124521732330322\n",
      "Training iteration: 768\n",
      "Improved validation loss from: 0.12124521732330322  to: 0.12121109962463379\n",
      "Training iteration: 769\n",
      "Improved validation loss from: 0.12121109962463379  to: 0.12117650508880615\n",
      "Training iteration: 770\n",
      "Improved validation loss from: 0.12117650508880615  to: 0.12114185094833374\n",
      "Training iteration: 771\n",
      "Improved validation loss from: 0.12114185094833374  to: 0.12110671997070313\n",
      "Training iteration: 772\n",
      "Improved validation loss from: 0.12110671997070313  to: 0.12107105255126953\n",
      "Training iteration: 773\n",
      "Improved validation loss from: 0.12107105255126953  to: 0.12103488445281982\n",
      "Training iteration: 774\n",
      "Improved validation loss from: 0.12103488445281982  to: 0.12099899053573608\n",
      "Training iteration: 775\n",
      "Improved validation loss from: 0.12099899053573608  to: 0.1209633469581604\n",
      "Training iteration: 776\n",
      "Improved validation loss from: 0.1209633469581604  to: 0.12092787027359009\n",
      "Training iteration: 777\n",
      "Improved validation loss from: 0.12092787027359009  to: 0.12089169025421143\n",
      "Training iteration: 778\n",
      "Improved validation loss from: 0.12089169025421143  to: 0.12085483074188233\n",
      "Training iteration: 779\n",
      "Improved validation loss from: 0.12085483074188233  to: 0.12081733942031861\n",
      "Training iteration: 780\n",
      "Improved validation loss from: 0.12081733942031861  to: 0.12077920436859131\n",
      "Training iteration: 781\n",
      "Improved validation loss from: 0.12077920436859131  to: 0.12074134349822999\n",
      "Training iteration: 782\n",
      "Improved validation loss from: 0.12074134349822999  to: 0.12070369720458984\n",
      "Training iteration: 783\n",
      "Improved validation loss from: 0.12070369720458984  to: 0.12066621780395508\n",
      "Training iteration: 784\n",
      "Improved validation loss from: 0.12066621780395508  to: 0.12062798738479615\n",
      "Training iteration: 785\n",
      "Improved validation loss from: 0.12062798738479615  to: 0.12058902978897094\n",
      "Training iteration: 786\n",
      "Improved validation loss from: 0.12058902978897094  to: 0.12054941654205323\n",
      "Training iteration: 787\n",
      "Improved validation loss from: 0.12054941654205323  to: 0.1205100655555725\n",
      "Training iteration: 788\n",
      "Improved validation loss from: 0.1205100655555725  to: 0.12047092914581299\n",
      "Training iteration: 789\n",
      "Improved validation loss from: 0.12047092914581299  to: 0.12043108940124511\n",
      "Training iteration: 790\n",
      "Improved validation loss from: 0.12043108940124511  to: 0.12039053440093994\n",
      "Training iteration: 791\n",
      "Improved validation loss from: 0.12039053440093994  to: 0.12035027742385865\n",
      "Training iteration: 792\n",
      "Improved validation loss from: 0.12035027742385865  to: 0.12031046152114869\n",
      "Training iteration: 793\n",
      "Improved validation loss from: 0.12031046152114869  to: 0.12027002573013305\n",
      "Training iteration: 794\n",
      "Improved validation loss from: 0.12027002573013305  to: 0.12022905349731446\n",
      "Training iteration: 795\n",
      "Improved validation loss from: 0.12022905349731446  to: 0.12018756866455078\n",
      "Training iteration: 796\n",
      "Improved validation loss from: 0.12018756866455078  to: 0.12014662027359009\n",
      "Training iteration: 797\n",
      "Improved validation loss from: 0.12014662027359009  to: 0.12010613679885865\n",
      "Training iteration: 798\n",
      "Improved validation loss from: 0.12010613679885865  to: 0.12006609439849854\n",
      "Training iteration: 799\n",
      "Improved validation loss from: 0.12006609439849854  to: 0.12002534866333008\n",
      "Training iteration: 800\n",
      "Improved validation loss from: 0.12002534866333008  to: 0.11998395919799805\n",
      "Training iteration: 801\n",
      "Improved validation loss from: 0.11998395919799805  to: 0.1199419617652893\n",
      "Training iteration: 802\n",
      "Improved validation loss from: 0.1199419617652893  to: 0.1199004054069519\n",
      "Training iteration: 803\n",
      "Improved validation loss from: 0.1199004054069519  to: 0.11985924243927001\n",
      "Training iteration: 804\n",
      "Improved validation loss from: 0.11985924243927001  to: 0.11981840133666992\n",
      "Training iteration: 805\n",
      "Improved validation loss from: 0.11981840133666992  to: 0.11977673768997192\n",
      "Training iteration: 806\n",
      "Improved validation loss from: 0.11977673768997192  to: 0.11973432302474976\n",
      "Training iteration: 807\n",
      "Improved validation loss from: 0.11973432302474976  to: 0.11969120502471924\n",
      "Training iteration: 808\n",
      "Improved validation loss from: 0.11969120502471924  to: 0.11964852809906006\n",
      "Training iteration: 809\n",
      "Improved validation loss from: 0.11964852809906006  to: 0.11960628032684326\n",
      "Training iteration: 810\n",
      "Improved validation loss from: 0.11960628032684326  to: 0.11956440210342408\n",
      "Training iteration: 811\n",
      "Improved validation loss from: 0.11956440210342408  to: 0.1195216417312622\n",
      "Training iteration: 812\n",
      "Improved validation loss from: 0.1195216417312622  to: 0.11947807073593139\n",
      "Training iteration: 813\n",
      "Improved validation loss from: 0.11947807073593139  to: 0.11943376064300537\n",
      "Training iteration: 814\n",
      "Improved validation loss from: 0.11943376064300537  to: 0.11938871145248413\n",
      "Training iteration: 815\n",
      "Improved validation loss from: 0.11938871145248413  to: 0.11934423446655273\n",
      "Training iteration: 816\n",
      "Improved validation loss from: 0.11934423446655273  to: 0.11930023431777954\n",
      "Training iteration: 817\n",
      "Improved validation loss from: 0.11930023431777954  to: 0.11925668716430664\n",
      "Training iteration: 818\n",
      "Improved validation loss from: 0.11925668716430664  to: 0.11921350955963135\n",
      "Training iteration: 819\n",
      "Improved validation loss from: 0.11921350955963135  to: 0.11916936635971069\n",
      "Training iteration: 820\n",
      "Improved validation loss from: 0.11916936635971069  to: 0.11912434101104737\n",
      "Training iteration: 821\n",
      "Improved validation loss from: 0.11912434101104737  to: 0.11907848119735717\n",
      "Training iteration: 822\n",
      "Improved validation loss from: 0.11907848119735717  to: 0.11903314590454102\n",
      "Training iteration: 823\n",
      "Improved validation loss from: 0.11903314590454102  to: 0.11898828744888305\n",
      "Training iteration: 824\n",
      "Improved validation loss from: 0.11898828744888305  to: 0.1189424753189087\n",
      "Training iteration: 825\n",
      "Improved validation loss from: 0.1189424753189087  to: 0.11889578104019165\n",
      "Training iteration: 826\n",
      "Improved validation loss from: 0.11889578104019165  to: 0.11884962320327759\n",
      "Training iteration: 827\n",
      "Improved validation loss from: 0.11884962320327759  to: 0.11880395412445069\n",
      "Training iteration: 828\n",
      "Improved validation loss from: 0.11880395412445069  to: 0.11875727176666259\n",
      "Training iteration: 829\n",
      "Improved validation loss from: 0.11875727176666259  to: 0.11870967149734497\n",
      "Training iteration: 830\n",
      "Improved validation loss from: 0.11870967149734497  to: 0.11866261959075927\n",
      "Training iteration: 831\n",
      "Improved validation loss from: 0.11866261959075927  to: 0.11861608028411866\n",
      "Training iteration: 832\n",
      "Improved validation loss from: 0.11861608028411866  to: 0.1185685157775879\n",
      "Training iteration: 833\n",
      "Improved validation loss from: 0.1185685157775879  to: 0.11851997375488281\n",
      "Training iteration: 834\n",
      "Improved validation loss from: 0.11851997375488281  to: 0.11847201585769654\n",
      "Training iteration: 835\n",
      "Improved validation loss from: 0.11847201585769654  to: 0.11842457056045533\n",
      "Training iteration: 836\n",
      "Improved validation loss from: 0.11842457056045533  to: 0.1183760404586792\n",
      "Training iteration: 837\n",
      "Improved validation loss from: 0.1183760404586792  to: 0.11832649707794189\n",
      "Training iteration: 838\n",
      "Improved validation loss from: 0.11832649707794189  to: 0.11827758550643921\n",
      "Training iteration: 839\n",
      "Improved validation loss from: 0.11827758550643921  to: 0.1182292103767395\n",
      "Training iteration: 840\n",
      "Improved validation loss from: 0.1182292103767395  to: 0.1181796908378601\n",
      "Training iteration: 841\n",
      "Improved validation loss from: 0.1181796908378601  to: 0.11812913417816162\n",
      "Training iteration: 842\n",
      "Improved validation loss from: 0.11812913417816162  to: 0.11807922124862671\n",
      "Training iteration: 843\n",
      "Improved validation loss from: 0.11807922124862671  to: 0.11802823543548584\n",
      "Training iteration: 844\n",
      "Improved validation loss from: 0.11802823543548584  to: 0.1179779052734375\n",
      "Training iteration: 845\n",
      "Improved validation loss from: 0.1179779052734375  to: 0.1179281234741211\n",
      "Training iteration: 846\n",
      "Improved validation loss from: 0.1179281234741211  to: 0.11787716150283814\n",
      "Training iteration: 847\n",
      "Improved validation loss from: 0.11787716150283814  to: 0.11782505512237548\n",
      "Training iteration: 848\n",
      "Improved validation loss from: 0.11782505512237548  to: 0.11777360439300537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 849\n",
      "Improved validation loss from: 0.11777360439300537  to: 0.11772100925445557\n",
      "Training iteration: 850\n",
      "Improved validation loss from: 0.11772100925445557  to: 0.11766908168792725\n",
      "Training iteration: 851\n",
      "Improved validation loss from: 0.11766908168792725  to: 0.11761776208877564\n",
      "Training iteration: 852\n",
      "Improved validation loss from: 0.11761776208877564  to: 0.11756513118743897\n",
      "Training iteration: 853\n",
      "Improved validation loss from: 0.11756513118743897  to: 0.11751127243041992\n",
      "Training iteration: 854\n",
      "Improved validation loss from: 0.11751127243041992  to: 0.11745626926422119\n",
      "Training iteration: 855\n",
      "Improved validation loss from: 0.11745626926422119  to: 0.1174020528793335\n",
      "Training iteration: 856\n",
      "Improved validation loss from: 0.1174020528793335  to: 0.11734856367111206\n",
      "Training iteration: 857\n",
      "Improved validation loss from: 0.11734856367111206  to: 0.11729590892791748\n",
      "Training iteration: 858\n",
      "Improved validation loss from: 0.11729590892791748  to: 0.11724400520324707\n",
      "Training iteration: 859\n",
      "Improved validation loss from: 0.11724400520324707  to: 0.11719073057174682\n",
      "Training iteration: 860\n",
      "Improved validation loss from: 0.11719073057174682  to: 0.11713621616363526\n",
      "Training iteration: 861\n",
      "Improved validation loss from: 0.11713621616363526  to: 0.1170804738998413\n",
      "Training iteration: 862\n",
      "Improved validation loss from: 0.1170804738998413  to: 0.11702361106872558\n",
      "Training iteration: 863\n",
      "Improved validation loss from: 0.11702361106872558  to: 0.11696600914001465\n",
      "Training iteration: 864\n",
      "Improved validation loss from: 0.11696600914001465  to: 0.11690816879272461\n",
      "Training iteration: 865\n",
      "Improved validation loss from: 0.11690816879272461  to: 0.11685030460357666\n",
      "Training iteration: 866\n",
      "Improved validation loss from: 0.11685030460357666  to: 0.11678969860076904\n",
      "Training iteration: 867\n",
      "Improved validation loss from: 0.11678969860076904  to: 0.11672673225402833\n",
      "Training iteration: 868\n",
      "Improved validation loss from: 0.11672673225402833  to: 0.11666383743286132\n",
      "Training iteration: 869\n",
      "Improved validation loss from: 0.11666383743286132  to: 0.11660120487213135\n",
      "Training iteration: 870\n",
      "Improved validation loss from: 0.11660120487213135  to: 0.11653661727905273\n",
      "Training iteration: 871\n",
      "Improved validation loss from: 0.11653661727905273  to: 0.11647000312805175\n",
      "Training iteration: 872\n",
      "Improved validation loss from: 0.11647000312805175  to: 0.11640440225601197\n",
      "Training iteration: 873\n",
      "Improved validation loss from: 0.11640440225601197  to: 0.11633973121643067\n",
      "Training iteration: 874\n",
      "Improved validation loss from: 0.11633973121643067  to: 0.11627362966537476\n",
      "Training iteration: 875\n",
      "Improved validation loss from: 0.11627362966537476  to: 0.11620628833770752\n",
      "Training iteration: 876\n",
      "Improved validation loss from: 0.11620628833770752  to: 0.11614029407501221\n",
      "Training iteration: 877\n",
      "Improved validation loss from: 0.11614029407501221  to: 0.11607562303543091\n",
      "Training iteration: 878\n",
      "Improved validation loss from: 0.11607562303543091  to: 0.11601219177246094\n",
      "Training iteration: 879\n",
      "Improved validation loss from: 0.11601219177246094  to: 0.115947425365448\n",
      "Training iteration: 880\n",
      "Improved validation loss from: 0.115947425365448  to: 0.11588121652603149\n",
      "Training iteration: 881\n",
      "Improved validation loss from: 0.11588121652603149  to: 0.11581366062164307\n",
      "Training iteration: 882\n",
      "Improved validation loss from: 0.11581366062164307  to: 0.11574728488922119\n",
      "Training iteration: 883\n",
      "Improved validation loss from: 0.11574728488922119  to: 0.11568204164505005\n",
      "Training iteration: 884\n",
      "Improved validation loss from: 0.11568204164505005  to: 0.11561784744262696\n",
      "Training iteration: 885\n",
      "Improved validation loss from: 0.11561784744262696  to: 0.1155545949935913\n",
      "Training iteration: 886\n",
      "Improved validation loss from: 0.1155545949935913  to: 0.11548967361450195\n",
      "Training iteration: 887\n",
      "Improved validation loss from: 0.11548967361450195  to: 0.11542317867279053\n",
      "Training iteration: 888\n",
      "Improved validation loss from: 0.11542317867279053  to: 0.11535521745681762\n",
      "Training iteration: 889\n",
      "Improved validation loss from: 0.11535521745681762  to: 0.11528877019882203\n",
      "Training iteration: 890\n",
      "Improved validation loss from: 0.11528877019882203  to: 0.11522371768951416\n",
      "Training iteration: 891\n",
      "Improved validation loss from: 0.11522371768951416  to: 0.11515983343124389\n",
      "Training iteration: 892\n",
      "Improved validation loss from: 0.11515983343124389  to: 0.1150970458984375\n",
      "Training iteration: 893\n",
      "Improved validation loss from: 0.1150970458984375  to: 0.1150321364402771\n",
      "Training iteration: 894\n",
      "Improved validation loss from: 0.1150321364402771  to: 0.1149652361869812\n",
      "Training iteration: 895\n",
      "Improved validation loss from: 0.1149652361869812  to: 0.11489641666412354\n",
      "Training iteration: 896\n",
      "Improved validation loss from: 0.11489641666412354  to: 0.11482870578765869\n",
      "Training iteration: 897\n",
      "Improved validation loss from: 0.11482870578765869  to: 0.11476194858551025\n",
      "Training iteration: 898\n",
      "Improved validation loss from: 0.11476194858551025  to: 0.11469603776931762\n",
      "Training iteration: 899\n",
      "Improved validation loss from: 0.11469603776931762  to: 0.11463086605072022\n",
      "Training iteration: 900\n",
      "Improved validation loss from: 0.11463086605072022  to: 0.11456329822540283\n",
      "Training iteration: 901\n",
      "Improved validation loss from: 0.11456329822540283  to: 0.11449340581893921\n",
      "Training iteration: 902\n",
      "Improved validation loss from: 0.11449340581893921  to: 0.11442480087280274\n",
      "Training iteration: 903\n",
      "Improved validation loss from: 0.11442480087280274  to: 0.11435736417770385\n",
      "Training iteration: 904\n",
      "Improved validation loss from: 0.11435736417770385  to: 0.11429097652435302\n",
      "Training iteration: 905\n",
      "Improved validation loss from: 0.11429097652435302  to: 0.1142220139503479\n",
      "Training iteration: 906\n",
      "Improved validation loss from: 0.1142220139503479  to: 0.1141506552696228\n",
      "Training iteration: 907\n",
      "Improved validation loss from: 0.1141506552696228  to: 0.11408077478408814\n",
      "Training iteration: 908\n",
      "Improved validation loss from: 0.11408077478408814  to: 0.11401231288909912\n",
      "Training iteration: 909\n",
      "Improved validation loss from: 0.11401231288909912  to: 0.11394518613815308\n",
      "Training iteration: 910\n",
      "Improved validation loss from: 0.11394518613815308  to: 0.11387512683868409\n",
      "Training iteration: 911\n",
      "Improved validation loss from: 0.11387512683868409  to: 0.11380622386932374\n",
      "Training iteration: 912\n",
      "Improved validation loss from: 0.11380622386932374  to: 0.11373443603515625\n",
      "Training iteration: 913\n",
      "Improved validation loss from: 0.11373443603515625  to: 0.11366430521011353\n",
      "Training iteration: 914\n",
      "Improved validation loss from: 0.11366430521011353  to: 0.11359562873840331\n",
      "Training iteration: 915\n",
      "Improved validation loss from: 0.11359562873840331  to: 0.11352829933166504\n",
      "Training iteration: 916\n",
      "Improved validation loss from: 0.11352829933166504  to: 0.11345791816711426\n",
      "Training iteration: 917\n",
      "Improved validation loss from: 0.11345791816711426  to: 0.11338891983032226\n",
      "Training iteration: 918\n",
      "Improved validation loss from: 0.11338891983032226  to: 0.11331678628921509\n",
      "Training iteration: 919\n",
      "Improved validation loss from: 0.11331678628921509  to: 0.11324604749679565\n",
      "Training iteration: 920\n",
      "Improved validation loss from: 0.11324604749679565  to: 0.11317651271820069\n",
      "Training iteration: 921\n",
      "Improved validation loss from: 0.11317651271820069  to: 0.11310575008392335\n",
      "Training iteration: 922\n",
      "Improved validation loss from: 0.11310575008392335  to: 0.11303352117538452\n",
      "Training iteration: 923\n",
      "Improved validation loss from: 0.11303352117538452  to: 0.11295526027679444\n",
      "Training iteration: 924\n",
      "Improved validation loss from: 0.11295526027679444  to: 0.11287181377410889\n",
      "Training iteration: 925\n",
      "Improved validation loss from: 0.11287181377410889  to: 0.11278846263885497\n",
      "Training iteration: 926\n",
      "Improved validation loss from: 0.11278846263885497  to: 0.11270571947097778\n",
      "Training iteration: 927\n",
      "Improved validation loss from: 0.11270571947097778  to: 0.11262381076812744\n",
      "Training iteration: 928\n",
      "Improved validation loss from: 0.11262381076812744  to: 0.11254236698150635\n",
      "Training iteration: 929\n",
      "Improved validation loss from: 0.11254236698150635  to: 0.11246154308319092\n",
      "Training iteration: 930\n",
      "Improved validation loss from: 0.11246154308319092  to: 0.11237581968307495\n",
      "Training iteration: 931\n",
      "Improved validation loss from: 0.11237581968307495  to: 0.1122860074043274\n",
      "Training iteration: 932\n",
      "Improved validation loss from: 0.1122860074043274  to: 0.11219780445098877\n",
      "Training iteration: 933\n",
      "Improved validation loss from: 0.11219780445098877  to: 0.1121112585067749\n",
      "Training iteration: 934\n",
      "Improved validation loss from: 0.1121112585067749  to: 0.11202630996704102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 935\n",
      "Improved validation loss from: 0.11202630996704102  to: 0.11194288730621338\n",
      "Training iteration: 936\n",
      "Improved validation loss from: 0.11194288730621338  to: 0.11186056137084961\n",
      "Training iteration: 937\n",
      "Improved validation loss from: 0.11186056137084961  to: 0.11177958250045776\n",
      "Training iteration: 938\n",
      "Improved validation loss from: 0.11177958250045776  to: 0.11169986724853516\n",
      "Training iteration: 939\n",
      "Improved validation loss from: 0.11169986724853516  to: 0.11161530017852783\n",
      "Training iteration: 940\n",
      "Improved validation loss from: 0.11161530017852783  to: 0.11152617931365967\n",
      "Training iteration: 941\n",
      "Improved validation loss from: 0.11152617931365967  to: 0.11143887042999268\n",
      "Training iteration: 942\n",
      "Improved validation loss from: 0.11143887042999268  to: 0.11135317087173462\n",
      "Training iteration: 943\n",
      "Improved validation loss from: 0.11135317087173462  to: 0.11126847267150879\n",
      "Training iteration: 944\n",
      "Improved validation loss from: 0.11126847267150879  to: 0.11118485927581787\n",
      "Training iteration: 945\n",
      "Improved validation loss from: 0.11118485927581787  to: 0.11110215187072754\n",
      "Training iteration: 946\n",
      "Improved validation loss from: 0.11110215187072754  to: 0.11102015972137451\n",
      "Training iteration: 947\n",
      "Improved validation loss from: 0.11102015972137451  to: 0.11093871593475342\n",
      "Training iteration: 948\n",
      "Improved validation loss from: 0.11093871593475342  to: 0.11085776090621949\n",
      "Training iteration: 949\n",
      "Improved validation loss from: 0.11085776090621949  to: 0.11077767610549927\n",
      "Training iteration: 950\n",
      "Improved validation loss from: 0.11077767610549927  to: 0.11069763898849487\n",
      "Training iteration: 951\n",
      "Improved validation loss from: 0.11069763898849487  to: 0.11061753034591675\n",
      "Training iteration: 952\n",
      "Improved validation loss from: 0.11061753034591675  to: 0.11052991151809692\n",
      "Training iteration: 953\n",
      "Improved validation loss from: 0.11052991151809692  to: 0.11044265031814575\n",
      "Training iteration: 954\n",
      "Improved validation loss from: 0.11044265031814575  to: 0.11035566329956055\n",
      "Training iteration: 955\n",
      "Improved validation loss from: 0.11035566329956055  to: 0.11026890277862549\n",
      "Training iteration: 956\n",
      "Improved validation loss from: 0.11026890277862549  to: 0.11018232107162476\n",
      "Training iteration: 957\n",
      "Improved validation loss from: 0.11018232107162476  to: 0.11009583473205567\n",
      "Training iteration: 958\n",
      "Improved validation loss from: 0.11009583473205567  to: 0.11000940799713135\n",
      "Training iteration: 959\n",
      "Improved validation loss from: 0.11000940799713135  to: 0.10992295742034912\n",
      "Training iteration: 960\n",
      "Improved validation loss from: 0.10992295742034912  to: 0.10983648300170898\n",
      "Training iteration: 961\n",
      "Improved validation loss from: 0.10983648300170898  to: 0.1097498893737793\n",
      "Training iteration: 962\n",
      "Improved validation loss from: 0.1097498893737793  to: 0.10966309309005737\n",
      "Training iteration: 963\n",
      "Improved validation loss from: 0.10966309309005737  to: 0.10957615375518799\n",
      "Training iteration: 964\n",
      "Improved validation loss from: 0.10957615375518799  to: 0.10948903560638427\n",
      "Training iteration: 965\n",
      "Improved validation loss from: 0.10948903560638427  to: 0.10940178632736205\n",
      "Training iteration: 966\n",
      "Improved validation loss from: 0.10940178632736205  to: 0.10931493043899536\n",
      "Training iteration: 967\n",
      "Improved validation loss from: 0.10931493043899536  to: 0.10922809839248657\n",
      "Training iteration: 968\n",
      "Improved validation loss from: 0.10922809839248657  to: 0.10914958715438842\n",
      "Training iteration: 969\n",
      "Improved validation loss from: 0.10914958715438842  to: 0.1090781569480896\n",
      "Training iteration: 970\n",
      "Improved validation loss from: 0.1090781569480896  to: 0.1090092658996582\n",
      "Training iteration: 971\n",
      "Improved validation loss from: 0.1090092658996582  to: 0.1089414119720459\n",
      "Training iteration: 972\n",
      "Improved validation loss from: 0.1089414119720459  to: 0.10887296199798584\n",
      "Training iteration: 973\n",
      "Improved validation loss from: 0.10887296199798584  to: 0.10879315137863159\n",
      "Training iteration: 974\n",
      "Improved validation loss from: 0.10879315137863159  to: 0.10870172977447509\n",
      "Training iteration: 975\n",
      "Improved validation loss from: 0.10870172977447509  to: 0.1085990309715271\n",
      "Training iteration: 976\n",
      "Improved validation loss from: 0.1085990309715271  to: 0.10848612785339355\n",
      "Training iteration: 977\n",
      "Improved validation loss from: 0.10848612785339355  to: 0.10837379693984986\n",
      "Training iteration: 978\n",
      "Improved validation loss from: 0.10837379693984986  to: 0.10826246738433838\n",
      "Training iteration: 979\n",
      "Improved validation loss from: 0.10826246738433838  to: 0.10815241336822509\n",
      "Training iteration: 980\n",
      "Improved validation loss from: 0.10815241336822509  to: 0.10804402828216553\n",
      "Training iteration: 981\n",
      "Improved validation loss from: 0.10804402828216553  to: 0.10793713331222535\n",
      "Training iteration: 982\n",
      "Improved validation loss from: 0.10793713331222535  to: 0.10783103704452515\n",
      "Training iteration: 983\n",
      "Improved validation loss from: 0.10783103704452515  to: 0.10772526264190674\n",
      "Training iteration: 984\n",
      "Improved validation loss from: 0.10772526264190674  to: 0.10761922597885132\n",
      "Training iteration: 985\n",
      "Improved validation loss from: 0.10761922597885132  to: 0.10751230716705322\n",
      "Training iteration: 986\n",
      "Improved validation loss from: 0.10751230716705322  to: 0.10740406513214111\n",
      "Training iteration: 987\n",
      "Improved validation loss from: 0.10740406513214111  to: 0.10728404521942139\n",
      "Training iteration: 988\n",
      "Improved validation loss from: 0.10728404521942139  to: 0.10715328454971314\n",
      "Training iteration: 989\n",
      "Improved validation loss from: 0.10715328454971314  to: 0.10701347589492798\n",
      "Training iteration: 990\n",
      "Improved validation loss from: 0.10701347589492798  to: 0.10686674118041992\n",
      "Training iteration: 991\n",
      "Improved validation loss from: 0.10686674118041992  to: 0.10671552419662475\n",
      "Training iteration: 992\n",
      "Improved validation loss from: 0.10671552419662475  to: 0.10656797885894775\n",
      "Training iteration: 993\n",
      "Improved validation loss from: 0.10656797885894775  to: 0.10642573833465577\n",
      "Training iteration: 994\n",
      "Improved validation loss from: 0.10642573833465577  to: 0.1062900185585022\n",
      "Training iteration: 995\n",
      "Improved validation loss from: 0.1062900185585022  to: 0.10616147518157959\n",
      "Training iteration: 996\n",
      "Improved validation loss from: 0.10616147518157959  to: 0.10604028701782227\n",
      "Training iteration: 997\n",
      "Improved validation loss from: 0.10604028701782227  to: 0.10592597723007202\n",
      "Training iteration: 998\n",
      "Improved validation loss from: 0.10592597723007202  to: 0.10581758022308349\n",
      "Training iteration: 999\n",
      "Improved validation loss from: 0.10581758022308349  to: 0.10571439266204834\n",
      "Training iteration: 1000\n",
      "Improved validation loss from: 0.10571439266204834  to: 0.10561521053314209\n",
      "Training iteration: 1001\n",
      "Improved validation loss from: 0.10561521053314209  to: 0.10551677942276001\n",
      "Training iteration: 1002\n",
      "Improved validation loss from: 0.10551677942276001  to: 0.10541695356369019\n",
      "Training iteration: 1003\n",
      "Improved validation loss from: 0.10541695356369019  to: 0.10531364679336548\n",
      "Training iteration: 1004\n",
      "Improved validation loss from: 0.10531364679336548  to: 0.1052049994468689\n",
      "Training iteration: 1005\n",
      "Improved validation loss from: 0.1052049994468689  to: 0.10508955717086792\n",
      "Training iteration: 1006\n",
      "Improved validation loss from: 0.10508955717086792  to: 0.10496623516082763\n",
      "Training iteration: 1007\n",
      "Improved validation loss from: 0.10496623516082763  to: 0.10483448505401612\n",
      "Training iteration: 1008\n",
      "Improved validation loss from: 0.10483448505401612  to: 0.10469433069229125\n",
      "Training iteration: 1009\n",
      "Improved validation loss from: 0.10469433069229125  to: 0.10454627275466918\n",
      "Training iteration: 1010\n",
      "Improved validation loss from: 0.10454627275466918  to: 0.10439121723175049\n",
      "Training iteration: 1011\n",
      "Improved validation loss from: 0.10439121723175049  to: 0.10423026084899903\n",
      "Training iteration: 1012\n",
      "Improved validation loss from: 0.10423026084899903  to: 0.10406489372253418\n",
      "Training iteration: 1013\n",
      "Improved validation loss from: 0.10406489372253418  to: 0.1038966178894043\n",
      "Training iteration: 1014\n",
      "Improved validation loss from: 0.1038966178894043  to: 0.1037266731262207\n",
      "Training iteration: 1015\n",
      "Improved validation loss from: 0.1037266731262207  to: 0.10355618000030517\n",
      "Training iteration: 1016\n",
      "Improved validation loss from: 0.10355618000030517  to: 0.10338577032089233\n",
      "Training iteration: 1017\n",
      "Improved validation loss from: 0.10338577032089233  to: 0.10321581363677979\n",
      "Training iteration: 1018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.10321581363677979  to: 0.10304625034332275\n",
      "Training iteration: 1019\n",
      "Improved validation loss from: 0.10304625034332275  to: 0.10287673473358154\n",
      "Training iteration: 1020\n",
      "Improved validation loss from: 0.10287673473358154  to: 0.1027066469192505\n",
      "Training iteration: 1021\n",
      "Improved validation loss from: 0.1027066469192505  to: 0.10253517627716065\n",
      "Training iteration: 1022\n",
      "Improved validation loss from: 0.10253517627716065  to: 0.10236146450042724\n",
      "Training iteration: 1023\n",
      "Improved validation loss from: 0.10236146450042724  to: 0.10218474864959717\n",
      "Training iteration: 1024\n",
      "Improved validation loss from: 0.10218474864959717  to: 0.10200430154800415\n",
      "Training iteration: 1025\n",
      "Improved validation loss from: 0.10200430154800415  to: 0.10181977748870849\n",
      "Training iteration: 1026\n",
      "Improved validation loss from: 0.10181977748870849  to: 0.10163099765777588\n",
      "Training iteration: 1027\n",
      "Improved validation loss from: 0.10163099765777588  to: 0.10143804550170898\n",
      "Training iteration: 1028\n",
      "Improved validation loss from: 0.10143804550170898  to: 0.10124132633209229\n",
      "Training iteration: 1029\n",
      "Improved validation loss from: 0.10124132633209229  to: 0.10104141235351563\n",
      "Training iteration: 1030\n",
      "Improved validation loss from: 0.10104141235351563  to: 0.10083904266357421\n",
      "Training iteration: 1031\n",
      "Improved validation loss from: 0.10083904266357421  to: 0.10063494443893432\n",
      "Training iteration: 1032\n",
      "Improved validation loss from: 0.10063494443893432  to: 0.10042980909347535\n",
      "Training iteration: 1033\n",
      "Improved validation loss from: 0.10042980909347535  to: 0.10022413730621338\n",
      "Training iteration: 1034\n",
      "Improved validation loss from: 0.10022413730621338  to: 0.10001829862594605\n",
      "Training iteration: 1035\n",
      "Improved validation loss from: 0.10001829862594605  to: 0.09981236457824708\n",
      "Training iteration: 1036\n",
      "Improved validation loss from: 0.09981236457824708  to: 0.09960620999336242\n",
      "Training iteration: 1037\n",
      "Improved validation loss from: 0.09960620999336242  to: 0.09939950704574585\n",
      "Training iteration: 1038\n",
      "Improved validation loss from: 0.09939950704574585  to: 0.09919187426567078\n",
      "Training iteration: 1039\n",
      "Improved validation loss from: 0.09919187426567078  to: 0.09898274540901184\n",
      "Training iteration: 1040\n",
      "Improved validation loss from: 0.09898274540901184  to: 0.0987716019153595\n",
      "Training iteration: 1041\n",
      "Improved validation loss from: 0.0987716019153595  to: 0.09855798482894898\n",
      "Training iteration: 1042\n",
      "Improved validation loss from: 0.09855798482894898  to: 0.09834165573120117\n",
      "Training iteration: 1043\n",
      "Improved validation loss from: 0.09834165573120117  to: 0.09812250137329101\n",
      "Training iteration: 1044\n",
      "Improved validation loss from: 0.09812250137329101  to: 0.09790068864822388\n",
      "Training iteration: 1045\n",
      "Improved validation loss from: 0.09790068864822388  to: 0.09767656326293946\n",
      "Training iteration: 1046\n",
      "Improved validation loss from: 0.09767656326293946  to: 0.09745063781738281\n",
      "Training iteration: 1047\n",
      "Improved validation loss from: 0.09745063781738281  to: 0.09722357988357544\n",
      "Training iteration: 1048\n",
      "Improved validation loss from: 0.09722357988357544  to: 0.09699599146842956\n",
      "Training iteration: 1049\n",
      "Improved validation loss from: 0.09699599146842956  to: 0.09676839709281922\n",
      "Training iteration: 1050\n",
      "Improved validation loss from: 0.09676839709281922  to: 0.09654129147529603\n",
      "Training iteration: 1051\n",
      "Improved validation loss from: 0.09654129147529603  to: 0.09631422758102418\n",
      "Training iteration: 1052\n",
      "Improved validation loss from: 0.09631422758102418  to: 0.0960874855518341\n",
      "Training iteration: 1053\n",
      "Improved validation loss from: 0.0960874855518341  to: 0.0958613097667694\n",
      "Training iteration: 1054\n",
      "Improved validation loss from: 0.0958613097667694  to: 0.09563608169555664\n",
      "Training iteration: 1055\n",
      "Improved validation loss from: 0.09563608169555664  to: 0.0954122245311737\n",
      "Training iteration: 1056\n",
      "Improved validation loss from: 0.0954122245311737  to: 0.0951901614665985\n",
      "Training iteration: 1057\n",
      "Improved validation loss from: 0.0951901614665985  to: 0.09496779441833496\n",
      "Training iteration: 1058\n",
      "Improved validation loss from: 0.09496779441833496  to: 0.09474624395370483\n",
      "Training iteration: 1059\n",
      "Improved validation loss from: 0.09474624395370483  to: 0.09452700614929199\n",
      "Training iteration: 1060\n",
      "Improved validation loss from: 0.09452700614929199  to: 0.09431158304214478\n",
      "Training iteration: 1061\n",
      "Improved validation loss from: 0.09431158304214478  to: 0.09409704208374023\n",
      "Training iteration: 1062\n",
      "Improved validation loss from: 0.09409704208374023  to: 0.09388052821159362\n",
      "Training iteration: 1063\n",
      "Improved validation loss from: 0.09388052821159362  to: 0.09366528391838073\n",
      "Training iteration: 1064\n",
      "Improved validation loss from: 0.09366528391838073  to: 0.09344615936279296\n",
      "Training iteration: 1065\n",
      "Improved validation loss from: 0.09344615936279296  to: 0.09322406649589539\n",
      "Training iteration: 1066\n",
      "Improved validation loss from: 0.09322406649589539  to: 0.09300465583801269\n",
      "Training iteration: 1067\n",
      "Improved validation loss from: 0.09300465583801269  to: 0.09278852343559266\n",
      "Training iteration: 1068\n",
      "Improved validation loss from: 0.09278852343559266  to: 0.0925756573677063\n",
      "Training iteration: 1069\n",
      "Improved validation loss from: 0.0925756573677063  to: 0.09236567616462707\n",
      "Training iteration: 1070\n",
      "Improved validation loss from: 0.09236567616462707  to: 0.09215806722640991\n",
      "Training iteration: 1071\n",
      "Improved validation loss from: 0.09215806722640991  to: 0.09195241928100586\n",
      "Training iteration: 1072\n",
      "Improved validation loss from: 0.09195241928100586  to: 0.09175332188606262\n",
      "Training iteration: 1073\n",
      "Improved validation loss from: 0.09175332188606262  to: 0.09155899286270142\n",
      "Training iteration: 1074\n",
      "Improved validation loss from: 0.09155899286270142  to: 0.09136797785758972\n",
      "Training iteration: 1075\n",
      "Improved validation loss from: 0.09136797785758972  to: 0.09117921590805053\n",
      "Training iteration: 1076\n",
      "Improved validation loss from: 0.09117921590805053  to: 0.09099220037460327\n",
      "Training iteration: 1077\n",
      "Improved validation loss from: 0.09099220037460327  to: 0.09080295562744141\n",
      "Training iteration: 1078\n",
      "Improved validation loss from: 0.09080295562744141  to: 0.09061360359191895\n",
      "Training iteration: 1079\n",
      "Improved validation loss from: 0.09061360359191895  to: 0.090426766872406\n",
      "Training iteration: 1080\n",
      "Improved validation loss from: 0.090426766872406  to: 0.09024616479873657\n",
      "Training iteration: 1081\n",
      "Improved validation loss from: 0.09024616479873657  to: 0.09005716443061829\n",
      "Training iteration: 1082\n",
      "Improved validation loss from: 0.09005716443061829  to: 0.08985153436660767\n",
      "Training iteration: 1083\n",
      "Improved validation loss from: 0.08985153436660767  to: 0.08964659571647644\n",
      "Training iteration: 1084\n",
      "Improved validation loss from: 0.08964659571647644  to: 0.08944635391235352\n",
      "Training iteration: 1085\n",
      "Improved validation loss from: 0.08944635391235352  to: 0.08924930691719055\n",
      "Training iteration: 1086\n",
      "Improved validation loss from: 0.08924930691719055  to: 0.08904882669448852\n",
      "Training iteration: 1087\n",
      "Improved validation loss from: 0.08904882669448852  to: 0.08886715769767761\n",
      "Training iteration: 1088\n",
      "Improved validation loss from: 0.08886715769767761  to: 0.08870784044265748\n",
      "Training iteration: 1089\n",
      "Improved validation loss from: 0.08870784044265748  to: 0.08855948448181153\n",
      "Training iteration: 1090\n",
      "Improved validation loss from: 0.08855948448181153  to: 0.08840158581733704\n",
      "Training iteration: 1091\n",
      "Improved validation loss from: 0.08840158581733704  to: 0.08821367025375366\n",
      "Training iteration: 1092\n",
      "Improved validation loss from: 0.08821367025375366  to: 0.08798584938049317\n",
      "Training iteration: 1093\n",
      "Improved validation loss from: 0.08798584938049317  to: 0.08773213624954224\n",
      "Training iteration: 1094\n",
      "Improved validation loss from: 0.08773213624954224  to: 0.0874725639820099\n",
      "Training iteration: 1095\n",
      "Improved validation loss from: 0.0874725639820099  to: 0.08722032308578491\n",
      "Training iteration: 1096\n",
      "Improved validation loss from: 0.08722032308578491  to: 0.08697986602783203\n",
      "Training iteration: 1097\n",
      "Improved validation loss from: 0.08697986602783203  to: 0.08675209283828736\n",
      "Training iteration: 1098\n",
      "Improved validation loss from: 0.08675209283828736  to: 0.08653554916381836\n",
      "Training iteration: 1099\n",
      "Improved validation loss from: 0.08653554916381836  to: 0.08632131814956664\n",
      "Training iteration: 1100\n",
      "Improved validation loss from: 0.08632131814956664  to: 0.08609172105789184\n",
      "Training iteration: 1101\n",
      "Improved validation loss from: 0.08609172105789184  to: 0.08582983016967774\n",
      "Training iteration: 1102\n",
      "Improved validation loss from: 0.08582983016967774  to: 0.08553956747055054\n",
      "Training iteration: 1103\n",
      "Improved validation loss from: 0.08553956747055054  to: 0.08523327708244324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 1104\n",
      "Improved validation loss from: 0.08523327708244324  to: 0.08493011593818664\n",
      "Training iteration: 1105\n",
      "Improved validation loss from: 0.08493011593818664  to: 0.08464458584785461\n",
      "Training iteration: 1106\n",
      "Improved validation loss from: 0.08464458584785461  to: 0.08438237905502319\n",
      "Training iteration: 1107\n",
      "Improved validation loss from: 0.08438237905502319  to: 0.08414276242256165\n",
      "Training iteration: 1108\n",
      "Improved validation loss from: 0.08414276242256165  to: 0.08392011523246765\n",
      "Training iteration: 1109\n",
      "Improved validation loss from: 0.08392011523246765  to: 0.08370348215103149\n",
      "Training iteration: 1110\n",
      "Improved validation loss from: 0.08370348215103149  to: 0.08347975015640259\n",
      "Training iteration: 1111\n",
      "Improved validation loss from: 0.08347975015640259  to: 0.0832415223121643\n",
      "Training iteration: 1112\n",
      "Improved validation loss from: 0.0832415223121643  to: 0.082992422580719\n",
      "Training iteration: 1113\n",
      "Improved validation loss from: 0.082992422580719  to: 0.0827436625957489\n",
      "Training iteration: 1114\n",
      "Improved validation loss from: 0.0827436625957489  to: 0.08250554800033569\n",
      "Training iteration: 1115\n",
      "Improved validation loss from: 0.08250554800033569  to: 0.08228257298469543\n",
      "Training iteration: 1116\n",
      "Improved validation loss from: 0.08228257298469543  to: 0.08207383155822753\n",
      "Training iteration: 1117\n",
      "Improved validation loss from: 0.08207383155822753  to: 0.08187462687492371\n",
      "Training iteration: 1118\n",
      "Improved validation loss from: 0.08187462687492371  to: 0.08167765736579895\n",
      "Training iteration: 1119\n",
      "Improved validation loss from: 0.08167765736579895  to: 0.08147560358047486\n",
      "Training iteration: 1120\n",
      "Improved validation loss from: 0.08147560358047486  to: 0.08126615285873413\n",
      "Training iteration: 1121\n",
      "Improved validation loss from: 0.08126615285873413  to: 0.08105384707450866\n",
      "Training iteration: 1122\n",
      "Improved validation loss from: 0.08105384707450866  to: 0.0808460533618927\n",
      "Training iteration: 1123\n",
      "Improved validation loss from: 0.0808460533618927  to: 0.08064780235290528\n",
      "Training iteration: 1124\n",
      "Improved validation loss from: 0.08064780235290528  to: 0.08045610189437866\n",
      "Training iteration: 1125\n",
      "Improved validation loss from: 0.08045610189437866  to: 0.08027125597000122\n",
      "Training iteration: 1126\n",
      "Improved validation loss from: 0.08027125597000122  to: 0.08009220957756043\n",
      "Training iteration: 1127\n",
      "Improved validation loss from: 0.08009220957756043  to: 0.07991777658462525\n",
      "Training iteration: 1128\n",
      "Improved validation loss from: 0.07991777658462525  to: 0.07974526286125183\n",
      "Training iteration: 1129\n",
      "Improved validation loss from: 0.07974526286125183  to: 0.07957078814506531\n",
      "Training iteration: 1130\n",
      "Improved validation loss from: 0.07957078814506531  to: 0.07939253449440002\n",
      "Training iteration: 1131\n",
      "Improved validation loss from: 0.07939253449440002  to: 0.07921084761619568\n",
      "Training iteration: 1132\n",
      "Improved validation loss from: 0.07921084761619568  to: 0.07902681827545166\n",
      "Training iteration: 1133\n",
      "Improved validation loss from: 0.07902681827545166  to: 0.07884141206741332\n",
      "Training iteration: 1134\n",
      "Improved validation loss from: 0.07884141206741332  to: 0.0786558747291565\n",
      "Training iteration: 1135\n",
      "Improved validation loss from: 0.0786558747291565  to: 0.07847203016281128\n",
      "Training iteration: 1136\n",
      "Improved validation loss from: 0.07847203016281128  to: 0.07829208374023437\n",
      "Training iteration: 1137\n",
      "Improved validation loss from: 0.07829208374023437  to: 0.0781328022480011\n",
      "Training iteration: 1138\n",
      "Improved validation loss from: 0.0781328022480011  to: 0.07800215482711792\n",
      "Training iteration: 1139\n",
      "Improved validation loss from: 0.07800215482711792  to: 0.07788456082344056\n",
      "Training iteration: 1140\n",
      "Improved validation loss from: 0.07788456082344056  to: 0.07777732014656066\n",
      "Training iteration: 1141\n",
      "Improved validation loss from: 0.07777732014656066  to: 0.07763458490371704\n",
      "Training iteration: 1142\n",
      "Improved validation loss from: 0.07763458490371704  to: 0.07737587690353394\n",
      "Training iteration: 1143\n",
      "Improved validation loss from: 0.07737587690353394  to: 0.07710694074630738\n",
      "Training iteration: 1144\n",
      "Improved validation loss from: 0.07710694074630738  to: 0.07681593894958497\n",
      "Training iteration: 1145\n",
      "Improved validation loss from: 0.07681593894958497  to: 0.07652497291564941\n",
      "Training iteration: 1146\n",
      "Improved validation loss from: 0.07652497291564941  to: 0.07624604105949402\n",
      "Training iteration: 1147\n",
      "Improved validation loss from: 0.07624604105949402  to: 0.07597046494483947\n",
      "Training iteration: 1148\n",
      "Improved validation loss from: 0.07597046494483947  to: 0.07568947672843933\n",
      "Training iteration: 1149\n",
      "Improved validation loss from: 0.07568947672843933  to: 0.0754071056842804\n",
      "Training iteration: 1150\n",
      "Improved validation loss from: 0.0754071056842804  to: 0.0751417875289917\n",
      "Training iteration: 1151\n",
      "Improved validation loss from: 0.0751417875289917  to: 0.07486879229545593\n",
      "Training iteration: 1152\n",
      "Improved validation loss from: 0.07486879229545593  to: 0.07458422780036926\n",
      "Training iteration: 1153\n",
      "Improved validation loss from: 0.07458422780036926  to: 0.0743195354938507\n",
      "Training iteration: 1154\n",
      "Improved validation loss from: 0.0743195354938507  to: 0.07407525777816773\n",
      "Training iteration: 1155\n",
      "Improved validation loss from: 0.07407525777816773  to: 0.07375034689903259\n",
      "Training iteration: 1156\n",
      "Improved validation loss from: 0.07375034689903259  to: 0.0734656274318695\n",
      "Training iteration: 1157\n",
      "Improved validation loss from: 0.0734656274318695  to: 0.07322045564651489\n",
      "Training iteration: 1158\n",
      "Improved validation loss from: 0.07322045564651489  to: 0.07279819250106812\n",
      "Training iteration: 1159\n",
      "Improved validation loss from: 0.07279819250106812  to: 0.0724459171295166\n",
      "Training iteration: 1160\n",
      "Improved validation loss from: 0.0724459171295166  to: 0.07215677499771118\n",
      "Training iteration: 1161\n",
      "Improved validation loss from: 0.07215677499771118  to: 0.0719287097454071\n",
      "Training iteration: 1162\n",
      "Improved validation loss from: 0.0719287097454071  to: 0.07160826921463012\n",
      "Training iteration: 1163\n",
      "Improved validation loss from: 0.07160826921463012  to: 0.07137815356254577\n",
      "Training iteration: 1164\n",
      "Improved validation loss from: 0.07137815356254577  to: 0.07096664309501648\n",
      "Training iteration: 1165\n",
      "Improved validation loss from: 0.07096664309501648  to: 0.07064296007156372\n",
      "Training iteration: 1166\n",
      "Improved validation loss from: 0.07064296007156372  to: 0.07021421194076538\n",
      "Training iteration: 1167\n",
      "Improved validation loss from: 0.07021421194076538  to: 0.0699781060218811\n",
      "Training iteration: 1168\n",
      "Improved validation loss from: 0.0699781060218811  to: 0.06978237032890319\n",
      "Training iteration: 1169\n",
      "Improved validation loss from: 0.06978237032890319  to: 0.06945697665214538\n",
      "Training iteration: 1170\n",
      "Improved validation loss from: 0.06945697665214538  to: 0.06928271651268006\n",
      "Training iteration: 1171\n",
      "Improved validation loss from: 0.06928271651268006  to: 0.06895748972892761\n",
      "Training iteration: 1172\n",
      "Improved validation loss from: 0.06895748972892761  to: 0.06859510540962219\n",
      "Training iteration: 1173\n",
      "Improved validation loss from: 0.06859510540962219  to: 0.0683094322681427\n",
      "Training iteration: 1174\n",
      "Improved validation loss from: 0.0683094322681427  to: 0.06808899641036988\n",
      "Training iteration: 1175\n",
      "Improved validation loss from: 0.06808899641036988  to: 0.06783596873283386\n",
      "Training iteration: 1176\n",
      "Validation loss (no improvement): 0.06784652471542359\n",
      "Training iteration: 1177\n",
      "Improved validation loss from: 0.06783596873283386  to: 0.0676562488079071\n",
      "Training iteration: 1178\n",
      "Improved validation loss from: 0.0676562488079071  to: 0.06736271977424621\n",
      "Training iteration: 1179\n",
      "Improved validation loss from: 0.06736271977424621  to: 0.06697429418563842\n",
      "Training iteration: 1180\n",
      "Improved validation loss from: 0.06697429418563842  to: 0.06667877435684204\n",
      "Training iteration: 1181\n",
      "Improved validation loss from: 0.06667877435684204  to: 0.0665138840675354\n",
      "Training iteration: 1182\n",
      "Improved validation loss from: 0.0665138840675354  to: 0.06632461547851562\n",
      "Training iteration: 1183\n",
      "Validation loss (no improvement): 0.06641589999198913\n",
      "Training iteration: 1184\n",
      "Validation loss (no improvement): 0.06650131940841675\n",
      "Training iteration: 1185\n",
      "Validation loss (no improvement): 0.06634766459465027\n",
      "Training iteration: 1186\n",
      "Improved validation loss from: 0.06632461547851562  to: 0.0659752607345581\n",
      "Training iteration: 1187\n",
      "Improved validation loss from: 0.0659752607345581  to: 0.0655339539051056\n",
      "Training iteration: 1188\n",
      "Improved validation loss from: 0.0655339539051056  to: 0.06544416546821594\n",
      "Training iteration: 1189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.06544416546821594  to: 0.06519351601600647\n",
      "Training iteration: 1190\n",
      "Improved validation loss from: 0.06519351601600647  to: 0.06496434211730957\n",
      "Training iteration: 1191\n",
      "Validation loss (no improvement): 0.06508108377456664\n",
      "Training iteration: 1192\n",
      "Validation loss (no improvement): 0.06522647142410279\n",
      "Training iteration: 1193\n",
      "Validation loss (no improvement): 0.06514996886253357\n",
      "Training iteration: 1194\n",
      "Improved validation loss from: 0.06496434211730957  to: 0.06484352350234986\n",
      "Training iteration: 1195\n",
      "Improved validation loss from: 0.06484352350234986  to: 0.06448808908462525\n",
      "Training iteration: 1196\n",
      "Improved validation loss from: 0.06448808908462525  to: 0.06435590982437134\n",
      "Training iteration: 1197\n",
      "Improved validation loss from: 0.06435590982437134  to: 0.0641009271144867\n",
      "Training iteration: 1198\n",
      "Improved validation loss from: 0.0641009271144867  to: 0.06406612396240234\n",
      "Training iteration: 1199\n",
      "Validation loss (no improvement): 0.0640916645526886\n",
      "Training iteration: 1200\n",
      "Improved validation loss from: 0.06406612396240234  to: 0.0640247106552124\n",
      "Training iteration: 1201\n",
      "Improved validation loss from: 0.0640247106552124  to: 0.0637769103050232\n",
      "Training iteration: 1202\n",
      "Improved validation loss from: 0.0637769103050232  to: 0.06352245211601257\n",
      "Training iteration: 1203\n",
      "Improved validation loss from: 0.06352245211601257  to: 0.06343790292739868\n",
      "Training iteration: 1204\n",
      "Improved validation loss from: 0.06343790292739868  to: 0.06326455473899842\n",
      "Training iteration: 1205\n",
      "Validation loss (no improvement): 0.06331214904785157\n",
      "Training iteration: 1206\n",
      "Validation loss (no improvement): 0.06341487765312195\n",
      "Training iteration: 1207\n",
      "Validation loss (no improvement): 0.06333956718444825\n",
      "Training iteration: 1208\n",
      "Improved validation loss from: 0.06326455473899842  to: 0.06307772397994996\n",
      "Training iteration: 1209\n",
      "Improved validation loss from: 0.06307772397994996  to: 0.06275922060012817\n",
      "Training iteration: 1210\n",
      "Improved validation loss from: 0.06275922060012817  to: 0.06263603568077088\n",
      "Training iteration: 1211\n",
      "Improved validation loss from: 0.06263603568077088  to: 0.06257055997848511\n",
      "Training iteration: 1212\n",
      "Validation loss (no improvement): 0.06258673667907715\n",
      "Training iteration: 1213\n",
      "Validation loss (no improvement): 0.0626975417137146\n",
      "Training iteration: 1214\n",
      "Validation loss (no improvement): 0.06264111995697022\n",
      "Training iteration: 1215\n",
      "Improved validation loss from: 0.06257055997848511  to: 0.062461328506469724\n",
      "Training iteration: 1216\n",
      "Improved validation loss from: 0.062461328506469724  to: 0.06225181221961975\n",
      "Training iteration: 1217\n",
      "Improved validation loss from: 0.06225181221961975  to: 0.06210607290267944\n",
      "Training iteration: 1218\n",
      "Improved validation loss from: 0.06210607290267944  to: 0.061895054578781125\n",
      "Training iteration: 1219\n",
      "Improved validation loss from: 0.061895054578781125  to: 0.061878430843353274\n",
      "Training iteration: 1220\n",
      "Validation loss (no improvement): 0.06205791234970093\n",
      "Training iteration: 1221\n",
      "Improved validation loss from: 0.061878430843353274  to: 0.06186195015907288\n",
      "Training iteration: 1222\n",
      "Validation loss (no improvement): 0.061935949325561526\n",
      "Training iteration: 1223\n",
      "Improved validation loss from: 0.06186195015907288  to: 0.061818373203277585\n",
      "Training iteration: 1224\n",
      "Improved validation loss from: 0.061818373203277585  to: 0.061809766292572024\n",
      "Training iteration: 1225\n",
      "Improved validation loss from: 0.061809766292572024  to: 0.06142634153366089\n",
      "Training iteration: 1226\n",
      "Improved validation loss from: 0.06142634153366089  to: 0.0614257276058197\n",
      "Training iteration: 1227\n",
      "Validation loss (no improvement): 0.0614759087562561\n",
      "Training iteration: 1228\n",
      "Validation loss (no improvement): 0.06170248985290527\n",
      "Training iteration: 1229\n",
      "Validation loss (no improvement): 0.06150793433189392\n",
      "Training iteration: 1230\n",
      "Validation loss (no improvement): 0.06156318783760071\n",
      "Training iteration: 1231\n",
      "Validation loss (no improvement): 0.06145155429840088\n",
      "Training iteration: 1232\n",
      "Validation loss (no improvement): 0.06160615682601929\n",
      "Training iteration: 1233\n",
      "Improved validation loss from: 0.0614257276058197  to: 0.061265099048614505\n",
      "Training iteration: 1234\n",
      "Improved validation loss from: 0.061265099048614505  to: 0.06116037964820862\n",
      "Training iteration: 1235\n",
      "Improved validation loss from: 0.06116037964820862  to: 0.06105336546897888\n",
      "Training iteration: 1236\n",
      "Validation loss (no improvement): 0.06121073961257935\n",
      "Training iteration: 1237\n",
      "Improved validation loss from: 0.06105336546897888  to: 0.06101018190383911\n",
      "Training iteration: 1238\n",
      "Improved validation loss from: 0.06101018190383911  to: 0.0609434723854065\n",
      "Training iteration: 1239\n",
      "Validation loss (no improvement): 0.060951721668243405\n",
      "Training iteration: 1240\n",
      "Validation loss (no improvement): 0.06099313497543335\n",
      "Training iteration: 1241\n",
      "Improved validation loss from: 0.0609434723854065  to: 0.06085959672927856\n",
      "Training iteration: 1242\n",
      "Improved validation loss from: 0.06085959672927856  to: 0.06085242629051209\n",
      "Training iteration: 1243\n",
      "Improved validation loss from: 0.06085242629051209  to: 0.060804557800292966\n",
      "Training iteration: 1244\n",
      "Improved validation loss from: 0.060804557800292966  to: 0.060658156871795654\n",
      "Training iteration: 1245\n",
      "Improved validation loss from: 0.060658156871795654  to: 0.060443985462188723\n",
      "Training iteration: 1246\n",
      "Improved validation loss from: 0.060443985462188723  to: 0.060262376070022584\n",
      "Training iteration: 1247\n",
      "Validation loss (no improvement): 0.060313951969146726\n",
      "Training iteration: 1248\n",
      "Improved validation loss from: 0.060262376070022584  to: 0.06000357270240784\n",
      "Training iteration: 1249\n",
      "Improved validation loss from: 0.06000357270240784  to: 0.059858697652816775\n",
      "Training iteration: 1250\n",
      "Improved validation loss from: 0.059858697652816775  to: 0.05982393622398376\n",
      "Training iteration: 1251\n",
      "Validation loss (no improvement): 0.05994423627853394\n",
      "Training iteration: 1252\n",
      "Improved validation loss from: 0.05982393622398376  to: 0.05962299108505249\n",
      "Training iteration: 1253\n",
      "Improved validation loss from: 0.05962299108505249  to: 0.05957466959953308\n",
      "Training iteration: 1254\n",
      "Validation loss (no improvement): 0.05960444211959839\n",
      "Training iteration: 1255\n",
      "Validation loss (no improvement): 0.059715795516967776\n",
      "Training iteration: 1256\n",
      "Improved validation loss from: 0.05957466959953308  to: 0.05935840010643005\n",
      "Training iteration: 1257\n",
      "Improved validation loss from: 0.05935840010643005  to: 0.05930498242378235\n",
      "Training iteration: 1258\n",
      "Validation loss (no improvement): 0.05939285159111023\n",
      "Training iteration: 1259\n",
      "Validation loss (no improvement): 0.059492182731628415\n",
      "Training iteration: 1260\n",
      "Improved validation loss from: 0.05930498242378235  to: 0.05912800431251526\n",
      "Training iteration: 1261\n",
      "Improved validation loss from: 0.05912800431251526  to: 0.058901858329772946\n",
      "Training iteration: 1262\n",
      "Improved validation loss from: 0.058901858329772946  to: 0.058780860900878903\n",
      "Training iteration: 1263\n",
      "Validation loss (no improvement): 0.05893560647964478\n",
      "Training iteration: 1264\n",
      "Validation loss (no improvement): 0.05901229381561279\n",
      "Training iteration: 1265\n",
      "Validation loss (no improvement): 0.05898253321647644\n",
      "Training iteration: 1266\n",
      "Validation loss (no improvement): 0.05887843370437622\n",
      "Training iteration: 1267\n",
      "Improved validation loss from: 0.058780860900878903  to: 0.05860008597373963\n",
      "Training iteration: 1268\n",
      "Improved validation loss from: 0.05860008597373963  to: 0.05840306282043457\n",
      "Training iteration: 1269\n",
      "Validation loss (no improvement): 0.058414268493652347\n",
      "Training iteration: 1270\n",
      "Improved validation loss from: 0.05840306282043457  to: 0.058177947998046875\n",
      "Training iteration: 1271\n",
      "Improved validation loss from: 0.058177947998046875  to: 0.05814119577407837\n",
      "Training iteration: 1272\n",
      "Validation loss (no improvement): 0.05831978321075439\n",
      "Training iteration: 1273\n",
      "Validation loss (no improvement): 0.058458340167999265\n",
      "Training iteration: 1274\n",
      "Validation loss (no improvement): 0.05816670656204224\n",
      "Training iteration: 1275\n",
      "Improved validation loss from: 0.05814119577407837  to: 0.057878583669662476\n",
      "Training iteration: 1276\n",
      "Improved validation loss from: 0.057878583669662476  to: 0.057768380641937254\n",
      "Training iteration: 1277\n",
      "Validation loss (no improvement): 0.05792030096054077\n",
      "Training iteration: 1278\n",
      "Validation loss (no improvement): 0.05780425071716309\n",
      "Training iteration: 1279\n",
      "Improved validation loss from: 0.057768380641937254  to: 0.05766122937202454\n",
      "Training iteration: 1280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.05766122937202454  to: 0.057530653476715085\n",
      "Training iteration: 1281\n",
      "Improved validation loss from: 0.057530653476715085  to: 0.05745817422866821\n",
      "Training iteration: 1282\n",
      "Improved validation loss from: 0.05745817422866821  to: 0.05720729827880859\n",
      "Training iteration: 1283\n",
      "Validation loss (no improvement): 0.05721355676651001\n",
      "Training iteration: 1284\n",
      "Validation loss (no improvement): 0.0574565052986145\n",
      "Training iteration: 1285\n",
      "Improved validation loss from: 0.05720729827880859  to: 0.057176119089126586\n",
      "Training iteration: 1286\n",
      "Validation loss (no improvement): 0.05720412135124207\n",
      "Training iteration: 1287\n",
      "Validation loss (no improvement): 0.05725414156913757\n",
      "Training iteration: 1288\n",
      "Improved validation loss from: 0.057176119089126586  to: 0.05714505910873413\n",
      "Training iteration: 1289\n",
      "Improved validation loss from: 0.05714505910873413  to: 0.056835442781448364\n",
      "Training iteration: 1290\n",
      "Improved validation loss from: 0.056835442781448364  to: 0.05681111216545105\n",
      "Training iteration: 1291\n",
      "Validation loss (no improvement): 0.05717822313308716\n",
      "Training iteration: 1292\n",
      "Validation loss (no improvement): 0.056905001401901245\n",
      "Training iteration: 1293\n",
      "Validation loss (no improvement): 0.05695459246635437\n",
      "Training iteration: 1294\n",
      "Validation loss (no improvement): 0.05715850591659546\n",
      "Training iteration: 1295\n",
      "Validation loss (no improvement): 0.0569807231426239\n",
      "Training iteration: 1296\n",
      "Validation loss (no improvement): 0.05693390369415283\n",
      "Training iteration: 1297\n",
      "Validation loss (no improvement): 0.05701593160629272\n",
      "Training iteration: 1298\n",
      "Validation loss (no improvement): 0.05717770457267761\n",
      "Training iteration: 1299\n",
      "Validation loss (no improvement): 0.05688199996948242\n",
      "Training iteration: 1300\n",
      "Validation loss (no improvement): 0.056822001934051514\n",
      "Training iteration: 1301\n",
      "Validation loss (no improvement): 0.056864726543426516\n",
      "Training iteration: 1302\n",
      "Validation loss (no improvement): 0.05717783570289612\n",
      "Training iteration: 1303\n",
      "Improved validation loss from: 0.05681111216545105  to: 0.05667592883110047\n",
      "Training iteration: 1304\n",
      "Improved validation loss from: 0.05667592883110047  to: 0.05664296150207519\n",
      "Training iteration: 1305\n",
      "Validation loss (no improvement): 0.056777065992355345\n",
      "Training iteration: 1306\n",
      "Validation loss (no improvement): 0.057094109058380124\n",
      "Training iteration: 1307\n",
      "Validation loss (no improvement): 0.056851398944854734\n",
      "Training iteration: 1308\n",
      "Validation loss (no improvement): 0.056687062978744505\n",
      "Training iteration: 1309\n",
      "Improved validation loss from: 0.05664296150207519  to: 0.05662606358528137\n",
      "Training iteration: 1310\n",
      "Validation loss (no improvement): 0.05671452283859253\n",
      "Training iteration: 1311\n",
      "Validation loss (no improvement): 0.05666082501411438\n",
      "Training iteration: 1312\n",
      "Validation loss (no improvement): 0.056661397218704224\n",
      "Training iteration: 1313\n",
      "Improved validation loss from: 0.05662606358528137  to: 0.05657395124435425\n",
      "Training iteration: 1314\n",
      "Improved validation loss from: 0.05657395124435425  to: 0.05637226104736328\n",
      "Training iteration: 1315\n",
      "Improved validation loss from: 0.05637226104736328  to: 0.056113308668136595\n",
      "Training iteration: 1316\n",
      "Validation loss (no improvement): 0.05617087483406067\n",
      "Training iteration: 1317\n",
      "Validation loss (no improvement): 0.05617497563362121\n",
      "Training iteration: 1318\n",
      "Improved validation loss from: 0.056113308668136595  to: 0.05609809756278992\n",
      "Training iteration: 1319\n",
      "Validation loss (no improvement): 0.05633726716041565\n",
      "Training iteration: 1320\n",
      "Validation loss (no improvement): 0.056336629390716556\n",
      "Training iteration: 1321\n",
      "Validation loss (no improvement): 0.05616493225097656\n",
      "Training iteration: 1322\n",
      "Improved validation loss from: 0.05609809756278992  to: 0.055652284622192384\n",
      "Training iteration: 1323\n",
      "Improved validation loss from: 0.055652284622192384  to: 0.05556807518005371\n",
      "Training iteration: 1324\n",
      "Validation loss (no improvement): 0.05592067837715149\n",
      "Training iteration: 1325\n",
      "Improved validation loss from: 0.05556807518005371  to: 0.0555464506149292\n",
      "Training iteration: 1326\n",
      "Validation loss (no improvement): 0.05588290095329285\n",
      "Training iteration: 1327\n",
      "Validation loss (no improvement): 0.05632426738739014\n",
      "Training iteration: 1328\n",
      "Validation loss (no improvement): 0.05644457340240479\n",
      "Training iteration: 1329\n",
      "Validation loss (no improvement): 0.05597372055053711\n",
      "Training iteration: 1330\n",
      "Improved validation loss from: 0.0555464506149292  to: 0.0553855836391449\n",
      "Training iteration: 1331\n",
      "Improved validation loss from: 0.0553855836391449  to: 0.0552940309047699\n",
      "Training iteration: 1332\n",
      "Validation loss (no improvement): 0.05543933510780334\n",
      "Training iteration: 1333\n",
      "Improved validation loss from: 0.0552940309047699  to: 0.05529311895370483\n",
      "Training iteration: 1334\n",
      "Improved validation loss from: 0.05529311895370483  to: 0.05523008108139038\n",
      "Training iteration: 1335\n",
      "Validation loss (no improvement): 0.05542880892753601\n",
      "Training iteration: 1336\n",
      "Validation loss (no improvement): 0.05537957549095154\n",
      "Training iteration: 1337\n",
      "Improved validation loss from: 0.05523008108139038  to: 0.0550781786441803\n",
      "Training iteration: 1338\n",
      "Improved validation loss from: 0.0550781786441803  to: 0.05482126474380493\n",
      "Training iteration: 1339\n",
      "Validation loss (no improvement): 0.05486143827438354\n",
      "Training iteration: 1340\n",
      "Validation loss (no improvement): 0.05490901470184326\n",
      "Training iteration: 1341\n",
      "Improved validation loss from: 0.05482126474380493  to: 0.054805290699005124\n",
      "Training iteration: 1342\n",
      "Improved validation loss from: 0.054805290699005124  to: 0.0546373188495636\n",
      "Training iteration: 1343\n",
      "Improved validation loss from: 0.0546373188495636  to: 0.05451495051383972\n",
      "Training iteration: 1344\n",
      "Improved validation loss from: 0.05451495051383972  to: 0.054444915056228636\n",
      "Training iteration: 1345\n",
      "Improved validation loss from: 0.054444915056228636  to: 0.05436273217201233\n",
      "Training iteration: 1346\n",
      "Improved validation loss from: 0.05436273217201233  to: 0.05432702898979187\n",
      "Training iteration: 1347\n",
      "Improved validation loss from: 0.05432702898979187  to: 0.05421636700630188\n",
      "Training iteration: 1348\n",
      "Improved validation loss from: 0.05421636700630188  to: 0.053994208574295044\n",
      "Training iteration: 1349\n",
      "Validation loss (no improvement): 0.05410149693489075\n",
      "Training iteration: 1350\n",
      "Validation loss (no improvement): 0.05402137041091919\n",
      "Training iteration: 1351\n",
      "Improved validation loss from: 0.053994208574295044  to: 0.05360874533653259\n",
      "Training iteration: 1352\n",
      "Improved validation loss from: 0.05360874533653259  to: 0.0536060631275177\n",
      "Training iteration: 1353\n",
      "Validation loss (no improvement): 0.05393560528755188\n",
      "Training iteration: 1354\n",
      "Improved validation loss from: 0.0536060631275177  to: 0.05338176488876343\n",
      "Training iteration: 1355\n",
      "Validation loss (no improvement): 0.053384536504745485\n",
      "Training iteration: 1356\n",
      "Validation loss (no improvement): 0.05350713729858399\n",
      "Training iteration: 1357\n",
      "Validation loss (no improvement): 0.053586161136627196\n",
      "Training iteration: 1358\n",
      "Validation loss (no improvement): 0.053597623109817506\n",
      "Training iteration: 1359\n",
      "Validation loss (no improvement): 0.05343538522720337\n",
      "Training iteration: 1360\n",
      "Improved validation loss from: 0.05338176488876343  to: 0.05325163602828979\n",
      "Training iteration: 1361\n",
      "Improved validation loss from: 0.05325163602828979  to: 0.05310482382774353\n",
      "Training iteration: 1362\n",
      "Improved validation loss from: 0.05310482382774353  to: 0.05302273035049439\n",
      "Training iteration: 1363\n",
      "Validation loss (no improvement): 0.05323338508605957\n",
      "Training iteration: 1364\n",
      "Validation loss (no improvement): 0.05355297327041626\n",
      "Training iteration: 1365\n",
      "Validation loss (no improvement): 0.05346969366073608\n",
      "Training iteration: 1366\n",
      "Validation loss (no improvement): 0.05317638516426086\n",
      "Training iteration: 1367\n",
      "Validation loss (no improvement): 0.05321599841117859\n",
      "Training iteration: 1368\n",
      "Improved validation loss from: 0.05302273035049439  to: 0.05296558141708374\n",
      "Training iteration: 1369\n",
      "Validation loss (no improvement): 0.05299188494682312\n",
      "Training iteration: 1370\n",
      "Validation loss (no improvement): 0.053081607818603514\n",
      "Training iteration: 1371\n",
      "Validation loss (no improvement): 0.05312243700027466\n",
      "Training iteration: 1372\n",
      "Validation loss (no improvement): 0.053046798706054686\n",
      "Training iteration: 1373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.05299059152603149\n",
      "Training iteration: 1374\n",
      "Improved validation loss from: 0.05296558141708374  to: 0.05264533162117004\n",
      "Training iteration: 1375\n",
      "Improved validation loss from: 0.05264533162117004  to: 0.05260123014450073\n",
      "Training iteration: 1376\n",
      "Validation loss (no improvement): 0.052861398458480834\n",
      "Training iteration: 1377\n",
      "Validation loss (no improvement): 0.05286377668380737\n",
      "Training iteration: 1378\n",
      "Validation loss (no improvement): 0.05263220071792603\n",
      "Training iteration: 1379\n",
      "Improved validation loss from: 0.05260123014450073  to: 0.052495318651199344\n",
      "Training iteration: 1380\n",
      "Validation loss (no improvement): 0.05252871513366699\n",
      "Training iteration: 1381\n",
      "Improved validation loss from: 0.052495318651199344  to: 0.05209850072860718\n",
      "Training iteration: 1382\n",
      "Improved validation loss from: 0.05209850072860718  to: 0.05198945999145508\n",
      "Training iteration: 1383\n",
      "Validation loss (no improvement): 0.052132368087768555\n",
      "Training iteration: 1384\n",
      "Validation loss (no improvement): 0.05227241516113281\n",
      "Training iteration: 1385\n",
      "Improved validation loss from: 0.05198945999145508  to: 0.051890474557876584\n",
      "Training iteration: 1386\n",
      "Improved validation loss from: 0.051890474557876584  to: 0.05165430307388306\n",
      "Training iteration: 1387\n",
      "Validation loss (no improvement): 0.052126514911651614\n",
      "Training iteration: 1388\n",
      "Validation loss (no improvement): 0.05192140936851501\n",
      "Training iteration: 1389\n",
      "Validation loss (no improvement): 0.05176004767417908\n",
      "Training iteration: 1390\n",
      "Validation loss (no improvement): 0.05207768678665161\n",
      "Training iteration: 1391\n",
      "Validation loss (no improvement): 0.05221034288406372\n",
      "Training iteration: 1392\n",
      "Validation loss (no improvement): 0.05198721289634704\n",
      "Training iteration: 1393\n",
      "Improved validation loss from: 0.05165430307388306  to: 0.05144658088684082\n",
      "Training iteration: 1394\n",
      "Improved validation loss from: 0.05144658088684082  to: 0.05127456784248352\n",
      "Training iteration: 1395\n",
      "Validation loss (no improvement): 0.0513707160949707\n",
      "Training iteration: 1396\n",
      "Validation loss (no improvement): 0.05161277651786804\n",
      "Training iteration: 1397\n",
      "Validation loss (no improvement): 0.05181070566177368\n",
      "Training iteration: 1398\n",
      "Validation loss (no improvement): 0.05189062356948852\n",
      "Training iteration: 1399\n",
      "Validation loss (no improvement): 0.0516065776348114\n",
      "Training iteration: 1400\n",
      "Validation loss (no improvement): 0.05138770341873169\n",
      "Training iteration: 1401\n",
      "Improved validation loss from: 0.05127456784248352  to: 0.051111352443695066\n",
      "Training iteration: 1402\n",
      "Validation loss (no improvement): 0.05111300349235535\n",
      "Training iteration: 1403\n",
      "Validation loss (no improvement): 0.051373451948165894\n",
      "Training iteration: 1404\n",
      "Improved validation loss from: 0.051111352443695066  to: 0.05102041959762573\n",
      "Training iteration: 1405\n",
      "Improved validation loss from: 0.05102041959762573  to: 0.05097534656524658\n",
      "Training iteration: 1406\n",
      "Validation loss (no improvement): 0.05120986104011536\n",
      "Training iteration: 1407\n",
      "Validation loss (no improvement): 0.05112292766571045\n",
      "Training iteration: 1408\n",
      "Improved validation loss from: 0.05097534656524658  to: 0.050926029682159424\n",
      "Training iteration: 1409\n",
      "Improved validation loss from: 0.050926029682159424  to: 0.050816076993942264\n",
      "Training iteration: 1410\n",
      "Validation loss (no improvement): 0.05109131932258606\n",
      "Training iteration: 1411\n",
      "Improved validation loss from: 0.050816076993942264  to: 0.05077018737792969\n",
      "Training iteration: 1412\n",
      "Improved validation loss from: 0.05077018737792969  to: 0.05067927241325378\n",
      "Training iteration: 1413\n",
      "Validation loss (no improvement): 0.05074413418769837\n",
      "Training iteration: 1414\n",
      "Validation loss (no improvement): 0.050994652509689334\n",
      "Training iteration: 1415\n",
      "Validation loss (no improvement): 0.05086179375648499\n",
      "Training iteration: 1416\n",
      "Validation loss (no improvement): 0.05075905919075012\n",
      "Training iteration: 1417\n",
      "Validation loss (no improvement): 0.05088075399398804\n",
      "Training iteration: 1418\n",
      "Improved validation loss from: 0.05067927241325378  to: 0.050564038753509524\n",
      "Training iteration: 1419\n",
      "Validation loss (no improvement): 0.05060068368911743\n",
      "Training iteration: 1420\n",
      "Validation loss (no improvement): 0.05071917772293091\n",
      "Training iteration: 1421\n",
      "Improved validation loss from: 0.050564038753509524  to: 0.05055740475654602\n",
      "Training iteration: 1422\n",
      "Improved validation loss from: 0.05055740475654602  to: 0.05011674165725708\n",
      "Training iteration: 1423\n",
      "Improved validation loss from: 0.05011674165725708  to: 0.050017702579498294\n",
      "Training iteration: 1424\n",
      "Validation loss (no improvement): 0.05020487904548645\n",
      "Training iteration: 1425\n",
      "Validation loss (no improvement): 0.05050690770149231\n",
      "Training iteration: 1426\n",
      "Validation loss (no improvement): 0.05030598640441895\n",
      "Training iteration: 1427\n",
      "Validation loss (no improvement): 0.050278091430664064\n",
      "Training iteration: 1428\n",
      "Validation loss (no improvement): 0.050481009483337405\n",
      "Training iteration: 1429\n",
      "Validation loss (no improvement): 0.050176167488098146\n",
      "Training iteration: 1430\n",
      "Improved validation loss from: 0.050017702579498294  to: 0.04995865225791931\n",
      "Training iteration: 1431\n",
      "Improved validation loss from: 0.04995865225791931  to: 0.04988411068916321\n",
      "Training iteration: 1432\n",
      "Validation loss (no improvement): 0.050313085317611694\n",
      "Training iteration: 1433\n",
      "Validation loss (no improvement): 0.050012075901031496\n",
      "Training iteration: 1434\n",
      "Validation loss (no improvement): 0.05006980299949646\n",
      "Training iteration: 1435\n",
      "Validation loss (no improvement): 0.050337857007980345\n",
      "Training iteration: 1436\n",
      "Validation loss (no improvement): 0.049966222047805785\n",
      "Training iteration: 1437\n",
      "Improved validation loss from: 0.04988411068916321  to: 0.04986714422702789\n",
      "Training iteration: 1438\n",
      "Validation loss (no improvement): 0.05016323924064636\n",
      "Training iteration: 1439\n",
      "Validation loss (no improvement): 0.04991548955440521\n",
      "Training iteration: 1440\n",
      "Validation loss (no improvement): 0.049916940927505496\n",
      "Training iteration: 1441\n",
      "Validation loss (no improvement): 0.05001248717308045\n",
      "Training iteration: 1442\n",
      "Validation loss (no improvement): 0.05008851289749146\n",
      "Training iteration: 1443\n",
      "Improved validation loss from: 0.04986714422702789  to: 0.04959175586700439\n",
      "Training iteration: 1444\n",
      "Improved validation loss from: 0.04959175586700439  to: 0.04943394660949707\n",
      "Training iteration: 1445\n",
      "Validation loss (no improvement): 0.049644798040390015\n",
      "Training iteration: 1446\n",
      "Validation loss (no improvement): 0.0498727411031723\n",
      "Training iteration: 1447\n",
      "Validation loss (no improvement): 0.049783945083618164\n",
      "Training iteration: 1448\n",
      "Validation loss (no improvement): 0.04966031014919281\n",
      "Training iteration: 1449\n",
      "Validation loss (no improvement): 0.04956381916999817\n",
      "Training iteration: 1450\n",
      "Improved validation loss from: 0.04943394660949707  to: 0.04937184453010559\n",
      "Training iteration: 1451\n",
      "Improved validation loss from: 0.04937184453010559  to: 0.0490126758813858\n",
      "Training iteration: 1452\n",
      "Improved validation loss from: 0.0490126758813858  to: 0.049010437726974485\n",
      "Training iteration: 1453\n",
      "Validation loss (no improvement): 0.04917638897895813\n",
      "Training iteration: 1454\n",
      "Validation loss (no improvement): 0.04934080243110657\n",
      "Training iteration: 1455\n",
      "Improved validation loss from: 0.049010437726974485  to: 0.048878532648086545\n",
      "Training iteration: 1456\n",
      "Improved validation loss from: 0.048878532648086545  to: 0.04865418970584869\n",
      "Training iteration: 1457\n",
      "Validation loss (no improvement): 0.04873364567756653\n",
      "Training iteration: 1458\n",
      "Validation loss (no improvement): 0.04904280304908752\n",
      "Training iteration: 1459\n",
      "Improved validation loss from: 0.04865418970584869  to: 0.048590803146362306\n",
      "Training iteration: 1460\n",
      "Validation loss (no improvement): 0.04864808917045593\n",
      "Training iteration: 1461\n",
      "Validation loss (no improvement): 0.04871583878993988\n",
      "Training iteration: 1462\n",
      "Validation loss (no improvement): 0.04878343939781189\n",
      "Training iteration: 1463\n",
      "Improved validation loss from: 0.048590803146362306  to: 0.04830698966979981\n",
      "Training iteration: 1464\n",
      "Improved validation loss from: 0.04830698966979981  to: 0.04820072054862976\n",
      "Training iteration: 1465\n",
      "Validation loss (no improvement): 0.048422640562057494\n",
      "Training iteration: 1466\n",
      "Validation loss (no improvement): 0.04894541800022125\n",
      "Training iteration: 1467\n",
      "Validation loss (no improvement): 0.04853702485561371\n",
      "Training iteration: 1468\n",
      "Validation loss (no improvement): 0.048578286170959474\n",
      "Training iteration: 1469\n",
      "Validation loss (no improvement): 0.04858735203742981\n",
      "Training iteration: 1470\n",
      "Validation loss (no improvement): 0.04878838062286377\n",
      "Training iteration: 1471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.048391017317771914\n",
      "Training iteration: 1472\n",
      "Validation loss (no improvement): 0.04826741218566895\n",
      "Training iteration: 1473\n",
      "Validation loss (no improvement): 0.04852171838283539\n",
      "Training iteration: 1474\n",
      "Validation loss (no improvement): 0.04881824553012848\n",
      "Training iteration: 1475\n",
      "Validation loss (no improvement): 0.04839784502983093\n",
      "Training iteration: 1476\n",
      "Validation loss (no improvement): 0.04824804365634918\n",
      "Training iteration: 1477\n",
      "Validation loss (no improvement): 0.04835403859615326\n",
      "Training iteration: 1478\n",
      "Validation loss (no improvement): 0.04871119856834412\n",
      "Training iteration: 1479\n",
      "Validation loss (no improvement): 0.04857622981071472\n",
      "Training iteration: 1480\n",
      "Validation loss (no improvement): 0.04862214028835297\n",
      "Training iteration: 1481\n",
      "Validation loss (no improvement): 0.04856635630130768\n",
      "Training iteration: 1482\n",
      "Validation loss (no improvement): 0.04828197360038757\n",
      "Training iteration: 1483\n",
      "Improved validation loss from: 0.04820072054862976  to: 0.04810191094875336\n",
      "Training iteration: 1484\n",
      "Validation loss (no improvement): 0.04811989367008209\n",
      "Training iteration: 1485\n",
      "Validation loss (no improvement): 0.04815531671047211\n",
      "Training iteration: 1486\n",
      "Validation loss (no improvement): 0.04811151921749115\n",
      "Training iteration: 1487\n",
      "Improved validation loss from: 0.04810191094875336  to: 0.04809907078742981\n",
      "Training iteration: 1488\n",
      "Validation loss (no improvement): 0.0481299638748169\n",
      "Training iteration: 1489\n",
      "Improved validation loss from: 0.04809907078742981  to: 0.047927451133728025\n",
      "Training iteration: 1490\n",
      "Improved validation loss from: 0.047927451133728025  to: 0.047904014587402344\n",
      "Training iteration: 1491\n",
      "Improved validation loss from: 0.047904014587402344  to: 0.0477637529373169\n",
      "Training iteration: 1492\n",
      "Improved validation loss from: 0.0477637529373169  to: 0.04768916964530945\n",
      "Training iteration: 1493\n",
      "Improved validation loss from: 0.04768916964530945  to: 0.04764311313629151\n",
      "Training iteration: 1494\n",
      "Improved validation loss from: 0.04764311313629151  to: 0.04753807187080383\n",
      "Training iteration: 1495\n",
      "Validation loss (no improvement): 0.04763264656066894\n",
      "Training iteration: 1496\n",
      "Validation loss (no improvement): 0.047694677114486696\n",
      "Training iteration: 1497\n",
      "Improved validation loss from: 0.04753807187080383  to: 0.04730286002159119\n",
      "Training iteration: 1498\n",
      "Improved validation loss from: 0.04730286002159119  to: 0.04714679718017578\n",
      "Training iteration: 1499\n",
      "Validation loss (no improvement): 0.047431477904319765\n",
      "Training iteration: 1500\n",
      "Validation loss (no improvement): 0.04736391603946686\n",
      "Training iteration: 1501\n",
      "Validation loss (no improvement): 0.047464889287948606\n",
      "Training iteration: 1502\n",
      "Validation loss (no improvement): 0.04762380123138428\n",
      "Training iteration: 1503\n",
      "Validation loss (no improvement): 0.04738808572292328\n",
      "Training iteration: 1504\n",
      "Validation loss (no improvement): 0.0473397433757782\n",
      "Training iteration: 1505\n",
      "Validation loss (no improvement): 0.04759612083435059\n",
      "Training iteration: 1506\n",
      "Validation loss (no improvement): 0.04728169441223144\n",
      "Training iteration: 1507\n",
      "Validation loss (no improvement): 0.04755130708217621\n",
      "Training iteration: 1508\n",
      "Validation loss (no improvement): 0.047877350449562074\n",
      "Training iteration: 1509\n",
      "Validation loss (no improvement): 0.04780144691467285\n",
      "Training iteration: 1510\n",
      "Validation loss (no improvement): 0.047618716955184937\n",
      "Training iteration: 1511\n",
      "Validation loss (no improvement): 0.04748329520225525\n",
      "Training iteration: 1512\n",
      "Validation loss (no improvement): 0.047505941987037656\n",
      "Training iteration: 1513\n",
      "Validation loss (no improvement): 0.04748232960700989\n",
      "Training iteration: 1514\n",
      "Validation loss (no improvement): 0.04755972921848297\n",
      "Training iteration: 1515\n",
      "Validation loss (no improvement): 0.04773455262184143\n",
      "Training iteration: 1516\n",
      "Validation loss (no improvement): 0.047636255621910095\n",
      "Training iteration: 1517\n",
      "Validation loss (no improvement): 0.047370442748069765\n",
      "Training iteration: 1518\n",
      "Validation loss (no improvement): 0.04734401106834411\n",
      "Training iteration: 1519\n",
      "Validation loss (no improvement): 0.04742850661277771\n",
      "Training iteration: 1520\n",
      "Validation loss (no improvement): 0.04718786776065827\n",
      "Training iteration: 1521\n",
      "Validation loss (no improvement): 0.04730921685695648\n",
      "Training iteration: 1522\n",
      "Validation loss (no improvement): 0.04742315709590912\n",
      "Training iteration: 1523\n",
      "Validation loss (no improvement): 0.04719486236572266\n",
      "Training iteration: 1524\n",
      "Improved validation loss from: 0.04714679718017578  to: 0.047016444802284243\n",
      "Training iteration: 1525\n",
      "Validation loss (no improvement): 0.04706001281738281\n",
      "Training iteration: 1526\n",
      "Validation loss (no improvement): 0.0470663458108902\n",
      "Training iteration: 1527\n",
      "Validation loss (no improvement): 0.047116023302078244\n",
      "Training iteration: 1528\n",
      "Validation loss (no improvement): 0.04722231924533844\n",
      "Training iteration: 1529\n",
      "Validation loss (no improvement): 0.04731612801551819\n",
      "Training iteration: 1530\n",
      "Improved validation loss from: 0.047016444802284243  to: 0.04693945050239563\n",
      "Training iteration: 1531\n",
      "Improved validation loss from: 0.04693945050239563  to: 0.04669139981269836\n",
      "Training iteration: 1532\n",
      "Validation loss (no improvement): 0.046763092279434204\n",
      "Training iteration: 1533\n",
      "Validation loss (no improvement): 0.04709874093532562\n",
      "Training iteration: 1534\n",
      "Validation loss (no improvement): 0.046973735094070435\n",
      "Training iteration: 1535\n",
      "Validation loss (no improvement): 0.04686996042728424\n",
      "Training iteration: 1536\n",
      "Validation loss (no improvement): 0.046884623169898984\n",
      "Training iteration: 1537\n",
      "Validation loss (no improvement): 0.047174683213233946\n",
      "Training iteration: 1538\n",
      "Validation loss (no improvement): 0.04690530300140381\n",
      "Training iteration: 1539\n",
      "Validation loss (no improvement): 0.04679044187068939\n",
      "Training iteration: 1540\n",
      "Validation loss (no improvement): 0.046751612424850465\n",
      "Training iteration: 1541\n",
      "Validation loss (no improvement): 0.04691241383552551\n",
      "Training iteration: 1542\n",
      "Improved validation loss from: 0.04669139981269836  to: 0.046652945876121524\n",
      "Training iteration: 1543\n",
      "Improved validation loss from: 0.046652945876121524  to: 0.046634578704833986\n",
      "Training iteration: 1544\n",
      "Validation loss (no improvement): 0.046718445420265195\n",
      "Training iteration: 1545\n",
      "Validation loss (no improvement): 0.04669831395149231\n",
      "Training iteration: 1546\n",
      "Improved validation loss from: 0.046634578704833986  to: 0.04658445715904236\n",
      "Training iteration: 1547\n",
      "Validation loss (no improvement): 0.046594268083572386\n",
      "Training iteration: 1548\n",
      "Validation loss (no improvement): 0.046624493598937986\n",
      "Training iteration: 1549\n",
      "Validation loss (no improvement): 0.04661454260349274\n",
      "Training iteration: 1550\n",
      "Improved validation loss from: 0.04658445715904236  to: 0.0462458610534668\n",
      "Training iteration: 1551\n",
      "Improved validation loss from: 0.0462458610534668  to: 0.04608737826347351\n",
      "Training iteration: 1552\n",
      "Validation loss (no improvement): 0.04641036093235016\n",
      "Training iteration: 1553\n",
      "Validation loss (no improvement): 0.04623330235481262\n",
      "Training iteration: 1554\n",
      "Improved validation loss from: 0.04608737826347351  to: 0.04607837796211243\n",
      "Training iteration: 1555\n",
      "Validation loss (no improvement): 0.04615567624568939\n",
      "Training iteration: 1556\n",
      "Validation loss (no improvement): 0.046388715505599976\n",
      "Training iteration: 1557\n",
      "Validation loss (no improvement): 0.04615585207939148\n",
      "Training iteration: 1558\n",
      "Improved validation loss from: 0.04607837796211243  to: 0.04603347182273865\n",
      "Training iteration: 1559\n",
      "Validation loss (no improvement): 0.04605847299098968\n",
      "Training iteration: 1560\n",
      "Validation loss (no improvement): 0.04637439846992493\n",
      "Training iteration: 1561\n",
      "Validation loss (no improvement): 0.04623584151268005\n",
      "Training iteration: 1562\n",
      "Validation loss (no improvement): 0.04617123007774353\n",
      "Training iteration: 1563\n",
      "Validation loss (no improvement): 0.04623899459838867\n",
      "Training iteration: 1564\n",
      "Validation loss (no improvement): 0.046556282043457034\n",
      "Training iteration: 1565\n",
      "Validation loss (no improvement): 0.04633203148841858\n",
      "Training iteration: 1566\n",
      "Improved validation loss from: 0.04603347182273865  to: 0.04598540663719177\n",
      "Training iteration: 1567\n",
      "Improved validation loss from: 0.04598540663719177  to: 0.04597703516483307\n",
      "Training iteration: 1568\n",
      "Validation loss (no improvement): 0.04627363681793213\n",
      "Training iteration: 1569\n",
      "Validation loss (no improvement): 0.046247997879981996\n",
      "Training iteration: 1570\n",
      "Validation loss (no improvement): 0.04606078565120697\n",
      "Training iteration: 1571\n",
      "Improved validation loss from: 0.04597703516483307  to: 0.04594656825065613\n",
      "Training iteration: 1572\n",
      "Validation loss (no improvement): 0.046141085028648374\n",
      "Training iteration: 1573\n",
      "Improved validation loss from: 0.04594656825065613  to: 0.0458662211894989\n",
      "Training iteration: 1574\n",
      "Validation loss (no improvement): 0.04596894383430481\n",
      "Training iteration: 1575\n",
      "Validation loss (no improvement): 0.04620529115200043\n",
      "Training iteration: 1576\n",
      "Validation loss (no improvement): 0.046168464422225955\n",
      "Training iteration: 1577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.0458662211894989  to: 0.04580141603946686\n",
      "Training iteration: 1578\n",
      "Improved validation loss from: 0.04580141603946686  to: 0.04568571448326111\n",
      "Training iteration: 1579\n",
      "Validation loss (no improvement): 0.04586043357849121\n",
      "Training iteration: 1580\n",
      "Validation loss (no improvement): 0.04576712548732757\n",
      "Training iteration: 1581\n",
      "Validation loss (no improvement): 0.04570055603981018\n",
      "Training iteration: 1582\n",
      "Validation loss (no improvement): 0.0457529217004776\n",
      "Training iteration: 1583\n",
      "Improved validation loss from: 0.04568571448326111  to: 0.04564752578735352\n",
      "Training iteration: 1584\n",
      "Validation loss (no improvement): 0.04576579928398132\n",
      "Training iteration: 1585\n",
      "Validation loss (no improvement): 0.04572274684906006\n",
      "Training iteration: 1586\n",
      "Improved validation loss from: 0.04564752578735352  to: 0.04557584822177887\n",
      "Training iteration: 1587\n",
      "Validation loss (no improvement): 0.04559842050075531\n",
      "Training iteration: 1588\n",
      "Validation loss (no improvement): 0.04571222364902496\n",
      "Training iteration: 1589\n",
      "Validation loss (no improvement): 0.04583853185176849\n",
      "Training iteration: 1590\n",
      "Validation loss (no improvement): 0.04561561048030853\n",
      "Training iteration: 1591\n",
      "Improved validation loss from: 0.04557584822177887  to: 0.04551906585693359\n",
      "Training iteration: 1592\n",
      "Validation loss (no improvement): 0.04555959105491638\n",
      "Training iteration: 1593\n",
      "Validation loss (no improvement): 0.045757293701171875\n",
      "Training iteration: 1594\n",
      "Validation loss (no improvement): 0.04575479030609131\n",
      "Training iteration: 1595\n",
      "Improved validation loss from: 0.04551906585693359  to: 0.045404759049415586\n",
      "Training iteration: 1596\n",
      "Improved validation loss from: 0.045404759049415586  to: 0.04527134895324707\n",
      "Training iteration: 1597\n",
      "Validation loss (no improvement): 0.04557226300239563\n",
      "Training iteration: 1598\n",
      "Validation loss (no improvement): 0.045492973923683164\n",
      "Training iteration: 1599\n",
      "Validation loss (no improvement): 0.045507964491844174\n",
      "Training iteration: 1600\n",
      "Validation loss (no improvement): 0.0455662339925766\n",
      "Training iteration: 1601\n",
      "Validation loss (no improvement): 0.04544830322265625\n",
      "Training iteration: 1602\n",
      "Validation loss (no improvement): 0.045337730646133424\n",
      "Training iteration: 1603\n",
      "Validation loss (no improvement): 0.045470109581947325\n",
      "Training iteration: 1604\n",
      "Validation loss (no improvement): 0.045423460006713864\n",
      "Training iteration: 1605\n",
      "Validation loss (no improvement): 0.04543121457099915\n",
      "Training iteration: 1606\n",
      "Validation loss (no improvement): 0.04546492993831634\n",
      "Training iteration: 1607\n",
      "Validation loss (no improvement): 0.04551137089729309\n",
      "Training iteration: 1608\n",
      "Validation loss (no improvement): 0.04563322961330414\n",
      "Training iteration: 1609\n",
      "Improved validation loss from: 0.04527134895324707  to: 0.045215314626693724\n",
      "Training iteration: 1610\n",
      "Validation loss (no improvement): 0.04524058699607849\n",
      "Training iteration: 1611\n",
      "Validation loss (no improvement): 0.045446079969406125\n",
      "Training iteration: 1612\n",
      "Validation loss (no improvement): 0.045452776551246646\n",
      "Training iteration: 1613\n",
      "Validation loss (no improvement): 0.045281720161437986\n",
      "Training iteration: 1614\n",
      "Validation loss (no improvement): 0.045227861404418944\n",
      "Training iteration: 1615\n",
      "Validation loss (no improvement): 0.045292457938194274\n",
      "Training iteration: 1616\n",
      "Improved validation loss from: 0.045215314626693724  to: 0.04514487385749817\n",
      "Training iteration: 1617\n",
      "Validation loss (no improvement): 0.045164459943771364\n",
      "Training iteration: 1618\n",
      "Validation loss (no improvement): 0.045246458053588866\n",
      "Training iteration: 1619\n",
      "Validation loss (no improvement): 0.045348328351974485\n",
      "Training iteration: 1620\n",
      "Validation loss (no improvement): 0.04519504606723786\n",
      "Training iteration: 1621\n",
      "Validation loss (no improvement): 0.045231693983078004\n",
      "Training iteration: 1622\n",
      "Validation loss (no improvement): 0.045332154631614684\n",
      "Training iteration: 1623\n",
      "Validation loss (no improvement): 0.04531158804893494\n",
      "Training iteration: 1624\n",
      "Validation loss (no improvement): 0.045247069001197814\n",
      "Training iteration: 1625\n",
      "Validation loss (no improvement): 0.04531327784061432\n",
      "Training iteration: 1626\n",
      "Validation loss (no improvement): 0.045359641313552856\n",
      "Training iteration: 1627\n",
      "Validation loss (no improvement): 0.045225852727890016\n",
      "Training iteration: 1628\n",
      "Improved validation loss from: 0.04514487385749817  to: 0.045124751329421994\n",
      "Training iteration: 1629\n",
      "Validation loss (no improvement): 0.045147284865379333\n",
      "Training iteration: 1630\n",
      "Validation loss (no improvement): 0.045197948813438416\n",
      "Training iteration: 1631\n",
      "Validation loss (no improvement): 0.04514266550540924\n",
      "Training iteration: 1632\n",
      "Improved validation loss from: 0.045124751329421994  to: 0.044921502470970154\n",
      "Training iteration: 1633\n",
      "Improved validation loss from: 0.044921502470970154  to: 0.04483870565891266\n",
      "Training iteration: 1634\n",
      "Validation loss (no improvement): 0.04491272568702698\n",
      "Training iteration: 1635\n",
      "Validation loss (no improvement): 0.045009249448776247\n",
      "Training iteration: 1636\n",
      "Improved validation loss from: 0.04483870565891266  to: 0.044798311591148374\n",
      "Training iteration: 1637\n",
      "Improved validation loss from: 0.044798311591148374  to: 0.044631201028823855\n",
      "Training iteration: 1638\n",
      "Validation loss (no improvement): 0.04471792578697205\n",
      "Training iteration: 1639\n",
      "Validation loss (no improvement): 0.044635277986526486\n",
      "Training iteration: 1640\n",
      "Improved validation loss from: 0.044631201028823855  to: 0.044522923231124875\n",
      "Training iteration: 1641\n",
      "Validation loss (no improvement): 0.04463010728359222\n",
      "Training iteration: 1642\n",
      "Validation loss (no improvement): 0.04485904574394226\n",
      "Training iteration: 1643\n",
      "Validation loss (no improvement): 0.044818168878555296\n",
      "Training iteration: 1644\n",
      "Validation loss (no improvement): 0.044723311066627504\n",
      "Training iteration: 1645\n",
      "Validation loss (no improvement): 0.04458521008491516\n",
      "Training iteration: 1646\n",
      "Validation loss (no improvement): 0.04455216526985169\n",
      "Training iteration: 1647\n",
      "Validation loss (no improvement): 0.044556325674057005\n",
      "Training iteration: 1648\n",
      "Validation loss (no improvement): 0.044568809866905215\n",
      "Training iteration: 1649\n",
      "Validation loss (no improvement): 0.0447397381067276\n",
      "Training iteration: 1650\n",
      "Validation loss (no improvement): 0.04489433169364929\n",
      "Training iteration: 1651\n",
      "Validation loss (no improvement): 0.04484111666679382\n",
      "Training iteration: 1652\n",
      "Improved validation loss from: 0.044522923231124875  to: 0.04445565342903137\n",
      "Training iteration: 1653\n",
      "Improved validation loss from: 0.04445565342903137  to: 0.044244876503944396\n",
      "Training iteration: 1654\n",
      "Validation loss (no improvement): 0.04437810778617859\n",
      "Training iteration: 1655\n",
      "Validation loss (no improvement): 0.04458602070808411\n",
      "Training iteration: 1656\n",
      "Validation loss (no improvement): 0.044569429755210874\n",
      "Training iteration: 1657\n",
      "Validation loss (no improvement): 0.04467053413391113\n",
      "Training iteration: 1658\n",
      "Validation loss (no improvement): 0.044686970114707944\n",
      "Training iteration: 1659\n",
      "Validation loss (no improvement): 0.044449234008789064\n",
      "Training iteration: 1660\n",
      "Improved validation loss from: 0.044244876503944396  to: 0.044147190451622007\n",
      "Training iteration: 1661\n",
      "Validation loss (no improvement): 0.04417291283607483\n",
      "Training iteration: 1662\n",
      "Validation loss (no improvement): 0.044508513808250424\n",
      "Training iteration: 1663\n",
      "Validation loss (no improvement): 0.04456204473972321\n",
      "Training iteration: 1664\n",
      "Validation loss (no improvement): 0.044737958908081056\n",
      "Training iteration: 1665\n",
      "Validation loss (no improvement): 0.044661450386047366\n",
      "Training iteration: 1666\n",
      "Validation loss (no improvement): 0.04447740018367767\n",
      "Training iteration: 1667\n",
      "Validation loss (no improvement): 0.04428090155124664\n",
      "Training iteration: 1668\n",
      "Validation loss (no improvement): 0.04425331950187683\n",
      "Training iteration: 1669\n",
      "Validation loss (no improvement): 0.04440471231937408\n",
      "Training iteration: 1670\n",
      "Validation loss (no improvement): 0.04463210999965668\n",
      "Training iteration: 1671\n",
      "Validation loss (no improvement): 0.044425034523010255\n",
      "Training iteration: 1672\n",
      "Validation loss (no improvement): 0.044319599866867065\n",
      "Training iteration: 1673\n",
      "Validation loss (no improvement): 0.04440726339817047\n",
      "Training iteration: 1674\n",
      "Improved validation loss from: 0.044147190451622007  to: 0.044122162461280826\n",
      "Training iteration: 1675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.044122162461280826  to: 0.044070130586624144\n",
      "Training iteration: 1676\n",
      "Validation loss (no improvement): 0.04430394172668457\n",
      "Training iteration: 1677\n",
      "Validation loss (no improvement): 0.04449889659881592\n",
      "Training iteration: 1678\n",
      "Validation loss (no improvement): 0.0443546861410141\n",
      "Training iteration: 1679\n",
      "Validation loss (no improvement): 0.04413272440433502\n",
      "Training iteration: 1680\n",
      "Validation loss (no improvement): 0.04436213076114655\n",
      "Training iteration: 1681\n",
      "Validation loss (no improvement): 0.04425894618034363\n",
      "Training iteration: 1682\n",
      "Improved validation loss from: 0.044070130586624144  to: 0.04397304654121399\n",
      "Training iteration: 1683\n",
      "Validation loss (no improvement): 0.044141346216201784\n",
      "Training iteration: 1684\n",
      "Validation loss (no improvement): 0.04451383948326111\n",
      "Training iteration: 1685\n",
      "Validation loss (no improvement): 0.04430171847343445\n",
      "Training iteration: 1686\n",
      "Validation loss (no improvement): 0.04411943554878235\n",
      "Training iteration: 1687\n",
      "Validation loss (no improvement): 0.04417975842952728\n",
      "Training iteration: 1688\n",
      "Validation loss (no improvement): 0.04420856535434723\n",
      "Training iteration: 1689\n",
      "Validation loss (no improvement): 0.04402938783168793\n",
      "Training iteration: 1690\n",
      "Validation loss (no improvement): 0.04407331049442291\n",
      "Training iteration: 1691\n",
      "Validation loss (no improvement): 0.04431498944759369\n",
      "Training iteration: 1692\n",
      "Validation loss (no improvement): 0.04440484046936035\n",
      "Training iteration: 1693\n",
      "Validation loss (no improvement): 0.0440216600894928\n",
      "Training iteration: 1694\n",
      "Improved validation loss from: 0.04397304654121399  to: 0.043796196579933167\n",
      "Training iteration: 1695\n",
      "Validation loss (no improvement): 0.0439643383026123\n",
      "Training iteration: 1696\n",
      "Validation loss (no improvement): 0.043882313370704654\n",
      "Training iteration: 1697\n",
      "Improved validation loss from: 0.043796196579933167  to: 0.043785113096237185\n",
      "Training iteration: 1698\n",
      "Validation loss (no improvement): 0.043963709473609926\n",
      "Training iteration: 1699\n",
      "Validation loss (no improvement): 0.0440680593252182\n",
      "Training iteration: 1700\n",
      "Validation loss (no improvement): 0.04388852715492249\n",
      "Training iteration: 1701\n",
      "Improved validation loss from: 0.043785113096237185  to: 0.04371857047080994\n",
      "Training iteration: 1702\n",
      "Improved validation loss from: 0.04371857047080994  to: 0.04366417825222015\n",
      "Training iteration: 1703\n",
      "Improved validation loss from: 0.04366417825222015  to: 0.0436616986989975\n",
      "Training iteration: 1704\n",
      "Validation loss (no improvement): 0.04368162155151367\n",
      "Training iteration: 1705\n",
      "Validation loss (no improvement): 0.043775469064712524\n",
      "Training iteration: 1706\n",
      "Validation loss (no improvement): 0.04379731714725495\n",
      "Training iteration: 1707\n",
      "Improved validation loss from: 0.0436616986989975  to: 0.043638023734092715\n",
      "Training iteration: 1708\n",
      "Improved validation loss from: 0.043638023734092715  to: 0.043560576438903806\n",
      "Training iteration: 1709\n",
      "Improved validation loss from: 0.043560576438903806  to: 0.043440356850624084\n",
      "Training iteration: 1710\n",
      "Improved validation loss from: 0.043440356850624084  to: 0.04338614940643311\n",
      "Training iteration: 1711\n",
      "Validation loss (no improvement): 0.0434785932302475\n",
      "Training iteration: 1712\n",
      "Validation loss (no improvement): 0.043598517775535583\n",
      "Training iteration: 1713\n",
      "Validation loss (no improvement): 0.04350980818271637\n",
      "Training iteration: 1714\n",
      "Validation loss (no improvement): 0.04344125688076019\n",
      "Training iteration: 1715\n",
      "Validation loss (no improvement): 0.043440216779708864\n",
      "Training iteration: 1716\n",
      "Validation loss (no improvement): 0.04344143867492676\n",
      "Training iteration: 1717\n",
      "Improved validation loss from: 0.04338614940643311  to: 0.04325072169303894\n",
      "Training iteration: 1718\n",
      "Validation loss (no improvement): 0.043302512168884276\n",
      "Training iteration: 1719\n",
      "Validation loss (no improvement): 0.0434704065322876\n",
      "Training iteration: 1720\n",
      "Validation loss (no improvement): 0.04325932562351227\n",
      "Training iteration: 1721\n",
      "Validation loss (no improvement): 0.043280473351478575\n",
      "Training iteration: 1722\n",
      "Validation loss (no improvement): 0.043444091081619264\n",
      "Training iteration: 1723\n",
      "Validation loss (no improvement): 0.043578559160232545\n",
      "Training iteration: 1724\n",
      "Validation loss (no improvement): 0.04337762892246246\n",
      "Training iteration: 1725\n",
      "Validation loss (no improvement): 0.04333465695381165\n",
      "Training iteration: 1726\n",
      "Validation loss (no improvement): 0.04350947737693787\n",
      "Training iteration: 1727\n",
      "Validation loss (no improvement): 0.043609461188316344\n",
      "Training iteration: 1728\n",
      "Improved validation loss from: 0.04325072169303894  to: 0.04324787259101868\n",
      "Training iteration: 1729\n",
      "Validation loss (no improvement): 0.043264803290367124\n",
      "Training iteration: 1730\n",
      "Validation loss (no improvement): 0.04347438812255859\n",
      "Training iteration: 1731\n",
      "Validation loss (no improvement): 0.04352457523345947\n",
      "Training iteration: 1732\n",
      "Validation loss (no improvement): 0.04341040551662445\n",
      "Training iteration: 1733\n",
      "Validation loss (no improvement): 0.0434195339679718\n",
      "Training iteration: 1734\n",
      "Validation loss (no improvement): 0.04354308545589447\n",
      "Training iteration: 1735\n",
      "Validation loss (no improvement): 0.0434122234582901\n",
      "Training iteration: 1736\n",
      "Validation loss (no improvement): 0.04344183802604675\n",
      "Training iteration: 1737\n",
      "Validation loss (no improvement): 0.04349575936794281\n",
      "Training iteration: 1738\n",
      "Validation loss (no improvement): 0.0435356080532074\n",
      "Training iteration: 1739\n",
      "Improved validation loss from: 0.04324787259101868  to: 0.043157529830932614\n",
      "Training iteration: 1740\n",
      "Improved validation loss from: 0.043157529830932614  to: 0.04308479428291321\n",
      "Training iteration: 1741\n",
      "Validation loss (no improvement): 0.04324983060359955\n",
      "Training iteration: 1742\n",
      "Validation loss (no improvement): 0.043319407105445865\n",
      "Training iteration: 1743\n",
      "Improved validation loss from: 0.04308479428291321  to: 0.042982012033462524\n",
      "Training iteration: 1744\n",
      "Improved validation loss from: 0.042982012033462524  to: 0.042934289574623107\n",
      "Training iteration: 1745\n",
      "Validation loss (no improvement): 0.04313877522945404\n",
      "Training iteration: 1746\n",
      "Validation loss (no improvement): 0.04315394461154938\n",
      "Training iteration: 1747\n",
      "Validation loss (no improvement): 0.04304178357124329\n",
      "Training iteration: 1748\n",
      "Validation loss (no improvement): 0.04300649762153626\n",
      "Training iteration: 1749\n",
      "Validation loss (no improvement): 0.04310852587223053\n",
      "Training iteration: 1750\n",
      "Validation loss (no improvement): 0.043112152814865114\n",
      "Training iteration: 1751\n",
      "Validation loss (no improvement): 0.04311185777187347\n",
      "Training iteration: 1752\n",
      "Validation loss (no improvement): 0.043070435523986816\n",
      "Training iteration: 1753\n",
      "Validation loss (no improvement): 0.04308485984802246\n",
      "Training iteration: 1754\n",
      "Validation loss (no improvement): 0.04316600263118744\n",
      "Training iteration: 1755\n",
      "Validation loss (no improvement): 0.04310739040374756\n",
      "Training iteration: 1756\n",
      "Validation loss (no improvement): 0.04310773313045502\n",
      "Training iteration: 1757\n",
      "Validation loss (no improvement): 0.043132027983665465\n",
      "Training iteration: 1758\n",
      "Validation loss (no improvement): 0.04309079051017761\n",
      "Training iteration: 1759\n",
      "Validation loss (no improvement): 0.04308513700962067\n",
      "Training iteration: 1760\n",
      "Validation loss (no improvement): 0.043139657378196715\n",
      "Training iteration: 1761\n",
      "Improved validation loss from: 0.042934289574623107  to: 0.042859941720962524\n",
      "Training iteration: 1762\n",
      "Improved validation loss from: 0.042859941720962524  to: 0.04276663661003113\n",
      "Training iteration: 1763\n",
      "Validation loss (no improvement): 0.04291907250881195\n",
      "Training iteration: 1764\n",
      "Validation loss (no improvement): 0.043176716566085814\n",
      "Training iteration: 1765\n",
      "Validation loss (no improvement): 0.04290346503257751\n",
      "Training iteration: 1766\n",
      "Validation loss (no improvement): 0.042861375212669375\n",
      "Training iteration: 1767\n",
      "Validation loss (no improvement): 0.04283981323242188\n",
      "Training iteration: 1768\n",
      "Validation loss (no improvement): 0.0431790828704834\n",
      "Training iteration: 1769\n",
      "Validation loss (no improvement): 0.04295720160007477\n",
      "Training iteration: 1770\n",
      "Validation loss (no improvement): 0.042935752868652345\n",
      "Training iteration: 1771\n",
      "Validation loss (no improvement): 0.04302653670310974\n",
      "Training iteration: 1772\n",
      "Validation loss (no improvement): 0.043184703588485716\n",
      "Training iteration: 1773\n",
      "Validation loss (no improvement): 0.04300931990146637\n",
      "Training iteration: 1774\n",
      "Validation loss (no improvement): 0.04292331337928772\n",
      "Training iteration: 1775\n",
      "Validation loss (no improvement): 0.042987316846847534\n",
      "Training iteration: 1776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.043236321210861205\n",
      "Training iteration: 1777\n",
      "Validation loss (no improvement): 0.04309158325195313\n",
      "Training iteration: 1778\n",
      "Validation loss (no improvement): 0.04289405345916748\n",
      "Training iteration: 1779\n",
      "Validation loss (no improvement): 0.04284311830997467\n",
      "Training iteration: 1780\n",
      "Validation loss (no improvement): 0.04286814630031586\n",
      "Training iteration: 1781\n",
      "Validation loss (no improvement): 0.04283769130706787\n",
      "Training iteration: 1782\n",
      "Validation loss (no improvement): 0.042795950174331666\n",
      "Training iteration: 1783\n",
      "Improved validation loss from: 0.04276663661003113  to: 0.042721137404441833\n",
      "Training iteration: 1784\n",
      "Improved validation loss from: 0.042721137404441833  to: 0.04263500273227692\n",
      "Training iteration: 1785\n",
      "Validation loss (no improvement): 0.042648178339004514\n",
      "Training iteration: 1786\n",
      "Validation loss (no improvement): 0.04271024763584137\n",
      "Training iteration: 1787\n",
      "Improved validation loss from: 0.04263500273227692  to: 0.04246310293674469\n",
      "Training iteration: 1788\n",
      "Improved validation loss from: 0.04246310293674469  to: 0.04235712885856628\n",
      "Training iteration: 1789\n",
      "Validation loss (no improvement): 0.04241316318511963\n",
      "Training iteration: 1790\n",
      "Validation loss (no improvement): 0.042656469345092776\n",
      "Training iteration: 1791\n",
      "Improved validation loss from: 0.04235712885856628  to: 0.042344307899475096\n",
      "Training iteration: 1792\n",
      "Improved validation loss from: 0.042344307899475096  to: 0.04230428636074066\n",
      "Training iteration: 1793\n",
      "Validation loss (no improvement): 0.042361751198768616\n",
      "Training iteration: 1794\n",
      "Validation loss (no improvement): 0.042524537444114684\n",
      "Training iteration: 1795\n",
      "Validation loss (no improvement): 0.04247540533542633\n",
      "Training iteration: 1796\n",
      "Validation loss (no improvement): 0.042471927404403684\n",
      "Training iteration: 1797\n",
      "Validation loss (no improvement): 0.04253026843070984\n",
      "Training iteration: 1798\n",
      "Validation loss (no improvement): 0.042626065015792844\n",
      "Training iteration: 1799\n",
      "Validation loss (no improvement): 0.04262729585170746\n",
      "Training iteration: 1800\n",
      "Validation loss (no improvement): 0.04267293810844421\n",
      "Training iteration: 1801\n",
      "Validation loss (no improvement): 0.04270029067993164\n",
      "Training iteration: 1802\n",
      "Validation loss (no improvement): 0.042643803358078006\n",
      "Training iteration: 1803\n",
      "Validation loss (no improvement): 0.042580407857894895\n",
      "Training iteration: 1804\n",
      "Validation loss (no improvement): 0.04240266680717468\n",
      "Training iteration: 1805\n",
      "Validation loss (no improvement): 0.042355766892433165\n",
      "Training iteration: 1806\n",
      "Validation loss (no improvement): 0.0424140602350235\n",
      "Training iteration: 1807\n",
      "Validation loss (no improvement): 0.04258384704589844\n",
      "Training iteration: 1808\n",
      "Validation loss (no improvement): 0.04236283302307129\n",
      "Training iteration: 1809\n",
      "Validation loss (no improvement): 0.04238477647304535\n",
      "Training iteration: 1810\n",
      "Validation loss (no improvement): 0.042423000931739806\n",
      "Training iteration: 1811\n",
      "Validation loss (no improvement): 0.04259181916713715\n",
      "Training iteration: 1812\n",
      "Improved validation loss from: 0.04230428636074066  to: 0.04229784905910492\n",
      "Training iteration: 1813\n",
      "Improved validation loss from: 0.04229784905910492  to: 0.042202049493789674\n",
      "Training iteration: 1814\n",
      "Validation loss (no improvement): 0.04240572452545166\n",
      "Training iteration: 1815\n",
      "Validation loss (no improvement): 0.042684349417686465\n",
      "Training iteration: 1816\n",
      "Validation loss (no improvement): 0.04251958429813385\n",
      "Training iteration: 1817\n",
      "Validation loss (no improvement): 0.04249181151390076\n",
      "Training iteration: 1818\n",
      "Validation loss (no improvement): 0.0425461620092392\n",
      "Training iteration: 1819\n",
      "Validation loss (no improvement): 0.04265432357788086\n",
      "Training iteration: 1820\n",
      "Validation loss (no improvement): 0.04242618978023529\n",
      "Training iteration: 1821\n",
      "Validation loss (no improvement): 0.04240882396697998\n",
      "Training iteration: 1822\n",
      "Validation loss (no improvement): 0.042614293098449704\n",
      "Training iteration: 1823\n",
      "Validation loss (no improvement): 0.04269675314426422\n",
      "Training iteration: 1824\n",
      "Validation loss (no improvement): 0.04232636392116547\n",
      "Training iteration: 1825\n",
      "Validation loss (no improvement): 0.04229739308357239\n",
      "Training iteration: 1826\n",
      "Validation loss (no improvement): 0.04244177341461182\n",
      "Training iteration: 1827\n",
      "Validation loss (no improvement): 0.04253258109092713\n",
      "Training iteration: 1828\n",
      "Validation loss (no improvement): 0.04232452511787414\n",
      "Training iteration: 1829\n",
      "Improved validation loss from: 0.042202049493789674  to: 0.04219123721122742\n",
      "Training iteration: 1830\n",
      "Validation loss (no improvement): 0.04229077696800232\n",
      "Training iteration: 1831\n",
      "Validation loss (no improvement): 0.04251126348972321\n",
      "Training iteration: 1832\n",
      "Validation loss (no improvement): 0.04231361448764801\n",
      "Training iteration: 1833\n",
      "Validation loss (no improvement): 0.042351236939430235\n",
      "Training iteration: 1834\n",
      "Validation loss (no improvement): 0.042339032888412474\n",
      "Training iteration: 1835\n",
      "Validation loss (no improvement): 0.04227374196052551\n",
      "Training iteration: 1836\n",
      "Validation loss (no improvement): 0.04231862425804138\n",
      "Training iteration: 1837\n",
      "Improved validation loss from: 0.04219123721122742  to: 0.0420119434595108\n",
      "Training iteration: 1838\n",
      "Improved validation loss from: 0.0420119434595108  to: 0.04190623164176941\n",
      "Training iteration: 1839\n",
      "Validation loss (no improvement): 0.04207201898097992\n",
      "Training iteration: 1840\n",
      "Validation loss (no improvement): 0.042320233583450315\n",
      "Training iteration: 1841\n",
      "Validation loss (no improvement): 0.04237262308597565\n",
      "Training iteration: 1842\n",
      "Validation loss (no improvement): 0.04241395890712738\n",
      "Training iteration: 1843\n",
      "Validation loss (no improvement): 0.04228913187980652\n",
      "Training iteration: 1844\n",
      "Validation loss (no improvement): 0.04232028424739838\n",
      "Training iteration: 1845\n",
      "Validation loss (no improvement): 0.042189115285873414\n",
      "Training iteration: 1846\n",
      "Validation loss (no improvement): 0.042113906145095824\n",
      "Training iteration: 1847\n",
      "Validation loss (no improvement): 0.04214658737182617\n",
      "Training iteration: 1848\n",
      "Validation loss (no improvement): 0.04219137728214264\n",
      "Training iteration: 1849\n",
      "Validation loss (no improvement): 0.0422268807888031\n",
      "Training iteration: 1850\n",
      "Validation loss (no improvement): 0.04205390810966492\n",
      "Training iteration: 1851\n",
      "Validation loss (no improvement): 0.04199400842189789\n",
      "Training iteration: 1852\n",
      "Validation loss (no improvement): 0.04210495352745056\n",
      "Training iteration: 1853\n",
      "Validation loss (no improvement): 0.04237351417541504\n",
      "Training iteration: 1854\n",
      "Validation loss (no improvement): 0.04205949902534485\n",
      "Training iteration: 1855\n",
      "Validation loss (no improvement): 0.04197742342948914\n",
      "Training iteration: 1856\n",
      "Validation loss (no improvement): 0.041969937086105344\n",
      "Training iteration: 1857\n",
      "Validation loss (no improvement): 0.04207475185394287\n",
      "Training iteration: 1858\n",
      "Improved validation loss from: 0.04190623164176941  to: 0.04190316200256348\n",
      "Training iteration: 1859\n",
      "Improved validation loss from: 0.04190316200256348  to: 0.04181860089302063\n",
      "Training iteration: 1860\n",
      "Validation loss (no improvement): 0.04185609221458435\n",
      "Training iteration: 1861\n",
      "Validation loss (no improvement): 0.04192148149013519\n",
      "Training iteration: 1862\n",
      "Validation loss (no improvement): 0.04198478162288666\n",
      "Training iteration: 1863\n",
      "Improved validation loss from: 0.04181860089302063  to: 0.04178885519504547\n",
      "Training iteration: 1864\n",
      "Improved validation loss from: 0.04178885519504547  to: 0.04174729287624359\n",
      "Training iteration: 1865\n",
      "Validation loss (no improvement): 0.0418008029460907\n",
      "Training iteration: 1866\n",
      "Validation loss (no improvement): 0.04200493693351746\n",
      "Training iteration: 1867\n",
      "Validation loss (no improvement): 0.04185851514339447\n",
      "Training iteration: 1868\n",
      "Validation loss (no improvement): 0.0418046623468399\n",
      "Training iteration: 1869\n",
      "Validation loss (no improvement): 0.04180495142936706\n",
      "Training iteration: 1870\n",
      "Validation loss (no improvement): 0.04195600152015686\n",
      "Training iteration: 1871\n",
      "Validation loss (no improvement): 0.041854724287986755\n",
      "Training iteration: 1872\n",
      "Validation loss (no improvement): 0.04186306893825531\n",
      "Training iteration: 1873\n",
      "Validation loss (no improvement): 0.04193158149719238\n",
      "Training iteration: 1874\n",
      "Validation loss (no improvement): 0.042015662789344786\n",
      "Training iteration: 1875\n",
      "Validation loss (no improvement): 0.04212849736213684\n",
      "Training iteration: 1876\n",
      "Validation loss (no improvement): 0.041844844818115234\n",
      "Training iteration: 1877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.04174729287624359  to: 0.04171294569969177\n",
      "Training iteration: 1878\n",
      "Validation loss (no improvement): 0.04180039465427399\n",
      "Training iteration: 1879\n",
      "Validation loss (no improvement): 0.041987499594688414\n",
      "Training iteration: 1880\n",
      "Validation loss (no improvement): 0.041828671097755434\n",
      "Training iteration: 1881\n",
      "Validation loss (no improvement): 0.0418963760137558\n",
      "Training iteration: 1882\n",
      "Validation loss (no improvement): 0.042073231935501096\n",
      "Training iteration: 1883\n",
      "Validation loss (no improvement): 0.04229077696800232\n",
      "Training iteration: 1884\n",
      "Validation loss (no improvement): 0.04200529456138611\n",
      "Training iteration: 1885\n",
      "Validation loss (no improvement): 0.041783961653709414\n",
      "Training iteration: 1886\n",
      "Improved validation loss from: 0.04171294569969177  to: 0.04167830348014832\n",
      "Training iteration: 1887\n",
      "Validation loss (no improvement): 0.041748923063278195\n",
      "Training iteration: 1888\n",
      "Validation loss (no improvement): 0.04186348915100098\n",
      "Training iteration: 1889\n",
      "Validation loss (no improvement): 0.041774940490722653\n",
      "Training iteration: 1890\n",
      "Validation loss (no improvement): 0.04169426560401916\n",
      "Training iteration: 1891\n",
      "Validation loss (no improvement): 0.04176217615604401\n",
      "Training iteration: 1892\n",
      "Validation loss (no improvement): 0.042005148530006406\n",
      "Training iteration: 1893\n",
      "Validation loss (no improvement): 0.041686263680458066\n",
      "Training iteration: 1894\n",
      "Improved validation loss from: 0.04167830348014832  to: 0.04149332940578461\n",
      "Training iteration: 1895\n",
      "Validation loss (no improvement): 0.04156374931335449\n",
      "Training iteration: 1896\n",
      "Validation loss (no improvement): 0.04167261719703674\n",
      "Training iteration: 1897\n",
      "Validation loss (no improvement): 0.04164827466011047\n",
      "Training iteration: 1898\n",
      "Validation loss (no improvement): 0.04162489771842957\n",
      "Training iteration: 1899\n",
      "Validation loss (no improvement): 0.041637292504310607\n",
      "Training iteration: 1900\n",
      "Validation loss (no improvement): 0.04175441861152649\n",
      "Training iteration: 1901\n",
      "Validation loss (no improvement): 0.041528350114822386\n",
      "Training iteration: 1902\n",
      "Improved validation loss from: 0.04149332940578461  to: 0.041430941224098204\n",
      "Training iteration: 1903\n",
      "Validation loss (no improvement): 0.04159363806247711\n",
      "Training iteration: 1904\n",
      "Validation loss (no improvement): 0.041865605115890506\n",
      "Training iteration: 1905\n",
      "Validation loss (no improvement): 0.04169846475124359\n",
      "Training iteration: 1906\n",
      "Validation loss (no improvement): 0.041642627120018004\n",
      "Training iteration: 1907\n",
      "Validation loss (no improvement): 0.04171590209007263\n",
      "Training iteration: 1908\n",
      "Validation loss (no improvement): 0.04178827404975891\n",
      "Training iteration: 1909\n",
      "Validation loss (no improvement): 0.04170191287994385\n",
      "Training iteration: 1910\n",
      "Validation loss (no improvement): 0.04159744381904602\n",
      "Training iteration: 1911\n",
      "Validation loss (no improvement): 0.04160242080688477\n",
      "Training iteration: 1912\n",
      "Validation loss (no improvement): 0.04169805943965912\n",
      "Training iteration: 1913\n",
      "Validation loss (no improvement): 0.041646677255630496\n",
      "Training iteration: 1914\n",
      "Validation loss (no improvement): 0.04154026508331299\n",
      "Training iteration: 1915\n",
      "Validation loss (no improvement): 0.041521602869033815\n",
      "Training iteration: 1916\n",
      "Improved validation loss from: 0.041430941224098204  to: 0.04135019779205322\n",
      "Training iteration: 1917\n",
      "Validation loss (no improvement): 0.041376131772995\n",
      "Training iteration: 1918\n",
      "Validation loss (no improvement): 0.04144667685031891\n",
      "Training iteration: 1919\n",
      "Validation loss (no improvement): 0.04155615866184235\n",
      "Training iteration: 1920\n",
      "Validation loss (no improvement): 0.041362470388412474\n",
      "Training iteration: 1921\n",
      "Improved validation loss from: 0.04135019779205322  to: 0.04122994840145111\n",
      "Training iteration: 1922\n",
      "Validation loss (no improvement): 0.0413828045129776\n",
      "Training iteration: 1923\n",
      "Validation loss (no improvement): 0.0415912002325058\n",
      "Training iteration: 1924\n",
      "Validation loss (no improvement): 0.04148360788822174\n",
      "Training iteration: 1925\n",
      "Validation loss (no improvement): 0.04127277433872223\n",
      "Training iteration: 1926\n",
      "Validation loss (no improvement): 0.04126161932945251\n",
      "Training iteration: 1927\n",
      "Validation loss (no improvement): 0.041364353895187375\n",
      "Training iteration: 1928\n",
      "Validation loss (no improvement): 0.041470497846603394\n",
      "Training iteration: 1929\n",
      "Validation loss (no improvement): 0.04140454232692718\n",
      "Training iteration: 1930\n",
      "Validation loss (no improvement): 0.04139846265316009\n",
      "Training iteration: 1931\n",
      "Validation loss (no improvement): 0.04154105186462402\n",
      "Training iteration: 1932\n",
      "Validation loss (no improvement): 0.041460394859313965\n",
      "Training iteration: 1933\n",
      "Validation loss (no improvement): 0.04143302440643311\n",
      "Training iteration: 1934\n",
      "Validation loss (no improvement): 0.04149309992790222\n",
      "Training iteration: 1935\n",
      "Validation loss (no improvement): 0.04152283668518066\n",
      "Training iteration: 1936\n",
      "Validation loss (no improvement): 0.04150007665157318\n",
      "Training iteration: 1937\n",
      "Validation loss (no improvement): 0.041489428281784056\n",
      "Training iteration: 1938\n",
      "Validation loss (no improvement): 0.04133753776550293\n",
      "Training iteration: 1939\n",
      "Validation loss (no improvement): 0.04127162396907806\n",
      "Training iteration: 1940\n",
      "Validation loss (no improvement): 0.04130329191684723\n",
      "Training iteration: 1941\n",
      "Validation loss (no improvement): 0.041423702239990236\n",
      "Training iteration: 1942\n",
      "Validation loss (no improvement): 0.04130219519138336\n",
      "Training iteration: 1943\n",
      "Improved validation loss from: 0.04122994840145111  to: 0.041139346361160276\n",
      "Training iteration: 1944\n",
      "Validation loss (no improvement): 0.041142520308494565\n",
      "Training iteration: 1945\n",
      "Validation loss (no improvement): 0.041277655959129335\n",
      "Training iteration: 1946\n",
      "Validation loss (no improvement): 0.041420990228652955\n",
      "Training iteration: 1947\n",
      "Validation loss (no improvement): 0.04132745862007141\n",
      "Training iteration: 1948\n",
      "Validation loss (no improvement): 0.041257017850875856\n",
      "Training iteration: 1949\n",
      "Validation loss (no improvement): 0.041227024793624875\n",
      "Training iteration: 1950\n",
      "Validation loss (no improvement): 0.041273146867752075\n",
      "Training iteration: 1951\n",
      "Improved validation loss from: 0.041139346361160276  to: 0.04113645553588867\n",
      "Training iteration: 1952\n",
      "Improved validation loss from: 0.04113645553588867  to: 0.04111031889915466\n",
      "Training iteration: 1953\n",
      "Validation loss (no improvement): 0.041120749711990354\n",
      "Training iteration: 1954\n",
      "Validation loss (no improvement): 0.04116533696651459\n",
      "Training iteration: 1955\n",
      "Validation loss (no improvement): 0.041215094923973086\n",
      "Training iteration: 1956\n",
      "Improved validation loss from: 0.04111031889915466  to: 0.0410790354013443\n",
      "Training iteration: 1957\n",
      "Improved validation loss from: 0.0410790354013443  to: 0.04101698398590088\n",
      "Training iteration: 1958\n",
      "Validation loss (no improvement): 0.04108619689941406\n",
      "Training iteration: 1959\n",
      "Validation loss (no improvement): 0.04129720330238342\n",
      "Training iteration: 1960\n",
      "Validation loss (no improvement): 0.041385531425476074\n",
      "Training iteration: 1961\n",
      "Validation loss (no improvement): 0.041136893630027774\n",
      "Training iteration: 1962\n",
      "Improved validation loss from: 0.04101698398590088  to: 0.040984305739402774\n",
      "Training iteration: 1963\n",
      "Validation loss (no improvement): 0.041036438941955564\n",
      "Training iteration: 1964\n",
      "Validation loss (no improvement): 0.04164704382419586\n",
      "Training iteration: 1965\n",
      "Validation loss (no improvement): 0.041400179266929626\n",
      "Training iteration: 1966\n",
      "Validation loss (no improvement): 0.04111860692501068\n",
      "Training iteration: 1967\n",
      "Validation loss (no improvement): 0.041259351372718814\n",
      "Training iteration: 1968\n",
      "Validation loss (no improvement): 0.04166719019412994\n",
      "Training iteration: 1969\n",
      "Validation loss (no improvement): 0.0420276939868927\n",
      "Training iteration: 1970\n",
      "Validation loss (no improvement): 0.041710454225540164\n",
      "Training iteration: 1971\n",
      "Validation loss (no improvement): 0.04139910638332367\n",
      "Training iteration: 1972\n",
      "Validation loss (no improvement): 0.041272124648094176\n",
      "Training iteration: 1973\n",
      "Validation loss (no improvement): 0.04154918789863586\n",
      "Training iteration: 1974\n",
      "Validation loss (no improvement): 0.041658297181129456\n",
      "Training iteration: 1975\n",
      "Validation loss (no improvement): 0.04138436317443848\n",
      "Training iteration: 1976\n",
      "Validation loss (no improvement): 0.04133215844631195\n",
      "Training iteration: 1977\n",
      "Validation loss (no improvement): 0.04160164296627045\n",
      "Training iteration: 1978\n",
      "Validation loss (no improvement): 0.04179181158542633\n",
      "Training iteration: 1979\n",
      "Validation loss (no improvement): 0.04157093167304993\n",
      "Training iteration: 1980\n",
      "Validation loss (no improvement): 0.04132718443870544\n",
      "Training iteration: 1981\n",
      "Validation loss (no improvement): 0.04117042124271393\n",
      "Training iteration: 1982\n",
      "Validation loss (no improvement): 0.04121887683868408\n",
      "Training iteration: 1983\n",
      "Validation loss (no improvement): 0.04127771258354187\n",
      "Training iteration: 1984\n",
      "Validation loss (no improvement): 0.041003718972206116\n",
      "Training iteration: 1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.040984305739402774  to: 0.040971598029136656\n",
      "Training iteration: 1986\n",
      "Improved validation loss from: 0.040971598029136656  to: 0.040965494513511655\n",
      "Training iteration: 1987\n",
      "Improved validation loss from: 0.040965494513511655  to: 0.04091826379299164\n",
      "Training iteration: 1988\n",
      "Improved validation loss from: 0.04091826379299164  to: 0.04064041674137116\n",
      "Training iteration: 1989\n",
      "Validation loss (no improvement): 0.040646728873252866\n",
      "Training iteration: 1990\n",
      "Validation loss (no improvement): 0.040917468070983884\n",
      "Training iteration: 1991\n",
      "Validation loss (no improvement): 0.04075906872749328\n",
      "Training iteration: 1992\n",
      "Validation loss (no improvement): 0.040735656023025514\n",
      "Training iteration: 1993\n",
      "Validation loss (no improvement): 0.040755876898765565\n",
      "Training iteration: 1994\n",
      "Validation loss (no improvement): 0.04093579649925232\n",
      "Training iteration: 1995\n",
      "Improved validation loss from: 0.04064041674137116  to: 0.040578094124794004\n",
      "Training iteration: 1996\n",
      "Improved validation loss from: 0.040578094124794004  to: 0.04034713208675385\n",
      "Training iteration: 1997\n",
      "Validation loss (no improvement): 0.040418806672096255\n",
      "Training iteration: 1998\n",
      "Validation loss (no improvement): 0.04054274559020996\n",
      "Training iteration: 1999\n",
      "Validation loss (no improvement): 0.040588158369064334\n",
      "Training iteration: 2000\n",
      "Improved validation loss from: 0.04034713208675385  to: 0.04034420847892761\n",
      "Training iteration: 2001\n",
      "Improved validation loss from: 0.04034420847892761  to: 0.04026626646518707\n",
      "Training iteration: 2002\n",
      "Validation loss (no improvement): 0.04033673405647278\n",
      "Training iteration: 2003\n",
      "Validation loss (no improvement): 0.04036117196083069\n",
      "Training iteration: 2004\n",
      "Validation loss (no improvement): 0.04031566679477692\n",
      "Training iteration: 2005\n",
      "Validation loss (no improvement): 0.040290698409080505\n",
      "Training iteration: 2006\n",
      "Validation loss (no improvement): 0.040291032195091246\n",
      "Training iteration: 2007\n",
      "Validation loss (no improvement): 0.04030672609806061\n",
      "Training iteration: 2008\n",
      "Improved validation loss from: 0.04026626646518707  to: 0.04000803530216217\n",
      "Training iteration: 2009\n",
      "Improved validation loss from: 0.04000803530216217  to: 0.039872124791145325\n",
      "Training iteration: 2010\n",
      "Validation loss (no improvement): 0.03989399671554565\n",
      "Training iteration: 2011\n",
      "Validation loss (no improvement): 0.039981549978256224\n",
      "Training iteration: 2012\n",
      "Validation loss (no improvement): 0.039929404854774475\n",
      "Training iteration: 2013\n",
      "Validation loss (no improvement): 0.03993433117866516\n",
      "Training iteration: 2014\n",
      "Validation loss (no improvement): 0.03997503817081451\n",
      "Training iteration: 2015\n",
      "Validation loss (no improvement): 0.03988954126834869\n",
      "Training iteration: 2016\n",
      "Improved validation loss from: 0.039872124791145325  to: 0.03981322944164276\n",
      "Training iteration: 2017\n",
      "Validation loss (no improvement): 0.03983793556690216\n",
      "Training iteration: 2018\n",
      "Validation loss (no improvement): 0.039928898215293884\n",
      "Training iteration: 2019\n",
      "Improved validation loss from: 0.03981322944164276  to: 0.03974489867687225\n",
      "Training iteration: 2020\n",
      "Improved validation loss from: 0.03974489867687225  to: 0.0396824836730957\n",
      "Training iteration: 2021\n",
      "Validation loss (no improvement): 0.03974317610263824\n",
      "Training iteration: 2022\n",
      "Validation loss (no improvement): 0.03990815281867981\n",
      "Training iteration: 2023\n",
      "Validation loss (no improvement): 0.03986353278160095\n",
      "Training iteration: 2024\n",
      "Validation loss (no improvement): 0.03989900946617127\n",
      "Training iteration: 2025\n",
      "Validation loss (no improvement): 0.03989811539649964\n",
      "Training iteration: 2026\n",
      "Validation loss (no improvement): 0.03997843861579895\n",
      "Training iteration: 2027\n",
      "Validation loss (no improvement): 0.04006076455116272\n",
      "Training iteration: 2028\n",
      "Improved validation loss from: 0.0396824836730957  to: 0.03963734805583954\n",
      "Training iteration: 2029\n",
      "Improved validation loss from: 0.03963734805583954  to: 0.03948041796684265\n",
      "Training iteration: 2030\n",
      "Validation loss (no improvement): 0.03975686132907867\n",
      "Training iteration: 2031\n",
      "Validation loss (no improvement): 0.04012798666954041\n",
      "Training iteration: 2032\n",
      "Validation loss (no improvement): 0.03964771628379822\n",
      "Training iteration: 2033\n",
      "Validation loss (no improvement): 0.03960248529911041\n",
      "Training iteration: 2034\n",
      "Validation loss (no improvement): 0.03965453505516052\n",
      "Training iteration: 2035\n",
      "Validation loss (no improvement): 0.039995431900024414\n",
      "Training iteration: 2036\n",
      "Validation loss (no improvement): 0.03993404507637024\n",
      "Training iteration: 2037\n",
      "Validation loss (no improvement): 0.03980019390583038\n",
      "Training iteration: 2038\n",
      "Validation loss (no improvement): 0.039978903532028195\n",
      "Training iteration: 2039\n",
      "Validation loss (no improvement): 0.04021604657173157\n",
      "Training iteration: 2040\n",
      "Validation loss (no improvement): 0.04005358815193176\n",
      "Training iteration: 2041\n",
      "Validation loss (no improvement): 0.03980852663516998\n",
      "Training iteration: 2042\n",
      "Validation loss (no improvement): 0.03976861536502838\n",
      "Training iteration: 2043\n",
      "Validation loss (no improvement): 0.03982635736465454\n",
      "Training iteration: 2044\n",
      "Validation loss (no improvement): 0.039970111846923825\n",
      "Training iteration: 2045\n",
      "Validation loss (no improvement): 0.03991919159889221\n",
      "Training iteration: 2046\n",
      "Validation loss (no improvement): 0.039830952882766724\n",
      "Training iteration: 2047\n",
      "Validation loss (no improvement): 0.039715471863746646\n",
      "Training iteration: 2048\n",
      "Validation loss (no improvement): 0.03962090611457825\n",
      "Training iteration: 2049\n",
      "Validation loss (no improvement): 0.03956848084926605\n",
      "Training iteration: 2050\n",
      "Improved validation loss from: 0.03948041796684265  to: 0.039251822233200076\n",
      "Training iteration: 2051\n",
      "Improved validation loss from: 0.039251822233200076  to: 0.03911601603031158\n",
      "Training iteration: 2052\n",
      "Validation loss (no improvement): 0.03917352557182312\n",
      "Training iteration: 2053\n",
      "Validation loss (no improvement): 0.0391323983669281\n",
      "Training iteration: 2054\n",
      "Improved validation loss from: 0.03911601603031158  to: 0.03892415165901184\n",
      "Training iteration: 2055\n",
      "Improved validation loss from: 0.03892415165901184  to: 0.0388790100812912\n",
      "Training iteration: 2056\n",
      "Validation loss (no improvement): 0.039077141880989076\n",
      "Training iteration: 2057\n",
      "Validation loss (no improvement): 0.03946002423763275\n",
      "Training iteration: 2058\n",
      "Validation loss (no improvement): 0.039167919754981996\n",
      "Training iteration: 2059\n",
      "Validation loss (no improvement): 0.039079874753952026\n",
      "Training iteration: 2060\n",
      "Validation loss (no improvement): 0.03920599520206451\n",
      "Training iteration: 2061\n",
      "Validation loss (no improvement): 0.03926818370819092\n",
      "Training iteration: 2062\n",
      "Validation loss (no improvement): 0.039201751351356506\n",
      "Training iteration: 2063\n",
      "Validation loss (no improvement): 0.039158877730369565\n",
      "Training iteration: 2064\n",
      "Validation loss (no improvement): 0.039125919342041016\n",
      "Training iteration: 2065\n",
      "Validation loss (no improvement): 0.03896874785423279\n",
      "Training iteration: 2066\n",
      "Validation loss (no improvement): 0.03890963196754456\n",
      "Training iteration: 2067\n",
      "Validation loss (no improvement): 0.0389857143163681\n",
      "Training iteration: 2068\n",
      "Validation loss (no improvement): 0.03896947503089905\n",
      "Training iteration: 2069\n",
      "Improved validation loss from: 0.0388790100812912  to: 0.03881896138191223\n",
      "Training iteration: 2070\n",
      "Improved validation loss from: 0.03881896138191223  to: 0.03877472281455994\n",
      "Training iteration: 2071\n",
      "Validation loss (no improvement): 0.038790386915206906\n",
      "Training iteration: 2072\n",
      "Validation loss (no improvement): 0.03884583115577698\n",
      "Training iteration: 2073\n",
      "Improved validation loss from: 0.03877472281455994  to: 0.03868571221828461\n",
      "Training iteration: 2074\n",
      "Improved validation loss from: 0.03868571221828461  to: 0.03861915171146393\n",
      "Training iteration: 2075\n",
      "Validation loss (no improvement): 0.0388004869222641\n",
      "Training iteration: 2076\n",
      "Validation loss (no improvement): 0.03916991353034973\n",
      "Training iteration: 2077\n",
      "Validation loss (no improvement): 0.03878537118434906\n",
      "Training iteration: 2078\n",
      "Validation loss (no improvement): 0.03864467740058899\n",
      "Training iteration: 2079\n",
      "Validation loss (no improvement): 0.038947421312332156\n",
      "Training iteration: 2080\n",
      "Validation loss (no improvement): 0.0393401950597763\n",
      "Training iteration: 2081\n",
      "Validation loss (no improvement): 0.03898668885231018\n",
      "Training iteration: 2082\n",
      "Validation loss (no improvement): 0.03879025280475616\n",
      "Training iteration: 2083\n",
      "Validation loss (no improvement): 0.03903898298740387\n",
      "Training iteration: 2084\n",
      "Validation loss (no improvement): 0.03948047757148743\n",
      "Training iteration: 2085\n",
      "Validation loss (no improvement): 0.03909599483013153\n",
      "Training iteration: 2086\n",
      "Validation loss (no improvement): 0.038970866799354555\n",
      "Training iteration: 2087\n",
      "Validation loss (no improvement): 0.039163604378700256\n",
      "Training iteration: 2088\n",
      "Validation loss (no improvement): 0.03951157033443451\n",
      "Training iteration: 2089\n",
      "Validation loss (no improvement): 0.03903575241565704\n",
      "Training iteration: 2090\n",
      "Validation loss (no improvement): 0.03896292746067047\n",
      "Training iteration: 2091\n",
      "Validation loss (no improvement): 0.03903552293777466\n",
      "Training iteration: 2092\n",
      "Validation loss (no improvement): 0.03959026336669922\n",
      "Training iteration: 2093\n",
      "Validation loss (no improvement): 0.039292937517166136\n",
      "Training iteration: 2094\n",
      "Validation loss (no improvement): 0.038991805911064145\n",
      "Training iteration: 2095\n",
      "Validation loss (no improvement): 0.03906714022159576\n",
      "Training iteration: 2096\n",
      "Validation loss (no improvement): 0.039392787218093875\n",
      "Training iteration: 2097\n",
      "Validation loss (no improvement): 0.03937754333019257\n",
      "Training iteration: 2098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.0389580100774765\n",
      "Training iteration: 2099\n",
      "Validation loss (no improvement): 0.03877098560333252\n",
      "Training iteration: 2100\n",
      "Validation loss (no improvement): 0.03896669745445251\n",
      "Training iteration: 2101\n",
      "Validation loss (no improvement): 0.03928344249725342\n",
      "Training iteration: 2102\n",
      "Validation loss (no improvement): 0.038956943154335025\n",
      "Training iteration: 2103\n",
      "Validation loss (no improvement): 0.03891846537590027\n",
      "Training iteration: 2104\n",
      "Validation loss (no improvement): 0.03895155489444733\n",
      "Training iteration: 2105\n",
      "Validation loss (no improvement): 0.03931276500225067\n",
      "Training iteration: 2106\n",
      "Validation loss (no improvement): 0.03874163627624512\n",
      "Training iteration: 2107\n",
      "Improved validation loss from: 0.03861915171146393  to: 0.03850160539150238\n",
      "Training iteration: 2108\n",
      "Validation loss (no improvement): 0.0386883556842804\n",
      "Training iteration: 2109\n",
      "Validation loss (no improvement): 0.03885785043239594\n",
      "Training iteration: 2110\n",
      "Validation loss (no improvement): 0.03857738077640534\n",
      "Training iteration: 2111\n",
      "Validation loss (no improvement): 0.03853926956653595\n",
      "Training iteration: 2112\n",
      "Validation loss (no improvement): 0.03859472870826721\n",
      "Training iteration: 2113\n",
      "Validation loss (no improvement): 0.03877537846565247\n",
      "Training iteration: 2114\n",
      "Improved validation loss from: 0.03850160539150238  to: 0.038500803709030154\n",
      "Training iteration: 2115\n",
      "Improved validation loss from: 0.038500803709030154  to: 0.038363152742385866\n",
      "Training iteration: 2116\n",
      "Validation loss (no improvement): 0.03846496045589447\n",
      "Training iteration: 2117\n",
      "Validation loss (no improvement): 0.038681036233901976\n",
      "Training iteration: 2118\n",
      "Validation loss (no improvement): 0.03852805197238922\n",
      "Training iteration: 2119\n",
      "Improved validation loss from: 0.038363152742385866  to: 0.038222378492355345\n",
      "Training iteration: 2120\n",
      "Validation loss (no improvement): 0.03825239837169647\n",
      "Training iteration: 2121\n",
      "Validation loss (no improvement): 0.038393497467041016\n",
      "Training iteration: 2122\n",
      "Validation loss (no improvement): 0.03822552561759949\n",
      "Training iteration: 2123\n",
      "Improved validation loss from: 0.038222378492355345  to: 0.038107562065124514\n",
      "Training iteration: 2124\n",
      "Improved validation loss from: 0.038107562065124514  to: 0.038095733523368834\n",
      "Training iteration: 2125\n",
      "Validation loss (no improvement): 0.038153433799743654\n",
      "Training iteration: 2126\n",
      "Improved validation loss from: 0.038095733523368834  to: 0.038074129819869997\n",
      "Training iteration: 2127\n",
      "Improved validation loss from: 0.038074129819869997  to: 0.0378623902797699\n",
      "Training iteration: 2128\n",
      "Improved validation loss from: 0.0378623902797699  to: 0.03773740530014038\n",
      "Training iteration: 2129\n",
      "Validation loss (no improvement): 0.03773845136165619\n",
      "Training iteration: 2130\n",
      "Validation loss (no improvement): 0.038001826405525206\n",
      "Training iteration: 2131\n",
      "Validation loss (no improvement): 0.03794825077056885\n",
      "Training iteration: 2132\n",
      "Validation loss (no improvement): 0.037967315316200255\n",
      "Training iteration: 2133\n",
      "Validation loss (no improvement): 0.03811989426612854\n",
      "Training iteration: 2134\n",
      "Validation loss (no improvement): 0.038271600008010866\n",
      "Training iteration: 2135\n",
      "Validation loss (no improvement): 0.03813530802726746\n",
      "Training iteration: 2136\n",
      "Validation loss (no improvement): 0.03804108500480652\n",
      "Training iteration: 2137\n",
      "Validation loss (no improvement): 0.037881973385810855\n",
      "Training iteration: 2138\n",
      "Improved validation loss from: 0.03773740530014038  to: 0.03769582509994507\n",
      "Training iteration: 2139\n",
      "Validation loss (no improvement): 0.03770269453525543\n",
      "Training iteration: 2140\n",
      "Improved validation loss from: 0.03769582509994507  to: 0.037542545795440675\n",
      "Training iteration: 2141\n",
      "Validation loss (no improvement): 0.03754965960979462\n",
      "Training iteration: 2142\n",
      "Improved validation loss from: 0.037542545795440675  to: 0.03753893375396729\n",
      "Training iteration: 2143\n",
      "Validation loss (no improvement): 0.03762736916542053\n",
      "Training iteration: 2144\n",
      "Validation loss (no improvement): 0.03772821426391602\n",
      "Training iteration: 2145\n",
      "Improved validation loss from: 0.03753893375396729  to: 0.037538358569145204\n",
      "Training iteration: 2146\n",
      "Improved validation loss from: 0.037538358569145204  to: 0.03749788999557495\n",
      "Training iteration: 2147\n",
      "Validation loss (no improvement): 0.03754716515541077\n",
      "Training iteration: 2148\n",
      "Validation loss (no improvement): 0.037616825103759764\n",
      "Training iteration: 2149\n",
      "Validation loss (no improvement): 0.037620139122009275\n",
      "Training iteration: 2150\n",
      "Improved validation loss from: 0.03749788999557495  to: 0.03742480278015137\n",
      "Training iteration: 2151\n",
      "Improved validation loss from: 0.03742480278015137  to: 0.03737679421901703\n",
      "Training iteration: 2152\n",
      "Validation loss (no improvement): 0.037473225593566896\n",
      "Training iteration: 2153\n",
      "Validation loss (no improvement): 0.03762632310390472\n",
      "Training iteration: 2154\n",
      "Improved validation loss from: 0.03737679421901703  to: 0.03720411658287048\n",
      "Training iteration: 2155\n",
      "Improved validation loss from: 0.03720411658287048  to: 0.03712294399738312\n",
      "Training iteration: 2156\n",
      "Validation loss (no improvement): 0.037479671835899356\n",
      "Training iteration: 2157\n",
      "Validation loss (no improvement): 0.0379289299249649\n",
      "Training iteration: 2158\n",
      "Validation loss (no improvement): 0.0374144434928894\n",
      "Training iteration: 2159\n",
      "Validation loss (no improvement): 0.03731160461902618\n",
      "Training iteration: 2160\n",
      "Validation loss (no improvement): 0.03750166893005371\n",
      "Training iteration: 2161\n",
      "Validation loss (no improvement): 0.03807154297828674\n",
      "Training iteration: 2162\n",
      "Validation loss (no improvement): 0.03757835328578949\n",
      "Training iteration: 2163\n",
      "Validation loss (no improvement): 0.037384039163589476\n",
      "Training iteration: 2164\n",
      "Validation loss (no improvement): 0.03761577904224396\n",
      "Training iteration: 2165\n",
      "Validation loss (no improvement): 0.03800649642944336\n",
      "Training iteration: 2166\n",
      "Validation loss (no improvement): 0.03786188960075378\n",
      "Training iteration: 2167\n",
      "Validation loss (no improvement): 0.03779719471931457\n",
      "Training iteration: 2168\n",
      "Validation loss (no improvement): 0.03782747387886047\n",
      "Training iteration: 2169\n",
      "Validation loss (no improvement): 0.0379979133605957\n",
      "Training iteration: 2170\n",
      "Validation loss (no improvement): 0.03789295256137848\n",
      "Training iteration: 2171\n",
      "Validation loss (no improvement): 0.037712326645851134\n",
      "Training iteration: 2172\n",
      "Validation loss (no improvement): 0.03766036629676819\n",
      "Training iteration: 2173\n",
      "Validation loss (no improvement): 0.037629663944244385\n",
      "Training iteration: 2174\n",
      "Validation loss (no improvement): 0.03745973706245422\n",
      "Training iteration: 2175\n",
      "Validation loss (no improvement): 0.03734051585197449\n",
      "Training iteration: 2176\n",
      "Validation loss (no improvement): 0.03718302249908447\n",
      "Training iteration: 2177\n",
      "Improved validation loss from: 0.03712294399738312  to: 0.03708134293556213\n",
      "Training iteration: 2178\n",
      "Improved validation loss from: 0.03708134293556213  to: 0.0368712306022644\n",
      "Training iteration: 2179\n",
      "Improved validation loss from: 0.0368712306022644  to: 0.03681010603904724\n",
      "Training iteration: 2180\n",
      "Validation loss (no improvement): 0.03691631555557251\n",
      "Training iteration: 2181\n",
      "Improved validation loss from: 0.03681010603904724  to: 0.036803397536277774\n",
      "Training iteration: 2182\n",
      "Improved validation loss from: 0.036803397536277774  to: 0.036681216955184934\n",
      "Training iteration: 2183\n",
      "Improved validation loss from: 0.036681216955184934  to: 0.036636799573898315\n",
      "Training iteration: 2184\n",
      "Validation loss (no improvement): 0.036754930019378663\n",
      "Training iteration: 2185\n",
      "Improved validation loss from: 0.036636799573898315  to: 0.03649615943431854\n",
      "Training iteration: 2186\n",
      "Improved validation loss from: 0.03649615943431854  to: 0.0364318311214447\n",
      "Training iteration: 2187\n",
      "Validation loss (no improvement): 0.036520248651504515\n",
      "Training iteration: 2188\n",
      "Validation loss (no improvement): 0.03663387894630432\n",
      "Training iteration: 2189\n",
      "Validation loss (no improvement): 0.03662547469139099\n",
      "Training iteration: 2190\n",
      "Validation loss (no improvement): 0.036680832505226135\n",
      "Training iteration: 2191\n",
      "Validation loss (no improvement): 0.036486309766769406\n",
      "Training iteration: 2192\n",
      "Validation loss (no improvement): 0.03648043870925903\n",
      "Training iteration: 2193\n",
      "Validation loss (no improvement): 0.03664244413375854\n",
      "Training iteration: 2194\n",
      "Validation loss (no improvement): 0.03688486218452454\n",
      "Training iteration: 2195\n",
      "Validation loss (no improvement): 0.03657428026199341\n",
      "Training iteration: 2196\n",
      "Validation loss (no improvement): 0.03653519153594971\n",
      "Training iteration: 2197\n",
      "Validation loss (no improvement): 0.03696019649505615\n",
      "Training iteration: 2198\n",
      "Validation loss (no improvement): 0.037376365065574645\n",
      "Training iteration: 2199\n",
      "Validation loss (no improvement): 0.03687910437583923\n",
      "Training iteration: 2200\n",
      "Validation loss (no improvement): 0.03687246441841126\n",
      "Training iteration: 2201\n",
      "Validation loss (no improvement): 0.0370360940694809\n",
      "Training iteration: 2202\n",
      "Validation loss (no improvement): 0.03754553198814392\n",
      "Training iteration: 2203\n",
      "Validation loss (no improvement): 0.037300515174865725\n",
      "Training iteration: 2204\n",
      "Validation loss (no improvement): 0.0372125506401062\n",
      "Training iteration: 2205\n",
      "Validation loss (no improvement): 0.0373323380947113\n",
      "Training iteration: 2206\n",
      "Validation loss (no improvement): 0.037492874264717105\n",
      "Training iteration: 2207\n",
      "Validation loss (no improvement): 0.03735756278038025\n",
      "Training iteration: 2208\n",
      "Validation loss (no improvement): 0.037239307165145875\n",
      "Training iteration: 2209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.037198790907859804\n",
      "Training iteration: 2210\n",
      "Validation loss (no improvement): 0.037172621488571166\n",
      "Training iteration: 2211\n",
      "Validation loss (no improvement): 0.03704265654087067\n",
      "Training iteration: 2212\n",
      "Validation loss (no improvement): 0.03697260320186615\n",
      "Training iteration: 2213\n",
      "Validation loss (no improvement): 0.036748528480529785\n",
      "Training iteration: 2214\n",
      "Validation loss (no improvement): 0.03666641712188721\n",
      "Training iteration: 2215\n",
      "Validation loss (no improvement): 0.03668930232524872\n",
      "Training iteration: 2216\n",
      "Improved validation loss from: 0.0364318311214447  to: 0.03638485074043274\n",
      "Training iteration: 2217\n",
      "Improved validation loss from: 0.03638485074043274  to: 0.03634697496891022\n",
      "Training iteration: 2218\n",
      "Validation loss (no improvement): 0.036454832553863524\n",
      "Training iteration: 2219\n",
      "Validation loss (no improvement): 0.036611175537109374\n",
      "Training iteration: 2220\n",
      "Improved validation loss from: 0.03634697496891022  to: 0.036114472150802615\n",
      "Training iteration: 2221\n",
      "Improved validation loss from: 0.036114472150802615  to: 0.03599388003349304\n",
      "Training iteration: 2222\n",
      "Validation loss (no improvement): 0.03633862137794495\n",
      "Training iteration: 2223\n",
      "Validation loss (no improvement): 0.036677560210227965\n",
      "Training iteration: 2224\n",
      "Validation loss (no improvement): 0.03613168001174927\n",
      "Training iteration: 2225\n",
      "Validation loss (no improvement): 0.03612138628959656\n",
      "Training iteration: 2226\n",
      "Validation loss (no improvement): 0.03632999062538147\n",
      "Training iteration: 2227\n",
      "Validation loss (no improvement): 0.0368544727563858\n",
      "Training iteration: 2228\n",
      "Validation loss (no improvement): 0.03663737177848816\n",
      "Training iteration: 2229\n",
      "Validation loss (no improvement): 0.03667309880256653\n",
      "Training iteration: 2230\n",
      "Validation loss (no improvement): 0.036831891536712645\n",
      "Training iteration: 2231\n",
      "Validation loss (no improvement): 0.03721562922000885\n",
      "Training iteration: 2232\n",
      "Validation loss (no improvement): 0.03690776228904724\n",
      "Training iteration: 2233\n",
      "Validation loss (no improvement): 0.0367894321680069\n",
      "Training iteration: 2234\n",
      "Validation loss (no improvement): 0.036964982748031616\n",
      "Training iteration: 2235\n",
      "Validation loss (no improvement): 0.03714128136634827\n",
      "Training iteration: 2236\n",
      "Validation loss (no improvement): 0.036937421560287474\n",
      "Training iteration: 2237\n",
      "Validation loss (no improvement): 0.036841607093811034\n",
      "Training iteration: 2238\n",
      "Validation loss (no improvement): 0.03687708973884583\n",
      "Training iteration: 2239\n",
      "Validation loss (no improvement): 0.03706111013889313\n",
      "Training iteration: 2240\n",
      "Validation loss (no improvement): 0.03670177459716797\n",
      "Training iteration: 2241\n",
      "Validation loss (no improvement): 0.03651170134544372\n",
      "Training iteration: 2242\n",
      "Validation loss (no improvement): 0.036512157320976256\n",
      "Training iteration: 2243\n",
      "Validation loss (no improvement): 0.03656194508075714\n",
      "Training iteration: 2244\n",
      "Validation loss (no improvement): 0.036412566900253296\n",
      "Training iteration: 2245\n",
      "Validation loss (no improvement): 0.036240598559379576\n",
      "Training iteration: 2246\n",
      "Validation loss (no improvement): 0.036145803332328794\n",
      "Training iteration: 2247\n",
      "Validation loss (no improvement): 0.036148452758789064\n",
      "Training iteration: 2248\n",
      "Validation loss (no improvement): 0.03623068630695343\n",
      "Training iteration: 2249\n",
      "Validation loss (no improvement): 0.03627603650093079\n",
      "Training iteration: 2250\n",
      "Improved validation loss from: 0.03599388003349304  to: 0.035823863744735715\n",
      "Training iteration: 2251\n",
      "Improved validation loss from: 0.035823863744735715  to: 0.035685965418815614\n",
      "Training iteration: 2252\n",
      "Validation loss (no improvement): 0.03592345714569092\n",
      "Training iteration: 2253\n",
      "Validation loss (no improvement): 0.036293810606002806\n",
      "Training iteration: 2254\n",
      "Validation loss (no improvement): 0.03595433533191681\n",
      "Training iteration: 2255\n",
      "Validation loss (no improvement): 0.036054092645645144\n",
      "Training iteration: 2256\n",
      "Validation loss (no improvement): 0.036217567324638364\n",
      "Training iteration: 2257\n",
      "Validation loss (no improvement): 0.03674797415733337\n",
      "Training iteration: 2258\n",
      "Validation loss (no improvement): 0.036433109641075136\n",
      "Training iteration: 2259\n",
      "Validation loss (no improvement): 0.036286816000938416\n",
      "Training iteration: 2260\n",
      "Validation loss (no improvement): 0.03644313216209412\n",
      "Training iteration: 2261\n",
      "Validation loss (no improvement): 0.03672001659870148\n",
      "Training iteration: 2262\n",
      "Validation loss (no improvement): 0.03662170767784119\n",
      "Training iteration: 2263\n",
      "Validation loss (no improvement): 0.03650465309619903\n",
      "Training iteration: 2264\n",
      "Validation loss (no improvement): 0.0365654319524765\n",
      "Training iteration: 2265\n",
      "Validation loss (no improvement): 0.03673657476902008\n",
      "Training iteration: 2266\n",
      "Validation loss (no improvement): 0.036566510796546936\n",
      "Training iteration: 2267\n",
      "Validation loss (no improvement): 0.03647223114967346\n",
      "Training iteration: 2268\n",
      "Validation loss (no improvement): 0.03648833930492401\n",
      "Training iteration: 2269\n",
      "Validation loss (no improvement): 0.03657906651496887\n",
      "Training iteration: 2270\n",
      "Validation loss (no improvement): 0.03635770380496979\n",
      "Training iteration: 2271\n",
      "Validation loss (no improvement): 0.03625123500823975\n",
      "Training iteration: 2272\n",
      "Validation loss (no improvement): 0.036290031671524045\n",
      "Training iteration: 2273\n",
      "Validation loss (no improvement): 0.03628701269626618\n",
      "Training iteration: 2274\n",
      "Validation loss (no improvement): 0.0361975371837616\n",
      "Training iteration: 2275\n",
      "Validation loss (no improvement): 0.03600098490715027\n",
      "Training iteration: 2276\n",
      "Validation loss (no improvement): 0.03587183952331543\n",
      "Training iteration: 2277\n",
      "Validation loss (no improvement): 0.035877594351768495\n",
      "Training iteration: 2278\n",
      "Validation loss (no improvement): 0.035933342576026914\n",
      "Training iteration: 2279\n",
      "Improved validation loss from: 0.035685965418815614  to: 0.03544195294380188\n",
      "Training iteration: 2280\n",
      "Improved validation loss from: 0.03544195294380188  to: 0.03537671864032745\n",
      "Training iteration: 2281\n",
      "Validation loss (no improvement): 0.03571602404117584\n",
      "Training iteration: 2282\n",
      "Validation loss (no improvement): 0.036208251118659975\n",
      "Training iteration: 2283\n",
      "Validation loss (no improvement): 0.0357588529586792\n",
      "Training iteration: 2284\n",
      "Validation loss (no improvement): 0.03567415177822113\n",
      "Training iteration: 2285\n",
      "Validation loss (no improvement): 0.03581360876560211\n",
      "Training iteration: 2286\n",
      "Validation loss (no improvement): 0.03627083897590637\n",
      "Training iteration: 2287\n",
      "Validation loss (no improvement): 0.03588920533657074\n",
      "Training iteration: 2288\n",
      "Validation loss (no improvement): 0.03585881292819977\n",
      "Training iteration: 2289\n",
      "Validation loss (no improvement): 0.036112836003303526\n",
      "Training iteration: 2290\n",
      "Validation loss (no improvement): 0.03640240728855133\n",
      "Training iteration: 2291\n",
      "Validation loss (no improvement): 0.03608022630214691\n",
      "Training iteration: 2292\n",
      "Validation loss (no improvement): 0.03594878911972046\n",
      "Training iteration: 2293\n",
      "Validation loss (no improvement): 0.03610780239105225\n",
      "Training iteration: 2294\n",
      "Validation loss (no improvement): 0.036250969767570494\n",
      "Training iteration: 2295\n",
      "Validation loss (no improvement): 0.035955867171287535\n",
      "Training iteration: 2296\n",
      "Validation loss (no improvement): 0.03584854006767273\n",
      "Training iteration: 2297\n",
      "Validation loss (no improvement): 0.035928168892860414\n",
      "Training iteration: 2298\n",
      "Validation loss (no improvement): 0.03599011301994324\n",
      "Training iteration: 2299\n",
      "Validation loss (no improvement): 0.03564424216747284\n",
      "Training iteration: 2300\n",
      "Validation loss (no improvement): 0.035494425892829896\n",
      "Training iteration: 2301\n",
      "Validation loss (no improvement): 0.03548081815242767\n",
      "Training iteration: 2302\n",
      "Validation loss (no improvement): 0.03556998074054718\n",
      "Training iteration: 2303\n",
      "Improved validation loss from: 0.03537671864032745  to: 0.035225123167037964\n",
      "Training iteration: 2304\n",
      "Improved validation loss from: 0.035225123167037964  to: 0.03507926762104034\n",
      "Training iteration: 2305\n",
      "Validation loss (no improvement): 0.035160231590271\n",
      "Training iteration: 2306\n",
      "Validation loss (no improvement): 0.03530744016170502\n",
      "Training iteration: 2307\n",
      "Improved validation loss from: 0.03507926762104034  to: 0.034965673089027406\n",
      "Training iteration: 2308\n",
      "Improved validation loss from: 0.034965673089027406  to: 0.03487652242183685\n",
      "Training iteration: 2309\n",
      "Validation loss (no improvement): 0.03497847318649292\n",
      "Training iteration: 2310\n",
      "Validation loss (no improvement): 0.03514086306095123\n",
      "Training iteration: 2311\n",
      "Improved validation loss from: 0.03487652242183685  to: 0.034624379873275754\n",
      "Training iteration: 2312\n",
      "Validation loss (no improvement): 0.03463258147239685\n",
      "Training iteration: 2313\n",
      "Validation loss (no improvement): 0.03497398793697357\n",
      "Training iteration: 2314\n",
      "Validation loss (no improvement): 0.035074570775032045\n",
      "Training iteration: 2315\n",
      "Validation loss (no improvement): 0.034956294298172\n",
      "Training iteration: 2316\n",
      "Validation loss (no improvement): 0.03499909043312073\n",
      "Training iteration: 2317\n",
      "Validation loss (no improvement): 0.035212987661361696\n",
      "Training iteration: 2318\n",
      "Validation loss (no improvement): 0.03526349067687988\n",
      "Training iteration: 2319\n",
      "Validation loss (no improvement): 0.03496560454368591\n",
      "Training iteration: 2320\n",
      "Validation loss (no improvement): 0.034842965006828305\n",
      "Training iteration: 2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.03513318002223968\n",
      "Training iteration: 2322\n",
      "Validation loss (no improvement): 0.035364919900894166\n",
      "Training iteration: 2323\n",
      "Validation loss (no improvement): 0.03488101959228516\n",
      "Training iteration: 2324\n",
      "Validation loss (no improvement): 0.03487108647823334\n",
      "Training iteration: 2325\n",
      "Validation loss (no improvement): 0.03506300747394562\n",
      "Training iteration: 2326\n",
      "Validation loss (no improvement): 0.03561154901981354\n",
      "Training iteration: 2327\n",
      "Validation loss (no improvement): 0.035307270288467404\n",
      "Training iteration: 2328\n",
      "Validation loss (no improvement): 0.035135012865066526\n",
      "Training iteration: 2329\n",
      "Validation loss (no improvement): 0.03518363833427429\n",
      "Training iteration: 2330\n",
      "Validation loss (no improvement): 0.035575708746910094\n",
      "Training iteration: 2331\n",
      "Validation loss (no improvement): 0.035430893301963806\n",
      "Training iteration: 2332\n",
      "Validation loss (no improvement): 0.03517555594444275\n",
      "Training iteration: 2333\n",
      "Validation loss (no improvement): 0.035049843788146975\n",
      "Training iteration: 2334\n",
      "Validation loss (no improvement): 0.035038036108016965\n",
      "Training iteration: 2335\n",
      "Validation loss (no improvement): 0.03496201634407044\n",
      "Training iteration: 2336\n",
      "Validation loss (no improvement): 0.03471929430961609\n",
      "Training iteration: 2337\n",
      "Validation loss (no improvement): 0.0346344143152237\n",
      "Training iteration: 2338\n",
      "Validation loss (no improvement): 0.03467037975788116\n",
      "Training iteration: 2339\n",
      "Validation loss (no improvement): 0.03479361534118652\n",
      "Training iteration: 2340\n",
      "Improved validation loss from: 0.034624379873275754  to: 0.034397050738334656\n",
      "Training iteration: 2341\n",
      "Improved validation loss from: 0.034397050738334656  to: 0.03424317240715027\n",
      "Training iteration: 2342\n",
      "Improved validation loss from: 0.03424317240715027  to: 0.03420785069465637\n",
      "Training iteration: 2343\n",
      "Improved validation loss from: 0.03420785069465637  to: 0.03411478698253632\n",
      "Training iteration: 2344\n",
      "Improved validation loss from: 0.03411478698253632  to: 0.03394035398960114\n",
      "Training iteration: 2345\n",
      "Validation loss (no improvement): 0.034021636843681334\n",
      "Training iteration: 2346\n",
      "Validation loss (no improvement): 0.03424021601676941\n",
      "Training iteration: 2347\n",
      "Validation loss (no improvement): 0.034164068102836606\n",
      "Training iteration: 2348\n",
      "Validation loss (no improvement): 0.0340240091085434\n",
      "Training iteration: 2349\n",
      "Improved validation loss from: 0.03394035398960114  to: 0.03391115665435791\n",
      "Training iteration: 2350\n",
      "Validation loss (no improvement): 0.034044128656387326\n",
      "Training iteration: 2351\n",
      "Validation loss (no improvement): 0.03405342996120453\n",
      "Training iteration: 2352\n",
      "Validation loss (no improvement): 0.03403087556362152\n",
      "Training iteration: 2353\n",
      "Validation loss (no improvement): 0.03395542502403259\n",
      "Training iteration: 2354\n",
      "Improved validation loss from: 0.03391115665435791  to: 0.03377065062522888\n",
      "Training iteration: 2355\n",
      "Improved validation loss from: 0.03377065062522888  to: 0.033760005235672\n",
      "Training iteration: 2356\n",
      "Validation loss (no improvement): 0.03378230631351471\n",
      "Training iteration: 2357\n",
      "Validation loss (no improvement): 0.033769986033439635\n",
      "Training iteration: 2358\n",
      "Validation loss (no improvement): 0.03385039269924164\n",
      "Training iteration: 2359\n",
      "Improved validation loss from: 0.033760005235672  to: 0.03367080092430115\n",
      "Training iteration: 2360\n",
      "Validation loss (no improvement): 0.03367812633514404\n",
      "Training iteration: 2361\n",
      "Validation loss (no improvement): 0.03379260897636414\n",
      "Training iteration: 2362\n",
      "Validation loss (no improvement): 0.03401536047458649\n",
      "Training iteration: 2363\n",
      "Validation loss (no improvement): 0.03377448916435242\n",
      "Training iteration: 2364\n",
      "Validation loss (no improvement): 0.0337110847234726\n",
      "Training iteration: 2365\n",
      "Validation loss (no improvement): 0.0341191828250885\n",
      "Training iteration: 2366\n",
      "Validation loss (no improvement): 0.03419778943061828\n",
      "Training iteration: 2367\n",
      "Validation loss (no improvement): 0.033852672576904295\n",
      "Training iteration: 2368\n",
      "Validation loss (no improvement): 0.033810168504714966\n",
      "Training iteration: 2369\n",
      "Validation loss (no improvement): 0.03397895395755768\n",
      "Training iteration: 2370\n",
      "Validation loss (no improvement): 0.034248510003089906\n",
      "Training iteration: 2371\n",
      "Validation loss (no improvement): 0.033898395299911496\n",
      "Training iteration: 2372\n",
      "Validation loss (no improvement): 0.03386102616786957\n",
      "Training iteration: 2373\n",
      "Validation loss (no improvement): 0.0340604692697525\n",
      "Training iteration: 2374\n",
      "Validation loss (no improvement): 0.034286117553710936\n",
      "Training iteration: 2375\n",
      "Validation loss (no improvement): 0.03415010571479797\n",
      "Training iteration: 2376\n",
      "Validation loss (no improvement): 0.03415028154850006\n",
      "Training iteration: 2377\n",
      "Validation loss (no improvement): 0.034328579902648926\n",
      "Training iteration: 2378\n",
      "Validation loss (no improvement): 0.03442208468914032\n",
      "Training iteration: 2379\n",
      "Validation loss (no improvement): 0.0342782586812973\n",
      "Training iteration: 2380\n",
      "Validation loss (no improvement): 0.034139609336853026\n",
      "Training iteration: 2381\n",
      "Validation loss (no improvement): 0.033954331278800966\n",
      "Training iteration: 2382\n",
      "Validation loss (no improvement): 0.03392227292060852\n",
      "Training iteration: 2383\n",
      "Validation loss (no improvement): 0.03391953408718109\n",
      "Training iteration: 2384\n",
      "Validation loss (no improvement): 0.03375080227851868\n",
      "Training iteration: 2385\n",
      "Improved validation loss from: 0.03367080092430115  to: 0.03363257050514221\n",
      "Training iteration: 2386\n",
      "Improved validation loss from: 0.03363257050514221  to: 0.033627310395240785\n",
      "Training iteration: 2387\n",
      "Validation loss (no improvement): 0.03370481431484222\n",
      "Training iteration: 2388\n",
      "Improved validation loss from: 0.033627310395240785  to: 0.03345757722854614\n",
      "Training iteration: 2389\n",
      "Improved validation loss from: 0.03345757722854614  to: 0.03337715864181519\n",
      "Training iteration: 2390\n",
      "Validation loss (no improvement): 0.03338359296321869\n",
      "Training iteration: 2391\n",
      "Validation loss (no improvement): 0.03341161608695984\n",
      "Training iteration: 2392\n",
      "Improved validation loss from: 0.03337715864181519  to: 0.03333495259284973\n",
      "Training iteration: 2393\n",
      "Improved validation loss from: 0.03333495259284973  to: 0.033285579085350035\n",
      "Training iteration: 2394\n",
      "Improved validation loss from: 0.033285579085350035  to: 0.033266004920005796\n",
      "Training iteration: 2395\n",
      "Improved validation loss from: 0.033266004920005796  to: 0.0330313503742218\n",
      "Training iteration: 2396\n",
      "Validation loss (no improvement): 0.03305549621582031\n",
      "Training iteration: 2397\n",
      "Validation loss (no improvement): 0.03322396874427795\n",
      "Training iteration: 2398\n",
      "Validation loss (no improvement): 0.03338846564292908\n",
      "Training iteration: 2399\n",
      "Validation loss (no improvement): 0.03319783210754394\n",
      "Training iteration: 2400\n",
      "Validation loss (no improvement): 0.03325543999671936\n",
      "Training iteration: 2401\n",
      "Validation loss (no improvement): 0.033577531576156616\n",
      "Training iteration: 2402\n",
      "Validation loss (no improvement): 0.033977660536766055\n",
      "Training iteration: 2403\n",
      "Validation loss (no improvement): 0.03346733450889587\n",
      "Training iteration: 2404\n",
      "Validation loss (no improvement): 0.033528226613998416\n",
      "Training iteration: 2405\n",
      "Validation loss (no improvement): 0.03377886414527893\n",
      "Training iteration: 2406\n",
      "Validation loss (no improvement): 0.03390626311302185\n",
      "Training iteration: 2407\n",
      "Validation loss (no improvement): 0.0335984468460083\n",
      "Training iteration: 2408\n",
      "Validation loss (no improvement): 0.03355282545089722\n",
      "Training iteration: 2409\n",
      "Validation loss (no improvement): 0.03380136787891388\n",
      "Training iteration: 2410\n",
      "Validation loss (no improvement): 0.034087148308753965\n",
      "Training iteration: 2411\n",
      "Validation loss (no improvement): 0.033806833624839785\n",
      "Training iteration: 2412\n",
      "Validation loss (no improvement): 0.03375117182731628\n",
      "Training iteration: 2413\n",
      "Validation loss (no improvement): 0.033764305710792544\n",
      "Training iteration: 2414\n",
      "Validation loss (no improvement): 0.03412176072597504\n",
      "Training iteration: 2415\n",
      "Validation loss (no improvement): 0.03381055891513825\n",
      "Training iteration: 2416\n",
      "Validation loss (no improvement): 0.03362342119216919\n",
      "Training iteration: 2417\n",
      "Validation loss (no improvement): 0.0338362455368042\n",
      "Training iteration: 2418\n",
      "Validation loss (no improvement): 0.03403317034244537\n",
      "Training iteration: 2419\n",
      "Validation loss (no improvement): 0.033654716610908506\n",
      "Training iteration: 2420\n",
      "Validation loss (no improvement): 0.03343712687492371\n",
      "Training iteration: 2421\n",
      "Validation loss (no improvement): 0.033587560057640076\n",
      "Training iteration: 2422\n",
      "Validation loss (no improvement): 0.03371151983737945\n",
      "Training iteration: 2423\n",
      "Validation loss (no improvement): 0.03327992856502533\n",
      "Training iteration: 2424\n",
      "Validation loss (no improvement): 0.03316274285316467\n",
      "Training iteration: 2425\n",
      "Validation loss (no improvement): 0.03330580294132233\n",
      "Training iteration: 2426\n",
      "Validation loss (no improvement): 0.03336875438690186\n",
      "Training iteration: 2427\n",
      "Validation loss (no improvement): 0.033044320344924924\n",
      "Training iteration: 2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.033036381006240845\n",
      "Training iteration: 2429\n",
      "Validation loss (no improvement): 0.03321486413478851\n",
      "Training iteration: 2430\n",
      "Validation loss (no improvement): 0.03357961475849151\n",
      "Training iteration: 2431\n",
      "Validation loss (no improvement): 0.03337681293487549\n",
      "Training iteration: 2432\n",
      "Validation loss (no improvement): 0.0331709623336792\n",
      "Training iteration: 2433\n",
      "Validation loss (no improvement): 0.03324974179267883\n",
      "Training iteration: 2434\n",
      "Validation loss (no improvement): 0.03342418074607849\n",
      "Training iteration: 2435\n",
      "Validation loss (no improvement): 0.0335516482591629\n",
      "Training iteration: 2436\n",
      "Validation loss (no improvement): 0.03332537710666657\n",
      "Training iteration: 2437\n",
      "Validation loss (no improvement): 0.033496469259262085\n",
      "Training iteration: 2438\n",
      "Validation loss (no improvement): 0.033933055400848386\n",
      "Training iteration: 2439\n",
      "Validation loss (no improvement): 0.03430902361869812\n",
      "Training iteration: 2440\n",
      "Validation loss (no improvement): 0.03396011590957641\n",
      "Training iteration: 2441\n",
      "Validation loss (no improvement): 0.0337076872587204\n",
      "Training iteration: 2442\n",
      "Validation loss (no improvement): 0.03358568549156189\n",
      "Training iteration: 2443\n",
      "Validation loss (no improvement): 0.033523231744766235\n",
      "Training iteration: 2444\n",
      "Validation loss (no improvement): 0.03355911374092102\n",
      "Training iteration: 2445\n",
      "Validation loss (no improvement): 0.03367102742195129\n",
      "Training iteration: 2446\n",
      "Validation loss (no improvement): 0.03346954882144928\n",
      "Training iteration: 2447\n",
      "Validation loss (no improvement): 0.03311172127723694\n",
      "Training iteration: 2448\n",
      "Improved validation loss from: 0.0330313503742218  to: 0.03299355804920197\n",
      "Training iteration: 2449\n",
      "Validation loss (no improvement): 0.03306010067462921\n",
      "Training iteration: 2450\n",
      "Improved validation loss from: 0.03299355804920197  to: 0.032963648438453674\n",
      "Training iteration: 2451\n",
      "Improved validation loss from: 0.032963648438453674  to: 0.03274226188659668\n",
      "Training iteration: 2452\n",
      "Improved validation loss from: 0.03274226188659668  to: 0.032725471258163455\n",
      "Training iteration: 2453\n",
      "Validation loss (no improvement): 0.03278286457061767\n",
      "Training iteration: 2454\n",
      "Improved validation loss from: 0.032725471258163455  to: 0.032656130194664\n",
      "Training iteration: 2455\n",
      "Improved validation loss from: 0.032656130194664  to: 0.0323052704334259\n",
      "Training iteration: 2456\n",
      "Improved validation loss from: 0.0323052704334259  to: 0.032190379500389096\n",
      "Training iteration: 2457\n",
      "Improved validation loss from: 0.032190379500389096  to: 0.032134875655174255\n",
      "Training iteration: 2458\n",
      "Validation loss (no improvement): 0.03219135403633118\n",
      "Training iteration: 2459\n",
      "Validation loss (no improvement): 0.03227569460868836\n",
      "Training iteration: 2460\n",
      "Improved validation loss from: 0.032134875655174255  to: 0.031981974840164185\n",
      "Training iteration: 2461\n",
      "Improved validation loss from: 0.031981974840164185  to: 0.03193248212337494\n",
      "Training iteration: 2462\n",
      "Validation loss (no improvement): 0.032162895798683165\n",
      "Training iteration: 2463\n",
      "Validation loss (no improvement): 0.03223469853401184\n",
      "Training iteration: 2464\n",
      "Validation loss (no improvement): 0.03204259276390076\n",
      "Training iteration: 2465\n",
      "Validation loss (no improvement): 0.03213692307472229\n",
      "Training iteration: 2466\n",
      "Validation loss (no improvement): 0.032311612367630006\n",
      "Training iteration: 2467\n",
      "Validation loss (no improvement): 0.03254702687263489\n",
      "Training iteration: 2468\n",
      "Validation loss (no improvement): 0.032200366258621216\n",
      "Training iteration: 2469\n",
      "Validation loss (no improvement): 0.03215780258178711\n",
      "Training iteration: 2470\n",
      "Validation loss (no improvement): 0.03232308030128479\n",
      "Training iteration: 2471\n",
      "Validation loss (no improvement): 0.03243436813354492\n",
      "Training iteration: 2472\n",
      "Validation loss (no improvement): 0.032282933592796326\n",
      "Training iteration: 2473\n",
      "Validation loss (no improvement): 0.03229174911975861\n",
      "Training iteration: 2474\n",
      "Validation loss (no improvement): 0.032466116547584536\n",
      "Training iteration: 2475\n",
      "Validation loss (no improvement): 0.032565879821777347\n",
      "Training iteration: 2476\n",
      "Validation loss (no improvement): 0.0323907196521759\n",
      "Training iteration: 2477\n",
      "Validation loss (no improvement): 0.03242517113685608\n",
      "Training iteration: 2478\n",
      "Validation loss (no improvement): 0.032390058040618896\n",
      "Training iteration: 2479\n",
      "Validation loss (no improvement): 0.03264938294887543\n",
      "Training iteration: 2480\n",
      "Validation loss (no improvement): 0.03221177458763123\n",
      "Training iteration: 2481\n",
      "Validation loss (no improvement): 0.03214431405067444\n",
      "Training iteration: 2482\n",
      "Validation loss (no improvement): 0.03239489495754242\n",
      "Training iteration: 2483\n",
      "Validation loss (no improvement): 0.032689023017883304\n",
      "Training iteration: 2484\n",
      "Validation loss (no improvement): 0.03226874470710754\n",
      "Training iteration: 2485\n",
      "Validation loss (no improvement): 0.03224640488624573\n",
      "Training iteration: 2486\n",
      "Validation loss (no improvement): 0.03224020600318909\n",
      "Training iteration: 2487\n",
      "Validation loss (no improvement): 0.03263189196586609\n",
      "Training iteration: 2488\n",
      "Validation loss (no improvement): 0.03241840898990631\n",
      "Training iteration: 2489\n",
      "Validation loss (no improvement): 0.032189327478408816\n",
      "Training iteration: 2490\n",
      "Validation loss (no improvement): 0.032223325967788694\n",
      "Training iteration: 2491\n",
      "Validation loss (no improvement): 0.03233657777309418\n",
      "Training iteration: 2492\n",
      "Validation loss (no improvement): 0.03218128085136414\n",
      "Training iteration: 2493\n",
      "Validation loss (no improvement): 0.032103431224822995\n",
      "Training iteration: 2494\n",
      "Validation loss (no improvement): 0.03209464848041534\n",
      "Training iteration: 2495\n",
      "Validation loss (no improvement): 0.032061013579368594\n",
      "Training iteration: 2496\n",
      "Improved validation loss from: 0.03193248212337494  to: 0.031711307168006894\n",
      "Training iteration: 2497\n",
      "Improved validation loss from: 0.031711307168006894  to: 0.03156937956809998\n",
      "Training iteration: 2498\n",
      "Improved validation loss from: 0.03156937956809998  to: 0.03155469298362732\n",
      "Training iteration: 2499\n",
      "Improved validation loss from: 0.03155469298362732  to: 0.03146353960037231\n",
      "Training iteration: 2500\n",
      "Validation loss (no improvement): 0.03151668906211853\n",
      "Training iteration: 2501\n",
      "Improved validation loss from: 0.03146353960037231  to: 0.031343096494674684\n",
      "Training iteration: 2502\n",
      "Improved validation loss from: 0.031343096494674684  to: 0.031218603253364563\n",
      "Training iteration: 2503\n",
      "Validation loss (no improvement): 0.031280866265296935\n",
      "Training iteration: 2504\n",
      "Validation loss (no improvement): 0.03147912621498108\n",
      "Training iteration: 2505\n",
      "Validation loss (no improvement): 0.03128144145011902\n",
      "Training iteration: 2506\n",
      "Validation loss (no improvement): 0.031232717633247375\n",
      "Training iteration: 2507\n",
      "Validation loss (no improvement): 0.03147358596324921\n",
      "Training iteration: 2508\n",
      "Validation loss (no improvement): 0.03184345960617065\n",
      "Training iteration: 2509\n",
      "Validation loss (no improvement): 0.03148417472839356\n",
      "Training iteration: 2510\n",
      "Validation loss (no improvement): 0.031440502405166625\n",
      "Training iteration: 2511\n",
      "Validation loss (no improvement): 0.03165034353733063\n",
      "Training iteration: 2512\n",
      "Validation loss (no improvement): 0.03195720016956329\n",
      "Training iteration: 2513\n",
      "Validation loss (no improvement): 0.03171565532684326\n",
      "Training iteration: 2514\n",
      "Validation loss (no improvement): 0.031686323881149295\n",
      "Training iteration: 2515\n",
      "Validation loss (no improvement): 0.03186904489994049\n",
      "Training iteration: 2516\n",
      "Validation loss (no improvement): 0.03205758631229401\n",
      "Training iteration: 2517\n",
      "Validation loss (no improvement): 0.03175590634346008\n",
      "Training iteration: 2518\n",
      "Validation loss (no improvement): 0.03170841336250305\n",
      "Training iteration: 2519\n",
      "Validation loss (no improvement): 0.031771183013916016\n",
      "Training iteration: 2520\n",
      "Validation loss (no improvement): 0.03221072256565094\n",
      "Training iteration: 2521\n",
      "Validation loss (no improvement): 0.03176650404930115\n",
      "Training iteration: 2522\n",
      "Validation loss (no improvement): 0.03148310780525208\n",
      "Training iteration: 2523\n",
      "Validation loss (no improvement): 0.03153865039348602\n",
      "Training iteration: 2524\n",
      "Validation loss (no improvement): 0.03182879090309143\n",
      "Training iteration: 2525\n",
      "Validation loss (no improvement): 0.0314453125\n",
      "Training iteration: 2526\n",
      "Validation loss (no improvement): 0.0312878429889679\n",
      "Training iteration: 2527\n",
      "Validation loss (no improvement): 0.031223392486572264\n",
      "Training iteration: 2528\n",
      "Validation loss (no improvement): 0.03129518628120422\n",
      "Training iteration: 2529\n",
      "Validation loss (no improvement): 0.03128840923309326\n",
      "Training iteration: 2530\n",
      "Improved validation loss from: 0.031218603253364563  to: 0.031021147966384888\n",
      "Training iteration: 2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved validation loss from: 0.031021147966384888  to: 0.03093518614768982\n",
      "Training iteration: 2532\n",
      "Validation loss (no improvement): 0.03114280104637146\n",
      "Training iteration: 2533\n",
      "Validation loss (no improvement): 0.031225797533988953\n",
      "Training iteration: 2534\n",
      "Validation loss (no improvement): 0.03107759952545166\n",
      "Training iteration: 2535\n",
      "Validation loss (no improvement): 0.030954617261886596\n",
      "Training iteration: 2536\n",
      "Improved validation loss from: 0.03093518614768982  to: 0.030927491188049317\n",
      "Training iteration: 2537\n",
      "Validation loss (no improvement): 0.031005162000656127\n",
      "Training iteration: 2538\n",
      "Improved validation loss from: 0.030927491188049317  to: 0.030869460105895995\n",
      "Training iteration: 2539\n",
      "Improved validation loss from: 0.030869460105895995  to: 0.030784335732460023\n",
      "Training iteration: 2540\n",
      "Validation loss (no improvement): 0.03089326024055481\n",
      "Training iteration: 2541\n",
      "Validation loss (no improvement): 0.031114417314529418\n",
      "Training iteration: 2542\n",
      "Validation loss (no improvement): 0.030995279550552368\n",
      "Training iteration: 2543\n",
      "Improved validation loss from: 0.030784335732460023  to: 0.03070334792137146\n",
      "Training iteration: 2544\n",
      "Validation loss (no improvement): 0.030730444192886352\n",
      "Training iteration: 2545\n",
      "Validation loss (no improvement): 0.030990871787071227\n",
      "Training iteration: 2546\n",
      "Validation loss (no improvement): 0.0310943603515625\n",
      "Training iteration: 2547\n",
      "Validation loss (no improvement): 0.030736881494522094\n",
      "Training iteration: 2548\n",
      "Validation loss (no improvement): 0.030761927366256714\n",
      "Training iteration: 2549\n",
      "Validation loss (no improvement): 0.031098484992980957\n",
      "Training iteration: 2550\n",
      "Validation loss (no improvement): 0.0313909113407135\n",
      "Training iteration: 2551\n",
      "Validation loss (no improvement): 0.03096051514148712\n",
      "Training iteration: 2552\n",
      "Validation loss (no improvement): 0.03092486262321472\n",
      "Training iteration: 2553\n",
      "Validation loss (no improvement): 0.03096466064453125\n",
      "Training iteration: 2554\n",
      "Validation loss (no improvement): 0.03129942417144775\n",
      "Training iteration: 2555\n",
      "Validation loss (no improvement): 0.031113561987876893\n",
      "Training iteration: 2556\n",
      "Validation loss (no improvement): 0.03094080090522766\n",
      "Training iteration: 2557\n",
      "Validation loss (no improvement): 0.03094201982021332\n",
      "Training iteration: 2558\n",
      "Validation loss (no improvement): 0.03098689317703247\n",
      "Training iteration: 2559\n",
      "Validation loss (no improvement): 0.030737149715423583\n",
      "Training iteration: 2560\n",
      "Improved validation loss from: 0.03070334792137146  to: 0.030539461970329286\n",
      "Training iteration: 2561\n",
      "Validation loss (no improvement): 0.030556055903434753\n",
      "Training iteration: 2562\n",
      "Improved validation loss from: 0.030539461970329286  to: 0.03045725226402283\n",
      "Training iteration: 2563\n",
      "Improved validation loss from: 0.03045725226402283  to: 0.03029232919216156\n",
      "Training iteration: 2564\n",
      "Improved validation loss from: 0.03029232919216156  to: 0.030246502161026\n",
      "Training iteration: 2565\n",
      "Improved validation loss from: 0.030246502161026  to: 0.030037182569503783\n",
      "Training iteration: 2566\n",
      "Improved validation loss from: 0.030037182569503783  to: 0.029940423369407655\n",
      "Training iteration: 2567\n",
      "Validation loss (no improvement): 0.03009524643421173\n",
      "Training iteration: 2568\n",
      "Improved validation loss from: 0.029940423369407655  to: 0.029893827438354493\n",
      "Training iteration: 2569\n",
      "Improved validation loss from: 0.029893827438354493  to: 0.029800659418106078\n",
      "Training iteration: 2570\n",
      "Improved validation loss from: 0.029800659418106078  to: 0.02975398600101471\n",
      "Training iteration: 2571\n",
      "Improved validation loss from: 0.02975398600101471  to: 0.02971498966217041\n",
      "Training iteration: 2572\n",
      "Validation loss (no improvement): 0.029744860529899598\n",
      "Training iteration: 2573\n",
      "Validation loss (no improvement): 0.02989199459552765\n",
      "Training iteration: 2574\n",
      "Validation loss (no improvement): 0.029855605959892274\n",
      "Training iteration: 2575\n",
      "Improved validation loss from: 0.02971498966217041  to: 0.02965947985649109\n",
      "Training iteration: 2576\n",
      "Validation loss (no improvement): 0.029678153991699218\n",
      "Training iteration: 2577\n",
      "Validation loss (no improvement): 0.029990673065185547\n",
      "Training iteration: 2578\n",
      "Validation loss (no improvement): 0.02990894913673401\n",
      "Training iteration: 2579\n",
      "Validation loss (no improvement): 0.029856720566749574\n",
      "Training iteration: 2580\n",
      "Validation loss (no improvement): 0.030069845914840698\n",
      "Training iteration: 2581\n",
      "Validation loss (no improvement): 0.030333435535430907\n",
      "Training iteration: 2582\n",
      "Validation loss (no improvement): 0.030137374997138977\n",
      "Training iteration: 2583\n",
      "Validation loss (no improvement): 0.030064266920089722\n",
      "Training iteration: 2584\n",
      "Validation loss (no improvement): 0.030196797847747803\n",
      "Training iteration: 2585\n",
      "Validation loss (no improvement): 0.03015075922012329\n",
      "Training iteration: 2586\n",
      "Validation loss (no improvement): 0.02997439503669739\n",
      "Training iteration: 2587\n",
      "Validation loss (no improvement): 0.029919379949569704\n",
      "Training iteration: 2588\n",
      "Validation loss (no improvement): 0.029951784014701843\n",
      "Training iteration: 2589\n",
      "Validation loss (no improvement): 0.029961809515953064\n",
      "Training iteration: 2590\n",
      "Validation loss (no improvement): 0.02967878580093384\n",
      "Training iteration: 2591\n",
      "Improved validation loss from: 0.02965947985649109  to: 0.02957102358341217\n",
      "Training iteration: 2592\n",
      "Validation loss (no improvement): 0.02972162663936615\n",
      "Training iteration: 2593\n",
      "Validation loss (no improvement): 0.030007347464561462\n",
      "Training iteration: 2594\n",
      "Improved validation loss from: 0.02957102358341217  to: 0.029566097259521484\n",
      "Training iteration: 2595\n",
      "Improved validation loss from: 0.029566097259521484  to: 0.029445862770080565\n",
      "Training iteration: 2596\n",
      "Validation loss (no improvement): 0.029580026865005493\n",
      "Training iteration: 2597\n",
      "Validation loss (no improvement): 0.02993537187576294\n",
      "Training iteration: 2598\n",
      "Validation loss (no improvement): 0.02963801920413971\n",
      "Training iteration: 2599\n",
      "Validation loss (no improvement): 0.0295438677072525\n",
      "Training iteration: 2600\n",
      "Validation loss (no improvement): 0.029597610235214233\n",
      "Training iteration: 2601\n",
      "Validation loss (no improvement): 0.029714202880859374\n",
      "Training iteration: 2602\n",
      "Validation loss (no improvement): 0.029644951224327087\n",
      "Training iteration: 2603\n",
      "Validation loss (no improvement): 0.029602816700935362\n",
      "Training iteration: 2604\n",
      "Validation loss (no improvement): 0.02970190644264221\n",
      "Training iteration: 2605\n",
      "Validation loss (no improvement): 0.02969786822795868\n",
      "Training iteration: 2606\n",
      "Validation loss (no improvement): 0.029531973600387573\n",
      "Training iteration: 2607\n",
      "Validation loss (no improvement): 0.029497942328453063\n",
      "Training iteration: 2608\n",
      "Validation loss (no improvement): 0.02945869565010071\n",
      "Training iteration: 2609\n",
      "Improved validation loss from: 0.029445862770080565  to: 0.029327785968780516\n",
      "Training iteration: 2610\n",
      "Improved validation loss from: 0.029327785968780516  to: 0.02919139266014099\n",
      "Training iteration: 2611\n",
      "Improved validation loss from: 0.02919139266014099  to: 0.029062944650650024\n",
      "Training iteration: 2612\n",
      "Improved validation loss from: 0.029062944650650024  to: 0.028930193185806273\n",
      "Training iteration: 2613\n",
      "Improved validation loss from: 0.028930193185806273  to: 0.028784888982772826\n",
      "Training iteration: 2614\n",
      "Improved validation loss from: 0.028784888982772826  to: 0.02871156930923462\n",
      "Training iteration: 2615\n",
      "Improved validation loss from: 0.02871156930923462  to: 0.02854783833026886\n",
      "Training iteration: 2616\n",
      "Improved validation loss from: 0.02854783833026886  to: 0.028513997793197632\n",
      "Training iteration: 2617\n",
      "Validation loss (no improvement): 0.028696924448013306\n",
      "Training iteration: 2618\n",
      "Improved validation loss from: 0.028513997793197632  to: 0.02837863564491272\n",
      "Training iteration: 2619\n",
      "Validation loss (no improvement): 0.02847767770290375\n",
      "Training iteration: 2620\n",
      "Validation loss (no improvement): 0.02888527512550354\n",
      "Training iteration: 2621\n",
      "Validation loss (no improvement): 0.029164153337478637\n",
      "Training iteration: 2622\n",
      "Validation loss (no improvement): 0.028741630911827087\n",
      "Training iteration: 2623\n",
      "Validation loss (no improvement): 0.0288408100605011\n",
      "Training iteration: 2624\n",
      "Validation loss (no improvement): 0.029201850295066833\n",
      "Training iteration: 2625\n",
      "Validation loss (no improvement): 0.029460877180099487\n",
      "Training iteration: 2626\n",
      "Validation loss (no improvement): 0.029153233766555785\n",
      "Training iteration: 2627\n",
      "Validation loss (no improvement): 0.02913716733455658\n",
      "Training iteration: 2628\n",
      "Validation loss (no improvement): 0.029391413927078246\n",
      "Training iteration: 2629\n",
      "Validation loss (no improvement): 0.029918605089187623\n",
      "Training iteration: 2630\n",
      "Validation loss (no improvement): 0.029405581951141357\n",
      "Training iteration: 2631\n",
      "Validation loss (no improvement): 0.029329118132591248\n",
      "Training iteration: 2632\n",
      "Validation loss (no improvement): 0.02943011522293091\n",
      "Training iteration: 2633\n",
      "Validation loss (no improvement): 0.029605332016944885\n",
      "Training iteration: 2634\n",
      "Validation loss (no improvement): 0.029231083393096925\n",
      "Training iteration: 2635\n",
      "Validation loss (no improvement): 0.029069870710372925\n",
      "Training iteration: 2636\n",
      "Validation loss (no improvement): 0.029194247722625733\n",
      "Training iteration: 2637\n",
      "Validation loss (no improvement): 0.029316014051437377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 2638\n",
      "Validation loss (no improvement): 0.028782194852828978\n",
      "Training iteration: 2639\n",
      "Validation loss (no improvement): 0.028615409135818483\n",
      "Training iteration: 2640\n",
      "Validation loss (no improvement): 0.028688448667526244\n",
      "Training iteration: 2641\n",
      "Validation loss (no improvement): 0.028829282522201537\n",
      "Training iteration: 2642\n",
      "Improved validation loss from: 0.02837863564491272  to: 0.028322505950927734\n",
      "Training iteration: 2643\n",
      "Validation loss (no improvement): 0.028353315591812134\n",
      "Training iteration: 2644\n",
      "Validation loss (no improvement): 0.028560933470726014\n",
      "Training iteration: 2645\n",
      "Validation loss (no improvement): 0.029081198573112487\n",
      "Training iteration: 2646\n",
      "Validation loss (no improvement): 0.028422838449478148\n",
      "Training iteration: 2647\n",
      "Improved validation loss from: 0.028322505950927734  to: 0.028224843740463256\n",
      "Training iteration: 2648\n",
      "Validation loss (no improvement): 0.028248518705368042\n",
      "Training iteration: 2649\n",
      "Validation loss (no improvement): 0.028609830141067504\n",
      "Training iteration: 2650\n",
      "Validation loss (no improvement): 0.02872527241706848\n",
      "Training iteration: 2651\n",
      "Validation loss (no improvement): 0.028423306345939637\n",
      "Training iteration: 2652\n",
      "Validation loss (no improvement): 0.028488430380821227\n",
      "Training iteration: 2653\n",
      "Validation loss (no improvement): 0.028829607367515563\n",
      "Training iteration: 2654\n",
      "Validation loss (no improvement): 0.029253455996513366\n",
      "Training iteration: 2655\n",
      "Validation loss (no improvement): 0.029048433899879454\n",
      "Training iteration: 2656\n",
      "Validation loss (no improvement): 0.029001536965370178\n",
      "Training iteration: 2657\n",
      "Validation loss (no improvement): 0.028781548142433167\n",
      "Training iteration: 2658\n",
      "Validation loss (no improvement): 0.029509904980659484\n",
      "Training iteration: 2659\n",
      "Validation loss (no improvement): 0.029395633935928346\n",
      "Training iteration: 2660\n",
      "Validation loss (no improvement): 0.028880050778388976\n",
      "Training iteration: 2661\n",
      "Validation loss (no improvement): 0.02884652018547058\n",
      "Training iteration: 2662\n",
      "Validation loss (no improvement): 0.029067644476890565\n",
      "Training iteration: 2663\n",
      "Validation loss (no improvement): 0.029132485389709473\n",
      "Training iteration: 2664\n",
      "Validation loss (no improvement): 0.02874661087989807\n",
      "Training iteration: 2665\n",
      "Validation loss (no improvement): 0.028523480892181395\n",
      "Training iteration: 2666\n",
      "Validation loss (no improvement): 0.028592008352279662\n",
      "Training iteration: 2667\n",
      "Validation loss (no improvement): 0.028913089632987977\n",
      "Training iteration: 2668\n",
      "Validation loss (no improvement): 0.028258198499679567\n",
      "Training iteration: 2669\n",
      "Improved validation loss from: 0.028224843740463256  to: 0.027941697835922243\n",
      "Training iteration: 2670\n",
      "Improved validation loss from: 0.027941697835922243  to: 0.027879363298416136\n",
      "Training iteration: 2671\n",
      "Validation loss (no improvement): 0.028011804819107054\n",
      "Training iteration: 2672\n",
      "Improved validation loss from: 0.027879363298416136  to: 0.027731260657310484\n",
      "Training iteration: 2673\n",
      "Improved validation loss from: 0.027731260657310484  to: 0.027613061666488647\n",
      "Training iteration: 2674\n",
      "Validation loss (no improvement): 0.027660587430000307\n",
      "Training iteration: 2675\n",
      "Validation loss (no improvement): 0.027781224250793456\n",
      "Training iteration: 2676\n",
      "Validation loss (no improvement): 0.027876219153404234\n",
      "Training iteration: 2677\n",
      "Improved validation loss from: 0.027613061666488647  to: 0.02740139067173004\n",
      "Training iteration: 2678\n",
      "Improved validation loss from: 0.02740139067173004  to: 0.027246591448783875\n",
      "Training iteration: 2679\n",
      "Validation loss (no improvement): 0.02757936418056488\n",
      "Training iteration: 2680\n",
      "Validation loss (no improvement): 0.02793392539024353\n",
      "Training iteration: 2681\n",
      "Validation loss (no improvement): 0.02744225561618805\n",
      "Training iteration: 2682\n",
      "Validation loss (no improvement): 0.027272921800613404\n",
      "Training iteration: 2683\n",
      "Validation loss (no improvement): 0.027389594912528993\n",
      "Training iteration: 2684\n",
      "Validation loss (no improvement): 0.027636504173278807\n",
      "Training iteration: 2685\n",
      "Validation loss (no improvement): 0.027408701181411744\n",
      "Training iteration: 2686\n",
      "Validation loss (no improvement): 0.027293199300765993\n",
      "Training iteration: 2687\n",
      "Validation loss (no improvement): 0.02737749516963959\n",
      "Training iteration: 2688\n",
      "Validation loss (no improvement): 0.027589958906173707\n",
      "Training iteration: 2689\n",
      "Validation loss (no improvement): 0.027500897645950317\n",
      "Training iteration: 2690\n",
      "Improved validation loss from: 0.027246591448783875  to: 0.027129364013671876\n",
      "Training iteration: 2691\n",
      "Improved validation loss from: 0.027129364013671876  to: 0.02703426480293274\n",
      "Training iteration: 2692\n",
      "Validation loss (no improvement): 0.027090397477149964\n",
      "Training iteration: 2693\n",
      "Improved validation loss from: 0.02703426480293274  to: 0.02686711847782135\n",
      "Training iteration: 2694\n",
      "Improved validation loss from: 0.02686711847782135  to: 0.026812401413917542\n",
      "Training iteration: 2695\n",
      "Validation loss (no improvement): 0.02689528465270996\n",
      "Training iteration: 2696\n",
      "Validation loss (no improvement): 0.026890179514884947\n",
      "Training iteration: 2697\n",
      "Validation loss (no improvement): 0.02683860659599304\n",
      "Training iteration: 2698\n",
      "Improved validation loss from: 0.026812401413917542  to: 0.026649484038352968\n",
      "Training iteration: 2699\n",
      "Improved validation loss from: 0.026649484038352968  to: 0.026524347066879273\n",
      "Training iteration: 2700\n",
      "Validation loss (no improvement): 0.027033790946006775\n",
      "Training iteration: 2701\n",
      "Validation loss (no improvement): 0.027249321341514587\n",
      "Training iteration: 2702\n",
      "Validation loss (no improvement): 0.02657018303871155\n",
      "Training iteration: 2703\n",
      "Validation loss (no improvement): 0.026600870490074157\n",
      "Training iteration: 2704\n",
      "Validation loss (no improvement): 0.027023571729660033\n",
      "Training iteration: 2705\n",
      "Validation loss (no improvement): 0.02716217339038849\n",
      "Training iteration: 2706\n",
      "Validation loss (no improvement): 0.026972398161888123\n",
      "Training iteration: 2707\n",
      "Validation loss (no improvement): 0.02706311047077179\n",
      "Training iteration: 2708\n",
      "Validation loss (no improvement): 0.02749796211719513\n",
      "Training iteration: 2709\n",
      "Validation loss (no improvement): 0.02785044312477112\n",
      "Training iteration: 2710\n",
      "Validation loss (no improvement): 0.027102911472320558\n",
      "Training iteration: 2711\n",
      "Validation loss (no improvement): 0.02708694040775299\n",
      "Training iteration: 2712\n",
      "Validation loss (no improvement): 0.027140861749649046\n",
      "Training iteration: 2713\n",
      "Validation loss (no improvement): 0.027399811148643493\n",
      "Training iteration: 2714\n",
      "Validation loss (no improvement): 0.027129477262496947\n",
      "Training iteration: 2715\n",
      "Validation loss (no improvement): 0.02699185013771057\n",
      "Training iteration: 2716\n",
      "Validation loss (no improvement): 0.027062228322029112\n",
      "Training iteration: 2717\n",
      "Validation loss (no improvement): 0.027063864469528198\n",
      "Training iteration: 2718\n",
      "Validation loss (no improvement): 0.026624128222465515\n",
      "Training iteration: 2719\n",
      "Improved validation loss from: 0.026524347066879273  to: 0.026450252532958983\n",
      "Training iteration: 2720\n",
      "Validation loss (no improvement): 0.026513734459877016\n",
      "Training iteration: 2721\n",
      "Validation loss (no improvement): 0.02658737897872925\n",
      "Training iteration: 2722\n",
      "Improved validation loss from: 0.026450252532958983  to: 0.026303309202194213\n",
      "Training iteration: 2723\n",
      "Improved validation loss from: 0.026303309202194213  to: 0.02621355652809143\n",
      "Training iteration: 2724\n",
      "Validation loss (no improvement): 0.02625386416912079\n",
      "Training iteration: 2725\n",
      "Validation loss (no improvement): 0.026289647817611693\n",
      "Training iteration: 2726\n",
      "Validation loss (no improvement): 0.0262281596660614\n",
      "Training iteration: 2727\n",
      "Improved validation loss from: 0.02621355652809143  to: 0.02581433653831482\n",
      "Training iteration: 2728\n",
      "Improved validation loss from: 0.02581433653831482  to: 0.025715777277946474\n",
      "Training iteration: 2729\n",
      "Validation loss (no improvement): 0.02603127360343933\n",
      "Training iteration: 2730\n",
      "Validation loss (no improvement): 0.02635383903980255\n",
      "Training iteration: 2731\n",
      "Improved validation loss from: 0.025715777277946474  to: 0.02562092840671539\n",
      "Training iteration: 2732\n",
      "Validation loss (no improvement): 0.025632232427597046\n",
      "Training iteration: 2733\n",
      "Validation loss (no improvement): 0.025946938991546632\n",
      "Training iteration: 2734\n",
      "Validation loss (no improvement): 0.026253721117973326\n",
      "Training iteration: 2735\n",
      "Validation loss (no improvement): 0.02584887444972992\n",
      "Training iteration: 2736\n",
      "Validation loss (no improvement): 0.025822314620018005\n",
      "Training iteration: 2737\n",
      "Validation loss (no improvement): 0.026154345273971556\n",
      "Training iteration: 2738\n",
      "Validation loss (no improvement): 0.026740309596061707\n",
      "Training iteration: 2739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.02621970772743225\n",
      "Training iteration: 2740\n",
      "Validation loss (no improvement): 0.026085391640663147\n",
      "Training iteration: 2741\n",
      "Validation loss (no improvement): 0.02623850107192993\n",
      "Training iteration: 2742\n",
      "Validation loss (no improvement): 0.026399478316307068\n",
      "Training iteration: 2743\n",
      "Validation loss (no improvement): 0.02625187337398529\n",
      "Training iteration: 2744\n",
      "Validation loss (no improvement): 0.025956562161445616\n",
      "Training iteration: 2745\n",
      "Validation loss (no improvement): 0.02566623091697693\n",
      "Training iteration: 2746\n",
      "Improved validation loss from: 0.02562092840671539  to: 0.025513356924057005\n",
      "Training iteration: 2747\n",
      "Improved validation loss from: 0.025513356924057005  to: 0.02543005645275116\n",
      "Training iteration: 2748\n",
      "Improved validation loss from: 0.02543005645275116  to: 0.025121107697486877\n",
      "Training iteration: 2749\n",
      "Improved validation loss from: 0.025121107697486877  to: 0.025035545229911804\n",
      "Training iteration: 2750\n",
      "Validation loss (no improvement): 0.02521021068096161\n",
      "Training iteration: 2751\n",
      "Validation loss (no improvement): 0.025318866968154906\n",
      "Training iteration: 2752\n",
      "Improved validation loss from: 0.025035545229911804  to: 0.024907413125038146\n",
      "Training iteration: 2753\n",
      "Improved validation loss from: 0.024907413125038146  to: 0.024828240275382996\n",
      "Training iteration: 2754\n",
      "Validation loss (no improvement): 0.0254329115152359\n",
      "Training iteration: 2755\n",
      "Validation loss (no improvement): 0.026194512844085693\n",
      "Training iteration: 2756\n",
      "Validation loss (no improvement): 0.025128096342086792\n",
      "Training iteration: 2757\n",
      "Validation loss (no improvement): 0.02504666745662689\n",
      "Training iteration: 2758\n",
      "Validation loss (no improvement): 0.025400656461715698\n",
      "Training iteration: 2759\n",
      "Validation loss (no improvement): 0.025698280334472655\n",
      "Training iteration: 2760\n",
      "Validation loss (no improvement): 0.025382685661315917\n",
      "Training iteration: 2761\n",
      "Validation loss (no improvement): 0.025336521863937377\n",
      "Training iteration: 2762\n",
      "Validation loss (no improvement): 0.0257571280002594\n",
      "Training iteration: 2763\n",
      "Validation loss (no improvement): 0.02639686465263367\n",
      "Training iteration: 2764\n",
      "Validation loss (no improvement): 0.02550508677959442\n",
      "Training iteration: 2765\n",
      "Validation loss (no improvement): 0.025273871421813966\n",
      "Training iteration: 2766\n",
      "Validation loss (no improvement): 0.025398030877113342\n",
      "Training iteration: 2767\n",
      "Validation loss (no improvement): 0.025581273436546325\n",
      "Training iteration: 2768\n",
      "Validation loss (no improvement): 0.025425666570663454\n",
      "Training iteration: 2769\n",
      "Validation loss (no improvement): 0.02542446255683899\n",
      "Training iteration: 2770\n",
      "Validation loss (no improvement): 0.025618019700050353\n",
      "Training iteration: 2771\n",
      "Validation loss (no improvement): 0.025411230325698853\n",
      "Training iteration: 2772\n",
      "Validation loss (no improvement): 0.02484036386013031\n",
      "Training iteration: 2773\n",
      "Improved validation loss from: 0.024828240275382996  to: 0.02453281432390213\n",
      "Training iteration: 2774\n",
      "Improved validation loss from: 0.02453281432390213  to: 0.024331621825695038\n",
      "Training iteration: 2775\n",
      "Validation loss (no improvement): 0.024691267311573027\n",
      "Training iteration: 2776\n",
      "Validation loss (no improvement): 0.024758605659008025\n",
      "Training iteration: 2777\n",
      "Validation loss (no improvement): 0.024345949292182922\n",
      "Training iteration: 2778\n",
      "Improved validation loss from: 0.024331621825695038  to: 0.024279527366161346\n",
      "Training iteration: 2779\n",
      "Validation loss (no improvement): 0.024413862824440004\n",
      "Training iteration: 2780\n",
      "Validation loss (no improvement): 0.024464991688728333\n",
      "Training iteration: 2781\n",
      "Validation loss (no improvement): 0.02448391616344452\n",
      "Training iteration: 2782\n",
      "Validation loss (no improvement): 0.024453321099281312\n",
      "Training iteration: 2783\n",
      "Improved validation loss from: 0.024279527366161346  to: 0.024110002815723418\n",
      "Training iteration: 2784\n",
      "Improved validation loss from: 0.024110002815723418  to: 0.023817233741283417\n",
      "Training iteration: 2785\n",
      "Improved validation loss from: 0.023817233741283417  to: 0.0236657053232193\n",
      "Training iteration: 2786\n",
      "Improved validation loss from: 0.0236657053232193  to: 0.023526456952095032\n",
      "Training iteration: 2787\n",
      "Improved validation loss from: 0.023526456952095032  to: 0.02338251769542694\n",
      "Training iteration: 2788\n",
      "Validation loss (no improvement): 0.023427848517894746\n",
      "Training iteration: 2789\n",
      "Improved validation loss from: 0.02338251769542694  to: 0.02317267656326294\n",
      "Training iteration: 2790\n",
      "Improved validation loss from: 0.02317267656326294  to: 0.02315252721309662\n",
      "Training iteration: 2791\n",
      "Validation loss (no improvement): 0.02323794662952423\n",
      "Training iteration: 2792\n",
      "Improved validation loss from: 0.02315252721309662  to: 0.02302607297897339\n",
      "Training iteration: 2793\n",
      "Validation loss (no improvement): 0.023234422504901885\n",
      "Training iteration: 2794\n",
      "Validation loss (no improvement): 0.0236486479640007\n",
      "Training iteration: 2795\n",
      "Validation loss (no improvement): 0.023805804550647736\n",
      "Training iteration: 2796\n",
      "Validation loss (no improvement): 0.023512573540210725\n",
      "Training iteration: 2797\n",
      "Validation loss (no improvement): 0.023417457938194275\n",
      "Training iteration: 2798\n",
      "Validation loss (no improvement): 0.023644647002220152\n",
      "Training iteration: 2799\n",
      "Validation loss (no improvement): 0.023859015107154845\n",
      "Training iteration: 2800\n",
      "Validation loss (no improvement): 0.023515982925891875\n",
      "Training iteration: 2801\n",
      "Validation loss (no improvement): 0.023340705037117004\n",
      "Training iteration: 2802\n",
      "Validation loss (no improvement): 0.023492607474327087\n",
      "Training iteration: 2803\n",
      "Validation loss (no improvement): 0.02353603094816208\n",
      "Training iteration: 2804\n",
      "Validation loss (no improvement): 0.023397549986839294\n",
      "Training iteration: 2805\n",
      "Validation loss (no improvement): 0.023405881226062776\n",
      "Training iteration: 2806\n",
      "Validation loss (no improvement): 0.023998753726482393\n",
      "Training iteration: 2807\n",
      "Validation loss (no improvement): 0.024325188994407655\n",
      "Training iteration: 2808\n",
      "Validation loss (no improvement): 0.023770542442798616\n",
      "Training iteration: 2809\n",
      "Validation loss (no improvement): 0.0237930566072464\n",
      "Training iteration: 2810\n",
      "Validation loss (no improvement): 0.02357449531555176\n",
      "Training iteration: 2811\n",
      "Validation loss (no improvement): 0.023887351155281067\n",
      "Training iteration: 2812\n",
      "Validation loss (no improvement): 0.024657659232616425\n",
      "Training iteration: 2813\n",
      "Validation loss (no improvement): 0.023946166038513184\n",
      "Training iteration: 2814\n",
      "Validation loss (no improvement): 0.023409828543663025\n",
      "Training iteration: 2815\n",
      "Validation loss (no improvement): 0.023323091864585876\n",
      "Training iteration: 2816\n",
      "Validation loss (no improvement): 0.02340788543224335\n",
      "Training iteration: 2817\n",
      "Improved validation loss from: 0.02302607297897339  to: 0.022896942496299744\n",
      "Training iteration: 2818\n",
      "Improved validation loss from: 0.022896942496299744  to: 0.022746749222278595\n",
      "Training iteration: 2819\n",
      "Validation loss (no improvement): 0.023148413002490997\n",
      "Training iteration: 2820\n",
      "Validation loss (no improvement): 0.023041710257530212\n",
      "Training iteration: 2821\n",
      "Improved validation loss from: 0.022746749222278595  to: 0.02233450412750244\n",
      "Training iteration: 2822\n",
      "Validation loss (no improvement): 0.02237476110458374\n",
      "Training iteration: 2823\n",
      "Validation loss (no improvement): 0.02273102253675461\n",
      "Training iteration: 2824\n",
      "Validation loss (no improvement): 0.022853155434131623\n",
      "Training iteration: 2825\n",
      "Improved validation loss from: 0.02233450412750244  to: 0.022209863364696502\n",
      "Training iteration: 2826\n",
      "Improved validation loss from: 0.022209863364696502  to: 0.022071699798107147\n",
      "Training iteration: 2827\n",
      "Validation loss (no improvement): 0.022384047508239746\n",
      "Training iteration: 2828\n",
      "Validation loss (no improvement): 0.02264329940080643\n",
      "Training iteration: 2829\n",
      "Improved validation loss from: 0.022071699798107147  to: 0.022028259932994843\n",
      "Training iteration: 2830\n",
      "Validation loss (no improvement): 0.022149887681007386\n",
      "Training iteration: 2831\n",
      "Validation loss (no improvement): 0.02284364402294159\n",
      "Training iteration: 2832\n",
      "Validation loss (no improvement): 0.02310118228197098\n",
      "Training iteration: 2833\n",
      "Validation loss (no improvement): 0.02238354682922363\n",
      "Training iteration: 2834\n",
      "Validation loss (no improvement): 0.022270965576171874\n",
      "Training iteration: 2835\n",
      "Validation loss (no improvement): 0.02264559268951416\n",
      "Training iteration: 2836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.022984004020690917\n",
      "Training iteration: 2837\n",
      "Validation loss (no improvement): 0.022704455256462096\n",
      "Training iteration: 2838\n",
      "Validation loss (no improvement): 0.022808580100536345\n",
      "Training iteration: 2839\n",
      "Validation loss (no improvement): 0.023132677376270293\n",
      "Training iteration: 2840\n",
      "Validation loss (no improvement): 0.023204588890075685\n",
      "Training iteration: 2841\n",
      "Validation loss (no improvement): 0.022603411972522736\n",
      "Training iteration: 2842\n",
      "Validation loss (no improvement): 0.022367818653583525\n",
      "Training iteration: 2843\n",
      "Validation loss (no improvement): 0.022193022072315216\n",
      "Training iteration: 2844\n",
      "Validation loss (no improvement): 0.022090010344982147\n",
      "Training iteration: 2845\n",
      "Improved validation loss from: 0.022028259932994843  to: 0.02172810137271881\n",
      "Training iteration: 2846\n",
      "Improved validation loss from: 0.02172810137271881  to: 0.021726472675800322\n",
      "Training iteration: 2847\n",
      "Validation loss (no improvement): 0.022205212712287904\n",
      "Training iteration: 2848\n",
      "Validation loss (no improvement): 0.022196002304553986\n",
      "Training iteration: 2849\n",
      "Improved validation loss from: 0.021726472675800322  to: 0.02171967029571533\n",
      "Training iteration: 2850\n",
      "Improved validation loss from: 0.02171967029571533  to: 0.021300017833709717\n",
      "Training iteration: 2851\n",
      "Validation loss (no improvement): 0.021320624649524687\n",
      "Training iteration: 2852\n",
      "Validation loss (no improvement): 0.0215790793299675\n",
      "Training iteration: 2853\n",
      "Validation loss (no improvement): 0.02144317626953125\n",
      "Training iteration: 2854\n",
      "Improved validation loss from: 0.021300017833709717  to: 0.02114345133304596\n",
      "Training iteration: 2855\n",
      "Improved validation loss from: 0.02114345133304596  to: 0.02109738290309906\n",
      "Training iteration: 2856\n",
      "Validation loss (no improvement): 0.021301765739917756\n",
      "Training iteration: 2857\n",
      "Improved validation loss from: 0.02109738290309906  to: 0.021031764149665833\n",
      "Training iteration: 2858\n",
      "Improved validation loss from: 0.021031764149665833  to: 0.021029821038246153\n",
      "Training iteration: 2859\n",
      "Validation loss (no improvement): 0.021479320526123048\n",
      "Training iteration: 2860\n",
      "Validation loss (no improvement): 0.02163875550031662\n",
      "Training iteration: 2861\n",
      "Validation loss (no improvement): 0.021034708619117735\n",
      "Training iteration: 2862\n",
      "Validation loss (no improvement): 0.02121376097202301\n",
      "Training iteration: 2863\n",
      "Validation loss (no improvement): 0.021721649169921874\n",
      "Training iteration: 2864\n",
      "Validation loss (no improvement): 0.021739272773265837\n",
      "Training iteration: 2865\n",
      "Improved validation loss from: 0.021029821038246153  to: 0.020876140892505647\n",
      "Training iteration: 2866\n",
      "Validation loss (no improvement): 0.020945525169372557\n",
      "Training iteration: 2867\n",
      "Validation loss (no improvement): 0.021570129692554472\n",
      "Training iteration: 2868\n",
      "Validation loss (no improvement): 0.021274706721305846\n",
      "Training iteration: 2869\n",
      "Validation loss (no improvement): 0.021373264491558075\n",
      "Training iteration: 2870\n",
      "Validation loss (no improvement): 0.02167210578918457\n",
      "Training iteration: 2871\n",
      "Validation loss (no improvement): 0.021716921031475066\n",
      "Training iteration: 2872\n",
      "Validation loss (no improvement): 0.021278946101665495\n",
      "Training iteration: 2873\n",
      "Improved validation loss from: 0.020876140892505647  to: 0.020851294696331023\n",
      "Training iteration: 2874\n",
      "Validation loss (no improvement): 0.020901450514793397\n",
      "Training iteration: 2875\n",
      "Validation loss (no improvement): 0.020955237746238708\n",
      "Training iteration: 2876\n",
      "Improved validation loss from: 0.020851294696331023  to: 0.02067941129207611\n",
      "Training iteration: 2877\n",
      "Improved validation loss from: 0.02067941129207611  to: 0.020374253392219543\n",
      "Training iteration: 2878\n",
      "Validation loss (no improvement): 0.020463724434375764\n",
      "Training iteration: 2879\n",
      "Validation loss (no improvement): 0.02102371007204056\n",
      "Training iteration: 2880\n",
      "Validation loss (no improvement): 0.0205922931432724\n",
      "Training iteration: 2881\n",
      "Improved validation loss from: 0.020374253392219543  to: 0.019941307604312897\n",
      "Training iteration: 2882\n",
      "Validation loss (no improvement): 0.01999003440141678\n",
      "Training iteration: 2883\n",
      "Validation loss (no improvement): 0.021182656288146973\n",
      "Training iteration: 2884\n",
      "Validation loss (no improvement): 0.02020042687654495\n",
      "Training iteration: 2885\n",
      "Improved validation loss from: 0.019941307604312897  to: 0.019841592013835906\n",
      "Training iteration: 2886\n",
      "Validation loss (no improvement): 0.020368489623069762\n",
      "Training iteration: 2887\n",
      "Validation loss (no improvement): 0.02075751721858978\n",
      "Training iteration: 2888\n",
      "Validation loss (no improvement): 0.019998374581336974\n",
      "Training iteration: 2889\n",
      "Validation loss (no improvement): 0.020041652023792267\n",
      "Training iteration: 2890\n",
      "Validation loss (no improvement): 0.020712156593799592\n",
      "Training iteration: 2891\n",
      "Validation loss (no improvement): 0.02123810052871704\n",
      "Training iteration: 2892\n",
      "Validation loss (no improvement): 0.02053288519382477\n",
      "Training iteration: 2893\n",
      "Validation loss (no improvement): 0.020346751809120177\n",
      "Training iteration: 2894\n",
      "Validation loss (no improvement): 0.021395310759544373\n",
      "Training iteration: 2895\n",
      "Validation loss (no improvement): 0.023227322101593017\n",
      "Training iteration: 2896\n",
      "Validation loss (no improvement): 0.021956229209899904\n",
      "Training iteration: 2897\n",
      "Validation loss (no improvement): 0.020908577740192412\n",
      "Training iteration: 2898\n",
      "Validation loss (no improvement): 0.021026864647865295\n",
      "Training iteration: 2899\n",
      "Validation loss (no improvement): 0.021386881172657014\n",
      "Training iteration: 2900\n",
      "Validation loss (no improvement): 0.021995869278907777\n",
      "Training iteration: 2901\n",
      "Validation loss (no improvement): 0.02092677801847458\n",
      "Training iteration: 2902\n",
      "Validation loss (no improvement): 0.020694883167743684\n",
      "Training iteration: 2903\n",
      "Validation loss (no improvement): 0.02086239606142044\n",
      "Training iteration: 2904\n",
      "Validation loss (no improvement): 0.02085227221250534\n",
      "Training iteration: 2905\n",
      "Validation loss (no improvement): 0.02004532366991043\n",
      "Training iteration: 2906\n",
      "Improved validation loss from: 0.019841592013835906  to: 0.019611044228076933\n",
      "Training iteration: 2907\n",
      "Validation loss (no improvement): 0.019646456837654112\n",
      "Training iteration: 2908\n",
      "Validation loss (no improvement): 0.020015239715576172\n",
      "Training iteration: 2909\n",
      "Improved validation loss from: 0.019611044228076933  to: 0.01950831711292267\n",
      "Training iteration: 2910\n",
      "Improved validation loss from: 0.01950831711292267  to: 0.019214868545532227\n",
      "Training iteration: 2911\n",
      "Validation loss (no improvement): 0.019275596737861632\n",
      "Training iteration: 2912\n",
      "Improved validation loss from: 0.019214868545532227  to: 0.01915100812911987\n",
      "Training iteration: 2913\n",
      "Improved validation loss from: 0.01915100812911987  to: 0.018605902791023254\n",
      "Training iteration: 2914\n",
      "Improved validation loss from: 0.018605902791023254  to: 0.01856618672609329\n",
      "Training iteration: 2915\n",
      "Validation loss (no improvement): 0.018777313828468322\n",
      "Training iteration: 2916\n",
      "Validation loss (no improvement): 0.018607111275196077\n",
      "Training iteration: 2917\n",
      "Improved validation loss from: 0.01856618672609329  to: 0.018482038378715517\n",
      "Training iteration: 2918\n",
      "Validation loss (no improvement): 0.018525584042072295\n",
      "Training iteration: 2919\n",
      "Validation loss (no improvement): 0.018669266998767853\n",
      "Training iteration: 2920\n",
      "Validation loss (no improvement): 0.01866925656795502\n",
      "Training iteration: 2921\n",
      "Improved validation loss from: 0.018482038378715517  to: 0.018141886591911315\n",
      "Training iteration: 2922\n",
      "Validation loss (no improvement): 0.0182049959897995\n",
      "Training iteration: 2923\n",
      "Validation loss (no improvement): 0.018299111723899843\n",
      "Training iteration: 2924\n",
      "Validation loss (no improvement): 0.0182206392288208\n",
      "Training iteration: 2925\n",
      "Validation loss (no improvement): 0.018431586027145386\n",
      "Training iteration: 2926\n",
      "Validation loss (no improvement): 0.018800541758537292\n",
      "Training iteration: 2927\n",
      "Validation loss (no improvement): 0.018750235438346863\n",
      "Training iteration: 2928\n",
      "Improved validation loss from: 0.018141886591911315  to: 0.01795344352722168\n",
      "Training iteration: 2929\n",
      "Improved validation loss from: 0.01795344352722168  to: 0.017860178649425507\n",
      "Training iteration: 2930\n",
      "Validation loss (no improvement): 0.0181375190615654\n",
      "Training iteration: 2931\n",
      "Validation loss (no improvement): 0.018328669667243957\n",
      "Training iteration: 2932\n",
      "Validation loss (no improvement): 0.018239039182662963\n",
      "Training iteration: 2933\n",
      "Validation loss (no improvement): 0.01809566468000412\n",
      "Training iteration: 2934\n",
      "Validation loss (no improvement): 0.01865215003490448\n",
      "Training iteration: 2935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.018323971331119536\n",
      "Training iteration: 2936\n",
      "Improved validation loss from: 0.017860178649425507  to: 0.01761174649000168\n",
      "Training iteration: 2937\n",
      "Validation loss (no improvement): 0.017672105133533476\n",
      "Training iteration: 2938\n",
      "Validation loss (no improvement): 0.01781298816204071\n",
      "Training iteration: 2939\n",
      "Validation loss (no improvement): 0.018898148834705353\n",
      "Training iteration: 2940\n",
      "Validation loss (no improvement): 0.018321025371551513\n",
      "Training iteration: 2941\n",
      "Validation loss (no improvement): 0.017804715037345886\n",
      "Training iteration: 2942\n",
      "Validation loss (no improvement): 0.017718467116355895\n",
      "Training iteration: 2943\n",
      "Validation loss (no improvement): 0.01892908662557602\n",
      "Training iteration: 2944\n",
      "Validation loss (no improvement): 0.01880793124437332\n",
      "Training iteration: 2945\n",
      "Improved validation loss from: 0.01761174649000168  to: 0.01753431111574173\n",
      "Training iteration: 2946\n",
      "Improved validation loss from: 0.01753431111574173  to: 0.017265674471855164\n",
      "Training iteration: 2947\n",
      "Validation loss (no improvement): 0.01778528094291687\n",
      "Training iteration: 2948\n",
      "Validation loss (no improvement): 0.01776755750179291\n",
      "Training iteration: 2949\n",
      "Validation loss (no improvement): 0.017349246144294738\n",
      "Training iteration: 2950\n",
      "Validation loss (no improvement): 0.01760401725769043\n",
      "Training iteration: 2951\n",
      "Validation loss (no improvement): 0.018282946944236756\n",
      "Training iteration: 2952\n",
      "Validation loss (no improvement): 0.018019355833530426\n",
      "Training iteration: 2953\n",
      "Validation loss (no improvement): 0.017783962190151215\n",
      "Training iteration: 2954\n",
      "Validation loss (no improvement): 0.017532262206077575\n",
      "Training iteration: 2955\n",
      "Validation loss (no improvement): 0.01752028465270996\n",
      "Training iteration: 2956\n",
      "Validation loss (no improvement): 0.017806749045848846\n",
      "Training iteration: 2957\n",
      "Improved validation loss from: 0.017265674471855164  to: 0.01694449931383133\n",
      "Training iteration: 2958\n",
      "Improved validation loss from: 0.01694449931383133  to: 0.01693032532930374\n",
      "Training iteration: 2959\n",
      "Validation loss (no improvement): 0.01801387667655945\n",
      "Training iteration: 2960\n",
      "Validation loss (no improvement): 0.018281613290309907\n",
      "Training iteration: 2961\n",
      "Validation loss (no improvement): 0.01726119816303253\n",
      "Training iteration: 2962\n",
      "Validation loss (no improvement): 0.016955380141735078\n",
      "Training iteration: 2963\n",
      "Validation loss (no improvement): 0.018474459648132324\n",
      "Training iteration: 2964\n",
      "Validation loss (no improvement): 0.020055410265922547\n",
      "Training iteration: 2965\n",
      "Validation loss (no improvement): 0.01800374984741211\n",
      "Training iteration: 2966\n",
      "Validation loss (no improvement): 0.017040900886058807\n",
      "Training iteration: 2967\n",
      "Validation loss (no improvement): 0.017710068821907045\n",
      "Training iteration: 2968\n",
      "Validation loss (no improvement): 0.018818345665931702\n",
      "Training iteration: 2969\n",
      "Validation loss (no improvement): 0.01846251040697098\n",
      "Training iteration: 2970\n",
      "Validation loss (no improvement): 0.017439359426498414\n",
      "Training iteration: 2971\n",
      "Validation loss (no improvement): 0.017364147305488586\n",
      "Training iteration: 2972\n",
      "Validation loss (no improvement): 0.018018582463264467\n",
      "Training iteration: 2973\n",
      "Validation loss (no improvement): 0.01830049753189087\n",
      "Training iteration: 2974\n",
      "Validation loss (no improvement): 0.017675893008708955\n",
      "Training iteration: 2975\n",
      "Validation loss (no improvement): 0.017809215188026428\n",
      "Training iteration: 2976\n",
      "Validation loss (no improvement): 0.018144106864929198\n",
      "Training iteration: 2977\n",
      "Validation loss (no improvement): 0.01798630952835083\n",
      "Training iteration: 2978\n",
      "Validation loss (no improvement): 0.017227597534656525\n",
      "Training iteration: 2979\n",
      "Validation loss (no improvement): 0.017001600563526155\n",
      "Training iteration: 2980\n",
      "Validation loss (no improvement): 0.0173242449760437\n",
      "Training iteration: 2981\n",
      "Validation loss (no improvement): 0.017103271186351778\n",
      "Training iteration: 2982\n",
      "Improved validation loss from: 0.01693032532930374  to: 0.016483725607395174\n",
      "Training iteration: 2983\n",
      "Improved validation loss from: 0.016483725607395174  to: 0.016371385753154756\n",
      "Training iteration: 2984\n",
      "Validation loss (no improvement): 0.01743362843990326\n",
      "Training iteration: 2985\n",
      "Validation loss (no improvement): 0.017010498046875\n",
      "Training iteration: 2986\n",
      "Improved validation loss from: 0.016371385753154756  to: 0.015896394848823547\n",
      "Training iteration: 2987\n",
      "Validation loss (no improvement): 0.016060562431812288\n",
      "Training iteration: 2988\n",
      "Validation loss (no improvement): 0.01639564335346222\n",
      "Training iteration: 2989\n",
      "Validation loss (no improvement): 0.01607833057641983\n",
      "Training iteration: 2990\n",
      "Validation loss (no improvement): 0.01592344343662262\n",
      "Training iteration: 2991\n",
      "Validation loss (no improvement): 0.016770565509796144\n",
      "Training iteration: 2992\n",
      "Validation loss (no improvement): 0.017102806270122527\n",
      "Training iteration: 2993\n",
      "Validation loss (no improvement): 0.01603967696428299\n",
      "Training iteration: 2994\n",
      "Improved validation loss from: 0.015896394848823547  to: 0.015762785077095033\n",
      "Training iteration: 2995\n",
      "Validation loss (no improvement): 0.016404007375240327\n",
      "Training iteration: 2996\n",
      "Validation loss (no improvement): 0.016835875809192657\n",
      "Training iteration: 2997\n",
      "Validation loss (no improvement): 0.01620706021785736\n",
      "Training iteration: 2998\n",
      "Validation loss (no improvement): 0.016370072960853577\n",
      "Training iteration: 2999\n",
      "Validation loss (no improvement): 0.01709568202495575\n",
      "Training iteration: 3000\n",
      "Validation loss (no improvement): 0.017065607011318207\n",
      "Training iteration: 3001\n",
      "Validation loss (no improvement): 0.016452038288116456\n",
      "Training iteration: 3002\n",
      "Validation loss (no improvement): 0.016018636524677277\n",
      "Training iteration: 3003\n",
      "Validation loss (no improvement): 0.01649131327867508\n",
      "Training iteration: 3004\n",
      "Validation loss (no improvement): 0.016579344868659973\n",
      "Training iteration: 3005\n",
      "Validation loss (no improvement): 0.015965776145458223\n",
      "Training iteration: 3006\n",
      "Validation loss (no improvement): 0.016111817955970765\n",
      "Training iteration: 3007\n",
      "Validation loss (no improvement): 0.01631919890642166\n",
      "Training iteration: 3008\n",
      "Validation loss (no improvement): 0.01609337031841278\n",
      "Training iteration: 3009\n",
      "Improved validation loss from: 0.015762785077095033  to: 0.015439505875110626\n",
      "Training iteration: 3010\n",
      "Validation loss (no improvement): 0.01559230387210846\n",
      "Training iteration: 3011\n",
      "Validation loss (no improvement): 0.016219492256641387\n",
      "Training iteration: 3012\n",
      "Validation loss (no improvement): 0.01545223742723465\n",
      "Training iteration: 3013\n",
      "Validation loss (no improvement): 0.015613631904125213\n",
      "Training iteration: 3014\n",
      "Validation loss (no improvement): 0.016875137388706208\n",
      "Training iteration: 3015\n",
      "Validation loss (no improvement): 0.016767077147960663\n",
      "Training iteration: 3016\n",
      "Validation loss (no improvement): 0.015575101971626282\n",
      "Training iteration: 3017\n",
      "Validation loss (no improvement): 0.015487918257713318\n",
      "Training iteration: 3018\n",
      "Validation loss (no improvement): 0.016063779592514038\n",
      "Training iteration: 3019\n",
      "Validation loss (no improvement): 0.015473195910453796\n",
      "Training iteration: 3020\n",
      "Validation loss (no improvement): 0.01581035852432251\n",
      "Training iteration: 3021\n",
      "Validation loss (no improvement): 0.0160783126950264\n",
      "Training iteration: 3022\n",
      "Validation loss (no improvement): 0.016507558524608612\n",
      "Training iteration: 3023\n",
      "Validation loss (no improvement): 0.016012142598628997\n",
      "Training iteration: 3024\n",
      "Validation loss (no improvement): 0.01591891348361969\n",
      "Training iteration: 3025\n",
      "Validation loss (no improvement): 0.01605747938156128\n",
      "Training iteration: 3026\n",
      "Validation loss (no improvement): 0.016545191407203674\n",
      "Training iteration: 3027\n",
      "Validation loss (no improvement): 0.016050511598587038\n",
      "Training iteration: 3028\n",
      "Validation loss (no improvement): 0.016140362620353697\n",
      "Training iteration: 3029\n",
      "Validation loss (no improvement): 0.016535413265228272\n",
      "Training iteration: 3030\n",
      "Validation loss (no improvement): 0.01644780784845352\n",
      "Training iteration: 3031\n",
      "Improved validation loss from: 0.015439505875110626  to: 0.015330883860588073\n",
      "Training iteration: 3032\n",
      "Improved validation loss from: 0.015330883860588073  to: 0.015115432441234589\n",
      "Training iteration: 3033\n",
      "Validation loss (no improvement): 0.01581781804561615\n",
      "Training iteration: 3034\n",
      "Validation loss (no improvement): 0.015931835770606993\n",
      "Training iteration: 3035\n",
      "Validation loss (no improvement): 0.016216251254081725\n",
      "Training iteration: 3036\n",
      "Validation loss (no improvement): 0.016834725439548493\n",
      "Training iteration: 3037\n",
      "Validation loss (no improvement): 0.017064113914966584\n",
      "Training iteration: 3038\n",
      "Validation loss (no improvement): 0.01605571061372757\n",
      "Training iteration: 3039\n",
      "Validation loss (no improvement): 0.015664520859718322\n",
      "Training iteration: 3040\n",
      "Validation loss (no improvement): 0.015974774956703186\n",
      "Training iteration: 3041\n",
      "Validation loss (no improvement): 0.016332896053791048\n",
      "Training iteration: 3042\n",
      "Validation loss (no improvement): 0.016529765725135804\n",
      "Training iteration: 3043\n",
      "Validation loss (no improvement): 0.01546010673046112\n",
      "Training iteration: 3044\n",
      "Validation loss (no improvement): 0.015295229852199554\n",
      "Training iteration: 3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.01546376496553421\n",
      "Training iteration: 3046\n",
      "Validation loss (no improvement): 0.01555849015712738\n",
      "Training iteration: 3047\n",
      "Improved validation loss from: 0.015115432441234589  to: 0.014902833104133605\n",
      "Training iteration: 3048\n",
      "Validation loss (no improvement): 0.015514105558395386\n",
      "Training iteration: 3049\n",
      "Validation loss (no improvement): 0.016608628630638122\n",
      "Training iteration: 3050\n",
      "Validation loss (no improvement): 0.016079017519950868\n",
      "Training iteration: 3051\n",
      "Validation loss (no improvement): 0.016834905743598937\n",
      "Training iteration: 3052\n",
      "Validation loss (no improvement): 0.016300585865974427\n",
      "Training iteration: 3053\n",
      "Validation loss (no improvement): 0.016933996975421906\n",
      "Training iteration: 3054\n",
      "Validation loss (no improvement): 0.017205724120140077\n",
      "Training iteration: 3055\n",
      "Validation loss (no improvement): 0.015571728348731995\n",
      "Training iteration: 3056\n",
      "Validation loss (no improvement): 0.015229044854640961\n",
      "Training iteration: 3057\n",
      "Validation loss (no improvement): 0.016903609037399292\n",
      "Training iteration: 3058\n",
      "Validation loss (no improvement): 0.017140577733516692\n",
      "Training iteration: 3059\n",
      "Validation loss (no improvement): 0.016701750457286835\n",
      "Training iteration: 3060\n",
      "Validation loss (no improvement): 0.017263756692409517\n",
      "Training iteration: 3061\n",
      "Validation loss (no improvement): 0.018141886591911315\n",
      "Training iteration: 3062\n",
      "Validation loss (no improvement): 0.018757760524749756\n",
      "Training iteration: 3063\n",
      "Validation loss (no improvement): 0.018026769161224365\n",
      "Training iteration: 3064\n",
      "Validation loss (no improvement): 0.01669631451368332\n",
      "Training iteration: 3065\n",
      "Validation loss (no improvement): 0.016617496311664582\n",
      "Training iteration: 3066\n",
      "Validation loss (no improvement): 0.01702294498682022\n",
      "Training iteration: 3067\n",
      "Validation loss (no improvement): 0.017029356956481934\n",
      "Training iteration: 3068\n",
      "Validation loss (no improvement): 0.01671154946088791\n",
      "Training iteration: 3069\n",
      "Validation loss (no improvement): 0.01676652431488037\n",
      "Training iteration: 3070\n",
      "Validation loss (no improvement): 0.017335686087608337\n",
      "Training iteration: 3071\n",
      "Validation loss (no improvement): 0.017773354053497316\n",
      "Training iteration: 3072\n",
      "Validation loss (no improvement): 0.01664532572031021\n",
      "Training iteration: 3073\n",
      "Validation loss (no improvement): 0.016058187186717986\n",
      "Training iteration: 3074\n",
      "Validation loss (no improvement): 0.015455496311187745\n",
      "Training iteration: 3075\n",
      "Validation loss (no improvement): 0.015505142509937286\n",
      "Training iteration: 3076\n",
      "Validation loss (no improvement): 0.015478704869747163\n",
      "Training iteration: 3077\n",
      "Validation loss (no improvement): 0.015116438269615173\n",
      "Training iteration: 3078\n",
      "Validation loss (no improvement): 0.015621998906135559\n",
      "Training iteration: 3079\n",
      "Validation loss (no improvement): 0.015238922834396363\n",
      "Training iteration: 3080\n",
      "Validation loss (no improvement): 0.01532263457775116\n",
      "Training iteration: 3081\n",
      "Validation loss (no improvement): 0.015445785224437713\n",
      "Training iteration: 3082\n",
      "Improved validation loss from: 0.014902833104133605  to: 0.014209596812725067\n",
      "Training iteration: 3083\n",
      "Improved validation loss from: 0.014209596812725067  to: 0.014075107872486115\n",
      "Training iteration: 3084\n",
      "Validation loss (no improvement): 0.01512121707201004\n",
      "Training iteration: 3085\n",
      "Validation loss (no improvement): 0.01602804958820343\n",
      "Training iteration: 3086\n",
      "Validation loss (no improvement): 0.016173923015594484\n",
      "Training iteration: 3087\n",
      "Validation loss (no improvement): 0.015603482723236084\n",
      "Training iteration: 3088\n",
      "Validation loss (no improvement): 0.015494026243686676\n",
      "Training iteration: 3089\n",
      "Validation loss (no improvement): 0.016058650612831116\n",
      "Training iteration: 3090\n",
      "Validation loss (no improvement): 0.016328611969947816\n",
      "Training iteration: 3091\n",
      "Validation loss (no improvement): 0.015564748644828796\n",
      "Training iteration: 3092\n",
      "Validation loss (no improvement): 0.015564948320388794\n",
      "Training iteration: 3093\n",
      "Validation loss (no improvement): 0.01678188443183899\n",
      "Training iteration: 3094\n",
      "Validation loss (no improvement): 0.01727592647075653\n",
      "Training iteration: 3095\n",
      "Validation loss (no improvement): 0.016733083128929137\n",
      "Training iteration: 3096\n",
      "Validation loss (no improvement): 0.017008662223815918\n",
      "Training iteration: 3097\n",
      "Validation loss (no improvement): 0.017331522703170777\n",
      "Training iteration: 3098\n",
      "Validation loss (no improvement): 0.017269375920295715\n",
      "Training iteration: 3099\n",
      "Validation loss (no improvement): 0.01630292981863022\n",
      "Training iteration: 3100\n",
      "Validation loss (no improvement): 0.015464341640472412\n",
      "Training iteration: 3101\n",
      "Validation loss (no improvement): 0.015049925446510315\n",
      "Training iteration: 3102\n",
      "Validation loss (no improvement): 0.015253688395023345\n",
      "Training iteration: 3103\n",
      "Validation loss (no improvement): 0.015148010849952698\n",
      "Training iteration: 3104\n",
      "Validation loss (no improvement): 0.015241701900959016\n",
      "Training iteration: 3105\n",
      "Validation loss (no improvement): 0.015244333446025849\n",
      "Training iteration: 3106\n",
      "Validation loss (no improvement): 0.015504077076911926\n",
      "Training iteration: 3107\n",
      "Validation loss (no improvement): 0.01534503698348999\n",
      "Training iteration: 3108\n",
      "Validation loss (no improvement): 0.014481818675994873\n",
      "Training iteration: 3109\n",
      "Validation loss (no improvement): 0.014525380730628968\n",
      "Training iteration: 3110\n",
      "Validation loss (no improvement): 0.015034833550453186\n",
      "Training iteration: 3111\n",
      "Validation loss (no improvement): 0.0149446040391922\n",
      "Training iteration: 3112\n",
      "Validation loss (no improvement): 0.014550086855888367\n",
      "Training iteration: 3113\n",
      "Validation loss (no improvement): 0.014549736678600312\n",
      "Training iteration: 3114\n",
      "Validation loss (no improvement): 0.014708806574344636\n",
      "Training iteration: 3115\n",
      "Validation loss (no improvement): 0.014138107001781464\n",
      "Training iteration: 3116\n",
      "Validation loss (no improvement): 0.014174413681030274\n",
      "Training iteration: 3117\n",
      "Validation loss (no improvement): 0.014672201871871949\n",
      "Training iteration: 3118\n",
      "Validation loss (no improvement): 0.014403066039085389\n",
      "Training iteration: 3119\n",
      "Validation loss (no improvement): 0.01484559029340744\n",
      "Training iteration: 3120\n",
      "Validation loss (no improvement): 0.0158064141869545\n",
      "Training iteration: 3121\n",
      "Validation loss (no improvement): 0.015921130776405334\n",
      "Training iteration: 3122\n",
      "Validation loss (no improvement): 0.015164747834205627\n",
      "Training iteration: 3123\n",
      "Validation loss (no improvement): 0.015101230144500733\n",
      "Training iteration: 3124\n",
      "Validation loss (no improvement): 0.015223781764507293\n",
      "Training iteration: 3125\n",
      "Validation loss (no improvement): 0.014639005064964294\n",
      "Training iteration: 3126\n",
      "Validation loss (no improvement): 0.014532849192619324\n",
      "Training iteration: 3127\n",
      "Validation loss (no improvement): 0.014374132454395293\n",
      "Training iteration: 3128\n",
      "Validation loss (no improvement): 0.015055815875530242\n",
      "Training iteration: 3129\n",
      "Validation loss (no improvement): 0.014839042723178864\n",
      "Training iteration: 3130\n",
      "Improved validation loss from: 0.014075107872486115  to: 0.014001227915287018\n",
      "Training iteration: 3131\n",
      "Validation loss (no improvement): 0.014217059314250945\n",
      "Training iteration: 3132\n",
      "Validation loss (no improvement): 0.014785392582416535\n",
      "Training iteration: 3133\n",
      "Validation loss (no improvement): 0.014685091376304627\n",
      "Training iteration: 3134\n",
      "Validation loss (no improvement): 0.015174469351768494\n",
      "Training iteration: 3135\n",
      "Validation loss (no improvement): 0.016211216151714326\n",
      "Training iteration: 3136\n",
      "Validation loss (no improvement): 0.016594724357128145\n",
      "Training iteration: 3137\n",
      "Validation loss (no improvement): 0.015617434680461884\n",
      "Training iteration: 3138\n",
      "Validation loss (no improvement): 0.014401082694530488\n",
      "Training iteration: 3139\n",
      "Validation loss (no improvement): 0.014115799963474274\n",
      "Training iteration: 3140\n",
      "Validation loss (no improvement): 0.014551523327827453\n",
      "Training iteration: 3141\n",
      "Validation loss (no improvement): 0.014492371678352356\n",
      "Training iteration: 3142\n",
      "Validation loss (no improvement): 0.014104212820529937\n",
      "Training iteration: 3143\n",
      "Validation loss (no improvement): 0.01434391587972641\n",
      "Training iteration: 3144\n",
      "Validation loss (no improvement): 0.0152350053191185\n",
      "Training iteration: 3145\n",
      "Validation loss (no improvement): 0.015626557171344757\n",
      "Training iteration: 3146\n",
      "Validation loss (no improvement): 0.015321335196495056\n",
      "Training iteration: 3147\n",
      "Validation loss (no improvement): 0.014663779735565185\n",
      "Training iteration: 3148\n",
      "Validation loss (no improvement): 0.014386124908924103\n",
      "Training iteration: 3149\n",
      "Validation loss (no improvement): 0.01439243257045746\n",
      "Training iteration: 3150\n",
      "Validation loss (no improvement): 0.014324942231178283\n",
      "Training iteration: 3151\n",
      "Validation loss (no improvement): 0.014251494407653808\n",
      "Training iteration: 3152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss (no improvement): 0.014584493637084962\n",
      "Training iteration: 3153\n",
      "Validation loss (no improvement): 0.014517535269260407\n",
      "Training iteration: 3154\n",
      "Improved validation loss from: 0.014001227915287018  to: 0.013861091434955597\n",
      "Training iteration: 3155\n",
      "Improved validation loss from: 0.013861091434955597  to: 0.01368635892868042\n",
      "Training iteration: 3156\n",
      "Validation loss (no improvement): 0.01378404200077057\n",
      "Training iteration: 3157\n",
      "Validation loss (no improvement): 0.014342036843299866\n",
      "Training iteration: 3158\n",
      "Validation loss (no improvement): 0.014307849109172821\n",
      "Training iteration: 3159\n",
      "Validation loss (no improvement): 0.014328014850616456\n",
      "Training iteration: 3160\n",
      "Validation loss (no improvement): 0.01489698588848114\n",
      "Training iteration: 3161\n",
      "Validation loss (no improvement): 0.014722613990306855\n",
      "Training iteration: 3162\n",
      "Validation loss (no improvement): 0.014006085693836212\n",
      "Training iteration: 3163\n",
      "Validation loss (no improvement): 0.014002834260463715\n",
      "Training iteration: 3164\n",
      "Validation loss (no improvement): 0.01463160216808319\n",
      "Training iteration: 3165\n",
      "Validation loss (no improvement): 0.014131172001361847\n",
      "Training iteration: 3166\n",
      "Validation loss (no improvement): 0.013871099054813384\n",
      "Training iteration: 3167\n",
      "Validation loss (no improvement): 0.014236117899417877\n",
      "Training iteration: 3168\n",
      "Validation loss (no improvement): 0.014867618680000305\n",
      "Training iteration: 3169\n",
      "Validation loss (no improvement): 0.014605338871479034\n",
      "Training iteration: 3170\n",
      "Validation loss (no improvement): 0.01488722413778305\n",
      "Training iteration: 3171\n",
      "Validation loss (no improvement): 0.015327973663806916\n",
      "Training iteration: 3172\n",
      "Validation loss (no improvement): 0.015393435955047607\n",
      "Training iteration: 3173\n",
      "Validation loss (no improvement): 0.014710882306098938\n",
      "Training iteration: 3174\n",
      "Validation loss (no improvement): 0.014645178616046906\n",
      "Training iteration: 3175\n",
      "Validation loss (no improvement): 0.015009976923465729\n",
      "Training iteration: 3176\n",
      "Validation loss (no improvement): 0.015160372853279114\n",
      "Training iteration: 3177\n",
      "Validation loss (no improvement): 0.01511678695678711\n",
      "Training iteration: 3178\n",
      "Validation loss (no improvement): 0.015075999498367309\n",
      "Training iteration: 3179\n",
      "Validation loss (no improvement): 0.015540331602096558\n",
      "Training iteration: 3180\n",
      "Validation loss (no improvement): 0.015198585391044617\n",
      "Training iteration: 3181\n",
      "Validation loss (no improvement): 0.014360031485557556\n",
      "Training iteration: 3182\n",
      "Validation loss (no improvement): 0.014145712554454803\n",
      "Training iteration: 3183\n",
      "Validation loss (no improvement): 0.01413806974887848\n",
      "Training iteration: 3184\n",
      "Validation loss (no improvement): 0.014883652329444885\n",
      "Training iteration: 3185\n",
      "Validation loss (no improvement): 0.015007129311561585\n",
      "Training iteration: 3186\n",
      "Validation loss (no improvement): 0.014233405888080596\n",
      "Training iteration: 3187\n",
      "Validation loss (no improvement): 0.014150106906890869\n",
      "Training iteration: 3188\n",
      "Validation loss (no improvement): 0.014704039692878723\n",
      "Training iteration: 3189\n",
      "Validation loss (no improvement): 0.014635972678661346\n",
      "Training iteration: 3190\n",
      "Validation loss (no improvement): 0.01382094919681549\n",
      "Training iteration: 3191\n",
      "Improved validation loss from: 0.01368635892868042  to: 0.013610953092575073\n",
      "Training iteration: 3192\n",
      "Validation loss (no improvement): 0.013755145668983459\n",
      "Training iteration: 3193\n",
      "Validation loss (no improvement): 0.014158549904823303\n",
      "Training iteration: 3194\n",
      "Validation loss (no improvement): 0.014522814750671386\n",
      "Training iteration: 3195\n",
      "Validation loss (no improvement): 0.014348366856575012\n",
      "Training iteration: 3196\n",
      "Validation loss (no improvement): 0.014506399631500244\n",
      "Training iteration: 3197\n",
      "Validation loss (no improvement): 0.014900538325309753\n",
      "Training iteration: 3198\n",
      "Validation loss (no improvement): 0.014807581901550293\n",
      "Training iteration: 3199\n",
      "Validation loss (no improvement): 0.014235606789588929\n",
      "Training iteration: 3200\n",
      "Validation loss (no improvement): 0.014269331097602844\n",
      "Training iteration: 3201\n",
      "Validation loss (no improvement): 0.01487785279750824\n",
      "Training iteration: 3202\n",
      "Validation loss (no improvement): 0.014754626154899596\n",
      "Training iteration: 3203\n",
      "Validation loss (no improvement): 0.014271526038646698\n",
      "Training iteration: 3204\n",
      "Validation loss (no improvement): 0.01448182761669159\n",
      "Training iteration: 3205\n",
      "Validation loss (no improvement): 0.015100058913230897\n",
      "Training iteration: 3206\n",
      "Validation loss (no improvement): 0.015183866024017334\n",
      "Training iteration: 3207\n",
      "Validation loss (no improvement): 0.01485254019498825\n",
      "Training iteration: 3208\n",
      "Validation loss (no improvement): 0.014806309342384338\n",
      "Training iteration: 3209\n",
      "Validation loss (no improvement): 0.014631395041942597\n",
      "Training iteration: 3210\n",
      "Validation loss (no improvement): 0.014112186431884766\n",
      "Training iteration: 3211\n",
      "Validation loss (no improvement): 0.013792750239372254\n",
      "Training iteration: 3212\n",
      "Validation loss (no improvement): 0.01408429890871048\n",
      "Training iteration: 3213\n",
      "Validation loss (no improvement): 0.014689616858959198\n",
      "Training iteration: 3214\n",
      "Validation loss (no improvement): 0.01435358077287674\n",
      "Training iteration: 3215\n",
      "Validation loss (no improvement): 0.014103420078754425\n",
      "Training iteration: 3216\n",
      "Validation loss (no improvement): 0.014315468072891236\n",
      "Training iteration: 3217\n",
      "Validation loss (no improvement): 0.01474740207195282\n",
      "Training iteration: 3218\n",
      "Validation loss (no improvement): 0.01419292837381363\n",
      "Training iteration: 3219\n",
      "Validation loss (no improvement): 0.014013127982616424\n",
      "Training iteration: 3220\n",
      "Validation loss (no improvement): 0.014894172549247742\n",
      "Training iteration: 3221\n",
      "Validation loss (no improvement): 0.015173229575157165\n",
      "Training iteration: 3222\n",
      "Validation loss (no improvement): 0.013982801139354706\n",
      "Training iteration: 3223\n",
      "Validation loss (no improvement): 0.01376875936985016\n",
      "Training iteration: 3224\n",
      "Validation loss (no improvement): 0.01473800539970398\n",
      "Training iteration: 3225\n",
      "Validation loss (no improvement): 0.014461064338684082\n",
      "Training iteration: 3226\n",
      "Validation loss (no improvement): 0.013990840315818787\n",
      "Training iteration: 3227\n",
      "Validation loss (no improvement): 0.014177520573139191\n",
      "Training iteration: 3228\n",
      "Validation loss (no improvement): 0.014649483561515807\n",
      "Training iteration: 3229\n",
      "Validation loss (no improvement): 0.01437186896800995\n",
      "Training iteration: 3230\n",
      "Validation loss (no improvement): 0.013902364671230317\n",
      "Training iteration: 3231\n",
      "Validation loss (no improvement): 0.014367467164993286\n",
      "Training iteration: 3232\n",
      "Validation loss (no improvement): 0.014965347945690155\n",
      "Training iteration: 3233\n",
      "Validation loss (no improvement): 0.014216800034046174\n",
      "Training iteration: 3234\n",
      "Validation loss (no improvement): 0.014412696659564971\n",
      "Training iteration: 3235\n",
      "Validation loss (no improvement): 0.015136145055294037\n",
      "Training iteration: 3236\n",
      "Validation loss (no improvement): 0.014824515581130982\n",
      "Training iteration: 3237\n",
      "Validation loss (no improvement): 0.014023560285568237\n",
      "Training iteration: 3238\n",
      "Validation loss (no improvement): 0.014043201506137849\n",
      "Training iteration: 3239\n",
      "Validation loss (no improvement): 0.01571856886148453\n",
      "Training iteration: 3240\n",
      "Validation loss (no improvement): 0.01570592522621155\n",
      "Training iteration: 3241\n",
      "Validation loss (no improvement): 0.014703957736492157\n",
      "Training iteration: 3242\n",
      "Validation loss (no improvement): 0.014822331070899964\n",
      "Training iteration: 3243\n",
      "Validation loss (no improvement): 0.015491679310798645\n",
      "Training iteration: 3244\n",
      "Validation loss (no improvement): 0.0153299480676651\n",
      "Training iteration: 3245\n",
      "Validation loss (no improvement): 0.01479625403881073\n",
      "Training iteration: 3246\n",
      "Validation loss (no improvement): 0.014414502680301667\n",
      "Training iteration: 3247\n",
      "Validation loss (no improvement): 0.014502641558647156\n",
      "Training iteration: 3248\n",
      "Validation loss (no improvement): 0.01465444713830948\n",
      "Training iteration: 3249\n",
      "Validation loss (no improvement): 0.014767993986606599\n",
      "Training iteration: 3250\n",
      "Validation loss (no improvement): 0.014326873421669006\n",
      "Training iteration: 3251\n",
      "Validation loss (no improvement): 0.014156214892864227\n",
      "Training iteration: 3252\n",
      "Validation loss (no improvement): 0.014404062926769257\n",
      "Training iteration: 3253\n",
      "Validation loss (no improvement): 0.013626801967620849\n",
      "Training iteration: 3254\n",
      "Validation loss (no improvement): 0.01381344199180603\n",
      "Training iteration: 3255\n",
      "Validation loss (no improvement): 0.014591722190380097\n",
      "Training iteration: 3256\n",
      "Validation loss (no improvement): 0.01445401757955551\n",
      "Training iteration: 3257\n",
      "Validation loss (no improvement): 0.01396298110485077\n",
      "Training iteration: 3258\n",
      "Validation loss (no improvement): 0.01377670168876648\n",
      "Training iteration: 3259\n",
      "Improved validation loss from: 0.013610953092575073  to: 0.0135569766163826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 3260\n",
      "Validation loss (no improvement): 0.013632836937904357\n",
      "Training iteration: 3261\n",
      "Improved validation loss from: 0.0135569766163826  to: 0.012918207049369811\n",
      "Training iteration: 3262\n",
      "Validation loss (no improvement): 0.013322648406028748\n",
      "Training iteration: 3263\n",
      "Validation loss (no improvement): 0.014861690998077392\n",
      "Training iteration: 3264\n",
      "Validation loss (no improvement): 0.014039555191993713\n",
      "Training iteration: 3265\n",
      "Validation loss (no improvement): 0.014002977311611176\n",
      "Training iteration: 3266\n",
      "Validation loss (no improvement): 0.01492244452238083\n",
      "Training iteration: 3267\n",
      "Validation loss (no improvement): 0.01542113721370697\n",
      "Training iteration: 3268\n",
      "Validation loss (no improvement): 0.014751291275024414\n",
      "Training iteration: 3269\n",
      "Validation loss (no improvement): 0.014140434563159943\n",
      "Training iteration: 3270\n",
      "Validation loss (no improvement): 0.014129748940467835\n",
      "Training iteration: 3271\n",
      "Validation loss (no improvement): 0.014109678566455841\n",
      "Training iteration: 3272\n",
      "Validation loss (no improvement): 0.013724243640899659\n",
      "Training iteration: 3273\n",
      "Validation loss (no improvement): 0.013882479071617127\n",
      "Training iteration: 3274\n",
      "Validation loss (no improvement): 0.014081747829914093\n",
      "Training iteration: 3275\n",
      "Validation loss (no improvement): 0.014239223301410675\n",
      "Training iteration: 3276\n",
      "Validation loss (no improvement): 0.01402295082807541\n",
      "Training iteration: 3277\n",
      "Validation loss (no improvement): 0.013858635723590852\n",
      "Training iteration: 3278\n",
      "Validation loss (no improvement): 0.014157414436340332\n",
      "Training iteration: 3279\n",
      "Validation loss (no improvement): 0.014085379242897034\n",
      "Training iteration: 3280\n",
      "Validation loss (no improvement): 0.013635209202766419\n",
      "Training iteration: 3281\n",
      "Validation loss (no improvement): 0.013686113059520721\n",
      "Training iteration: 3282\n",
      "Validation loss (no improvement): 0.01404922902584076\n",
      "Training iteration: 3283\n",
      "Validation loss (no improvement): 0.013747183978557587\n",
      "Training iteration: 3284\n",
      "Validation loss (no improvement): 0.01313454657793045\n",
      "Training iteration: 3285\n",
      "Validation loss (no improvement): 0.013639140129089355\n",
      "Training iteration: 3286\n",
      "Validation loss (no improvement): 0.01447945088148117\n",
      "Training iteration: 3287\n",
      "Validation loss (no improvement): 0.013916224241256714\n",
      "Training iteration: 3288\n",
      "Validation loss (no improvement): 0.013238272070884705\n",
      "Training iteration: 3289\n",
      "Validation loss (no improvement): 0.013521933555603027\n",
      "Training iteration: 3290\n",
      "Validation loss (no improvement): 0.014338819682598114\n",
      "Training iteration: 3291\n",
      "Validation loss (no improvement): 0.01397479921579361\n",
      "Training iteration: 3292\n",
      "Validation loss (no improvement): 0.01393604576587677\n",
      "Training iteration: 3293\n",
      "Validation loss (no improvement): 0.014307357370853424\n",
      "Training iteration: 3294\n",
      "Validation loss (no improvement): 0.014816150069236755\n",
      "Training iteration: 3295\n",
      "Validation loss (no improvement): 0.01452207863330841\n",
      "Training iteration: 3296\n",
      "Validation loss (no improvement): 0.014366617798805237\n",
      "Training iteration: 3297\n",
      "Validation loss (no improvement): 0.014341272413730621\n",
      "Training iteration: 3298\n",
      "Validation loss (no improvement): 0.014354509115219117\n",
      "Training iteration: 3299\n",
      "Validation loss (no improvement): 0.013979962468147278\n",
      "Training iteration: 3300\n",
      "Validation loss (no improvement): 0.014521270990371704\n",
      "Training iteration: 3301\n",
      "Validation loss (no improvement): 0.014201831817626954\n",
      "Training iteration: 3302\n",
      "Validation loss (no improvement): 0.01358916014432907\n",
      "Training iteration: 3303\n",
      "Validation loss (no improvement): 0.013631013035774232\n",
      "Training iteration: 3304\n",
      "Validation loss (no improvement): 0.014084574580192567\n",
      "Training iteration: 3305\n",
      "Validation loss (no improvement): 0.013666245341300964\n",
      "Training iteration: 3306\n",
      "Validation loss (no improvement): 0.014003615081310272\n",
      "Training iteration: 3307\n",
      "Validation loss (no improvement): 0.01470150500535965\n",
      "Training iteration: 3308\n",
      "Validation loss (no improvement): 0.014655511081218719\n",
      "Training iteration: 3309\n",
      "Validation loss (no improvement): 0.013537418842315675\n",
      "Training iteration: 3310\n",
      "Validation loss (no improvement): 0.013603302836418151\n",
      "Training iteration: 3311\n",
      "Validation loss (no improvement): 0.014031882584095\n",
      "Training iteration: 3312\n",
      "Validation loss (no improvement): 0.014229361712932587\n",
      "Training iteration: 3313\n",
      "Validation loss (no improvement): 0.013612815737724304\n",
      "Training iteration: 3314\n",
      "Validation loss (no improvement): 0.01386253535747528\n",
      "Training iteration: 3315\n",
      "Validation loss (no improvement): 0.01484476625919342\n",
      "Training iteration: 3316\n",
      "Validation loss (no improvement): 0.014930570125579834\n",
      "Training iteration: 3317\n",
      "Validation loss (no improvement): 0.013922402262687683\n",
      "Training iteration: 3318\n",
      "Validation loss (no improvement): 0.013893866539001464\n",
      "Training iteration: 3319\n",
      "Validation loss (no improvement): 0.014638307690620422\n",
      "Training iteration: 3320\n",
      "Validation loss (no improvement): 0.014958527684211732\n",
      "Training iteration: 3321\n",
      "Validation loss (no improvement): 0.014263655245304107\n",
      "Training iteration: 3322\n",
      "Validation loss (no improvement): 0.014641641080379486\n",
      "Training iteration: 3323\n",
      "Validation loss (no improvement): 0.01567336469888687\n",
      "Training iteration: 3324\n",
      "Validation loss (no improvement): 0.015608660876750946\n",
      "Training iteration: 3325\n",
      "Validation loss (no improvement): 0.014248065650463104\n",
      "Training iteration: 3326\n",
      "Validation loss (no improvement): 0.014087027311325074\n",
      "Training iteration: 3327\n",
      "Validation loss (no improvement): 0.014952388405799866\n",
      "Training iteration: 3328\n",
      "Validation loss (no improvement): 0.015399019420146941\n",
      "Training iteration: 3329\n",
      "Validation loss (no improvement): 0.013830454647541046\n",
      "Training iteration: 3330\n",
      "Validation loss (no improvement): 0.01369108259677887\n",
      "Training iteration: 3331\n",
      "Validation loss (no improvement): 0.014415265619754791\n",
      "Training iteration: 3332\n",
      "Validation loss (no improvement): 0.014893606305122375\n",
      "Training iteration: 3333\n",
      "Validation loss (no improvement): 0.013644149899482727\n",
      "Training iteration: 3334\n",
      "Validation loss (no improvement): 0.013329717516899108\n",
      "Training iteration: 3335\n",
      "Validation loss (no improvement): 0.014007334411144257\n",
      "Training iteration: 3336\n",
      "Validation loss (no improvement): 0.014367882907390595\n",
      "Training iteration: 3337\n",
      "Validation loss (no improvement): 0.013957229256629945\n",
      "Training iteration: 3338\n",
      "Validation loss (no improvement): 0.014271540939807892\n",
      "Training iteration: 3339\n",
      "Validation loss (no improvement): 0.0146733358502388\n",
      "Training iteration: 3340\n",
      "Validation loss (no improvement): 0.014376072585582734\n",
      "Training iteration: 3341\n",
      "Validation loss (no improvement): 0.013519665598869324\n",
      "Training iteration: 3342\n",
      "Validation loss (no improvement): 0.013369004428386688\n",
      "Training iteration: 3343\n",
      "Validation loss (no improvement): 0.01373279094696045\n",
      "Training iteration: 3344\n",
      "Validation loss (no improvement): 0.013343897461891175\n",
      "Training iteration: 3345\n",
      "Validation loss (no improvement): 0.013424333930015565\n",
      "Training iteration: 3346\n",
      "Validation loss (no improvement): 0.014259424805641175\n",
      "Training iteration: 3347\n",
      "Validation loss (no improvement): 0.01410207450389862\n",
      "Training iteration: 3348\n",
      "Validation loss (no improvement): 0.013213767111301422\n",
      "Training iteration: 3349\n",
      "Validation loss (no improvement): 0.01406002789735794\n",
      "Training iteration: 3350\n",
      "Validation loss (no improvement): 0.015106268227100372\n",
      "Training iteration: 3351\n",
      "Validation loss (no improvement): 0.013879568874835968\n",
      "Training iteration: 3352\n",
      "Validation loss (no improvement): 0.013298907876014709\n",
      "Training iteration: 3353\n",
      "Validation loss (no improvement): 0.013566844165325165\n",
      "Training iteration: 3354\n",
      "Validation loss (no improvement): 0.014047415554523468\n",
      "Training iteration: 3355\n",
      "Validation loss (no improvement): 0.01404975652694702\n",
      "Training iteration: 3356\n",
      "Validation loss (no improvement): 0.013467006385326385\n",
      "Training iteration: 3357\n",
      "Validation loss (no improvement): 0.013273680210113525\n",
      "Training iteration: 3358\n",
      "Validation loss (no improvement): 0.013354411721229554\n",
      "Training iteration: 3359\n",
      "Validation loss (no improvement): 0.013496728241443634\n",
      "Training iteration: 3360\n",
      "Validation loss (no improvement): 0.013584661483764648\n",
      "Training iteration: 3361\n",
      "Validation loss (no improvement): 0.013291260600090027\n"
     ]
    }
   ],
   "source": [
    "ensemble_model_5 = krishnan_model(data_tuple,arch_type='ensemble',lrate=0.01,num_epochs=4000,eps_range=eps_range)\n",
    "ensemble_model_5.train_model()\n",
    "ensemble_model_5.model_inference()\n",
    "\n",
    "ensemble_mean_5 = np.load('ensemble_mean_validation.npy')\n",
    "ensemble_logvar_5 = np.load('ensemble_var_validation.npy')\n",
    "ensemble_std_5 = np.sqrt(np.exp(ensemble_logvar_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_mean = (ensemble_mean_1+ensemble_mean_2+ensemble_mean_3+ensemble_mean_4+ensemble_mean_5)/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_var = ensemble_mean_1**2 + ensemble_std_1**2 + ensemble_mean_2**2 + ensemble_std_2**2 + ensemble_mean_3**2 + ensemble_std_3**2 + ensemble_mean_4**2 + ensemble_std_4**2 + ensemble_mean_5**2 + ensemble_std_5**2\n",
    "ensemble_var = ensemble_var/5.0 - ensemble_mean**2\n",
    "ensemble_std = np.sqrt(ensemble_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1inds = data_tuple[2][:,0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],baseline[arr1inds,0],label='Baseline')\n",
    "plt.plot(data_tuple[2][arr1inds,0],dropout_mean[arr1inds,0],label='Dropout mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Mixture mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],ensemble_mean[arr1inds,0],label='Ensemble mean')\n",
    "plt.plot(data_tuple[2][arr1inds,0],swa_mean[arr1inds,0],label='SWA mean')\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],dropout_mean[arr1inds,0],label='Dropout mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=dropout_mean[arr1inds,0]-3*dropout_std[arr1inds,0],y2=dropout_mean[arr1inds,0]+3*dropout_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Mixture mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=mixture_mean[arr1inds,0]-3*mixture_std[arr1inds,0],y2=mixture_mean[arr1inds,0]+3*mixture_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],mixture_mean[arr1inds,0],label='Ensemble mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=ensemble_mean[arr1inds,0]-3*ensemble_std[arr1inds,0],y2=ensemble_mean[arr1inds,0]+3*ensemble_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(data_tuple[2][arr1inds,0],data_tuple[2][arr1inds,1],label='Truth')\n",
    "plt.plot(data_tuple[2][arr1inds,0],swa_mean[arr1inds,0],label='SWA mean')\n",
    "plt.fill_between(x=data_tuple[2][arr1inds,0],y1=swa_mean[arr1inds,0]-3*swa_std[arr1inds,0],y2=swa_mean[arr1inds,0]+3*swa_std[arr1inds,0])\n",
    "plt.legend()\n",
    "plt.ylim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
