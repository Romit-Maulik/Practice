{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p0GaHRGUg5Ux"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(10)\n",
    "import matplotlib.pyplot as plt\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "# Load MNIST data set\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1,28*28)/255\n",
    "test_images = test_images.reshape(-1,28*28)/255\n",
    "\n",
    "np.random.shuffle(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vkn-RMBqhBtS"
   },
   "outputs": [],
   "source": [
    "class vi_nf_model(tf.keras.Model):\n",
    "    def __init__(self,num_latent,num_flow_iters):\n",
    "        super(vi_nf_model,self).__init__()\n",
    "        \n",
    "        # Constants\n",
    "        self.num_latent = num_latent\n",
    "        self.num_flow_iters = num_flow_iters\n",
    "                \n",
    "        # Initializer\n",
    "        xavier = tf.keras.initializers.GlorotUniform()\n",
    "        \n",
    "        # Define inference architecture\n",
    "        self.l1 = tf.keras.layers.Dense(150,activation='swish')\n",
    "        self.l2 = tf.keras.layers.Dense(100,activation='swish')\n",
    "        self.l3 = tf.keras.layers.Dense(50,activation='swish')\n",
    "        \n",
    "        # Variational stuff\n",
    "        self.zz_mu = tf.keras.layers.Dense(num_latent,activation='swish')\n",
    "        self.zz_logvar = tf.keras.layers.Dense(num_latent,activation='swish')\n",
    "        \n",
    "        # Normalizing flow stuff\n",
    "        self.ww_list = []\n",
    "        self.uu_list = []\n",
    "        self.bb_list = []\n",
    "        for _ in range(num_flow_iters):\n",
    "            self.ww_list.append(xavier(shape=(num_latent,1),dtype='float64'))\n",
    "            self.uu_list.append(xavier(shape=(num_latent,1),dtype='float64'))\n",
    "            self.bb_list.append(xavier(shape=(1,),dtype='float64'))\n",
    "            \n",
    "        \n",
    "        # Define reconstruction architecture\n",
    "        self.l5 = tf.keras.layers.Dense(50,activation='swish')\n",
    "        self.l6 = tf.keras.layers.Dense(100,activation='swish')\n",
    "        self.l7 = tf.keras.layers.Dense(150,activation='swish')\n",
    "        self.l8 = tf.keras.layers.Dense(28*28,activation='sigmoid')\n",
    "        \n",
    "        # Optimizer\n",
    "        self.train_op = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "        \n",
    "    def call(self,X):\n",
    "        # Call inference\n",
    "        h1 = self.l1(X)\n",
    "        h2 = self.l2(h1)\n",
    "        h3 = self.l3(h2)\n",
    "               \n",
    "        # Latent space quantities\n",
    "        self.mu = self.zz_mu(h3)\n",
    "        self.logvar = self.zz_logvar(h3)\n",
    "        \n",
    "        # Shapes\n",
    "        batch_size = tf.shape(self.mu)[0]\n",
    "        nl = self.num_latent\n",
    "        \n",
    "        # Batch wise latent space operations\n",
    "        eps = tf.random.normal(shape=(batch_size,nl),mean=0.0,stddev=1.0,dtype='float64')\n",
    "        self.Z = self.mu + tf.math.exp(0.5*self.logvar)*eps\n",
    "        \n",
    "        # Find density value of initial sample \n",
    "        self.logZ0 = -0.5*(tf.cast(tf.math.log(2.0*np.pi),dtype='float64') - self.logvar) - (0.5*(self.Z-self.mu)**2)/(2.0*tf.exp(self.logvar))\n",
    "        \n",
    "        # Use normalizing flow and get energy (needed for loss function)\n",
    "        self.Zk, flow_energy = self.normalizing_flow(self.Z,self.logZ0)\n",
    "                \n",
    "        # Decode\n",
    "        h5 = self.l5(self.Zk)\n",
    "        h6 = self.l6(h5)\n",
    "        h7 = self.l7(h6)\n",
    "        out = self.l8(h7)\n",
    "        \n",
    "        return out, flow_energy\n",
    "\n",
    "        \n",
    "    def normalizing_flow(self,Z,logstart):\n",
    "        \n",
    "        # Number of transformations\n",
    "        Kval = self.num_flow_iters\n",
    "        zprev = Z\n",
    "        \n",
    "        for i in range(Kval):\n",
    "            w_i = self.ww_list[i]\n",
    "            u_i = self.uu_list[i]\n",
    "            b_i = self.bb_list[i]\n",
    "            \n",
    "            wz = tf.matmul(zprev,w_i)\n",
    "            \n",
    "            tt = tf.math.tanh(wz+b_i)\n",
    "            dtt = 1.0-(tf.math.tanh(wz+b_i))**2\n",
    "            \n",
    "            zprev = zprev + tf.matmul(tt,tf.transpose(u_i))\n",
    "            phi = tf.matmul(tt,tf.transpose(w_i))\n",
    "            \n",
    "            utphi = tf.matmul(phi,u_i)\n",
    "            \n",
    "            logstart = logstart - tf.math.log(tf.math.abs(1.0+utphi))\n",
    "        \n",
    "        return zprev, logstart\n",
    "    \n",
    "    \n",
    "    def get_loss(self,X):\n",
    "        Ypred, flow_energy = self.call(X)        \n",
    "        return self.bce(X,Ypred) + tf.reduce_mean(flow_energy)\n",
    "\n",
    "    \n",
    "    def get_grad(self,X):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            L = self.get_loss(X)\n",
    "            g = tape.gradient(L, self.trainable_variables)\n",
    "        return g\n",
    "    \n",
    "    # perform gradient descent - regular\n",
    "    def network_learn(self,X):\n",
    "        g = self.get_grad(X)\n",
    "        self.train_op.apply_gradients(zip(g, self.trainable_variables))\n",
    "        \n",
    "    # Train the model\n",
    "    def train_model(self,Xtrain):\n",
    "        plot_iter = 0\n",
    "        stop_iter = 0\n",
    "        patience = 10\n",
    "        best_loss = 999999.0 # Some large number \n",
    "\n",
    "        self.num_batches = 10\n",
    "        self.train_batch_size = int(Xtrain.shape[0]/self.num_batches)\n",
    "        \n",
    "        for i in range(200):\n",
    "            # Training loss\n",
    "            print('Training iteration:',i)\n",
    "            \n",
    "            loss = 0.0\n",
    "            for batch in range(self.num_batches):\n",
    "                input_batch = Xtrain[batch*self.train_batch_size:(batch+1)*self.train_batch_size]\n",
    "                self.network_learn(input_batch)\n",
    "                batch_loss = self.get_loss(input_batch).numpy()\n",
    "                loss+=batch_loss\n",
    "                print('Batch loss:',batch_loss)\n",
    "            \n",
    "            loss = loss/self.num_batches\n",
    "            print('Epoch loss:',loss)\n",
    "\n",
    "            # Check early stopping criteria\n",
    "            if loss < best_loss:\n",
    "                \n",
    "                print('Improved loss from:',best_loss,' to:', loss)                \n",
    "                best_loss = loss\n",
    "                self.save_weights('./checkpoints/my_checkpoint')\n",
    "                \n",
    "                stop_iter = 0\n",
    "            else:\n",
    "                print('Loss (no improvement):',loss)\n",
    "                stop_iter = stop_iter + 1\n",
    "\n",
    "            if stop_iter == patience:\n",
    "                break\n",
    "        \n",
    "    def test_model(self,Xtest):\n",
    "        # Check accuracy on test\n",
    "        print('Test loss:',self.get_loss(Xtest).numpy())\n",
    "\n",
    "    # Load weights\n",
    "    def restore_model(self):\n",
    "        self.load_weights('./checkpoints/my_checkpoint') # Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e0tg7kl2hEO4"
   },
   "outputs": [],
   "source": [
    "model = vi_nf_model(40,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afqVyWPNhGkj",
    "outputId": "4d73a370-795b-436b-9064-6a564870cd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration: 0\n",
      "Batch loss: -0.24259146900067252\n",
      "Batch loss: -0.3199071579010371\n",
      "Batch loss: -0.4097357182591643\n",
      "Batch loss: -0.5166686235990237\n",
      "Batch loss: -0.6336146522777204\n",
      "Batch loss: -0.7637171344111295\n",
      "Batch loss: -0.8901944754374167\n",
      "Batch loss: -1.0482860531297231\n",
      "Batch loss: -1.1921763688756128\n",
      "Batch loss: -1.3324420791076217\n",
      "Epoch loss: -0.7349333731999123\n",
      "Improved loss from: 999999.0  to: -0.7349333731999123\n",
      "Training iteration: 1\n",
      "Batch loss: -1.4746936707145624\n",
      "Batch loss: -1.6407186596656982\n",
      "Batch loss: -1.7912244133600148\n",
      "Batch loss: -1.9742710038878273\n",
      "Batch loss: -2.1783285759787385\n",
      "Batch loss: -2.4103945182833306\n",
      "Batch loss: -2.6084202236179737\n",
      "Batch loss: -2.800940424566765\n",
      "Batch loss: -2.9660836655706277\n",
      "Batch loss: -3.074305411549417\n",
      "Epoch loss: -2.291938056719496\n",
      "Improved loss from: -0.7349333731999123  to: -2.291938056719496\n",
      "Training iteration: 2\n",
      "Batch loss: -3.181973686734712\n",
      "Batch loss: -3.2771812069127795\n",
      "Batch loss: -3.3665596807311706\n",
      "Batch loss: -3.429129053953638\n",
      "Batch loss: -3.5218516925701557\n",
      "Batch loss: -3.6081049359017228\n",
      "Batch loss: -3.6864971477507438\n",
      "Batch loss: -3.752882558802776\n",
      "Batch loss: -3.798065536544599\n",
      "Batch loss: -3.831398856010691\n",
      "Epoch loss: -3.5453644355912983\n",
      "Improved loss from: -2.291938056719496  to: -3.5453644355912983\n",
      "Training iteration: 3\n",
      "Batch loss: -3.8551703925795957\n",
      "Batch loss: -3.877650059599808\n",
      "Batch loss: -3.898539840787488\n",
      "Batch loss: -3.9147405943212314\n",
      "Batch loss: -3.932024381142912\n",
      "Batch loss: -3.942739112800992\n",
      "Batch loss: -3.9549658126065026\n",
      "Batch loss: -3.964076423586544\n",
      "Batch loss: -3.9724823958158257\n",
      "Batch loss: -3.9799168876040474\n",
      "Epoch loss: -3.929230590084495\n",
      "Improved loss from: -3.5453644355912983  to: -3.929230590084495\n",
      "Training iteration: 4\n",
      "Batch loss: -3.9831163592833345\n",
      "Batch loss: -3.9901556698398517\n",
      "Batch loss: -3.9935122153022764\n",
      "Batch loss: -3.9947926891015397\n",
      "Batch loss: -4.001974610376729\n",
      "Batch loss: -4.002347069395676\n",
      "Batch loss: -4.005437745383508\n",
      "Batch loss: -4.004772095352619\n",
      "Batch loss: -4.004914639125876\n",
      "Batch loss: -4.0085251288473245\n",
      "Epoch loss: -3.9989548222008744\n",
      "Improved loss from: -3.929230590084495  to: -3.9989548222008744\n",
      "Training iteration: 5\n",
      "Batch loss: -4.008231346869334\n",
      "Batch loss: -4.010993464909852\n",
      "Batch loss: -4.014586723696661\n",
      "Batch loss: -4.0124497605695675\n",
      "Batch loss: -4.019333128061907\n",
      "Batch loss: -4.0209558726270185\n",
      "Batch loss: -4.02424156332048\n",
      "Batch loss: -4.024145465739109\n",
      "Batch loss: -4.025461245211426\n",
      "Batch loss: -4.028765391828646\n",
      "Epoch loss: -4.0189163962834\n",
      "Improved loss from: -3.9989548222008744  to: -4.0189163962834\n",
      "Training iteration: 6\n",
      "Batch loss: -4.028336369280829\n",
      "Batch loss: -4.030784857842696\n",
      "Batch loss: -4.031202975202942\n",
      "Batch loss: -4.0310299721913925\n",
      "Batch loss: -4.035594706626362\n",
      "Batch loss: -4.03510918798719\n",
      "Batch loss: -4.035878096782582\n",
      "Batch loss: -4.037148982557302\n",
      "Batch loss: -4.037077701095091\n",
      "Batch loss: -4.0399380429908\n",
      "Epoch loss: -4.034210089255719\n",
      "Improved loss from: -4.0189163962834  to: -4.034210089255719\n",
      "Training iteration: 7\n",
      "Batch loss: -4.0395558699752385\n",
      "Batch loss: -4.039780536963314\n",
      "Batch loss: -4.040128342459795\n",
      "Batch loss: -4.041890402658575\n",
      "Batch loss: -4.043857356252331\n",
      "Batch loss: -4.044736186738544\n",
      "Batch loss: -4.044673645971415\n",
      "Batch loss: -4.044616895594896\n",
      "Batch loss: -4.0452150515393805\n",
      "Batch loss: -4.046355633841584\n",
      "Epoch loss: -4.043080992199507\n",
      "Improved loss from: -4.034210089255719  to: -4.043080992199507\n",
      "Training iteration: 8\n",
      "Batch loss: -4.047548233831998\n",
      "Batch loss: -4.046763686095215\n",
      "Batch loss: -4.048185309725896\n",
      "Batch loss: -4.048724235288731\n",
      "Batch loss: -4.051397163077468\n",
      "Batch loss: -4.051775231439133\n",
      "Batch loss: -4.052140152102519\n",
      "Batch loss: -4.05247081350863\n",
      "Batch loss: -4.053014925261319\n",
      "Batch loss: -4.051880366497603\n",
      "Epoch loss: -4.05039001168285\n",
      "Improved loss from: -4.043080992199507  to: -4.05039001168285\n",
      "Training iteration: 9\n",
      "Batch loss: -4.051041218838418\n",
      "Batch loss: -4.054908227083879\n",
      "Batch loss: -4.053296695500954\n",
      "Batch loss: -4.053409063556708\n",
      "Batch loss: -4.055573187643262\n",
      "Batch loss: -4.055751179929787\n",
      "Batch loss: -4.055599842022064\n",
      "Batch loss: -4.055540043921658\n",
      "Batch loss: -4.056426026542503\n",
      "Batch loss: -4.056923474970385\n",
      "Epoch loss: -4.054846896000962\n",
      "Improved loss from: -4.05039001168285  to: -4.054846896000962\n",
      "Training iteration: 10\n",
      "Batch loss: -4.055781896849774\n",
      "Batch loss: -4.057292569231064\n",
      "Batch loss: -4.056913790772219\n",
      "Batch loss: -4.056894883396914\n",
      "Batch loss: -4.05923836177069\n",
      "Batch loss: -4.060755538856898\n",
      "Batch loss: -4.060619563694407\n",
      "Batch loss: -4.05878992333644\n",
      "Batch loss: -4.061456985909195\n",
      "Batch loss: -4.061137504010312\n",
      "Epoch loss: -4.058888101782792\n",
      "Improved loss from: -4.054846896000962  to: -4.058888101782792\n",
      "Training iteration: 11\n",
      "Batch loss: -4.058291388840321\n",
      "Batch loss: -4.060855324263796\n",
      "Batch loss: -4.0612293189479\n",
      "Batch loss: -4.061921131591398\n",
      "Batch loss: -4.062504631463328\n",
      "Batch loss: -4.062045854351502\n",
      "Batch loss: -4.06406384148736\n",
      "Batch loss: -4.062763541886772\n",
      "Batch loss: -4.063346593095755\n",
      "Batch loss: -4.064436247837212\n",
      "Epoch loss: -4.062145787376536\n",
      "Improved loss from: -4.058888101782792  to: -4.062145787376536\n",
      "Training iteration: 12\n",
      "Batch loss: -4.062560942930991\n",
      "Batch loss: -4.063031934890943\n",
      "Batch loss: -4.0638307200167745\n",
      "Batch loss: -4.063156989309551\n",
      "Batch loss: -4.0657995827232725\n",
      "Batch loss: -4.065537162925023\n",
      "Batch loss: -4.065472121327549\n",
      "Batch loss: -4.065957750863309\n",
      "Batch loss: -4.064350515923753\n",
      "Batch loss: -4.065511953669285\n",
      "Epoch loss: -4.064520967458046\n",
      "Improved loss from: -4.062145787376536  to: -4.064520967458046\n",
      "Training iteration: 13\n",
      "Batch loss: -4.06531200074155\n",
      "Batch loss: -4.066574963035714\n",
      "Batch loss: -4.065427696632504\n",
      "Batch loss: -4.0666694783041155\n",
      "Batch loss: -4.068557025389096\n",
      "Batch loss: -4.069261297673628\n",
      "Batch loss: -4.068707173171731\n",
      "Batch loss: -4.06882400274264\n",
      "Batch loss: -4.068499269508706\n",
      "Batch loss: -4.070381426805894\n",
      "Epoch loss: -4.0678214334005585\n",
      "Improved loss from: -4.064520967458046  to: -4.0678214334005585\n",
      "Training iteration: 14\n",
      "Batch loss: -4.0692638460429755\n",
      "Batch loss: -4.069793252245762\n",
      "Batch loss: -4.068006501539056\n",
      "Batch loss: -4.068903358740533\n",
      "Batch loss: -4.069411683103205\n",
      "Batch loss: -4.071800007501025\n",
      "Batch loss: -4.071270885250568\n",
      "Batch loss: -4.069320290423862\n",
      "Batch loss: -4.070735504899503\n",
      "Batch loss: -4.072695353320965\n",
      "Epoch loss: -4.070120068306745\n",
      "Improved loss from: -4.0678214334005585  to: -4.070120068306745\n",
      "Training iteration: 15\n",
      "Batch loss: -4.070325613227099\n",
      "Batch loss: -4.073309607050499\n",
      "Batch loss: -4.071250346257682\n",
      "Batch loss: -4.0724071514772096\n",
      "Batch loss: -4.074358899741919\n",
      "Batch loss: -4.074494658396652\n",
      "Batch loss: -4.072980809148179\n",
      "Batch loss: -4.07349901785666\n",
      "Batch loss: -4.073942741371258\n",
      "Batch loss: -4.074515469385271\n",
      "Epoch loss: -4.073108431391242\n",
      "Improved loss from: -4.070120068306745  to: -4.073108431391242\n",
      "Training iteration: 16\n",
      "Batch loss: -4.072685888286593\n",
      "Batch loss: -4.07421329552391\n",
      "Batch loss: -4.073392181050633\n",
      "Batch loss: -4.074542352353335\n",
      "Batch loss: -4.076433043273461\n",
      "Batch loss: -4.0773381062257394\n",
      "Batch loss: -4.076082085160339\n",
      "Batch loss: -4.076311202444934\n",
      "Batch loss: -4.075156096668376\n",
      "Batch loss: -4.078317747168615\n",
      "Epoch loss: -4.075447199815594\n",
      "Improved loss from: -4.073108431391242  to: -4.075447199815594\n",
      "Training iteration: 17\n",
      "Batch loss: -4.075280252531918\n",
      "Batch loss: -4.075572229951255\n",
      "Batch loss: -4.076648327443446\n",
      "Batch loss: -4.075531626166912\n",
      "Batch loss: -4.079243936950001\n",
      "Batch loss: -4.078386540330509\n",
      "Batch loss: -4.07971148733169\n",
      "Batch loss: -4.078832738576962\n",
      "Batch loss: -4.078606619360764\n",
      "Batch loss: -4.0803574957452104\n",
      "Epoch loss: -4.077817125438867\n",
      "Improved loss from: -4.075447199815594  to: -4.077817125438867\n",
      "Training iteration: 18\n",
      "Batch loss: -4.0782203510036465\n",
      "Batch loss: -4.079037501130083\n",
      "Batch loss: -4.077623393609112\n",
      "Batch loss: -4.078182045689707\n",
      "Batch loss: -4.079718796059542\n",
      "Batch loss: -4.080562628751094\n",
      "Batch loss: -4.081297890370563\n",
      "Batch loss: -4.080549862000498\n",
      "Batch loss: -4.078994460635078\n",
      "Batch loss: -4.081515043498273\n",
      "Epoch loss: -4.07957019727476\n",
      "Improved loss from: -4.077817125438867  to: -4.07957019727476\n",
      "Training iteration: 19\n",
      "Batch loss: -4.079394499535926\n",
      "Batch loss: -4.079227939964971\n",
      "Batch loss: -4.080871973010927\n",
      "Batch loss: -4.079200527753855\n",
      "Batch loss: -4.081332900860709\n",
      "Batch loss: -4.082731751725228\n",
      "Batch loss: -4.082080545804028\n",
      "Batch loss: -4.08176187586419\n",
      "Batch loss: -4.081980122810578\n",
      "Batch loss: -4.082994147311129\n",
      "Epoch loss: -4.081157628464154\n",
      "Improved loss from: -4.07957019727476  to: -4.081157628464154\n",
      "Training iteration: 20\n",
      "Batch loss: -4.082463663231549\n",
      "Batch loss: -4.08220538755759\n",
      "Batch loss: -4.082663377652313\n",
      "Batch loss: -4.082191678146676\n",
      "Batch loss: -4.083541089100528\n",
      "Batch loss: -4.0843646165839935\n",
      "Batch loss: -4.083692607898148\n",
      "Batch loss: -4.082452999363646\n",
      "Batch loss: -4.083974119463063\n",
      "Batch loss: -4.0852284891640895\n",
      "Epoch loss: -4.083277802816159\n",
      "Improved loss from: -4.081157628464154  to: -4.083277802816159\n",
      "Training iteration: 21\n",
      "Batch loss: -4.082280627786633\n",
      "Batch loss: -4.083316673986554\n",
      "Batch loss: -4.084086818083273\n",
      "Batch loss: -4.082684824513591\n",
      "Batch loss: -4.084862240917137\n",
      "Batch loss: -4.084866592052335\n",
      "Batch loss: -4.085173971168365\n",
      "Batch loss: -4.083607093391777\n",
      "Batch loss: -4.085355343324634\n",
      "Batch loss: -4.085018992964294\n",
      "Epoch loss: -4.0841253178188595\n",
      "Improved loss from: -4.083277802816159  to: -4.0841253178188595\n",
      "Training iteration: 22\n",
      "Batch loss: -4.0833454478619995\n",
      "Batch loss: -4.08563999765378\n",
      "Batch loss: -4.083858049307727\n",
      "Batch loss: -4.084924337852041\n",
      "Batch loss: -4.086639042913657\n",
      "Batch loss: -4.086104855813045\n",
      "Batch loss: -4.087715279832418\n",
      "Batch loss: -4.084556329516891\n",
      "Batch loss: -4.086930020541864\n",
      "Batch loss: -4.088014026226102\n",
      "Epoch loss: -4.0857727387519525\n",
      "Improved loss from: -4.0841253178188595  to: -4.0857727387519525\n",
      "Training iteration: 23\n",
      "Batch loss: -4.084438221442713\n",
      "Batch loss: -4.086912903272728\n",
      "Batch loss: -4.084061447652041\n",
      "Batch loss: -4.085613695977122\n",
      "Batch loss: -4.086370921145981\n",
      "Batch loss: -4.085420641794355\n",
      "Batch loss: -4.0874258102587255\n",
      "Batch loss: -4.086142594925422\n",
      "Batch loss: -4.087499004187581\n",
      "Batch loss: -4.087831852601724\n",
      "Epoch loss: -4.086171709325839\n",
      "Improved loss from: -4.0857727387519525  to: -4.086171709325839\n",
      "Training iteration: 24\n",
      "Batch loss: -4.088413123464686\n",
      "Batch loss: -4.086519583228206\n",
      "Batch loss: -4.086785437973113\n",
      "Batch loss: -4.086489842491204\n",
      "Batch loss: -4.0885480332146615\n",
      "Batch loss: -4.0884480919271935\n",
      "Batch loss: -4.089396066771045\n",
      "Batch loss: -4.088191911085908\n",
      "Batch loss: -4.087264112719366\n",
      "Batch loss: -4.089533489550802\n",
      "Epoch loss: -4.087958969242619\n",
      "Improved loss from: -4.086171709325839  to: -4.087958969242619\n",
      "Training iteration: 25\n",
      "Batch loss: -4.087529299592496\n",
      "Batch loss: -4.087865992078804\n",
      "Batch loss: -4.087024689163713\n",
      "Batch loss: -4.087394853588751\n",
      "Batch loss: -4.089537199295686\n",
      "Batch loss: -4.0904487814623085\n",
      "Batch loss: -4.08857838452337\n",
      "Batch loss: -4.088354262000192\n",
      "Batch loss: -4.088216919834446\n",
      "Batch loss: -4.090786193248516\n",
      "Epoch loss: -4.088573657478828\n",
      "Improved loss from: -4.087958969242619  to: -4.088573657478828\n",
      "Training iteration: 26\n",
      "Batch loss: -4.088685228935282\n",
      "Batch loss: -4.088203745238048\n",
      "Batch loss: -4.088323934010607\n",
      "Batch loss: -4.08846755307106\n",
      "Batch loss: -4.090831364290691\n",
      "Batch loss: -4.091142980646004\n",
      "Batch loss: -4.090419226866106\n",
      "Batch loss: -4.089039682113643\n",
      "Batch loss: -4.08981498763953\n",
      "Batch loss: -4.091376259893831\n",
      "Epoch loss: -4.08963049627048\n",
      "Improved loss from: -4.088573657478828  to: -4.08963049627048\n",
      "Training iteration: 27\n",
      "Batch loss: -4.088570455936619\n",
      "Batch loss: -4.090520958497964\n",
      "Batch loss: -4.0891215464544475\n",
      "Batch loss: -4.089377600099019\n",
      "Batch loss: -4.089586162129573\n",
      "Batch loss: -4.091400295671038\n",
      "Batch loss: -4.090379280090264\n",
      "Batch loss: -4.09157003786727\n",
      "Batch loss: -4.089167705359642\n",
      "Batch loss: -4.091820182411132\n",
      "Epoch loss: -4.090151422451696\n",
      "Improved loss from: -4.08963049627048  to: -4.090151422451696\n",
      "Training iteration: 28\n",
      "Batch loss: -4.089425088521015\n",
      "Batch loss: -4.090107751228116\n",
      "Batch loss: -4.091378214101507\n",
      "Batch loss: -4.090429007086184\n",
      "Batch loss: -4.092277203717304\n",
      "Batch loss: -4.0928138772978215\n",
      "Batch loss: -4.0905384180033915\n",
      "Batch loss: -4.091160853854568\n",
      "Batch loss: -4.091730307440566\n",
      "Batch loss: -4.093054739127779\n",
      "Epoch loss: -4.091291546037825\n",
      "Improved loss from: -4.090151422451696  to: -4.091291546037825\n",
      "Training iteration: 29\n",
      "Batch loss: -4.091112529402266\n",
      "Batch loss: -4.090160633705128\n",
      "Batch loss: -4.091321653181969\n",
      "Batch loss: -4.090804705067668\n",
      "Batch loss: -4.092282142672477\n",
      "Batch loss: -4.092174355649233\n",
      "Batch loss: -4.092231873047728\n",
      "Batch loss: -4.091624011130222\n",
      "Batch loss: -4.0906748638949955\n",
      "Batch loss: -4.093946160619199\n",
      "Epoch loss: -4.091633292837089\n",
      "Improved loss from: -4.091291546037825  to: -4.091633292837089\n",
      "Training iteration: 30\n",
      "Batch loss: -4.092034822535097\n",
      "Batch loss: -4.09213768319048\n",
      "Batch loss: -4.092708355301178\n",
      "Batch loss: -4.091818482443458\n",
      "Batch loss: -4.093568763443241\n",
      "Batch loss: -4.093094835888271\n",
      "Batch loss: -4.093227308326742\n",
      "Batch loss: -4.092576946318306\n",
      "Batch loss: -4.093832182318299\n",
      "Batch loss: -4.095637049527728\n",
      "Epoch loss: -4.09306364292928\n",
      "Improved loss from: -4.091633292837089  to: -4.09306364292928\n",
      "Training iteration: 31\n",
      "Batch loss: -4.093534882980092\n",
      "Batch loss: -4.0929814428716025\n",
      "Batch loss: -4.091730085722511\n",
      "Batch loss: -4.092655882957591\n",
      "Batch loss: -4.0935720630024415\n",
      "Batch loss: -4.094278465812341\n",
      "Batch loss: -4.0938557258649455\n",
      "Batch loss: -4.09314088771333\n",
      "Batch loss: -4.093783481393835\n",
      "Batch loss: -4.09487782505607\n",
      "Epoch loss: -4.093441074337475\n",
      "Improved loss from: -4.09306364292928  to: -4.093441074337475\n",
      "Training iteration: 32\n",
      "Batch loss: -4.093696861988642\n",
      "Batch loss: -4.093622114715853\n",
      "Batch loss: -4.093262718805724\n",
      "Batch loss: -4.093350634697049\n",
      "Batch loss: -4.095319249200242\n",
      "Batch loss: -4.09407772302804\n",
      "Batch loss: -4.0954774781783305\n",
      "Batch loss: -4.094566123903879\n",
      "Batch loss: -4.095298838512508\n",
      "Batch loss: -4.096063321315618\n",
      "Epoch loss: -4.094473506434588\n",
      "Improved loss from: -4.093441074337475  to: -4.094473506434588\n",
      "Training iteration: 33\n",
      "Batch loss: -4.094850106150312\n",
      "Batch loss: -4.094017538260247\n",
      "Batch loss: -4.095513155602027\n",
      "Batch loss: -4.094917936015519\n",
      "Batch loss: -4.096764618902655\n",
      "Batch loss: -4.096504665596444\n",
      "Batch loss: -4.096478633891224\n",
      "Batch loss: -4.0965696317733205\n",
      "Batch loss: -4.095891233670075\n",
      "Batch loss: -4.098147529109273\n",
      "Epoch loss: -4.095965504897109\n",
      "Improved loss from: -4.094473506434588  to: -4.095965504897109\n",
      "Training iteration: 34\n",
      "Batch loss: -4.096401480059202\n",
      "Batch loss: -4.096758202068783\n",
      "Batch loss: -4.096853870218518\n",
      "Batch loss: -4.096327684963072\n",
      "Batch loss: -4.097766994881774\n",
      "Batch loss: -4.098881861814553\n",
      "Batch loss: -4.098648379136521\n",
      "Batch loss: -4.096940925151247\n",
      "Batch loss: -4.100312054552965\n",
      "Batch loss: -4.101144551549299\n",
      "Epoch loss: -4.0980036004395926\n",
      "Improved loss from: -4.095965504897109  to: -4.0980036004395926\n",
      "Training iteration: 35\n",
      "Batch loss: -4.09817822964867\n",
      "Batch loss: -4.099050863543737\n",
      "Batch loss: -4.0993180055014395\n",
      "Batch loss: -4.100853229235409\n",
      "Batch loss: -4.1014091608865275\n",
      "Batch loss: -4.1006617490570525\n",
      "Batch loss: -4.101478717384881\n",
      "Batch loss: -4.101981065245011\n",
      "Batch loss: -4.101594066726245\n",
      "Batch loss: -4.10381257337387\n",
      "Epoch loss: -4.100833766060284\n",
      "Improved loss from: -4.0980036004395926  to: -4.100833766060284\n",
      "Training iteration: 36\n",
      "Batch loss: -4.102523301088841\n",
      "Batch loss: -4.102704022973426\n",
      "Batch loss: -4.101226545402257\n",
      "Batch loss: -4.101524978980742\n",
      "Batch loss: -4.103161142479063\n",
      "Batch loss: -4.104113898752079\n",
      "Batch loss: -4.103132500395812\n",
      "Batch loss: -4.102014063084591\n",
      "Batch loss: -4.103720599807651\n",
      "Batch loss: -4.105462290441654\n",
      "Epoch loss: -4.102958334340611\n",
      "Improved loss from: -4.100833766060284  to: -4.102958334340611\n",
      "Training iteration: 37\n",
      "Batch loss: -4.104410034823016\n",
      "Batch loss: -4.104079542837103\n",
      "Batch loss: -4.10463320453778\n",
      "Batch loss: -4.103999007154613\n",
      "Batch loss: -4.105889305194703\n",
      "Batch loss: -4.1054510683396055\n",
      "Batch loss: -4.10596440332792\n",
      "Batch loss: -4.106031894571167\n",
      "Batch loss: -4.106294678765801\n",
      "Batch loss: -4.107229064602111\n",
      "Epoch loss: -4.105398220415382\n",
      "Improved loss from: -4.102958334340611  to: -4.105398220415382\n",
      "Training iteration: 38\n",
      "Batch loss: -4.10589166983202\n",
      "Batch loss: -4.104967978888545\n",
      "Batch loss: -4.10654450188534\n",
      "Batch loss: -4.106157198985266\n",
      "Batch loss: -4.1079200487178404\n",
      "Batch loss: -4.107803533116859\n",
      "Batch loss: -4.10792568253457\n",
      "Batch loss: -4.106788951724786\n",
      "Batch loss: -4.108207658642941\n",
      "Batch loss: -4.109488240153075\n",
      "Epoch loss: -4.107169546448125\n",
      "Improved loss from: -4.105398220415382  to: -4.107169546448125\n",
      "Training iteration: 39\n",
      "Batch loss: -4.108178384330471\n",
      "Batch loss: -4.108102805079468\n",
      "Batch loss: -4.107103783286549\n",
      "Batch loss: -4.10914143001129\n",
      "Batch loss: -4.108588972256458\n",
      "Batch loss: -4.108685736793316\n",
      "Batch loss: -4.108158984054498\n",
      "Batch loss: -4.108250770050205\n",
      "Batch loss: -4.10892977879427\n",
      "Batch loss: -4.110345151932865\n",
      "Epoch loss: -4.108548579658939\n",
      "Improved loss from: -4.107169546448125  to: -4.108548579658939\n",
      "Training iteration: 40\n",
      "Batch loss: -4.1078923149592965\n",
      "Batch loss: -4.107875435713194\n",
      "Batch loss: -4.108454043236762\n",
      "Batch loss: -4.109727244630692\n",
      "Batch loss: -4.110084247563751\n",
      "Batch loss: -4.109415868172607\n",
      "Batch loss: -4.1101215256826364\n",
      "Batch loss: -4.10946796796651\n",
      "Batch loss: -4.109231617230907\n",
      "Batch loss: -4.110926322927292\n",
      "Epoch loss: -4.1093196588083645\n",
      "Improved loss from: -4.108548579658939  to: -4.1093196588083645\n",
      "Training iteration: 41\n",
      "Batch loss: -4.108315175950926\n",
      "Batch loss: -4.110840564978352\n",
      "Batch loss: -4.109368046461124\n",
      "Batch loss: -4.109691918732136\n",
      "Batch loss: -4.110353953079776\n",
      "Batch loss: -4.109475564881443\n",
      "Batch loss: -4.110255781691249\n",
      "Batch loss: -4.110821844855061\n",
      "Batch loss: -4.110539300110613\n",
      "Batch loss: -4.11181479960829\n",
      "Epoch loss: -4.110147695034898\n",
      "Improved loss from: -4.1093196588083645  to: -4.110147695034898\n",
      "Training iteration: 42\n",
      "Batch loss: -4.111542486563552\n",
      "Batch loss: -4.109372634140727\n",
      "Batch loss: -4.111069810118463\n",
      "Batch loss: -4.1123063876066634\n",
      "Batch loss: -4.113682444157104\n",
      "Batch loss: -4.111223543722679\n",
      "Batch loss: -4.11198309796934\n",
      "Batch loss: -4.1112313133318965\n",
      "Batch loss: -4.1110443566913775\n",
      "Batch loss: -4.113610491050917\n",
      "Epoch loss: -4.111706656535273\n",
      "Improved loss from: -4.110147695034898  to: -4.111706656535273\n",
      "Training iteration: 43\n",
      "Batch loss: -4.111425845008799\n",
      "Batch loss: -4.111523526252709\n",
      "Batch loss: -4.112293229313897\n",
      "Batch loss: -4.112415791622041\n",
      "Batch loss: -4.113353154315148\n",
      "Batch loss: -4.11180307128721\n",
      "Batch loss: -4.113973945293652\n",
      "Batch loss: -4.111672557256326\n",
      "Batch loss: -4.113182541460211\n",
      "Batch loss: -4.11552853142341\n",
      "Epoch loss: -4.11271721932334\n",
      "Improved loss from: -4.111706656535273  to: -4.11271721932334\n",
      "Training iteration: 44\n",
      "Batch loss: -4.112302283803167\n",
      "Batch loss: -4.112498389715897\n",
      "Batch loss: -4.112693529187282\n",
      "Batch loss: -4.112511165289061\n",
      "Batch loss: -4.115108570595564\n",
      "Batch loss: -4.114142202944219\n",
      "Batch loss: -4.1136915645091605\n",
      "Batch loss: -4.114409831272013\n",
      "Batch loss: -4.112761803383749\n",
      "Batch loss: -4.114801738032219\n",
      "Epoch loss: -4.113492107873233\n",
      "Improved loss from: -4.11271721932334  to: -4.113492107873233\n",
      "Training iteration: 45\n",
      "Batch loss: -4.114706901724472\n",
      "Batch loss: -4.113750853083513\n",
      "Batch loss: -4.113932561322559\n",
      "Batch loss: -4.114468039237854\n",
      "Batch loss: -4.113946291957641\n",
      "Batch loss: -4.115184525256764\n",
      "Batch loss: -4.114215018848455\n",
      "Batch loss: -4.114265348459257\n",
      "Batch loss: -4.114486538991509\n",
      "Batch loss: -4.115866196268794\n",
      "Epoch loss: -4.114482227515081\n",
      "Improved loss from: -4.113492107873233  to: -4.114482227515081\n",
      "Training iteration: 46\n",
      "Batch loss: -4.115793662211074\n",
      "Batch loss: -4.115092759890301\n",
      "Batch loss: -4.114813351736014\n",
      "Batch loss: -4.115497357841931\n",
      "Batch loss: -4.117265651255178\n",
      "Batch loss: -4.115534238472926\n",
      "Batch loss: -4.115739205108865\n",
      "Batch loss: -4.114588721312355\n",
      "Batch loss: -4.115742383934813\n",
      "Batch loss: -4.1176517213191195\n",
      "Epoch loss: -4.115771905308257\n",
      "Improved loss from: -4.114482227515081  to: -4.115771905308257\n",
      "Training iteration: 47\n",
      "Batch loss: -4.114328747360133\n",
      "Batch loss: -4.114678392014974\n",
      "Batch loss: -4.1148091499163755\n",
      "Batch loss: -4.115423050321169\n",
      "Batch loss: -4.117753616637842\n",
      "Batch loss: -4.11736130868539\n",
      "Batch loss: -4.115080744886032\n",
      "Batch loss: -4.116220070664372\n",
      "Batch loss: -4.116594633238619\n",
      "Batch loss: -4.1185067438512375\n",
      "Epoch loss: -4.116075645757614\n",
      "Improved loss from: -4.115771905308257  to: -4.116075645757614\n",
      "Training iteration: 48\n",
      "Batch loss: -4.115019305564073\n",
      "Batch loss: -4.115698611571916\n",
      "Batch loss: -4.116863776269067\n",
      "Batch loss: -4.11656356151285\n",
      "Batch loss: -4.118497732338405\n",
      "Batch loss: -4.118414137268618\n",
      "Batch loss: -4.11731848178366\n",
      "Batch loss: -4.117248751522531\n",
      "Batch loss: -4.117098809447302\n",
      "Batch loss: -4.118057731914174\n",
      "Epoch loss: -4.117078089919259\n",
      "Improved loss from: -4.116075645757614  to: -4.117078089919259\n",
      "Training iteration: 49\n",
      "Batch loss: -4.11626646954916\n",
      "Batch loss: -4.117821150793473\n",
      "Batch loss: -4.11773983716217\n",
      "Batch loss: -4.1171085930451845\n",
      "Batch loss: -4.120165062184124\n",
      "Batch loss: -4.117790601332724\n",
      "Batch loss: -4.1185852028549155\n",
      "Batch loss: -4.117666817447328\n",
      "Batch loss: -4.1184062287249406\n",
      "Batch loss: -4.119961583328447\n",
      "Epoch loss: -4.118151154642247\n",
      "Improved loss from: -4.117078089919259  to: -4.118151154642247\n",
      "Training iteration: 50\n",
      "Batch loss: -4.117837317316025\n",
      "Batch loss: -4.116892601551504\n",
      "Batch loss: -4.117196250467606\n",
      "Batch loss: -4.1178059498056205\n",
      "Batch loss: -4.121229420650436\n",
      "Batch loss: -4.119926135689884\n",
      "Batch loss: -4.118699988672353\n",
      "Batch loss: -4.11938291013714\n",
      "Batch loss: -4.1186274398392815\n",
      "Batch loss: -4.121096815779076\n",
      "Epoch loss: -4.118869482990893\n",
      "Improved loss from: -4.118151154642247  to: -4.118869482990893\n",
      "Training iteration: 51\n",
      "Batch loss: -4.119444593407573\n",
      "Batch loss: -4.118614243880823\n",
      "Batch loss: -4.119588578984185\n",
      "Batch loss: -4.119333502397863\n",
      "Batch loss: -4.120620054274791\n",
      "Batch loss: -4.120758805312718\n",
      "Batch loss: -4.121549273079569\n",
      "Batch loss: -4.12019247518982\n",
      "Batch loss: -4.119673840538843\n",
      "Batch loss: -4.122029423708675\n",
      "Epoch loss: -4.120180479077486\n",
      "Improved loss from: -4.118869482990893  to: -4.120180479077486\n",
      "Training iteration: 52\n",
      "Batch loss: -4.118842566514975\n",
      "Batch loss: -4.121476040238007\n",
      "Batch loss: -4.1202667743760895\n",
      "Batch loss: -4.118991503596538\n",
      "Batch loss: -4.121359254709256\n",
      "Batch loss: -4.1231539007884015\n",
      "Batch loss: -4.121380725816011\n",
      "Batch loss: -4.1212624608828605\n",
      "Batch loss: -4.119922905569994\n",
      "Batch loss: -4.122738920268115\n",
      "Epoch loss: -4.1209395052760245\n",
      "Improved loss from: -4.120180479077486  to: -4.1209395052760245\n",
      "Training iteration: 53\n",
      "Batch loss: -4.12095926399166\n",
      "Batch loss: -4.120316683847124\n",
      "Batch loss: -4.120759008920794\n",
      "Batch loss: -4.119609283755236\n",
      "Batch loss: -4.121680945535034\n",
      "Batch loss: -4.1219710655578385\n",
      "Batch loss: -4.121920546772921\n",
      "Batch loss: -4.120868674559948\n",
      "Batch loss: -4.1231723322975915\n",
      "Batch loss: -4.123586149276077\n",
      "Epoch loss: -4.121484395451423\n",
      "Improved loss from: -4.1209395052760245  to: -4.121484395451423\n",
      "Training iteration: 54\n",
      "Batch loss: -4.120830552745781\n",
      "Batch loss: -4.119972740268689\n",
      "Batch loss: -4.121383666236148\n",
      "Batch loss: -4.121079722711686\n",
      "Batch loss: -4.122595386789677\n",
      "Batch loss: -4.122442780682667\n",
      "Batch loss: -4.122458381940783\n",
      "Batch loss: -4.122555231544681\n",
      "Batch loss: -4.124226330314933\n",
      "Batch loss: -4.124133097075791\n",
      "Epoch loss: -4.122167789031083\n",
      "Improved loss from: -4.121484395451423  to: -4.122167789031083\n",
      "Training iteration: 55\n",
      "Batch loss: -4.121685292499006\n",
      "Batch loss: -4.1218356771949365\n",
      "Batch loss: -4.121305806690641\n",
      "Batch loss: -4.121629674060932\n",
      "Batch loss: -4.123889985373433\n",
      "Batch loss: -4.1225718353048295\n",
      "Batch loss: -4.121913061144173\n",
      "Batch loss: -4.122638582203802\n",
      "Batch loss: -4.122532559482174\n",
      "Batch loss: -4.123604505733525\n",
      "Epoch loss: -4.1223606979687455\n",
      "Improved loss from: -4.122167789031083  to: -4.1223606979687455\n",
      "Training iteration: 56\n",
      "Batch loss: -4.121845986138349\n",
      "Batch loss: -4.123081459421275\n",
      "Batch loss: -4.1233642774638435\n",
      "Batch loss: -4.1220950074085705\n",
      "Batch loss: -4.12400122019995\n",
      "Batch loss: -4.123720237670581\n",
      "Batch loss: -4.123838709766903\n",
      "Batch loss: -4.122249773707928\n",
      "Batch loss: -4.123479432933925\n",
      "Batch loss: -4.124989729065247\n",
      "Epoch loss: -4.123266583377657\n",
      "Improved loss from: -4.1223606979687455  to: -4.123266583377657\n",
      "Training iteration: 57\n",
      "Batch loss: -4.123079948437276\n",
      "Batch loss: -4.122495318131971\n",
      "Batch loss: -4.123827009478925\n",
      "Batch loss: -4.123984339057371\n",
      "Batch loss: -4.125047088776779\n",
      "Batch loss: -4.124746561365873\n",
      "Batch loss: -4.122613723847941\n",
      "Batch loss: -4.123435256222018\n",
      "Batch loss: -4.125246190651821\n",
      "Batch loss: -4.125956997390061\n",
      "Epoch loss: -4.124043243336003\n",
      "Improved loss from: -4.123266583377657  to: -4.124043243336003\n",
      "Training iteration: 58\n",
      "Batch loss: -4.122582830387982\n",
      "Batch loss: -4.123967417385372\n",
      "Batch loss: -4.124755670403305\n",
      "Batch loss: -4.123134247397195\n",
      "Batch loss: -4.125452954088499\n",
      "Batch loss: -4.124550981370852\n",
      "Batch loss: -4.125674721382246\n",
      "Batch loss: -4.124292196510479\n",
      "Batch loss: -4.125668043057934\n",
      "Batch loss: -4.124600276646233\n",
      "Epoch loss: -4.12446793386301\n",
      "Improved loss from: -4.124043243336003  to: -4.12446793386301\n",
      "Training iteration: 59\n",
      "Batch loss: -4.1242438764554565\n",
      "Batch loss: -4.124531030003274\n",
      "Batch loss: -4.12473621312445\n",
      "Batch loss: -4.125042063128622\n",
      "Batch loss: -4.125697277104118\n",
      "Batch loss: -4.126283347071646\n",
      "Batch loss: -4.125469302475112\n",
      "Batch loss: -4.124907123103391\n",
      "Batch loss: -4.12549159394127\n",
      "Batch loss: -4.126139480827198\n",
      "Epoch loss: -4.125254130723453\n",
      "Improved loss from: -4.12446793386301  to: -4.125254130723453\n",
      "Training iteration: 60\n",
      "Batch loss: -4.126136755928793\n",
      "Batch loss: -4.125734472405738\n",
      "Batch loss: -4.126074554535013\n",
      "Batch loss: -4.125957394266458\n",
      "Batch loss: -4.12757261614656\n",
      "Batch loss: -4.126487975928017\n",
      "Batch loss: -4.127930341331627\n",
      "Batch loss: -4.126015462365513\n",
      "Batch loss: -4.127329609272938\n",
      "Batch loss: -4.129173158004847\n",
      "Epoch loss: -4.126841234018551\n",
      "Improved loss from: -4.125254130723453  to: -4.126841234018551\n",
      "Training iteration: 61\n",
      "Batch loss: -4.125002000711862\n",
      "Batch loss: -4.1275403321066015\n",
      "Batch loss: -4.125799423515703\n",
      "Batch loss: -4.125747557245561\n",
      "Batch loss: -4.12727336732452\n",
      "Batch loss: -4.128336875441071\n",
      "Batch loss: -4.12748524880425\n",
      "Batch loss: -4.128311525759127\n",
      "Batch loss: -4.127534694660243\n",
      "Batch loss: -4.129248413148921\n",
      "Epoch loss: -4.1272279438717865\n",
      "Improved loss from: -4.126841234018551  to: -4.1272279438717865\n",
      "Training iteration: 62\n",
      "Batch loss: -4.126968802926373\n",
      "Batch loss: -4.12836584994381\n",
      "Batch loss: -4.127125944959861\n",
      "Batch loss: -4.127961656995219\n",
      "Batch loss: -4.129532358578671\n",
      "Batch loss: -4.129200936463031\n",
      "Batch loss: -4.128939325054586\n",
      "Batch loss: -4.126947573078856\n",
      "Batch loss: -4.128845122767493\n",
      "Batch loss: -4.130837600299856\n",
      "Epoch loss: -4.1284725171067755\n",
      "Improved loss from: -4.1272279438717865  to: -4.1284725171067755\n",
      "Training iteration: 63\n",
      "Batch loss: -4.128648234203898\n",
      "Batch loss: -4.12814783172382\n",
      "Batch loss: -4.128624327589063\n",
      "Batch loss: -4.128106480801445\n",
      "Batch loss: -4.130376585330634\n",
      "Batch loss: -4.131330480223055\n",
      "Batch loss: -4.12906258853013\n",
      "Batch loss: -4.129620205228504\n",
      "Batch loss: -4.1308940139786205\n",
      "Batch loss: -4.1324412886061985\n",
      "Epoch loss: -4.129725203621537\n",
      "Improved loss from: -4.1284725171067755  to: -4.129725203621537\n",
      "Training iteration: 64\n",
      "Batch loss: -4.130611869603991\n",
      "Batch loss: -4.130197533987195\n",
      "Batch loss: -4.1291845857467955\n",
      "Batch loss: -4.130061449546756\n",
      "Batch loss: -4.131087205007421\n",
      "Batch loss: -4.1334221538950855\n",
      "Batch loss: -4.130801207475159\n",
      "Batch loss: -4.129855763995698\n",
      "Batch loss: -4.132585588842636\n",
      "Batch loss: -4.133574917807322\n",
      "Epoch loss: -4.131138227590806\n",
      "Improved loss from: -4.129725203621537  to: -4.131138227590806\n",
      "Training iteration: 65\n",
      "Batch loss: -4.132509503765833\n",
      "Batch loss: -4.131501990311018\n",
      "Batch loss: -4.131802484374414\n",
      "Batch loss: -4.130573736800201\n",
      "Batch loss: -4.133607609318357\n",
      "Batch loss: -4.132644253738791\n",
      "Batch loss: -4.133713650551602\n",
      "Batch loss: -4.131745629522648\n",
      "Batch loss: -4.134047001538574\n",
      "Batch loss: -4.135118004425632\n",
      "Epoch loss: -4.132726386434707\n",
      "Improved loss from: -4.131138227590806  to: -4.132726386434707\n",
      "Training iteration: 66\n",
      "Batch loss: -4.132688274578099\n",
      "Batch loss: -4.134728622381584\n",
      "Batch loss: -4.132498706481171\n",
      "Batch loss: -4.1328764407348295\n",
      "Batch loss: -4.135394993272966\n",
      "Batch loss: -4.134636919133936\n",
      "Batch loss: -4.134478423198489\n",
      "Batch loss: -4.133641818444651\n",
      "Batch loss: -4.133067034744468\n",
      "Batch loss: -4.135670104957201\n",
      "Epoch loss: -4.13396813379274\n",
      "Improved loss from: -4.132726386434707  to: -4.13396813379274\n",
      "Training iteration: 67\n",
      "Batch loss: -4.133202137090726\n",
      "Batch loss: -4.134129132862095\n",
      "Batch loss: -4.1357357306745275\n",
      "Batch loss: -4.136024790548304\n",
      "Batch loss: -4.136207971312779\n",
      "Batch loss: -4.136166001545393\n",
      "Batch loss: -4.1353725433322115\n",
      "Batch loss: -4.134486014031697\n",
      "Batch loss: -4.13609762564985\n",
      "Batch loss: -4.137139870244998\n",
      "Epoch loss: -4.135456181729259\n",
      "Improved loss from: -4.13396813379274  to: -4.135456181729259\n",
      "Training iteration: 68\n",
      "Batch loss: -4.135773186719041\n",
      "Batch loss: -4.137236902301797\n",
      "Batch loss: -4.135491768450316\n",
      "Batch loss: -4.135390801961319\n",
      "Batch loss: -4.138291514331674\n",
      "Batch loss: -4.138471150542505\n",
      "Batch loss: -4.137070446233749\n",
      "Batch loss: -4.136724127483634\n",
      "Batch loss: -4.136580007396998\n",
      "Batch loss: -4.138867889817251\n",
      "Epoch loss: -4.136989779523828\n",
      "Improved loss from: -4.135456181729259  to: -4.136989779523828\n",
      "Training iteration: 69\n",
      "Batch loss: -4.136568116778571\n",
      "Batch loss: -4.137859094666042\n",
      "Batch loss: -4.137117532286625\n",
      "Batch loss: -4.140713420876095\n",
      "Batch loss: -4.139130611906633\n",
      "Batch loss: -4.138449734248934\n",
      "Batch loss: -4.137810093274433\n",
      "Batch loss: -4.136545836678948\n",
      "Batch loss: -4.138237512147443\n",
      "Batch loss: -4.1390918045237814\n",
      "Epoch loss: -4.13815237573875\n",
      "Improved loss from: -4.136989779523828  to: -4.13815237573875\n",
      "Training iteration: 70\n",
      "Batch loss: -4.138060742828253\n",
      "Batch loss: -4.138467047628106\n",
      "Batch loss: -4.137338180710903\n",
      "Batch loss: -4.139702022250953\n",
      "Batch loss: -4.140230546172505\n",
      "Batch loss: -4.13818053202779\n",
      "Batch loss: -4.139030054308093\n",
      "Batch loss: -4.138498256450601\n",
      "Batch loss: -4.140708527751496\n",
      "Batch loss: -4.140808635582408\n",
      "Epoch loss: -4.139102454571111\n",
      "Improved loss from: -4.13815237573875  to: -4.139102454571111\n",
      "Training iteration: 71\n",
      "Batch loss: -4.139622659713073\n",
      "Batch loss: -4.14057933904732\n",
      "Batch loss: -4.138824284529907\n",
      "Batch loss: -4.139620618582352\n",
      "Batch loss: -4.141988992062656\n",
      "Batch loss: -4.141965592341593\n",
      "Batch loss: -4.140677915578737\n",
      "Batch loss: -4.138441321430814\n",
      "Batch loss: -4.140095627940294\n",
      "Batch loss: -4.14161766704319\n",
      "Epoch loss: -4.140343401826994\n",
      "Improved loss from: -4.139102454571111  to: -4.140343401826994\n",
      "Training iteration: 72\n",
      "Batch loss: -4.139212397897889\n",
      "Batch loss: -4.141694066335516\n",
      "Batch loss: -4.140539436813046\n",
      "Batch loss: -4.140661096710644\n",
      "Batch loss: -4.141192969547461\n",
      "Batch loss: -4.14265784178138\n",
      "Batch loss: -4.142065939919801\n",
      "Batch loss: -4.140445138762474\n",
      "Batch loss: -4.141787432048145\n",
      "Batch loss: -4.142898540342912\n",
      "Epoch loss: -4.141315486015928\n",
      "Improved loss from: -4.140343401826994  to: -4.141315486015928\n",
      "Training iteration: 73\n",
      "Batch loss: -4.140225513381502\n",
      "Batch loss: -4.142461367280112\n",
      "Batch loss: -4.141619228264597\n",
      "Batch loss: -4.141444935366006\n",
      "Batch loss: -4.143873875930975\n",
      "Batch loss: -4.1408984172572145\n",
      "Batch loss: -4.140988020727298\n",
      "Batch loss: -4.142927781236883\n",
      "Batch loss: -4.143146021994907\n",
      "Batch loss: -4.143609146434973\n",
      "Epoch loss: -4.142119430787448\n",
      "Improved loss from: -4.141315486015928  to: -4.142119430787448\n",
      "Training iteration: 74\n",
      "Batch loss: -4.139836914512226\n",
      "Batch loss: -4.142778915706199\n",
      "Batch loss: -4.142042755625816\n",
      "Batch loss: -4.141230754394604\n",
      "Batch loss: -4.1420209521530955\n",
      "Batch loss: -4.144309567028368\n",
      "Batch loss: -4.143342358266042\n",
      "Batch loss: -4.142527140286937\n",
      "Batch loss: -4.141255889194639\n",
      "Batch loss: -4.1437741457691875\n",
      "Epoch loss: -4.1423119392937116\n",
      "Improved loss from: -4.142119430787448  to: -4.1423119392937116\n",
      "Training iteration: 75\n",
      "Batch loss: -4.143411761612579\n",
      "Batch loss: -4.142775794477495\n",
      "Batch loss: -4.142432337130482\n",
      "Batch loss: -4.142892342065522\n",
      "Batch loss: -4.143643338825939\n",
      "Batch loss: -4.144142785713942\n",
      "Batch loss: -4.143884414084506\n",
      "Batch loss: -4.143074181860498\n",
      "Batch loss: -4.1444936021906615\n",
      "Batch loss: -4.144529166353373\n",
      "Epoch loss: -4.1435279724315\n",
      "Improved loss from: -4.1423119392937116  to: -4.1435279724315\n",
      "Training iteration: 76\n",
      "Batch loss: -4.142286087072111\n",
      "Batch loss: -4.1430512098154315\n",
      "Batch loss: -4.142542575276817\n",
      "Batch loss: -4.1432117938648405\n",
      "Batch loss: -4.145456794598585\n",
      "Batch loss: -4.1450114922197585\n",
      "Batch loss: -4.144735005422239\n",
      "Batch loss: -4.143895227586745\n",
      "Batch loss: -4.144878147843914\n",
      "Batch loss: -4.145721038743379\n",
      "Epoch loss: -4.144078937244382\n",
      "Improved loss from: -4.1435279724315  to: -4.144078937244382\n",
      "Training iteration: 77\n",
      "Batch loss: -4.144774322344834\n",
      "Batch loss: -4.145005860689496\n",
      "Batch loss: -4.144483798536714\n",
      "Batch loss: -4.145612677371089\n",
      "Batch loss: -4.145600997168709\n",
      "Batch loss: -4.144625347993269\n",
      "Batch loss: -4.144347522935531\n",
      "Batch loss: -4.1443303133855816\n",
      "Batch loss: -4.144909327120338\n",
      "Batch loss: -4.146196247171901\n",
      "Epoch loss: -4.1449886414717465\n",
      "Improved loss from: -4.144078937244382  to: -4.1449886414717465\n",
      "Training iteration: 78\n",
      "Batch loss: -4.144013906985839\n",
      "Batch loss: -4.145891939033309\n",
      "Batch loss: -4.144535768163073\n",
      "Batch loss: -4.144435544527559\n",
      "Batch loss: -4.146930918159308\n",
      "Batch loss: -4.1467700202117985\n",
      "Batch loss: -4.145645823545051\n",
      "Batch loss: -4.145241121828998\n",
      "Batch loss: -4.144688557338952\n",
      "Batch loss: -4.146146603095357\n",
      "Epoch loss: -4.1454300202889245\n",
      "Improved loss from: -4.1449886414717465  to: -4.1454300202889245\n",
      "Training iteration: 79\n",
      "Batch loss: -4.146197469143391\n",
      "Batch loss: -4.1455436227400755\n",
      "Batch loss: -4.144386956216099\n",
      "Batch loss: -4.144588007954908\n",
      "Batch loss: -4.146615225903216\n",
      "Batch loss: -4.146581179955555\n",
      "Batch loss: -4.145490280850514\n",
      "Batch loss: -4.144220589217165\n",
      "Batch loss: -4.145823498109931\n",
      "Batch loss: -4.14630416508491\n",
      "Epoch loss: -4.145575099517576\n",
      "Improved loss from: -4.1454300202889245  to: -4.145575099517576\n",
      "Training iteration: 80\n",
      "Batch loss: -4.145747139346785\n",
      "Batch loss: -4.147130254644417\n",
      "Batch loss: -4.14558080210065\n",
      "Batch loss: -4.146253717599881\n",
      "Batch loss: -4.14841001497714\n",
      "Batch loss: -4.14706193133629\n",
      "Batch loss: -4.145411826977749\n",
      "Batch loss: -4.146509176980903\n",
      "Batch loss: -4.147200006005087\n",
      "Batch loss: -4.147912510471331\n",
      "Epoch loss: -4.146721738044023\n",
      "Improved loss from: -4.145575099517576  to: -4.146721738044023\n",
      "Training iteration: 81\n",
      "Batch loss: -4.14547169881951\n",
      "Batch loss: -4.146643819586651\n",
      "Batch loss: -4.144486105686331\n",
      "Batch loss: -4.145896994571381\n",
      "Batch loss: -4.147237581777909\n",
      "Batch loss: -4.14715818064989\n",
      "Batch loss: -4.147002141536865\n",
      "Batch loss: -4.146986348596405\n",
      "Batch loss: -4.147392902430279\n",
      "Batch loss: -4.147811879934984\n",
      "Epoch loss: -4.14660876535902\n",
      "Loss (no improvement): -4.14660876535902\n",
      "Training iteration: 82\n",
      "Batch loss: -4.145025939638097\n",
      "Batch loss: -4.147051065400954\n",
      "Batch loss: -4.146967086091625\n",
      "Batch loss: -4.147638501292237\n",
      "Batch loss: -4.146913322400862\n",
      "Batch loss: -4.147607535229407\n",
      "Batch loss: -4.147274476620511\n",
      "Batch loss: -4.146202665846518\n",
      "Batch loss: -4.147995014018182\n",
      "Batch loss: -4.148348598195141\n",
      "Epoch loss: -4.147102420473354\n",
      "Improved loss from: -4.146721738044023  to: -4.147102420473354\n",
      "Training iteration: 83\n",
      "Batch loss: -4.1468302359823115\n",
      "Batch loss: -4.146854787283176\n",
      "Batch loss: -4.147266474994786\n",
      "Batch loss: -4.148897257743503\n",
      "Batch loss: -4.147300572782575\n",
      "Batch loss: -4.15004215745932\n",
      "Batch loss: -4.149150575538355\n",
      "Batch loss: -4.146966181051306\n",
      "Batch loss: -4.147194095390206\n",
      "Batch loss: -4.149528162731051\n",
      "Epoch loss: -4.148003050095658\n",
      "Improved loss from: -4.147102420473354  to: -4.148003050095658\n",
      "Training iteration: 84\n",
      "Batch loss: -4.147390834709524\n",
      "Batch loss: -4.148174411795278\n",
      "Batch loss: -4.148253444142536\n",
      "Batch loss: -4.147281372697805\n",
      "Batch loss: -4.1499205049218535\n",
      "Batch loss: -4.148331744549597\n",
      "Batch loss: -4.147294834520208\n",
      "Batch loss: -4.148145525617304\n",
      "Batch loss: -4.1480364576748565\n",
      "Batch loss: -4.14994209721941\n",
      "Epoch loss: -4.148277122784837\n",
      "Improved loss from: -4.148003050095658  to: -4.148277122784837\n",
      "Training iteration: 85\n",
      "Batch loss: -4.146900585463338\n",
      "Batch loss: -4.148549911190518\n",
      "Batch loss: -4.146740842121707\n",
      "Batch loss: -4.148923556259333\n",
      "Batch loss: -4.150178766047063\n",
      "Batch loss: -4.150162848623754\n",
      "Batch loss: -4.148143682893573\n",
      "Batch loss: -4.148256911057668\n",
      "Batch loss: -4.150001129710566\n",
      "Batch loss: -4.149904515608876\n",
      "Epoch loss: -4.14877627489764\n",
      "Improved loss from: -4.148277122784837  to: -4.14877627489764\n",
      "Training iteration: 86\n",
      "Batch loss: -4.148291723903391\n",
      "Batch loss: -4.148891513308988\n",
      "Batch loss: -4.14855398542651\n",
      "Batch loss: -4.149440979101915\n",
      "Batch loss: -4.149552612200282\n",
      "Batch loss: -4.1511956737921585\n",
      "Batch loss: -4.149177505474954\n",
      "Batch loss: -4.148221963698914\n",
      "Batch loss: -4.148415676796059\n",
      "Batch loss: -4.149200113625838\n",
      "Epoch loss: -4.149094174732902\n",
      "Improved loss from: -4.14877627489764  to: -4.149094174732902\n",
      "Training iteration: 87\n",
      "Batch loss: -4.149393802284943\n",
      "Batch loss: -4.149748598039958\n",
      "Batch loss: -4.147964583218396\n",
      "Batch loss: -4.148662483914678\n",
      "Batch loss: -4.151227689704917\n",
      "Batch loss: -4.149397375723914\n",
      "Batch loss: -4.149490602288432\n",
      "Batch loss: -4.149440882436074\n",
      "Batch loss: -4.150343002570955\n",
      "Batch loss: -4.150764055787665\n",
      "Epoch loss: -4.149643307596993\n",
      "Improved loss from: -4.149094174732902  to: -4.149643307596993\n",
      "Training iteration: 88\n",
      "Batch loss: -4.148285090131288\n",
      "Batch loss: -4.150338440057518\n",
      "Batch loss: -4.147417718384062\n",
      "Batch loss: -4.148355388462008\n",
      "Batch loss: -4.150436338996588\n",
      "Batch loss: -4.151594540561161\n",
      "Batch loss: -4.149164824588849\n",
      "Batch loss: -4.148337710682324\n",
      "Batch loss: -4.149212984184511\n",
      "Batch loss: -4.151965913728596\n",
      "Epoch loss: -4.1495108949776895\n",
      "Loss (no improvement): -4.1495108949776895\n",
      "Training iteration: 89\n",
      "Batch loss: -4.149359503637848\n",
      "Batch loss: -4.149580949272298\n",
      "Batch loss: -4.149150776863434\n",
      "Batch loss: -4.14966393749813\n",
      "Batch loss: -4.150859291457049\n",
      "Batch loss: -4.149684250179376\n",
      "Batch loss: -4.151275753630386\n",
      "Batch loss: -4.149830106454089\n",
      "Batch loss: -4.14995991652615\n",
      "Batch loss: -4.152422051521415\n",
      "Epoch loss: -4.1501786537040175\n",
      "Improved loss from: -4.149643307596993  to: -4.1501786537040175\n",
      "Training iteration: 90\n",
      "Batch loss: -4.150764360895332\n",
      "Batch loss: -4.1513242573260305\n",
      "Batch loss: -4.149683651712641\n",
      "Batch loss: -4.150617005289537\n",
      "Batch loss: -4.152811478296463\n",
      "Batch loss: -4.1498330112784885\n",
      "Batch loss: -4.150900333162978\n",
      "Batch loss: -4.149212214760703\n",
      "Batch loss: -4.150813368709381\n",
      "Batch loss: -4.152842121992755\n",
      "Epoch loss: -4.150880180342431\n",
      "Improved loss from: -4.1501786537040175  to: -4.150880180342431\n",
      "Training iteration: 91\n",
      "Batch loss: -4.150179713774665\n",
      "Batch loss: -4.149293646437046\n",
      "Batch loss: -4.1499079747985315\n",
      "Batch loss: -4.1495611623506345\n",
      "Batch loss: -4.152147843558337\n",
      "Batch loss: -4.152083826367881\n",
      "Batch loss: -4.151304421793539\n",
      "Batch loss: -4.149358341654588\n",
      "Batch loss: -4.1513735849849756\n",
      "Batch loss: -4.151898441361308\n",
      "Epoch loss: -4.1507108957081496\n",
      "Loss (no improvement): -4.1507108957081496\n",
      "Training iteration: 92\n",
      "Batch loss: -4.151424337386819\n",
      "Batch loss: -4.151586799667661\n",
      "Batch loss: -4.150837936613035\n",
      "Batch loss: -4.150798129458092\n",
      "Batch loss: -4.151242936857634\n",
      "Batch loss: -4.1518085653758785\n",
      "Batch loss: -4.150448782241923\n",
      "Batch loss: -4.149449851521597\n",
      "Batch loss: -4.15149655551989\n",
      "Batch loss: -4.1523180978967575\n",
      "Epoch loss: -4.15114119925393\n",
      "Improved loss from: -4.150880180342431  to: -4.15114119925393\n",
      "Training iteration: 93\n",
      "Batch loss: -4.150560855487087\n",
      "Batch loss: -4.151337804965588\n",
      "Batch loss: -4.1516419092003165\n",
      "Batch loss: -4.151303162411204\n",
      "Batch loss: -4.1532513668239055\n",
      "Batch loss: -4.1525502537225565\n",
      "Batch loss: -4.152239085782537\n",
      "Batch loss: -4.14904683347391\n",
      "Batch loss: -4.151712408271072\n",
      "Batch loss: -4.152756365285961\n",
      "Epoch loss: -4.151640004542413\n",
      "Improved loss from: -4.15114119925393  to: -4.151640004542413\n",
      "Training iteration: 94\n",
      "Batch loss: -4.151980356080853\n",
      "Batch loss: -4.1530594924166415\n",
      "Batch loss: -4.151978612994295\n",
      "Batch loss: -4.152651025913411\n",
      "Batch loss: -4.153457617540203\n",
      "Batch loss: -4.152430981089879\n",
      "Batch loss: -4.152101348271054\n",
      "Batch loss: -4.151260560560556\n",
      "Batch loss: -4.150459790051806\n",
      "Batch loss: -4.151694424423596\n",
      "Epoch loss: -4.15210742093423\n",
      "Improved loss from: -4.151640004542413  to: -4.15210742093423\n",
      "Training iteration: 95\n",
      "Batch loss: -4.15126809532015\n",
      "Batch loss: -4.153266942321984\n",
      "Batch loss: -4.151475842485454\n",
      "Batch loss: -4.151756119474296\n",
      "Batch loss: -4.152121185722919\n",
      "Batch loss: -4.153517500678686\n",
      "Batch loss: -4.152354339393297\n",
      "Batch loss: -4.151598331780507\n",
      "Batch loss: -4.152594892160742\n",
      "Batch loss: -4.154689406049498\n",
      "Epoch loss: -4.152464265538754\n",
      "Improved loss from: -4.15210742093423  to: -4.152464265538754\n",
      "Training iteration: 96\n",
      "Batch loss: -4.152110887313564\n",
      "Batch loss: -4.151826603602061\n",
      "Batch loss: -4.152253336276697\n",
      "Batch loss: -4.1525895166589235\n",
      "Batch loss: -4.15292003361696\n",
      "Batch loss: -4.152694315664053\n",
      "Batch loss: -4.152563322024551\n",
      "Batch loss: -4.151356897271218\n",
      "Batch loss: -4.153496937974868\n",
      "Batch loss: -4.154742928880319\n",
      "Epoch loss: -4.152655477928322\n",
      "Improved loss from: -4.152464265538754  to: -4.152655477928322\n",
      "Training iteration: 97\n",
      "Batch loss: -4.1532279271114385\n",
      "Batch loss: -4.153071764522142\n",
      "Batch loss: -4.152212415703168\n",
      "Batch loss: -4.152089680977687\n",
      "Batch loss: -4.153273679020401\n",
      "Batch loss: -4.154053104594656\n",
      "Batch loss: -4.152628992900544\n",
      "Batch loss: -4.152329758225324\n",
      "Batch loss: -4.153622122811552\n",
      "Batch loss: -4.154135839039701\n",
      "Epoch loss: -4.1530645284906615\n",
      "Improved loss from: -4.152655477928322  to: -4.1530645284906615\n",
      "Training iteration: 98\n",
      "Batch loss: -4.152273479580296\n",
      "Batch loss: -4.152550961561308\n",
      "Batch loss: -4.153831132169276\n",
      "Batch loss: -4.152382132954637\n",
      "Batch loss: -4.155767503617679\n",
      "Batch loss: -4.153718425210443\n",
      "Batch loss: -4.153062809998035\n",
      "Batch loss: -4.152648161901301\n",
      "Batch loss: -4.152792197852021\n",
      "Batch loss: -4.155801760674069\n",
      "Epoch loss: -4.153482856551906\n",
      "Improved loss from: -4.1530645284906615  to: -4.153482856551906\n",
      "Training iteration: 99\n",
      "Batch loss: -4.152580025250334\n",
      "Batch loss: -4.153587021152551\n",
      "Batch loss: -4.152272782219325\n",
      "Batch loss: -4.152794626759439\n",
      "Batch loss: -4.1537268719887726\n",
      "Batch loss: -4.153031663161604\n",
      "Batch loss: -4.153715207378917\n",
      "Batch loss: -4.151835461446925\n",
      "Batch loss: -4.153628490815265\n",
      "Batch loss: -4.155858834757788\n",
      "Epoch loss: -4.153303098493092\n",
      "Loss (no improvement): -4.153303098493092\n",
      "Training iteration: 100\n",
      "Batch loss: -4.1533036036442414\n",
      "Batch loss: -4.154078446453176\n",
      "Batch loss: -4.153693477640711\n",
      "Batch loss: -4.154668168030823\n",
      "Batch loss: -4.155339888409144\n",
      "Batch loss: -4.153777185616826\n",
      "Batch loss: -4.154273421134297\n",
      "Batch loss: -4.154287722111649\n",
      "Batch loss: -4.153696087824776\n",
      "Batch loss: -4.154410414071384\n",
      "Epoch loss: -4.154152841493703\n",
      "Improved loss from: -4.153482856551906  to: -4.154152841493703\n",
      "Training iteration: 101\n",
      "Batch loss: -4.15280543164701\n",
      "Batch loss: -4.153232729443204\n",
      "Batch loss: -4.1523387592697985\n",
      "Batch loss: -4.153545888953197\n",
      "Batch loss: -4.154835600063892\n",
      "Batch loss: -4.154460961364184\n",
      "Batch loss: -4.153497625127843\n",
      "Batch loss: -4.153105744013055\n",
      "Batch loss: -4.154319527590893\n",
      "Batch loss: -4.155998252745935\n",
      "Epoch loss: -4.153814052021902\n",
      "Loss (no improvement): -4.153814052021902\n",
      "Training iteration: 102\n",
      "Batch loss: -4.15295760101975\n",
      "Batch loss: -4.15413591246013\n",
      "Batch loss: -4.154304995335265\n",
      "Batch loss: -4.154021214996765\n",
      "Batch loss: -4.154903145029946\n",
      "Batch loss: -4.155158255301123\n",
      "Batch loss: -4.153273046624475\n",
      "Batch loss: -4.152377274585409\n",
      "Batch loss: -4.154560877704661\n",
      "Batch loss: -4.156174573041928\n",
      "Epoch loss: -4.154186689609946\n",
      "Improved loss from: -4.154152841493703  to: -4.154186689609946\n",
      "Training iteration: 103\n",
      "Batch loss: -4.155683027560481\n",
      "Batch loss: -4.15298631505737\n",
      "Batch loss: -4.154379072325514\n",
      "Batch loss: -4.153901602336903\n",
      "Batch loss: -4.155940300075098\n",
      "Batch loss: -4.155513648154029\n",
      "Batch loss: -4.154776635674637\n",
      "Batch loss: -4.154217869796758\n",
      "Batch loss: -4.154479085159098\n",
      "Batch loss: -4.156915902345591\n",
      "Epoch loss: -4.154879345848547\n",
      "Improved loss from: -4.154186689609946  to: -4.154879345848547\n",
      "Training iteration: 104\n",
      "Batch loss: -4.152791976874907\n",
      "Batch loss: -4.154565107195125\n",
      "Batch loss: -4.15533822980736\n",
      "Batch loss: -4.15533316826106\n",
      "Batch loss: -4.156535921045716\n",
      "Batch loss: -4.155417940176705\n",
      "Batch loss: -4.154843789739655\n",
      "Batch loss: -4.154174001656286\n",
      "Batch loss: -4.15519804440306\n",
      "Batch loss: -4.156601132422634\n",
      "Epoch loss: -4.15507993115825\n",
      "Improved loss from: -4.154879345848547  to: -4.15507993115825\n",
      "Training iteration: 105\n",
      "Batch loss: -4.155581214473069\n",
      "Batch loss: -4.1552673290601145\n",
      "Batch loss: -4.15426528889745\n",
      "Batch loss: -4.154666246068056\n",
      "Batch loss: -4.1556319500460255\n",
      "Batch loss: -4.155095460914488\n",
      "Batch loss: -4.155672921848847\n",
      "Batch loss: -4.153999028282971\n",
      "Batch loss: -4.155572576906522\n",
      "Batch loss: -4.158392209899121\n",
      "Epoch loss: -4.155414422639666\n",
      "Improved loss from: -4.15507993115825  to: -4.155414422639666\n",
      "Training iteration: 106\n",
      "Batch loss: -4.1560119663985935\n",
      "Batch loss: -4.155683573090511\n",
      "Batch loss: -4.154600549367807\n",
      "Batch loss: -4.154824260468596\n",
      "Batch loss: -4.156106843707936\n",
      "Batch loss: -4.156528277553149\n",
      "Batch loss: -4.1555932322124045\n",
      "Batch loss: -4.154979113402691\n",
      "Batch loss: -4.156746617672057\n",
      "Batch loss: -4.158634114559499\n",
      "Epoch loss: -4.155970854843324\n",
      "Improved loss from: -4.155414422639666  to: -4.155970854843324\n",
      "Training iteration: 107\n",
      "Batch loss: -4.155388207334083\n",
      "Batch loss: -4.154860260350605\n",
      "Batch loss: -4.1558357217172945\n",
      "Batch loss: -4.156343490067357\n",
      "Batch loss: -4.157017064378204\n",
      "Batch loss: -4.157342134407542\n",
      "Batch loss: -4.156171968649731\n",
      "Batch loss: -4.154732671537196\n",
      "Batch loss: -4.155480422920258\n",
      "Batch loss: -4.157783219046492\n",
      "Epoch loss: -4.156095516040876\n",
      "Improved loss from: -4.155970854843324  to: -4.156095516040876\n",
      "Training iteration: 108\n",
      "Batch loss: -4.156070424830845\n",
      "Batch loss: -4.15644666455037\n",
      "Batch loss: -4.155842150534713\n",
      "Batch loss: -4.15590331707317\n",
      "Batch loss: -4.158066711756363\n",
      "Batch loss: -4.156820492180353\n",
      "Batch loss: -4.155139726681589\n",
      "Batch loss: -4.154313693192719\n",
      "Batch loss: -4.155785672068421\n",
      "Batch loss: -4.159152056683054\n",
      "Epoch loss: -4.15635409095516\n",
      "Improved loss from: -4.156095516040876  to: -4.15635409095516\n",
      "Training iteration: 109\n",
      "Batch loss: -4.15618074795716\n",
      "Batch loss: -4.155967811637982\n",
      "Batch loss: -4.154785074712461\n",
      "Batch loss: -4.156313724347259\n",
      "Batch loss: -4.157456000226181\n",
      "Batch loss: -4.15802119879529\n",
      "Batch loss: -4.156844295643078\n",
      "Batch loss: -4.15614406895621\n",
      "Batch loss: -4.158557294221457\n",
      "Batch loss: -4.159189633324324\n",
      "Epoch loss: -4.15694598498214\n",
      "Improved loss from: -4.15635409095516  to: -4.15694598498214\n",
      "Training iteration: 110\n",
      "Batch loss: -4.156231527381266\n",
      "Batch loss: -4.156474465085149\n",
      "Batch loss: -4.156828486855769\n",
      "Batch loss: -4.157152992921288\n",
      "Batch loss: -4.1577863957652985\n",
      "Batch loss: -4.15769066664714\n",
      "Batch loss: -4.158778704352871\n",
      "Batch loss: -4.156382252680479\n",
      "Batch loss: -4.1573893875696815\n",
      "Batch loss: -4.157963764389693\n",
      "Epoch loss: -4.157267864364863\n",
      "Improved loss from: -4.15694598498214  to: -4.157267864364863\n",
      "Training iteration: 111\n",
      "Batch loss: -4.156327151085224\n",
      "Batch loss: -4.157350363809535\n",
      "Batch loss: -4.157877775868153\n",
      "Batch loss: -4.158851975311648\n",
      "Batch loss: -4.157791141128127\n",
      "Batch loss: -4.158400198910893\n",
      "Batch loss: -4.1572335634616575\n",
      "Batch loss: -4.157581898964514\n",
      "Batch loss: -4.158971856070014\n",
      "Batch loss: -4.159546414825829\n",
      "Epoch loss: -4.157993233943559\n",
      "Improved loss from: -4.157267864364863  to: -4.157993233943559\n",
      "Training iteration: 112\n",
      "Batch loss: -4.157996536527899\n",
      "Batch loss: -4.15870495753187\n",
      "Batch loss: -4.158904964693762\n",
      "Batch loss: -4.158829417316263\n",
      "Batch loss: -4.158945719018681\n",
      "Batch loss: -4.1588499055409205\n",
      "Batch loss: -4.159924019656728\n",
      "Batch loss: -4.156619369286558\n",
      "Batch loss: -4.158076147782365\n",
      "Batch loss: -4.159540328444679\n",
      "Epoch loss: -4.158639136579973\n",
      "Improved loss from: -4.157993233943559  to: -4.158639136579973\n",
      "Training iteration: 113\n",
      "Batch loss: -4.158616073610631\n",
      "Batch loss: -4.159174211985175\n",
      "Batch loss: -4.157524003392256\n",
      "Batch loss: -4.157272527516521\n",
      "Batch loss: -4.1601200703274746\n",
      "Batch loss: -4.16023772696708\n",
      "Batch loss: -4.159079786874463\n",
      "Batch loss: -4.156765523446045\n",
      "Batch loss: -4.156353421007417\n",
      "Batch loss: -4.158356401438799\n",
      "Epoch loss: -4.158349974656586\n",
      "Loss (no improvement): -4.158349974656586\n",
      "Training iteration: 114\n",
      "Batch loss: -4.158821123026793\n",
      "Batch loss: -4.15870965055123\n",
      "Batch loss: -4.157772729010171\n",
      "Batch loss: -4.158304168359422\n",
      "Batch loss: -4.161177149144641\n",
      "Batch loss: -4.1577724538793905\n",
      "Batch loss: -4.159075028352144\n",
      "Batch loss: -4.158203783167842\n",
      "Batch loss: -4.160124088762377\n",
      "Batch loss: -4.159787828518558\n",
      "Epoch loss: -4.158974800277257\n",
      "Improved loss from: -4.158639136579973  to: -4.158974800277257\n",
      "Training iteration: 115\n",
      "Batch loss: -4.159119765472738\n",
      "Batch loss: -4.1609415811754555\n",
      "Batch loss: -4.16014279624856\n",
      "Batch loss: -4.160627240890508\n",
      "Batch loss: -4.160520139970299\n",
      "Batch loss: -4.160306887846868\n",
      "Batch loss: -4.158349394933924\n",
      "Batch loss: -4.157689718350339\n",
      "Batch loss: -4.160211500329531\n",
      "Batch loss: -4.162396922397837\n",
      "Epoch loss: -4.160030594761606\n",
      "Improved loss from: -4.158974800277257  to: -4.160030594761606\n",
      "Training iteration: 116\n",
      "Batch loss: -4.161535389566207\n",
      "Batch loss: -4.16003592598623\n",
      "Batch loss: -4.159899886657842\n",
      "Batch loss: -4.160715826908747\n",
      "Batch loss: -4.161977278364635\n",
      "Batch loss: -4.161084534837673\n",
      "Batch loss: -4.160501125744822\n",
      "Batch loss: -4.159212039879572\n",
      "Batch loss: -4.160320152592076\n",
      "Batch loss: -4.160949196999821\n",
      "Epoch loss: -4.160623135753763\n",
      "Improved loss from: -4.160030594761606  to: -4.160623135753763\n",
      "Training iteration: 117\n",
      "Batch loss: -4.160280847947317\n",
      "Batch loss: -4.16006476885519\n",
      "Batch loss: -4.160984402293979\n",
      "Batch loss: -4.160511794280596\n",
      "Batch loss: -4.161746196205432\n",
      "Batch loss: -4.16125878694384\n",
      "Batch loss: -4.162100885755575\n",
      "Batch loss: -4.160455785204838\n",
      "Batch loss: -4.161345304868937\n",
      "Batch loss: -4.162077310003977\n",
      "Epoch loss: -4.161082608235968\n",
      "Improved loss from: -4.160623135753763  to: -4.161082608235968\n",
      "Training iteration: 118\n",
      "Batch loss: -4.160723294798812\n",
      "Batch loss: -4.16072541135193\n",
      "Batch loss: -4.161378930107279\n",
      "Batch loss: -4.162832928597362\n",
      "Batch loss: -4.162922621880535\n",
      "Batch loss: -4.160610590111172\n",
      "Batch loss: -4.160924266005309\n",
      "Batch loss: -4.159750743153871\n",
      "Batch loss: -4.161789915577696\n",
      "Batch loss: -4.163675757567068\n",
      "Epoch loss: -4.161533445915103\n",
      "Improved loss from: -4.161082608235968  to: -4.161533445915103\n",
      "Training iteration: 119\n",
      "Batch loss: -4.160961008781805\n",
      "Batch loss: -4.162973482313219\n",
      "Batch loss: -4.162413234495556\n",
      "Batch loss: -4.16148592588726\n",
      "Batch loss: -4.161952758673306\n",
      "Batch loss: -4.161172167697465\n",
      "Batch loss: -4.160447808463122\n",
      "Batch loss: -4.159678291827241\n",
      "Batch loss: -4.162362533544611\n",
      "Batch loss: -4.162049934637504\n",
      "Epoch loss: -4.161549714632108\n",
      "Improved loss from: -4.161533445915103  to: -4.161549714632108\n",
      "Training iteration: 120\n",
      "Batch loss: -4.162320282610739\n",
      "Batch loss: -4.163032380488989\n",
      "Batch loss: -4.161421837974692\n",
      "Batch loss: -4.163341748195984\n",
      "Batch loss: -4.162178144760588\n",
      "Batch loss: -4.163236997128391\n",
      "Batch loss: -4.162783739988816\n",
      "Batch loss: -4.162374055836104\n",
      "Batch loss: -4.161570639708765\n",
      "Batch loss: -4.163187202837825\n",
      "Epoch loss: -4.162544702953089\n",
      "Improved loss from: -4.161549714632108  to: -4.162544702953089\n",
      "Training iteration: 121\n",
      "Batch loss: -4.161742768462868\n",
      "Batch loss: -4.16239203492112\n",
      "Batch loss: -4.161638934960137\n",
      "Batch loss: -4.16213053881308\n",
      "Batch loss: -4.162865431588366\n",
      "Batch loss: -4.162573513711848\n",
      "Batch loss: -4.162234566846728\n",
      "Batch loss: -4.162258492823601\n",
      "Batch loss: -4.162025233789463\n",
      "Batch loss: -4.162808297401718\n",
      "Epoch loss: -4.162266981331893\n",
      "Loss (no improvement): -4.162266981331893\n",
      "Training iteration: 122\n",
      "Batch loss: -4.162988626068318\n",
      "Batch loss: -4.163532028523203\n",
      "Batch loss: -4.161885373280587\n",
      "Batch loss: -4.163007133472356\n",
      "Batch loss: -4.163927626666178\n",
      "Batch loss: -4.163373086910824\n",
      "Batch loss: -4.163822072280789\n",
      "Batch loss: -4.162954401288884\n",
      "Batch loss: -4.162942065780173\n",
      "Batch loss: -4.164723431213582\n",
      "Epoch loss: -4.1633155845484895\n",
      "Improved loss from: -4.162544702953089  to: -4.1633155845484895\n",
      "Training iteration: 123\n",
      "Batch loss: -4.163071778947572\n",
      "Batch loss: -4.163525381707233\n",
      "Batch loss: -4.162152256095203\n",
      "Batch loss: -4.161581724659908\n",
      "Batch loss: -4.1625879417896225\n",
      "Batch loss: -4.162209201284127\n",
      "Batch loss: -4.163277950124123\n",
      "Batch loss: -4.162348932048166\n",
      "Batch loss: -4.162535082408296\n",
      "Batch loss: -4.165120110242626\n",
      "Epoch loss: -4.162841035930688\n",
      "Loss (no improvement): -4.162841035930688\n",
      "Training iteration: 124\n",
      "Batch loss: -4.162842074748923\n",
      "Batch loss: -4.162888138464111\n",
      "Batch loss: -4.162378359326054\n",
      "Batch loss: -4.165616052214105\n",
      "Batch loss: -4.164140624501001\n",
      "Batch loss: -4.163752570171539\n",
      "Batch loss: -4.164141181582648\n",
      "Batch loss: -4.163448359566495\n",
      "Batch loss: -4.163579250219425\n",
      "Batch loss: -4.164908342258895\n",
      "Epoch loss: -4.163769495305321\n",
      "Improved loss from: -4.1633155845484895  to: -4.163769495305321\n",
      "Training iteration: 125\n",
      "Batch loss: -4.16429978084793\n",
      "Batch loss: -4.162528369248279\n",
      "Batch loss: -4.164898757072761\n",
      "Batch loss: -4.164563620617578\n",
      "Batch loss: -4.163108356898504\n",
      "Batch loss: -4.164591257156469\n",
      "Batch loss: -4.163830769454452\n",
      "Batch loss: -4.16228843077946\n",
      "Batch loss: -4.164611424265076\n",
      "Batch loss: -4.165872246155652\n",
      "Epoch loss: -4.164059301249616\n",
      "Improved loss from: -4.163769495305321  to: -4.164059301249616\n",
      "Training iteration: 126\n",
      "Batch loss: -4.163489585489701\n",
      "Batch loss: -4.164626449976347\n",
      "Batch loss: -4.164318958968209\n",
      "Batch loss: -4.16399681065033\n",
      "Batch loss: -4.165750022311918\n",
      "Batch loss: -4.16425817197097\n",
      "Batch loss: -4.163524072638164\n",
      "Batch loss: -4.162780358799229\n",
      "Batch loss: -4.164022570817194\n",
      "Batch loss: -4.166178311186482\n",
      "Epoch loss: -4.164294531280855\n",
      "Improved loss from: -4.164059301249616  to: -4.164294531280855\n",
      "Training iteration: 127\n",
      "Batch loss: -4.16441194982626\n",
      "Batch loss: -4.16499266648086\n",
      "Batch loss: -4.16556284137484\n",
      "Batch loss: -4.164624068484494\n",
      "Batch loss: -4.1646096546377676\n",
      "Batch loss: -4.164624379769349\n",
      "Batch loss: -4.164994829453488\n",
      "Batch loss: -4.164397104921524\n",
      "Batch loss: -4.1656948463336025\n",
      "Batch loss: -4.166032777391331\n",
      "Epoch loss: -4.164994511867351\n",
      "Improved loss from: -4.164294531280855  to: -4.164994511867351\n",
      "Training iteration: 128\n",
      "Batch loss: -4.16423182932501\n",
      "Batch loss: -4.164401214960104\n",
      "Batch loss: -4.164154664256598\n",
      "Batch loss: -4.164439098027862\n",
      "Batch loss: -4.16514126463828\n",
      "Batch loss: -4.164715647186477\n",
      "Batch loss: -4.1656780256367885\n",
      "Batch loss: -4.163392940320543\n",
      "Batch loss: -4.165415411922322\n",
      "Batch loss: -4.166235976093474\n",
      "Epoch loss: -4.164780607236746\n",
      "Loss (no improvement): -4.164780607236746\n",
      "Training iteration: 129\n",
      "Batch loss: -4.165141847922654\n",
      "Batch loss: -4.164896393455693\n",
      "Batch loss: -4.163722774032171\n",
      "Batch loss: -4.16498788836124\n",
      "Batch loss: -4.1662356497835\n",
      "Batch loss: -4.165909638480057\n",
      "Batch loss: -4.164553537809143\n",
      "Batch loss: -4.1648323421072995\n",
      "Batch loss: -4.165850392934837\n",
      "Batch loss: -4.1663989576596085\n",
      "Epoch loss: -4.16525294225462\n",
      "Improved loss from: -4.164994511867351  to: -4.16525294225462\n",
      "Training iteration: 130\n",
      "Batch loss: -4.164743457354447\n",
      "Batch loss: -4.166168424287936\n",
      "Batch loss: -4.165840889031136\n",
      "Batch loss: -4.165814735414661\n",
      "Batch loss: -4.165848082200673\n",
      "Batch loss: -4.165733657577022\n",
      "Batch loss: -4.165823591784889\n",
      "Batch loss: -4.166034257060211\n",
      "Batch loss: -4.165245218478989\n",
      "Batch loss: -4.166859583833746\n",
      "Epoch loss: -4.165811189702371\n",
      "Improved loss from: -4.16525294225462  to: -4.165811189702371\n",
      "Training iteration: 131\n",
      "Batch loss: -4.164790056461323\n",
      "Batch loss: -4.165597962396523\n",
      "Batch loss: -4.165020906332477\n",
      "Batch loss: -4.166991978088448\n",
      "Batch loss: -4.164542152138225\n",
      "Batch loss: -4.165509596011134\n",
      "Batch loss: -4.166164876792807\n",
      "Batch loss: -4.165525781144452\n",
      "Batch loss: -4.165261869367938\n",
      "Batch loss: -4.166737255494173\n",
      "Epoch loss: -4.165614243422749\n",
      "Loss (no improvement): -4.165614243422749\n",
      "Training iteration: 132\n",
      "Batch loss: -4.164996701575731\n",
      "Batch loss: -4.1677412689631375\n",
      "Batch loss: -4.16601109034875\n",
      "Batch loss: -4.166569852932901\n",
      "Batch loss: -4.166736730037333\n",
      "Batch loss: -4.1656004546810435\n",
      "Batch loss: -4.166272118321542\n",
      "Batch loss: -4.16577240669214\n",
      "Batch loss: -4.165631561893916\n",
      "Batch loss: -4.167669151133722\n",
      "Epoch loss: -4.166300133658022\n",
      "Improved loss from: -4.165811189702371  to: -4.166300133658022\n",
      "Training iteration: 133\n",
      "Batch loss: -4.167090919039566\n",
      "Batch loss: -4.166203203512938\n",
      "Batch loss: -4.165438165136029\n",
      "Batch loss: -4.165620949064329\n",
      "Batch loss: -4.167297162849916\n",
      "Batch loss: -4.165610602204825\n",
      "Batch loss: -4.166532767489302\n",
      "Batch loss: -4.166187416255931\n",
      "Batch loss: -4.165731487403851\n",
      "Batch loss: -4.166466382689911\n",
      "Epoch loss: -4.16621790556466\n",
      "Loss (no improvement): -4.16621790556466\n",
      "Training iteration: 134\n",
      "Batch loss: -4.1664245497348595\n",
      "Batch loss: -4.165531481981031\n",
      "Batch loss: -4.167857506054016\n",
      "Batch loss: -4.167185164553582\n",
      "Batch loss: -4.167395134429638\n",
      "Batch loss: -4.165992152185231\n",
      "Batch loss: -4.166708979866008\n",
      "Batch loss: -4.164914875069875\n",
      "Batch loss: -4.1658084354714475\n",
      "Batch loss: -4.1688566078041465\n",
      "Epoch loss: -4.1666674887149835\n",
      "Improved loss from: -4.166300133658022  to: -4.1666674887149835\n",
      "Training iteration: 135\n",
      "Batch loss: -4.165533209530509\n",
      "Batch loss: -4.166100843190885\n",
      "Batch loss: -4.165895659569839\n",
      "Batch loss: -4.166394360411545\n",
      "Batch loss: -4.167244638925348\n",
      "Batch loss: -4.164900805673245\n",
      "Batch loss: -4.166954333523749\n",
      "Batch loss: -4.165562440885872\n",
      "Batch loss: -4.1688588760468335\n",
      "Batch loss: -4.167286459469657\n",
      "Epoch loss: -4.166473162722748\n",
      "Loss (no improvement): -4.166473162722748\n",
      "Training iteration: 136\n",
      "Batch loss: -4.166128994541238\n",
      "Batch loss: -4.167690278224399\n",
      "Batch loss: -4.166665251382705\n",
      "Batch loss: -4.167228664636536\n",
      "Batch loss: -4.167597080751986\n",
      "Batch loss: -4.166228115118244\n",
      "Batch loss: -4.167554536079529\n",
      "Batch loss: -4.165879511763957\n",
      "Batch loss: -4.166973984506056\n",
      "Batch loss: -4.168188179992894\n",
      "Epoch loss: -4.167013459699755\n",
      "Improved loss from: -4.1666674887149835  to: -4.167013459699755\n",
      "Training iteration: 137\n",
      "Batch loss: -4.1683207071335975\n",
      "Batch loss: -4.16738809188426\n",
      "Batch loss: -4.167472424506061\n",
      "Batch loss: -4.1675697969763705\n",
      "Batch loss: -4.167144165979355\n",
      "Batch loss: -4.168055894108731\n",
      "Batch loss: -4.166452155467536\n",
      "Batch loss: -4.166199743248408\n",
      "Batch loss: -4.1668555019968405\n",
      "Batch loss: -4.169564702490676\n",
      "Epoch loss: -4.167502318379183\n",
      "Improved loss from: -4.167013459699755  to: -4.167502318379183\n",
      "Training iteration: 138\n",
      "Batch loss: -4.166750554411779\n",
      "Batch loss: -4.168648621423657\n",
      "Batch loss: -4.16608730892666\n",
      "Batch loss: -4.167417643440637\n",
      "Batch loss: -4.168626323995048\n",
      "Batch loss: -4.168051814673086\n",
      "Batch loss: -4.168197019805467\n",
      "Batch loss: -4.166864588831033\n",
      "Batch loss: -4.168627750633914\n",
      "Batch loss: -4.169235990286686\n",
      "Epoch loss: -4.167850761642797\n",
      "Improved loss from: -4.167502318379183  to: -4.167850761642797\n",
      "Training iteration: 139\n",
      "Batch loss: -4.166202059099614\n",
      "Batch loss: -4.167006688650414\n",
      "Batch loss: -4.166317166792914\n",
      "Batch loss: -4.167745821686004\n",
      "Batch loss: -4.1680155526460965\n",
      "Batch loss: -4.168183119735828\n",
      "Batch loss: -4.167379589295791\n",
      "Batch loss: -4.167882974006594\n",
      "Batch loss: -4.168172069030814\n",
      "Batch loss: -4.168911474551565\n",
      "Epoch loss: -4.167581651549564\n",
      "Loss (no improvement): -4.167581651549564\n",
      "Training iteration: 140\n",
      "Batch loss: -4.167958018781317\n",
      "Batch loss: -4.167285130117945\n",
      "Batch loss: -4.1669648107923525\n",
      "Batch loss: -4.167786056768883\n",
      "Batch loss: -4.168961491433737\n",
      "Batch loss: -4.1682537526249845\n",
      "Batch loss: -4.169590904514767\n",
      "Batch loss: -4.166713661038605\n",
      "Batch loss: -4.169420204558432\n",
      "Batch loss: -4.168977430011468\n",
      "Epoch loss: -4.16819114606425\n",
      "Improved loss from: -4.167850761642797  to: -4.16819114606425\n",
      "Training iteration: 141\n",
      "Batch loss: -4.1694348281320295\n",
      "Batch loss: -4.169300972152235\n",
      "Batch loss: -4.168550478253285\n",
      "Batch loss: -4.16675138705425\n",
      "Batch loss: -4.169590187859814\n",
      "Batch loss: -4.169528480929374\n",
      "Batch loss: -4.167052958829228\n",
      "Batch loss: -4.1664545560367845\n",
      "Batch loss: -4.167879004264336\n",
      "Batch loss: -4.169912602470118\n",
      "Epoch loss: -4.1684455455981455\n",
      "Improved loss from: -4.16819114606425  to: -4.1684455455981455\n",
      "Training iteration: 142\n",
      "Batch loss: -4.168636113516902\n",
      "Batch loss: -4.169839523891418\n",
      "Batch loss: -4.168676387642055\n",
      "Batch loss: -4.169136035454502\n",
      "Batch loss: -4.169550306839122\n",
      "Batch loss: -4.168301028590163\n",
      "Batch loss: -4.167905686753604\n",
      "Batch loss: -4.167371135664869\n",
      "Batch loss: -4.1685365622597015\n",
      "Batch loss: -4.169834059813513\n",
      "Epoch loss: -4.168778684042586\n",
      "Improved loss from: -4.1684455455981455  to: -4.168778684042586\n",
      "Training iteration: 143\n",
      "Batch loss: -4.168470330004845\n",
      "Batch loss: -4.168239147819371\n",
      "Batch loss: -4.167902268600363\n",
      "Batch loss: -4.167560757780191\n",
      "Batch loss: -4.170533486752243\n",
      "Batch loss: -4.170476149552304\n",
      "Batch loss: -4.169523750637341\n",
      "Batch loss: -4.167873435909527\n",
      "Batch loss: -4.169128855709619\n",
      "Batch loss: -4.169957741629902\n",
      "Epoch loss: -4.168966592439571\n",
      "Improved loss from: -4.168778684042586  to: -4.168966592439571\n",
      "Training iteration: 144\n",
      "Batch loss: -4.169683087382274\n",
      "Batch loss: -4.168603074503601\n",
      "Batch loss: -4.168632887860117\n",
      "Batch loss: -4.168524743098353\n",
      "Batch loss: -4.169254535176667\n",
      "Batch loss: -4.170480518822731\n",
      "Batch loss: -4.1689212070238275\n",
      "Batch loss: -4.1685587702657\n",
      "Batch loss: -4.1690421642025495\n",
      "Batch loss: -4.170503796307785\n",
      "Epoch loss: -4.169220478464361\n",
      "Improved loss from: -4.168966592439571  to: -4.169220478464361\n",
      "Training iteration: 145\n",
      "Batch loss: -4.1686855630581885\n",
      "Batch loss: -4.169526717073731\n",
      "Batch loss: -4.168129615258997\n",
      "Batch loss: -4.169249113280706\n",
      "Batch loss: -4.168995371951589\n",
      "Batch loss: -4.168834706784733\n",
      "Batch loss: -4.169640598356866\n",
      "Batch loss: -4.167818673491256\n",
      "Batch loss: -4.168827237380373\n",
      "Batch loss: -4.169324154437935\n",
      "Epoch loss: -4.168903175107437\n",
      "Loss (no improvement): -4.168903175107437\n",
      "Training iteration: 146\n",
      "Batch loss: -4.170074381242521\n",
      "Batch loss: -4.170141134014028\n",
      "Batch loss: -4.168242032891962\n",
      "Batch loss: -4.170056581434211\n",
      "Batch loss: -4.169686058326413\n",
      "Batch loss: -4.169844714998991\n",
      "Batch loss: -4.1681706418709235\n",
      "Batch loss: -4.169714049188336\n",
      "Batch loss: -4.170540850993242\n",
      "Batch loss: -4.1711097789833245\n",
      "Epoch loss: -4.1697580223943955\n",
      "Improved loss from: -4.169220478464361  to: -4.1697580223943955\n",
      "Training iteration: 147\n",
      "Batch loss: -4.169382274428285\n",
      "Batch loss: -4.170293695194018\n",
      "Batch loss: -4.1688521779205\n",
      "Batch loss: -4.168787748691754\n",
      "Batch loss: -4.1708637529426875\n",
      "Batch loss: -4.169272024185511\n",
      "Batch loss: -4.16927220771322\n",
      "Batch loss: -4.169391995841049\n",
      "Batch loss: -4.169292090174095\n",
      "Batch loss: -4.172081571454039\n",
      "Epoch loss: -4.169748953854515\n",
      "Loss (no improvement): -4.169748953854515\n",
      "Training iteration: 148\n",
      "Batch loss: -4.170089566188795\n",
      "Batch loss: -4.1698976356858415\n",
      "Batch loss: -4.168167621990356\n",
      "Batch loss: -4.169766713104533\n",
      "Batch loss: -4.170203507414121\n",
      "Batch loss: -4.17041203445298\n",
      "Batch loss: -4.170396479908361\n",
      "Batch loss: -4.1687626468339545\n",
      "Batch loss: -4.170016347804392\n",
      "Batch loss: -4.170684378999414\n",
      "Epoch loss: -4.169839693238275\n",
      "Improved loss from: -4.1697580223943955  to: -4.169839693238275\n",
      "Training iteration: 149\n",
      "Batch loss: -4.170847879309338\n",
      "Batch loss: -4.170125445102401\n",
      "Batch loss: -4.169344386048482\n",
      "Batch loss: -4.170509624347006\n",
      "Batch loss: -4.170011739411082\n",
      "Batch loss: -4.170019402074571\n",
      "Batch loss: -4.169774212157767\n",
      "Batch loss: -4.170630978567233\n",
      "Batch loss: -4.1710331162066785\n",
      "Batch loss: -4.171171169509095\n",
      "Epoch loss: -4.170346795273366\n",
      "Improved loss from: -4.169839693238275  to: -4.170346795273366\n",
      "Training iteration: 150\n",
      "Batch loss: -4.170084420788346\n",
      "Batch loss: -4.17163723745962\n",
      "Batch loss: -4.1703831895428305\n",
      "Batch loss: -4.1694371459350394\n",
      "Batch loss: -4.169822147090313\n",
      "Batch loss: -4.1709798440429315\n",
      "Batch loss: -4.171491242674431\n",
      "Batch loss: -4.169755217635786\n",
      "Batch loss: -4.169911445822626\n",
      "Batch loss: -4.171944708717011\n",
      "Epoch loss: -4.170544659970894\n",
      "Improved loss from: -4.170346795273366  to: -4.170544659970894\n",
      "Training iteration: 151\n",
      "Batch loss: -4.170943829622931\n",
      "Batch loss: -4.170496403977821\n",
      "Batch loss: -4.170197092190393\n",
      "Batch loss: -4.169456233195189\n",
      "Batch loss: -4.171965374290271\n",
      "Batch loss: -4.171283105101069\n",
      "Batch loss: -4.169182055715443\n",
      "Batch loss: -4.169547479850157\n",
      "Batch loss: -4.170636069603437\n",
      "Batch loss: -4.17240989131119\n",
      "Epoch loss: -4.17061175348579\n",
      "Improved loss from: -4.170544659970894  to: -4.17061175348579\n",
      "Training iteration: 152\n",
      "Batch loss: -4.171127872797771\n",
      "Batch loss: -4.171723020706109\n",
      "Batch loss: -4.1702495054222855\n",
      "Batch loss: -4.1703450791546075\n",
      "Batch loss: -4.1705800437059475\n",
      "Batch loss: -4.171112327323598\n",
      "Batch loss: -4.171317337944795\n",
      "Batch loss: -4.169886196350092\n",
      "Batch loss: -4.16930309386749\n",
      "Batch loss: -4.172242446585427\n",
      "Epoch loss: -4.170788692385812\n",
      "Improved loss from: -4.17061175348579  to: -4.170788692385812\n",
      "Training iteration: 153\n",
      "Batch loss: -4.170444968548605\n",
      "Batch loss: -4.171328239878412\n",
      "Batch loss: -4.16989077071688\n",
      "Batch loss: -4.169411343687424\n",
      "Batch loss: -4.17217392669946\n",
      "Batch loss: -4.171484748001371\n",
      "Batch loss: -4.170791540579672\n",
      "Batch loss: -4.169812944392498\n",
      "Batch loss: -4.171974356490515\n",
      "Batch loss: -4.172810847545591\n",
      "Epoch loss: -4.171012368654043\n",
      "Improved loss from: -4.170788692385812  to: -4.171012368654043\n",
      "Training iteration: 154\n",
      "Batch loss: -4.171992719980949\n",
      "Batch loss: -4.171373822569682\n",
      "Batch loss: -4.16991966254271\n",
      "Batch loss: -4.1707837829357555\n",
      "Batch loss: -4.171887594936501\n",
      "Batch loss: -4.170380728734054\n",
      "Batch loss: -4.172108811171529\n",
      "Batch loss: -4.169136099691058\n",
      "Batch loss: -4.170853507866125\n",
      "Batch loss: -4.172463254547662\n",
      "Epoch loss: -4.171089998497602\n",
      "Improved loss from: -4.171012368654043  to: -4.171089998497602\n",
      "Training iteration: 155\n",
      "Batch loss: -4.171597297835468\n",
      "Batch loss: -4.171801028836064\n",
      "Batch loss: -4.169744470787462\n",
      "Batch loss: -4.1708797823966215\n",
      "Batch loss: -4.171318484471118\n",
      "Batch loss: -4.171008331465123\n",
      "Batch loss: -4.171750718552162\n",
      "Batch loss: -4.168968498375747\n",
      "Batch loss: -4.172035277763808\n",
      "Batch loss: -4.173097151988614\n",
      "Epoch loss: -4.171220104247219\n",
      "Improved loss from: -4.171089998497602  to: -4.171220104247219\n",
      "Training iteration: 156\n",
      "Batch loss: -4.170577632024862\n",
      "Batch loss: -4.171186328395893\n",
      "Batch loss: -4.170343161598991\n",
      "Batch loss: -4.172242976495491\n",
      "Batch loss: -4.1722268608103565\n",
      "Batch loss: -4.1722419558014\n",
      "Batch loss: -4.172075221354025\n",
      "Batch loss: -4.170163032410635\n",
      "Batch loss: -4.1709854071900265\n",
      "Batch loss: -4.173648289024933\n",
      "Epoch loss: -4.171569086510661\n",
      "Improved loss from: -4.171220104247219  to: -4.171569086510661\n",
      "Training iteration: 157\n",
      "Batch loss: -4.170226689307342\n",
      "Batch loss: -4.171310750189493\n",
      "Batch loss: -4.17125108942889\n",
      "Batch loss: -4.171802965859203\n",
      "Batch loss: -4.172724539268791\n",
      "Batch loss: -4.17074917221472\n",
      "Batch loss: -4.170329154578593\n",
      "Batch loss: -4.172445335958043\n",
      "Batch loss: -4.172128863762102\n",
      "Batch loss: -4.173082561448118\n",
      "Epoch loss: -4.17160511220153\n",
      "Improved loss from: -4.171569086510661  to: -4.17160511220153\n",
      "Training iteration: 158\n",
      "Batch loss: -4.171391812906943\n",
      "Batch loss: -4.1727685232188\n",
      "Batch loss: -4.170198147018485\n",
      "Batch loss: -4.1714892669787496\n",
      "Batch loss: -4.173914499445257\n",
      "Batch loss: -4.172958559522145\n",
      "Batch loss: -4.172308772838926\n",
      "Batch loss: -4.170006379745806\n",
      "Batch loss: -4.171996397402871\n",
      "Batch loss: -4.172658699166378\n",
      "Epoch loss: -4.171969105824436\n",
      "Improved loss from: -4.17160511220153  to: -4.171969105824436\n",
      "Training iteration: 159\n",
      "Batch loss: -4.170897584751556\n",
      "Batch loss: -4.171078050726734\n",
      "Batch loss: -4.171203719411167\n",
      "Batch loss: -4.171296616461303\n",
      "Batch loss: -4.173133022663353\n",
      "Batch loss: -4.17163354607041\n",
      "Batch loss: -4.172678662290617\n",
      "Batch loss: -4.1712639082101015\n",
      "Batch loss: -4.17195181243355\n",
      "Batch loss: -4.173125654528578\n",
      "Epoch loss: -4.171826257754737\n",
      "Loss (no improvement): -4.171826257754737\n",
      "Training iteration: 160\n",
      "Batch loss: -4.17257289479379\n",
      "Batch loss: -4.172038096651265\n",
      "Batch loss: -4.1721055251893056\n",
      "Batch loss: -4.173044839702303\n",
      "Batch loss: -4.1734442877098\n",
      "Batch loss: -4.171605491967942\n",
      "Batch loss: -4.173121476294272\n",
      "Batch loss: -4.1711465824556955\n",
      "Batch loss: -4.172414840863481\n",
      "Batch loss: -4.174207767393055\n",
      "Epoch loss: -4.172570180302091\n",
      "Improved loss from: -4.171969105824436  to: -4.172570180302091\n",
      "Training iteration: 161\n",
      "Batch loss: -4.1734304387794925\n",
      "Batch loss: -4.173107863310151\n",
      "Batch loss: -4.172045118080787\n",
      "Batch loss: -4.172421267785606\n",
      "Batch loss: -4.173690445770713\n",
      "Batch loss: -4.171767538958772\n",
      "Batch loss: -4.173009776859598\n",
      "Batch loss: -4.171929267675002\n",
      "Batch loss: -4.172252745342595\n",
      "Batch loss: -4.173253278770517\n",
      "Epoch loss: -4.172690774133324\n",
      "Improved loss from: -4.172570180302091  to: -4.172690774133324\n",
      "Training iteration: 162\n",
      "Batch loss: -4.1730166701074936\n",
      "Batch loss: -4.172913240380252\n",
      "Batch loss: -4.17274109004874\n",
      "Batch loss: -4.172234494782372\n",
      "Batch loss: -4.1725373208594245\n",
      "Batch loss: -4.172520439073161\n",
      "Batch loss: -4.172039669323483\n",
      "Batch loss: -4.171742186260313\n",
      "Batch loss: -4.172025722260166\n",
      "Batch loss: -4.172961027425313\n",
      "Epoch loss: -4.172473186052072\n",
      "Loss (no improvement): -4.172473186052072\n",
      "Training iteration: 163\n",
      "Batch loss: -4.170317101071679\n",
      "Batch loss: -4.171352099707155\n",
      "Batch loss: -4.1719062931321815\n",
      "Batch loss: -4.173031814077839\n",
      "Batch loss: -4.172815666496093\n",
      "Batch loss: -4.171670682940491\n",
      "Batch loss: -4.17184902654125\n",
      "Batch loss: -4.170562548464056\n",
      "Batch loss: -4.17241215136752\n",
      "Batch loss: -4.172718348176806\n",
      "Epoch loss: -4.171863573197507\n",
      "Loss (no improvement): -4.171863573197507\n",
      "Training iteration: 164\n",
      "Batch loss: -4.174892898917812\n",
      "Batch loss: -4.173803897505389\n",
      "Batch loss: -4.170878139498158\n",
      "Batch loss: -4.17289655767993\n",
      "Batch loss: -4.173179003294648\n",
      "Batch loss: -4.173588316549403\n",
      "Batch loss: -4.173522199074765\n",
      "Batch loss: -4.1728180607888525\n",
      "Batch loss: -4.173534213291929\n",
      "Batch loss: -4.17582625435253\n",
      "Epoch loss: -4.173493954095342\n",
      "Improved loss from: -4.172690774133324  to: -4.173493954095342\n",
      "Training iteration: 165\n",
      "Batch loss: -4.174250349862329\n",
      "Batch loss: -4.173393134927724\n",
      "Batch loss: -4.172028697694692\n",
      "Batch loss: -4.1730300883925615\n",
      "Batch loss: -4.173996972150229\n",
      "Batch loss: -4.176106071705793\n",
      "Batch loss: -4.173805504281466\n",
      "Batch loss: -4.171350504183171\n",
      "Batch loss: -4.17325315556256\n",
      "Batch loss: -4.1751424113216595\n",
      "Epoch loss: -4.1736356890082185\n",
      "Improved loss from: -4.173493954095342  to: -4.1736356890082185\n",
      "Training iteration: 166\n",
      "Batch loss: -4.174488620679295\n",
      "Batch loss: -4.173741759180289\n",
      "Batch loss: -4.1734183053399\n",
      "Batch loss: -4.173341355116754\n",
      "Batch loss: -4.175263023341547\n",
      "Batch loss: -4.1773543607841\n",
      "Batch loss: -4.174435682758633\n",
      "Batch loss: -4.173541074595382\n",
      "Batch loss: -4.174939261450182\n",
      "Batch loss: -4.176429421038909\n",
      "Epoch loss: -4.1746952864285\n",
      "Improved loss from: -4.1736356890082185  to: -4.1746952864285\n",
      "Training iteration: 167\n",
      "Batch loss: -4.173740107412036\n",
      "Batch loss: -4.1749696377200936\n",
      "Batch loss: -4.172877112801421\n",
      "Batch loss: -4.174750597962406\n",
      "Batch loss: -4.174953507906833\n",
      "Batch loss: -4.176037220770295\n",
      "Batch loss: -4.175137351286785\n",
      "Batch loss: -4.173590323672916\n",
      "Batch loss: -4.174149553352067\n",
      "Batch loss: -4.176057901068681\n",
      "Epoch loss: -4.174626331395354\n",
      "Loss (no improvement): -4.174626331395354\n",
      "Training iteration: 168\n",
      "Batch loss: -4.175474384867153\n",
      "Batch loss: -4.1743454448752715\n",
      "Batch loss: -4.173749224250646\n",
      "Batch loss: -4.176053212065389\n",
      "Batch loss: -4.175587471441741\n",
      "Batch loss: -4.175495522883649\n",
      "Batch loss: -4.177068739161752\n",
      "Batch loss: -4.174828562861194\n",
      "Batch loss: -4.17544699642022\n",
      "Batch loss: -4.176165325310855\n",
      "Epoch loss: -4.175421488413788\n",
      "Improved loss from: -4.1746952864285  to: -4.175421488413788\n",
      "Training iteration: 169\n",
      "Batch loss: -4.17571076169244\n",
      "Batch loss: -4.175350370272739\n",
      "Batch loss: -4.175429795443286\n",
      "Batch loss: -4.175052650824009\n",
      "Batch loss: -4.177078376367329\n",
      "Batch loss: -4.175255191072921\n",
      "Batch loss: -4.175120994634433\n",
      "Batch loss: -4.174906127619067\n",
      "Batch loss: -4.175696498414396\n",
      "Batch loss: -4.17689697630037\n",
      "Epoch loss: -4.175649774264099\n",
      "Improved loss from: -4.175421488413788  to: -4.175649774264099\n",
      "Training iteration: 170\n",
      "Batch loss: -4.174989525888913\n",
      "Batch loss: -4.175871497798938\n",
      "Batch loss: -4.174884369641073\n",
      "Batch loss: -4.176183489145581\n",
      "Batch loss: -4.175630514287796\n",
      "Batch loss: -4.175259062565244\n",
      "Batch loss: -4.174367344332062\n",
      "Batch loss: -4.173369114842802\n",
      "Batch loss: -4.174566882984482\n",
      "Batch loss: -4.176715491778514\n",
      "Epoch loss: -4.17518372932654\n",
      "Loss (no improvement): -4.17518372932654\n",
      "Training iteration: 171\n",
      "Batch loss: -4.175018774385963\n",
      "Batch loss: -4.175330636285213\n",
      "Batch loss: -4.174186380862178\n",
      "Batch loss: -4.175129753932801\n",
      "Batch loss: -4.175845081357253\n",
      "Batch loss: -4.176061414125138\n",
      "Batch loss: -4.175571604830213\n",
      "Batch loss: -4.175029536954765\n",
      "Batch loss: -4.177158607248852\n",
      "Batch loss: -4.17610605649974\n",
      "Epoch loss: -4.175543784648211\n",
      "Loss (no improvement): -4.175543784648211\n",
      "Training iteration: 172\n",
      "Batch loss: -4.1749910383461595\n",
      "Batch loss: -4.176421443493163\n",
      "Batch loss: -4.175473609965511\n",
      "Batch loss: -4.17621391528146\n",
      "Batch loss: -4.176869400717278\n",
      "Batch loss: -4.175811168135674\n",
      "Batch loss: -4.1760908112149515\n",
      "Batch loss: -4.176405000031853\n",
      "Batch loss: -4.1773765138495556\n",
      "Batch loss: -4.177168962351436\n",
      "Epoch loss: -4.176282186338704\n",
      "Improved loss from: -4.175649774264099  to: -4.176282186338704\n",
      "Training iteration: 173\n",
      "Batch loss: -4.176204716565986\n",
      "Batch loss: -4.176585996993851\n",
      "Batch loss: -4.176517901351057\n",
      "Batch loss: -4.175477236767573\n",
      "Batch loss: -4.176098083693167\n",
      "Batch loss: -4.175920438327504\n",
      "Batch loss: -4.176933447475421\n",
      "Batch loss: -4.174748246862797\n",
      "Batch loss: -4.17722681509233\n",
      "Batch loss: -4.177357982470672\n",
      "Epoch loss: -4.176307086560036\n",
      "Improved loss from: -4.176282186338704  to: -4.176307086560036\n",
      "Training iteration: 174\n",
      "Batch loss: -4.175680362465387\n",
      "Batch loss: -4.176418428060561\n",
      "Batch loss: -4.177066690997467\n",
      "Batch loss: -4.175590095364508\n",
      "Batch loss: -4.176455074897237\n",
      "Batch loss: -4.178016191640726\n",
      "Batch loss: -4.177402063739426\n",
      "Batch loss: -4.175280517881442\n",
      "Batch loss: -4.178158446438404\n",
      "Batch loss: -4.177566347950195\n",
      "Epoch loss: -4.1767634219435354\n",
      "Improved loss from: -4.176307086560036  to: -4.1767634219435354\n",
      "Training iteration: 175\n",
      "Batch loss: -4.1765424051997435\n",
      "Batch loss: -4.1775766579350915\n",
      "Batch loss: -4.176316898439147\n",
      "Batch loss: -4.177046440141706\n",
      "Batch loss: -4.177596706650408\n",
      "Batch loss: -4.17693650561058\n",
      "Batch loss: -4.177962641353593\n",
      "Batch loss: -4.175697457650992\n",
      "Batch loss: -4.177254736058356\n",
      "Batch loss: -4.178306977604828\n",
      "Epoch loss: -4.1771237426644445\n",
      "Improved loss from: -4.1767634219435354  to: -4.1771237426644445\n",
      "Training iteration: 176\n",
      "Batch loss: -4.17734738989158\n",
      "Batch loss: -4.178182873723864\n",
      "Batch loss: -4.1756989670296285\n",
      "Batch loss: -4.176091528386485\n",
      "Batch loss: -4.177492461147804\n",
      "Batch loss: -4.176753009941988\n",
      "Batch loss: -4.176240357733774\n",
      "Batch loss: -4.1754170440177285\n",
      "Batch loss: -4.177477368252855\n",
      "Batch loss: -4.178186131640448\n",
      "Epoch loss: -4.176888713176615\n",
      "Loss (no improvement): -4.176888713176615\n",
      "Training iteration: 177\n",
      "Batch loss: -4.177369383336935\n",
      "Batch loss: -4.1765636797497585\n",
      "Batch loss: -4.176681836986654\n",
      "Batch loss: -4.17676030900114\n",
      "Batch loss: -4.177890662834782\n",
      "Batch loss: -4.17715762726458\n",
      "Batch loss: -4.177853374505943\n",
      "Batch loss: -4.177667423734366\n",
      "Batch loss: -4.177020394583241\n",
      "Batch loss: -4.179338431753759\n",
      "Epoch loss: -4.177430312375117\n",
      "Improved loss from: -4.1771237426644445  to: -4.177430312375117\n",
      "Training iteration: 178\n",
      "Batch loss: -4.178069091655235\n",
      "Batch loss: -4.17775690221093\n",
      "Batch loss: -4.177178149364842\n",
      "Batch loss: -4.178142651132558\n",
      "Batch loss: -4.1781771406474695\n",
      "Batch loss: -4.178369575076807\n",
      "Batch loss: -4.178227242586667\n",
      "Batch loss: -4.176249823634014\n",
      "Batch loss: -4.177535364137645\n",
      "Batch loss: -4.179980661495927\n",
      "Epoch loss: -4.177968660194209\n",
      "Improved loss from: -4.177430312375117  to: -4.177968660194209\n",
      "Training iteration: 179\n",
      "Batch loss: -4.178318560307012\n",
      "Batch loss: -4.178587390128836\n",
      "Batch loss: -4.176521778355845\n",
      "Batch loss: -4.178853503047278\n",
      "Batch loss: -4.179675307277161\n",
      "Batch loss: -4.177688102880842\n",
      "Batch loss: -4.177999682227402\n",
      "Batch loss: -4.176747420180517\n",
      "Batch loss: -4.177279956124054\n",
      "Batch loss: -4.179411913267076\n",
      "Epoch loss: -4.178108361379602\n",
      "Improved loss from: -4.177968660194209  to: -4.178108361379602\n",
      "Training iteration: 180\n",
      "Batch loss: -4.177199973457177\n",
      "Batch loss: -4.178933827651407\n",
      "Batch loss: -4.176848619215166\n",
      "Batch loss: -4.177701712821726\n",
      "Batch loss: -4.178305632709862\n",
      "Batch loss: -4.1773388777357\n",
      "Batch loss: -4.177922734551854\n",
      "Batch loss: -4.176569633764499\n",
      "Batch loss: -4.178538918550094\n",
      "Batch loss: -4.180454022573953\n",
      "Epoch loss: -4.177981395303144\n",
      "Loss (no improvement): -4.177981395303144\n",
      "Training iteration: 181\n",
      "Batch loss: -4.177751848219969\n",
      "Batch loss: -4.179300889516017\n",
      "Batch loss: -4.1774301951609525\n",
      "Batch loss: -4.177611226498347\n",
      "Batch loss: -4.17885905529973\n",
      "Batch loss: -4.179149343356413\n",
      "Batch loss: -4.178577454931456\n",
      "Batch loss: -4.178496619788543\n",
      "Batch loss: -4.177145147109515\n",
      "Batch loss: -4.178826383359107\n",
      "Epoch loss: -4.178314816324006\n",
      "Improved loss from: -4.178108361379602  to: -4.178314816324006\n",
      "Training iteration: 182\n",
      "Batch loss: -4.177402263351053\n",
      "Batch loss: -4.178797540345189\n",
      "Batch loss: -4.176918718509212\n",
      "Batch loss: -4.178703542914543\n",
      "Batch loss: -4.180586963057427\n",
      "Batch loss: -4.178343609152229\n",
      "Batch loss: -4.1796794137721385\n",
      "Batch loss: -4.177702698685948\n",
      "Batch loss: -4.17787923064941\n",
      "Batch loss: -4.179691563663774\n",
      "Epoch loss: -4.178570554410092\n",
      "Improved loss from: -4.178314816324006  to: -4.178570554410092\n",
      "Training iteration: 183\n",
      "Batch loss: -4.179103000912996\n",
      "Batch loss: -4.1805547337996005\n",
      "Batch loss: -4.178401380827057\n",
      "Batch loss: -4.179173573872507\n",
      "Batch loss: -4.178910526601582\n",
      "Batch loss: -4.177144407158442\n",
      "Batch loss: -4.179344258014143\n",
      "Batch loss: -4.178052539631092\n",
      "Batch loss: -4.179304746921152\n",
      "Batch loss: -4.179902750150481\n",
      "Epoch loss: -4.178989191788906\n",
      "Improved loss from: -4.178570554410092  to: -4.178989191788906\n",
      "Training iteration: 184\n",
      "Batch loss: -4.1803272480149065\n",
      "Batch loss: -4.178494175989262\n",
      "Batch loss: -4.177852134323089\n",
      "Batch loss: -4.178972063881426\n",
      "Batch loss: -4.179896098124641\n",
      "Batch loss: -4.177698238259395\n",
      "Batch loss: -4.178502622496121\n",
      "Batch loss: -4.177951692924\n",
      "Batch loss: -4.178820487174306\n",
      "Batch loss: -4.179661187338574\n",
      "Epoch loss: -4.178817594852572\n",
      "Loss (no improvement): -4.178817594852572\n",
      "Training iteration: 185\n",
      "Batch loss: -4.1783258835025565\n",
      "Batch loss: -4.179815532146231\n",
      "Batch loss: -4.178459336341017\n",
      "Batch loss: -4.179070227346519\n",
      "Batch loss: -4.178901883024043\n",
      "Batch loss: -4.180233602233616\n",
      "Batch loss: -4.179766284120552\n",
      "Batch loss: -4.177394570819437\n",
      "Batch loss: -4.1798247676565445\n",
      "Batch loss: -4.178227390574975\n",
      "Epoch loss: -4.17900194777655\n",
      "Improved loss from: -4.178989191788906  to: -4.17900194777655\n",
      "Training iteration: 186\n",
      "Batch loss: -4.1793643195257335\n",
      "Batch loss: -4.178787606727192\n",
      "Batch loss: -4.178468903868895\n",
      "Batch loss: -4.179508721523581\n",
      "Batch loss: -4.178958553606253\n",
      "Batch loss: -4.179314691142444\n",
      "Batch loss: -4.178943383731375\n",
      "Batch loss: -4.177717983930201\n",
      "Batch loss: -4.180108868467289\n",
      "Batch loss: -4.180941381682813\n",
      "Epoch loss: -4.179211441420578\n",
      "Improved loss from: -4.17900194777655  to: -4.179211441420578\n",
      "Training iteration: 187\n",
      "Batch loss: -4.178301494408253\n",
      "Batch loss: -4.1798237250089585\n",
      "Batch loss: -4.17911827842375\n",
      "Batch loss: -4.179600036909299\n",
      "Batch loss: -4.1800592092376805\n",
      "Batch loss: -4.180001614190955\n",
      "Batch loss: -4.179683750537391\n",
      "Batch loss: -4.178355055286006\n",
      "Batch loss: -4.178491144736179\n",
      "Batch loss: -4.179718762831067\n",
      "Epoch loss: -4.179315307156953\n",
      "Improved loss from: -4.179211441420578  to: -4.179315307156953\n",
      "Training iteration: 188\n",
      "Batch loss: -4.179043615982676\n",
      "Batch loss: -4.179047329456604\n",
      "Batch loss: -4.1780536492541325\n",
      "Batch loss: -4.179264734123198\n",
      "Batch loss: -4.18111432912044\n",
      "Batch loss: -4.180126153821039\n",
      "Batch loss: -4.178255314056754\n",
      "Batch loss: -4.178879693706158\n",
      "Batch loss: -4.180503515122529\n",
      "Batch loss: -4.182334886962799\n",
      "Epoch loss: -4.179662322160633\n",
      "Improved loss from: -4.179315307156953  to: -4.179662322160633\n",
      "Training iteration: 189\n",
      "Batch loss: -4.180343653268935\n",
      "Batch loss: -4.179850410293825\n",
      "Batch loss: -4.179639735210505\n",
      "Batch loss: -4.177860428948068\n",
      "Batch loss: -4.181159609787076\n",
      "Batch loss: -4.180349931858024\n",
      "Batch loss: -4.179791258504171\n",
      "Batch loss: -4.178164919768419\n",
      "Batch loss: -4.180735671900192\n",
      "Batch loss: -4.181263189923255\n",
      "Epoch loss: -4.179915880946248\n",
      "Improved loss from: -4.179662322160633  to: -4.179915880946248\n",
      "Training iteration: 190\n",
      "Batch loss: -4.180092404918882\n",
      "Batch loss: -4.181006621469517\n",
      "Batch loss: -4.179541638784723\n",
      "Batch loss: -4.179867524195027\n",
      "Batch loss: -4.180616333930608\n",
      "Batch loss: -4.179493224477915\n",
      "Batch loss: -4.181420841605292\n",
      "Batch loss: -4.178284054332722\n",
      "Batch loss: -4.178237247321144\n",
      "Batch loss: -4.181844635772248\n",
      "Epoch loss: -4.180040452680808\n",
      "Improved loss from: -4.179915880946248  to: -4.180040452680808\n",
      "Training iteration: 191\n",
      "Batch loss: -4.179838749504523\n",
      "Batch loss: -4.180885634763504\n",
      "Batch loss: -4.179426197103583\n",
      "Batch loss: -4.1788660362985\n",
      "Batch loss: -4.180385880574385\n",
      "Batch loss: -4.1816619649243\n",
      "Batch loss: -4.179221119980512\n",
      "Batch loss: -4.181262548874185\n",
      "Batch loss: -4.180119335924359\n",
      "Batch loss: -4.18189342280287\n",
      "Epoch loss: -4.180356089075072\n",
      "Improved loss from: -4.180040452680808  to: -4.180356089075072\n",
      "Training iteration: 192\n",
      "Batch loss: -4.180007192466321\n",
      "Batch loss: -4.179487495415822\n",
      "Batch loss: -4.179462381032843\n",
      "Batch loss: -4.1795367030051045\n",
      "Batch loss: -4.18221837747379\n",
      "Batch loss: -4.180665568712516\n",
      "Batch loss: -4.181359155497812\n",
      "Batch loss: -4.179479512256411\n",
      "Batch loss: -4.1821309029063745\n",
      "Batch loss: -4.18150826660075\n",
      "Epoch loss: -4.180585555536775\n",
      "Improved loss from: -4.180356089075072  to: -4.180585555536775\n",
      "Training iteration: 193\n",
      "Batch loss: -4.180312164849099\n",
      "Batch loss: -4.1808885043485855\n",
      "Batch loss: -4.177922637359011\n",
      "Batch loss: -4.18023768323659\n",
      "Batch loss: -4.179967078005695\n",
      "Batch loss: -4.180079158486117\n",
      "Batch loss: -4.1802345266243774\n",
      "Batch loss: -4.178573128132218\n",
      "Batch loss: -4.179537863530957\n",
      "Batch loss: -4.181734504281922\n",
      "Epoch loss: -4.179948724885456\n",
      "Loss (no improvement): -4.179948724885456\n",
      "Training iteration: 194\n",
      "Batch loss: -4.179555484973027\n",
      "Batch loss: -4.180495574331563\n",
      "Batch loss: -4.179289744822208\n",
      "Batch loss: -4.1814453234937465\n",
      "Batch loss: -4.181168997764854\n",
      "Batch loss: -4.179051285591669\n",
      "Batch loss: -4.180934110882076\n",
      "Batch loss: -4.180443556151144\n",
      "Batch loss: -4.1802110298235\n",
      "Batch loss: -4.181276419103245\n",
      "Epoch loss: -4.180387152693703\n",
      "Loss (no improvement): -4.180387152693703\n",
      "Training iteration: 195\n",
      "Batch loss: -4.180458790778221\n",
      "Batch loss: -4.1803394662058215\n",
      "Batch loss: -4.181355820463945\n",
      "Batch loss: -4.180647131166913\n",
      "Batch loss: -4.180793894203027\n",
      "Batch loss: -4.180588250450595\n",
      "Batch loss: -4.179895461208807\n",
      "Batch loss: -4.180248561606442\n",
      "Batch loss: -4.180523932124624\n",
      "Batch loss: -4.181387546334851\n",
      "Epoch loss: -4.180623885454326\n",
      "Improved loss from: -4.180585555536775  to: -4.180623885454326\n",
      "Training iteration: 196\n",
      "Batch loss: -4.179961517127539\n",
      "Batch loss: -4.182587097960521\n",
      "Batch loss: -4.181171024168107\n",
      "Batch loss: -4.180471939997001\n",
      "Batch loss: -4.180983498843144\n",
      "Batch loss: -4.181336151321239\n",
      "Batch loss: -4.180066317534984\n",
      "Batch loss: -4.180329505641979\n",
      "Batch loss: -4.18082115268064\n",
      "Batch loss: -4.181360527889938\n",
      "Epoch loss: -4.1809088733165085\n",
      "Improved loss from: -4.180623885454326  to: -4.1809088733165085\n",
      "Training iteration: 197\n",
      "Batch loss: -4.181451025307062\n",
      "Batch loss: -4.181221973850953\n",
      "Batch loss: -4.1803123978345305\n",
      "Batch loss: -4.181069146123055\n",
      "Batch loss: -4.181351118670224\n",
      "Batch loss: -4.181671124232444\n",
      "Batch loss: -4.180494780065056\n",
      "Batch loss: -4.180759458091713\n",
      "Batch loss: -4.180893348746988\n",
      "Batch loss: -4.182647060213148\n",
      "Epoch loss: -4.181187143313517\n",
      "Improved loss from: -4.1809088733165085  to: -4.181187143313517\n",
      "Training iteration: 198\n",
      "Batch loss: -4.180442320867773\n",
      "Batch loss: -4.180323786331\n",
      "Batch loss: -4.180490400567029\n",
      "Batch loss: -4.180407647235337\n",
      "Batch loss: -4.180001189984803\n",
      "Batch loss: -4.180761251852016\n",
      "Batch loss: -4.181307338254349\n",
      "Batch loss: -4.179656784853762\n",
      "Batch loss: -4.181438916480901\n",
      "Batch loss: -4.1819287150213365\n",
      "Epoch loss: -4.1806758351448305\n",
      "Loss (no improvement): -4.1806758351448305\n",
      "Training iteration: 199\n",
      "Batch loss: -4.182092907159503\n",
      "Batch loss: -4.181995343663967\n",
      "Batch loss: -4.181233890893591\n",
      "Batch loss: -4.180886100405058\n",
      "Batch loss: -4.1823930543114685\n",
      "Batch loss: -4.182574024472625\n",
      "Batch loss: -4.180829427145659\n",
      "Batch loss: -4.180470642118934\n",
      "Batch loss: -4.180767257690528\n",
      "Batch loss: -4.182851197995935\n",
      "Epoch loss: -4.181609384585727\n",
      "Improved loss from: -4.181187143313517  to: -4.181609384585727\n"
     ]
    }
   ],
   "source": [
    "model.train_model(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fR_YKOWllVs_"
   },
   "outputs": [],
   "source": [
    "model.restore_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "QtQLJBP3hIxz",
    "outputId": "5e17e4bb-7c1b-4f42-d905-c8ce35dbe544"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 0\n",
    "output = model.call(train_images[sample:sample+1])[0].numpy().reshape(28,28)\n",
    "plt.figure()\n",
    "plt.imshow(output)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[sample].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ernq5YhF1kT0",
    "outputId": "8202209e-e323-48de-96e9-216a7d3129ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.14151352, -4.41144114, -4.71427114, -5.21232843, -4.1079271 ,\n",
       "        -4.07452473, -4.25784372, -4.19393901, -4.09258392, -4.13648822,\n",
       "        -4.36854611, -4.09061567, -5.20204314, -4.47448768, -4.82258114,\n",
       "        -4.16273526, -4.15452149, -4.70838276, -4.72218288, -5.08561871,\n",
       "        -4.07890714, -4.19437142, -4.36126627, -4.10655928, -5.10302288,\n",
       "        -4.26269724, -4.64140494, -4.24490996, -4.67171631, -4.08281594,\n",
       "        -4.10416932, -4.42693438, -4.22723072, -4.82002326, -5.44526381,\n",
       "        -4.3612911 , -4.17372803, -4.51944128, -4.18230535, -4.09294723]])"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.call(train_images[sample:sample+1])[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "KMIdobK9hyjH",
    "outputId": "527cc2ea-3257-430a-afeb-3a16d0faf28c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 2000\n",
    "output = model.call(train_images[sample:sample+1])[0].numpy().reshape(28,28)\n",
    "output2 = model.call(train_images[sample:sample+1])[0].numpy().reshape(28,28)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(output)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(output2)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(output-output2)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_MHHKwEIpTtT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "VI_NF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
